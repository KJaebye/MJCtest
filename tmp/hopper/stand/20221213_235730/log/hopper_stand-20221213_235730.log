[32m[20221213 23:57:30 @logger.py:105][0m Log file set to ./tmp/hopper/stand/20221213_235730/log/hopper_stand-20221213_235730.log
[32m[20221213 23:57:30 @agent_ppo2.py:79][0m [4m[34mCRITICAL[0m Loading model from checkpoint: ./tmp/hopper/stand/20221213_231232/models/iter_4000.p
[32m[20221213 23:57:30 @agent_ppo2.py:121][0m #------------------------ Iteration 4000 --------------------------#
[32m[20221213 23:57:31 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 23:57:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:57:31 @agent_ppo2.py:185][0m |           0.0023 |          65.5836 |          11.3468 |
[32m[20221213 23:57:31 @agent_ppo2.py:185][0m |           0.0007 |          64.3332 |          11.3435 |
[32m[20221213 23:57:31 @agent_ppo2.py:185][0m |          -0.0041 |          62.8309 |          11.3877 |
[32m[20221213 23:57:31 @agent_ppo2.py:185][0m |          -0.0054 |          62.9077 |          11.3355 |
[32m[20221213 23:57:31 @agent_ppo2.py:185][0m |          -0.0069 |          61.8684 |          11.3476 |
[32m[20221213 23:57:31 @agent_ppo2.py:185][0m |          -0.0098 |          61.5412 |          11.3549 |
[32m[20221213 23:57:31 @agent_ppo2.py:185][0m |          -0.0095 |          61.4282 |          11.3523 |
[32m[20221213 23:57:31 @agent_ppo2.py:185][0m |          -0.0111 |          61.2008 |          11.3470 |
[32m[20221213 23:57:31 @agent_ppo2.py:185][0m |          -0.0103 |          60.7729 |          11.3887 |
[32m[20221213 23:57:32 @agent_ppo2.py:185][0m |          -0.0070 |          64.4524 |          11.3962 |
[32m[20221213 23:57:32 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:57:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 501.28
[32m[20221213 23:57:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 526.62
[32m[20221213 23:57:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 458.02
[32m[20221213 23:57:32 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 458.02
[32m[20221213 23:57:32 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 458.02
[32m[20221213 23:57:32 @agent_ppo2.py:143][0m Total time:       0.02 min
[32m[20221213 23:57:32 @agent_ppo2.py:145][0m 2048 total steps have happened
[32m[20221213 23:57:32 @agent_ppo2.py:121][0m #------------------------ Iteration 4001 --------------------------#
[32m[20221213 23:57:32 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:57:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:57:32 @agent_ppo2.py:185][0m |           0.0031 |          57.6289 |          11.1055 |
[32m[20221213 23:57:32 @agent_ppo2.py:185][0m |           0.0004 |          51.7873 |          11.1551 |
[32m[20221213 23:57:32 @agent_ppo2.py:185][0m |          -0.0035 |          50.1151 |          11.1884 |
[32m[20221213 23:57:32 @agent_ppo2.py:185][0m |          -0.0118 |          48.9324 |          11.1345 |
[32m[20221213 23:57:32 @agent_ppo2.py:185][0m |          -0.0124 |          48.3930 |          11.1828 |
[32m[20221213 23:57:32 @agent_ppo2.py:185][0m |          -0.0127 |          47.4840 |          11.2062 |
[32m[20221213 23:57:33 @agent_ppo2.py:185][0m |          -0.0168 |          47.1621 |          11.2451 |
[32m[20221213 23:57:33 @agent_ppo2.py:185][0m |          -0.0163 |          46.7602 |          11.2618 |
[32m[20221213 23:57:33 @agent_ppo2.py:185][0m |          -0.0163 |          46.3904 |          11.2645 |
[32m[20221213 23:57:33 @agent_ppo2.py:185][0m |          -0.0179 |          46.0786 |          11.3077 |
[32m[20221213 23:57:33 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:57:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 456.87
[32m[20221213 23:57:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 498.53
[32m[20221213 23:57:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 343.01
[32m[20221213 23:57:33 @agent_ppo2.py:143][0m Total time:       0.04 min
[32m[20221213 23:57:33 @agent_ppo2.py:145][0m 4096 total steps have happened
[32m[20221213 23:57:33 @agent_ppo2.py:121][0m #------------------------ Iteration 4002 --------------------------#
[32m[20221213 23:57:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:57:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:57:33 @agent_ppo2.py:185][0m |           0.0033 |          64.0930 |          11.4572 |
[32m[20221213 23:57:33 @agent_ppo2.py:185][0m |           0.0038 |          63.2413 |          11.4278 |
[32m[20221213 23:57:33 @agent_ppo2.py:185][0m |          -0.0104 |          58.0410 |          11.3820 |
[32m[20221213 23:57:34 @agent_ppo2.py:185][0m |          -0.0123 |          57.0046 |          11.4452 |
[32m[20221213 23:57:34 @agent_ppo2.py:185][0m |          -0.0153 |          56.3311 |          11.4016 |
[32m[20221213 23:57:34 @agent_ppo2.py:185][0m |          -0.0124 |          55.9766 |          11.4505 |
[32m[20221213 23:57:34 @agent_ppo2.py:185][0m |          -0.0144 |          55.6003 |          11.4096 |
[32m[20221213 23:57:34 @agent_ppo2.py:185][0m |          -0.0196 |          55.3190 |          11.3786 |
[32m[20221213 23:57:34 @agent_ppo2.py:185][0m |          -0.0188 |          55.0421 |          11.3624 |
[32m[20221213 23:57:34 @agent_ppo2.py:185][0m |          -0.0171 |          54.7205 |          11.3856 |
[32m[20221213 23:57:34 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:57:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 331.84
[32m[20221213 23:57:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 463.60
[32m[20221213 23:57:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 498.40
[32m[20221213 23:57:34 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 498.40
[32m[20221213 23:57:34 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 498.40
[32m[20221213 23:57:34 @agent_ppo2.py:143][0m Total time:       0.06 min
[32m[20221213 23:57:34 @agent_ppo2.py:145][0m 6144 total steps have happened
[32m[20221213 23:57:34 @agent_ppo2.py:121][0m #------------------------ Iteration 4003 --------------------------#
[32m[20221213 23:57:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:57:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:57:35 @agent_ppo2.py:185][0m |           0.0058 |          52.9612 |          11.7409 |
[32m[20221213 23:57:35 @agent_ppo2.py:185][0m |          -0.0076 |          47.8837 |          11.7415 |
[32m[20221213 23:57:35 @agent_ppo2.py:185][0m |          -0.0087 |          46.2172 |          11.7426 |
[32m[20221213 23:57:35 @agent_ppo2.py:185][0m |          -0.0027 |          46.5532 |          11.7247 |
[32m[20221213 23:57:35 @agent_ppo2.py:185][0m |          -0.0126 |          45.1386 |          11.6738 |
[32m[20221213 23:57:35 @agent_ppo2.py:185][0m |          -0.0144 |          44.6235 |          11.6968 |
[32m[20221213 23:57:35 @agent_ppo2.py:185][0m |          -0.0165 |          44.3063 |          11.6761 |
[32m[20221213 23:57:35 @agent_ppo2.py:185][0m |          -0.0171 |          43.8580 |          11.6622 |
[32m[20221213 23:57:35 @agent_ppo2.py:185][0m |          -0.0182 |          43.6156 |          11.6366 |
[32m[20221213 23:57:35 @agent_ppo2.py:185][0m |          -0.0206 |          43.5039 |          11.6480 |
[32m[20221213 23:57:35 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:57:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 429.79
[32m[20221213 23:57:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 475.02
[32m[20221213 23:57:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 502.13
[32m[20221213 23:57:36 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 502.13
[32m[20221213 23:57:36 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 502.13
[32m[20221213 23:57:36 @agent_ppo2.py:143][0m Total time:       0.09 min
[32m[20221213 23:57:36 @agent_ppo2.py:145][0m 8192 total steps have happened
[32m[20221213 23:57:36 @agent_ppo2.py:121][0m #------------------------ Iteration 4004 --------------------------#
[32m[20221213 23:57:36 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:57:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:57:36 @agent_ppo2.py:185][0m |           0.0045 |          45.9903 |          11.1550 |
[32m[20221213 23:57:36 @agent_ppo2.py:185][0m |          -0.0018 |          41.0802 |          11.1720 |
[32m[20221213 23:57:36 @agent_ppo2.py:185][0m |          -0.0100 |          39.0143 |          11.1661 |
[32m[20221213 23:57:36 @agent_ppo2.py:185][0m |          -0.0174 |          37.9265 |          11.1476 |
[32m[20221213 23:57:36 @agent_ppo2.py:185][0m |          -0.0150 |          37.1238 |          11.1897 |
[32m[20221213 23:57:36 @agent_ppo2.py:185][0m |          -0.0172 |          36.1452 |          11.1990 |
[32m[20221213 23:57:36 @agent_ppo2.py:185][0m |          -0.0141 |          35.7446 |          11.1804 |
[32m[20221213 23:57:36 @agent_ppo2.py:185][0m |          -0.0099 |          36.5614 |          11.2071 |
[32m[20221213 23:57:37 @agent_ppo2.py:185][0m |          -0.0164 |          34.7590 |          11.2017 |
[32m[20221213 23:57:37 @agent_ppo2.py:185][0m |          -0.0206 |          34.0788 |          11.1567 |
[32m[20221213 23:57:37 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:57:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 374.53
[32m[20221213 23:57:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 456.67
[32m[20221213 23:57:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 439.33
[32m[20221213 23:57:37 @agent_ppo2.py:143][0m Total time:       0.11 min
[32m[20221213 23:57:37 @agent_ppo2.py:145][0m 10240 total steps have happened
[32m[20221213 23:57:37 @agent_ppo2.py:121][0m #------------------------ Iteration 4005 --------------------------#
[32m[20221213 23:57:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:57:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:57:37 @agent_ppo2.py:185][0m |           0.0059 |          65.4913 |          11.2375 |
[32m[20221213 23:57:37 @agent_ppo2.py:185][0m |           0.0177 |          71.5003 |          11.2517 |
[32m[20221213 23:57:37 @agent_ppo2.py:185][0m |          -0.0056 |          60.1870 |          11.2877 |
[32m[20221213 23:57:37 @agent_ppo2.py:185][0m |          -0.0083 |          58.6464 |          11.2698 |
[32m[20221213 23:57:37 @agent_ppo2.py:185][0m |          -0.0059 |          58.3379 |          11.3003 |
[32m[20221213 23:57:38 @agent_ppo2.py:185][0m |          -0.0100 |          57.7089 |          11.3060 |
[32m[20221213 23:57:38 @agent_ppo2.py:185][0m |          -0.0052 |          58.5694 |          11.2791 |
[32m[20221213 23:57:38 @agent_ppo2.py:185][0m |          -0.0043 |          57.6981 |          11.3282 |
[32m[20221213 23:57:38 @agent_ppo2.py:185][0m |          -0.0094 |          57.1789 |          11.2907 |
[32m[20221213 23:57:38 @agent_ppo2.py:185][0m |          -0.0116 |          56.8286 |          11.3000 |
[32m[20221213 23:57:38 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:57:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 482.20
[32m[20221213 23:57:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 497.49
[32m[20221213 23:57:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 454.67
[32m[20221213 23:57:38 @agent_ppo2.py:143][0m Total time:       0.13 min
[32m[20221213 23:57:38 @agent_ppo2.py:145][0m 12288 total steps have happened
[32m[20221213 23:57:38 @agent_ppo2.py:121][0m #------------------------ Iteration 4006 --------------------------#
[32m[20221213 23:57:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:57:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:57:38 @agent_ppo2.py:185][0m |          -0.0050 |          50.5110 |          11.6101 |
[32m[20221213 23:57:38 @agent_ppo2.py:185][0m |          -0.0054 |          45.8622 |          11.5824 |
[32m[20221213 23:57:39 @agent_ppo2.py:185][0m |          -0.0144 |          43.8601 |          11.5898 |
[32m[20221213 23:57:39 @agent_ppo2.py:185][0m |          -0.0142 |          42.7738 |          11.5655 |
[32m[20221213 23:57:39 @agent_ppo2.py:185][0m |          -0.0139 |          42.2660 |          11.5357 |
[32m[20221213 23:57:39 @agent_ppo2.py:185][0m |          -0.0182 |          41.4973 |          11.5282 |
[32m[20221213 23:57:39 @agent_ppo2.py:185][0m |          -0.0183 |          40.9912 |          11.5251 |
[32m[20221213 23:57:39 @agent_ppo2.py:185][0m |          -0.0105 |          43.7644 |          11.4754 |
[32m[20221213 23:57:39 @agent_ppo2.py:185][0m |          -0.0103 |          40.6045 |          11.4817 |
[32m[20221213 23:57:39 @agent_ppo2.py:185][0m |          -0.0191 |          39.8266 |          11.4322 |
[32m[20221213 23:57:39 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:57:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 417.39
[32m[20221213 23:57:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 510.09
[32m[20221213 23:57:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 447.17
[32m[20221213 23:57:39 @agent_ppo2.py:143][0m Total time:       0.15 min
[32m[20221213 23:57:39 @agent_ppo2.py:145][0m 14336 total steps have happened
[32m[20221213 23:57:39 @agent_ppo2.py:121][0m #------------------------ Iteration 4007 --------------------------#
[32m[20221213 23:57:39 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:57:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:57:40 @agent_ppo2.py:185][0m |           0.0033 |          65.2455 |          10.8942 |
[32m[20221213 23:57:40 @agent_ppo2.py:185][0m |          -0.0056 |          60.4130 |          10.9496 |
[32m[20221213 23:57:40 @agent_ppo2.py:185][0m |          -0.0068 |          59.6164 |          10.9083 |
[32m[20221213 23:57:40 @agent_ppo2.py:185][0m |          -0.0122 |          58.7396 |          10.9694 |
[32m[20221213 23:57:40 @agent_ppo2.py:185][0m |          -0.0167 |          58.0317 |          11.0090 |
[32m[20221213 23:57:40 @agent_ppo2.py:185][0m |          -0.0168 |          57.5841 |          11.0191 |
[32m[20221213 23:57:40 @agent_ppo2.py:185][0m |          -0.0117 |          57.7747 |          11.0514 |
[32m[20221213 23:57:40 @agent_ppo2.py:185][0m |          -0.0136 |          56.9395 |          11.0472 |
[32m[20221213 23:57:40 @agent_ppo2.py:185][0m |          -0.0163 |          56.7294 |          11.0646 |
[32m[20221213 23:57:40 @agent_ppo2.py:185][0m |          -0.0106 |          57.2575 |          11.0920 |
[32m[20221213 23:57:40 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:57:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 383.39
[32m[20221213 23:57:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 439.31
[32m[20221213 23:57:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 523.72
[32m[20221213 23:57:41 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 523.72
[32m[20221213 23:57:41 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 523.72
[32m[20221213 23:57:41 @agent_ppo2.py:143][0m Total time:       0.17 min
[32m[20221213 23:57:41 @agent_ppo2.py:145][0m 16384 total steps have happened
[32m[20221213 23:57:41 @agent_ppo2.py:121][0m #------------------------ Iteration 4008 --------------------------#
[32m[20221213 23:57:41 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:57:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:57:41 @agent_ppo2.py:185][0m |           0.0017 |          55.5851 |          11.0070 |
[32m[20221213 23:57:41 @agent_ppo2.py:185][0m |          -0.0062 |          49.8253 |          11.0189 |
[32m[20221213 23:57:41 @agent_ppo2.py:185][0m |          -0.0096 |          48.0941 |          11.0127 |
[32m[20221213 23:57:41 @agent_ppo2.py:185][0m |          -0.0087 |          47.4759 |          11.0609 |
[32m[20221213 23:57:41 @agent_ppo2.py:185][0m |          -0.0055 |          46.8905 |          11.0331 |
[32m[20221213 23:57:41 @agent_ppo2.py:185][0m |          -0.0122 |          46.2405 |          11.0406 |
[32m[20221213 23:57:41 @agent_ppo2.py:185][0m |          -0.0073 |          45.6758 |          11.0442 |
[32m[20221213 23:57:42 @agent_ppo2.py:185][0m |          -0.0159 |          45.3780 |          11.0438 |
[32m[20221213 23:57:42 @agent_ppo2.py:185][0m |          -0.0189 |          44.9334 |          11.0429 |
[32m[20221213 23:57:42 @agent_ppo2.py:185][0m |          -0.0180 |          44.6535 |          11.0538 |
[32m[20221213 23:57:42 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:57:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 380.27
[32m[20221213 23:57:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 442.30
[32m[20221213 23:57:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 476.24
[32m[20221213 23:57:42 @agent_ppo2.py:143][0m Total time:       0.19 min
[32m[20221213 23:57:42 @agent_ppo2.py:145][0m 18432 total steps have happened
[32m[20221213 23:57:42 @agent_ppo2.py:121][0m #------------------------ Iteration 4009 --------------------------#
[32m[20221213 23:57:42 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:57:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:57:42 @agent_ppo2.py:185][0m |          -0.0025 |          50.1095 |          11.3426 |
[32m[20221213 23:57:42 @agent_ppo2.py:185][0m |          -0.0107 |          46.9063 |          11.3470 |
[32m[20221213 23:57:42 @agent_ppo2.py:185][0m |          -0.0100 |          46.2164 |          11.3740 |
[32m[20221213 23:57:42 @agent_ppo2.py:185][0m |          -0.0117 |          45.2908 |          11.3566 |
[32m[20221213 23:57:43 @agent_ppo2.py:185][0m |          -0.0124 |          44.9576 |          11.3663 |
[32m[20221213 23:57:43 @agent_ppo2.py:185][0m |          -0.0192 |          44.5584 |          11.3783 |
[32m[20221213 23:57:43 @agent_ppo2.py:185][0m |          -0.0173 |          44.4611 |          11.3600 |
[32m[20221213 23:57:43 @agent_ppo2.py:185][0m |          -0.0207 |          43.9676 |          11.3236 |
[32m[20221213 23:57:43 @agent_ppo2.py:185][0m |          -0.0169 |          43.6893 |          11.3531 |
[32m[20221213 23:57:43 @agent_ppo2.py:185][0m |          -0.0205 |          43.6292 |          11.3417 |
[32m[20221213 23:57:43 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:57:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 376.77
[32m[20221213 23:57:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 484.06
[32m[20221213 23:57:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 443.58
[32m[20221213 23:57:43 @agent_ppo2.py:143][0m Total time:       0.21 min
[32m[20221213 23:57:43 @agent_ppo2.py:145][0m 20480 total steps have happened
[32m[20221213 23:57:43 @agent_ppo2.py:121][0m #------------------------ Iteration 4010 --------------------------#
[32m[20221213 23:57:43 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:57:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:57:43 @agent_ppo2.py:185][0m |           0.0019 |          60.6414 |          11.4485 |
[32m[20221213 23:57:44 @agent_ppo2.py:185][0m |          -0.0054 |          55.9115 |          11.4797 |
[32m[20221213 23:57:44 @agent_ppo2.py:185][0m |          -0.0018 |          55.2046 |          11.4130 |
[32m[20221213 23:57:44 @agent_ppo2.py:185][0m |          -0.0041 |          53.2101 |          11.4461 |
[32m[20221213 23:57:44 @agent_ppo2.py:185][0m |          -0.0040 |          58.0131 |          11.4260 |
[32m[20221213 23:57:44 @agent_ppo2.py:185][0m |          -0.0046 |          53.7978 |          11.4543 |
[32m[20221213 23:57:44 @agent_ppo2.py:185][0m |          -0.0121 |          50.1728 |          11.4079 |
[32m[20221213 23:57:44 @agent_ppo2.py:185][0m |          -0.0011 |          51.0409 |          11.4119 |
[32m[20221213 23:57:44 @agent_ppo2.py:185][0m |          -0.0138 |          49.1204 |          11.4367 |
[32m[20221213 23:57:44 @agent_ppo2.py:185][0m |          -0.0132 |          48.6869 |          11.4419 |
[32m[20221213 23:57:44 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:57:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 480.81
[32m[20221213 23:57:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 515.78
[32m[20221213 23:57:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 418.27
[32m[20221213 23:57:44 @agent_ppo2.py:143][0m Total time:       0.23 min
[32m[20221213 23:57:44 @agent_ppo2.py:145][0m 22528 total steps have happened
[32m[20221213 23:57:44 @agent_ppo2.py:121][0m #------------------------ Iteration 4011 --------------------------#
[32m[20221213 23:57:45 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:57:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:57:45 @agent_ppo2.py:185][0m |           0.0019 |          61.4701 |          11.0936 |
[32m[20221213 23:57:45 @agent_ppo2.py:185][0m |          -0.0049 |          57.5694 |          11.1015 |
[32m[20221213 23:57:45 @agent_ppo2.py:185][0m |          -0.0080 |          54.9683 |          11.1323 |
[32m[20221213 23:57:45 @agent_ppo2.py:185][0m |          -0.0095 |          53.7238 |          11.0855 |
[32m[20221213 23:57:45 @agent_ppo2.py:185][0m |          -0.0187 |          53.2078 |          11.0988 |
[32m[20221213 23:57:45 @agent_ppo2.py:185][0m |          -0.0134 |          53.2025 |          11.0544 |
[32m[20221213 23:57:45 @agent_ppo2.py:185][0m |          -0.0152 |          52.1347 |          11.0807 |
[32m[20221213 23:57:45 @agent_ppo2.py:185][0m |          -0.0152 |          51.9033 |          11.0486 |
[32m[20221213 23:57:45 @agent_ppo2.py:185][0m |          -0.0205 |          51.5807 |          11.0450 |
[32m[20221213 23:57:46 @agent_ppo2.py:185][0m |          -0.0159 |          51.2425 |          11.0141 |
[32m[20221213 23:57:46 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:57:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 413.08
[32m[20221213 23:57:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 470.47
[32m[20221213 23:57:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 516.96
[32m[20221213 23:57:46 @agent_ppo2.py:143][0m Total time:       0.26 min
[32m[20221213 23:57:46 @agent_ppo2.py:145][0m 24576 total steps have happened
[32m[20221213 23:57:46 @agent_ppo2.py:121][0m #------------------------ Iteration 4012 --------------------------#
[32m[20221213 23:57:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:57:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:57:46 @agent_ppo2.py:185][0m |          -0.0008 |          58.9426 |          10.9863 |
[32m[20221213 23:57:46 @agent_ppo2.py:185][0m |          -0.0022 |          51.3129 |          11.0799 |
[32m[20221213 23:57:46 @agent_ppo2.py:185][0m |          -0.0092 |          49.4950 |          11.0674 |
[32m[20221213 23:57:46 @agent_ppo2.py:185][0m |          -0.0107 |          48.6507 |          11.1016 |
[32m[20221213 23:57:46 @agent_ppo2.py:185][0m |          -0.0032 |          49.8794 |          11.1011 |
[32m[20221213 23:57:46 @agent_ppo2.py:185][0m |          -0.0120 |          47.4968 |          11.1358 |
[32m[20221213 23:57:47 @agent_ppo2.py:185][0m |          -0.0120 |          47.2374 |          11.1132 |
[32m[20221213 23:57:47 @agent_ppo2.py:185][0m |          -0.0150 |          46.6465 |          11.1266 |
[32m[20221213 23:57:47 @agent_ppo2.py:185][0m |          -0.0150 |          46.4286 |          11.1550 |
[32m[20221213 23:57:47 @agent_ppo2.py:185][0m |          -0.0193 |          46.3143 |          11.2064 |
[32m[20221213 23:57:47 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:57:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 458.10
[32m[20221213 23:57:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 482.47
[32m[20221213 23:57:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 400.35
[32m[20221213 23:57:47 @agent_ppo2.py:143][0m Total time:       0.28 min
[32m[20221213 23:57:47 @agent_ppo2.py:145][0m 26624 total steps have happened
[32m[20221213 23:57:47 @agent_ppo2.py:121][0m #------------------------ Iteration 4013 --------------------------#
[32m[20221213 23:57:47 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:57:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:57:47 @agent_ppo2.py:185][0m |          -0.0015 |          57.9376 |          11.2196 |
[32m[20221213 23:57:47 @agent_ppo2.py:185][0m |           0.0037 |          57.2321 |          11.1564 |
[32m[20221213 23:57:47 @agent_ppo2.py:185][0m |          -0.0006 |          51.1692 |          11.1367 |
[32m[20221213 23:57:48 @agent_ppo2.py:185][0m |          -0.0121 |          49.2214 |          11.0867 |
[32m[20221213 23:57:48 @agent_ppo2.py:185][0m |          -0.0145 |          48.5219 |          11.0829 |
[32m[20221213 23:57:48 @agent_ppo2.py:185][0m |          -0.0155 |          48.1569 |          11.0411 |
[32m[20221213 23:57:48 @agent_ppo2.py:185][0m |          -0.0176 |          47.6833 |          11.0161 |
[32m[20221213 23:57:48 @agent_ppo2.py:185][0m |          -0.0152 |          47.2675 |          11.0129 |
[32m[20221213 23:57:48 @agent_ppo2.py:185][0m |          -0.0157 |          46.9748 |          10.9816 |
[32m[20221213 23:57:48 @agent_ppo2.py:185][0m |          -0.0200 |          46.5590 |          10.9569 |
[32m[20221213 23:57:48 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:57:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 449.12
[32m[20221213 23:57:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 489.97
[32m[20221213 23:57:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 499.27
[32m[20221213 23:57:48 @agent_ppo2.py:143][0m Total time:       0.30 min
[32m[20221213 23:57:48 @agent_ppo2.py:145][0m 28672 total steps have happened
[32m[20221213 23:57:48 @agent_ppo2.py:121][0m #------------------------ Iteration 4014 --------------------------#
[32m[20221213 23:57:48 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:57:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:57:49 @agent_ppo2.py:185][0m |           0.0151 |          74.2608 |          10.5892 |
[32m[20221213 23:57:49 @agent_ppo2.py:185][0m |          -0.0066 |          61.5640 |          10.6451 |
[32m[20221213 23:57:49 @agent_ppo2.py:185][0m |          -0.0085 |          60.8484 |          10.6071 |
[32m[20221213 23:57:49 @agent_ppo2.py:185][0m |          -0.0047 |          61.1196 |          10.6368 |
[32m[20221213 23:57:49 @agent_ppo2.py:185][0m |          -0.0123 |          60.0505 |          10.6107 |
[32m[20221213 23:57:49 @agent_ppo2.py:185][0m |          -0.0085 |          60.1478 |          10.6290 |
[32m[20221213 23:57:49 @agent_ppo2.py:185][0m |          -0.0089 |          60.1363 |          10.6655 |
[32m[20221213 23:57:49 @agent_ppo2.py:185][0m |          -0.0143 |          59.5666 |          10.6475 |
[32m[20221213 23:57:49 @agent_ppo2.py:185][0m |          -0.0115 |          59.5952 |          10.6942 |
[32m[20221213 23:57:49 @agent_ppo2.py:185][0m |          -0.0137 |          59.3301 |          10.6895 |
[32m[20221213 23:57:49 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:57:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 456.85
[32m[20221213 23:57:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 481.87
[32m[20221213 23:57:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 471.44
[32m[20221213 23:57:50 @agent_ppo2.py:143][0m Total time:       0.32 min
[32m[20221213 23:57:50 @agent_ppo2.py:145][0m 30720 total steps have happened
[32m[20221213 23:57:50 @agent_ppo2.py:121][0m #------------------------ Iteration 4015 --------------------------#
[32m[20221213 23:57:50 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:57:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:57:50 @agent_ppo2.py:185][0m |           0.0129 |          72.2775 |          10.9415 |
[32m[20221213 23:57:50 @agent_ppo2.py:185][0m |          -0.0050 |          64.2204 |          10.8742 |
[32m[20221213 23:57:50 @agent_ppo2.py:185][0m |          -0.0140 |          62.5646 |          10.8915 |
[32m[20221213 23:57:50 @agent_ppo2.py:185][0m |          -0.0076 |          62.1656 |          10.8648 |
[32m[20221213 23:57:50 @agent_ppo2.py:185][0m |          -0.0104 |          61.2814 |          10.8075 |
[32m[20221213 23:57:50 @agent_ppo2.py:185][0m |          -0.0038 |          66.6375 |          10.7669 |
[32m[20221213 23:57:50 @agent_ppo2.py:185][0m |          -0.0156 |          60.8538 |          10.7591 |
[32m[20221213 23:57:50 @agent_ppo2.py:185][0m |          -0.0150 |          60.0112 |          10.7681 |
[32m[20221213 23:57:51 @agent_ppo2.py:185][0m |          -0.0174 |          59.4781 |          10.7150 |
[32m[20221213 23:57:51 @agent_ppo2.py:185][0m |          -0.0207 |          59.2880 |          10.7157 |
[32m[20221213 23:57:51 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:57:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 358.33
[32m[20221213 23:57:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 425.20
[32m[20221213 23:57:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 453.27
[32m[20221213 23:57:51 @agent_ppo2.py:143][0m Total time:       0.34 min
[32m[20221213 23:57:51 @agent_ppo2.py:145][0m 32768 total steps have happened
[32m[20221213 23:57:51 @agent_ppo2.py:121][0m #------------------------ Iteration 4016 --------------------------#
[32m[20221213 23:57:51 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:57:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:57:51 @agent_ppo2.py:185][0m |          -0.0005 |          62.1918 |          10.7160 |
[32m[20221213 23:57:51 @agent_ppo2.py:185][0m |          -0.0019 |          58.5585 |          10.7742 |
[32m[20221213 23:57:51 @agent_ppo2.py:185][0m |          -0.0082 |          56.8901 |          10.7830 |
[32m[20221213 23:57:51 @agent_ppo2.py:185][0m |          -0.0070 |          56.4424 |          10.7914 |
[32m[20221213 23:57:51 @agent_ppo2.py:185][0m |          -0.0060 |          55.9039 |          10.8324 |
[32m[20221213 23:57:52 @agent_ppo2.py:185][0m |           0.0060 |          60.7396 |          10.8283 |
[32m[20221213 23:57:52 @agent_ppo2.py:185][0m |          -0.0082 |          55.0883 |          10.8763 |
[32m[20221213 23:57:52 @agent_ppo2.py:185][0m |          -0.0073 |          54.9631 |          10.8803 |
[32m[20221213 23:57:52 @agent_ppo2.py:185][0m |          -0.0081 |          55.1439 |          10.9042 |
[32m[20221213 23:57:52 @agent_ppo2.py:185][0m |          -0.0146 |          54.4428 |          10.9466 |
[32m[20221213 23:57:52 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:57:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 455.70
[32m[20221213 23:57:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 477.68
[32m[20221213 23:57:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 468.77
[32m[20221213 23:57:52 @agent_ppo2.py:143][0m Total time:       0.36 min
[32m[20221213 23:57:52 @agent_ppo2.py:145][0m 34816 total steps have happened
[32m[20221213 23:57:52 @agent_ppo2.py:121][0m #------------------------ Iteration 4017 --------------------------#
[32m[20221213 23:57:52 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:57:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:57:52 @agent_ppo2.py:185][0m |           0.0040 |          69.9779 |          10.8635 |
[32m[20221213 23:57:53 @agent_ppo2.py:185][0m |          -0.0046 |          66.4214 |          10.8844 |
[32m[20221213 23:57:53 @agent_ppo2.py:185][0m |          -0.0090 |          65.3215 |          10.8920 |
[32m[20221213 23:57:53 @agent_ppo2.py:185][0m |          -0.0093 |          64.5790 |          10.9705 |
[32m[20221213 23:57:53 @agent_ppo2.py:185][0m |          -0.0062 |          67.0229 |          10.9216 |
[32m[20221213 23:57:53 @agent_ppo2.py:185][0m |          -0.0083 |          63.8486 |          10.9374 |
[32m[20221213 23:57:53 @agent_ppo2.py:185][0m |          -0.0135 |          63.3384 |          10.9587 |
[32m[20221213 23:57:53 @agent_ppo2.py:185][0m |          -0.0154 |          63.1608 |          10.9502 |
[32m[20221213 23:57:53 @agent_ppo2.py:185][0m |          -0.0152 |          62.9446 |          10.9459 |
[32m[20221213 23:57:53 @agent_ppo2.py:185][0m |          -0.0151 |          62.6789 |          10.9447 |
[32m[20221213 23:57:53 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:57:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 449.10
[32m[20221213 23:57:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 500.33
[32m[20221213 23:57:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 334.94
[32m[20221213 23:57:53 @agent_ppo2.py:143][0m Total time:       0.38 min
[32m[20221213 23:57:53 @agent_ppo2.py:145][0m 36864 total steps have happened
[32m[20221213 23:57:53 @agent_ppo2.py:121][0m #------------------------ Iteration 4018 --------------------------#
[32m[20221213 23:57:54 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:57:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:57:54 @agent_ppo2.py:185][0m |           0.0024 |          47.6637 |          10.9274 |
[32m[20221213 23:57:54 @agent_ppo2.py:185][0m |          -0.0047 |          42.4992 |          10.9388 |
[32m[20221213 23:57:54 @agent_ppo2.py:185][0m |          -0.0090 |          40.9210 |          10.9406 |
[32m[20221213 23:57:54 @agent_ppo2.py:185][0m |          -0.0127 |          40.0025 |          10.8993 |
[32m[20221213 23:57:54 @agent_ppo2.py:185][0m |          -0.0089 |          39.4565 |          10.8964 |
[32m[20221213 23:57:54 @agent_ppo2.py:185][0m |          -0.0120 |          39.1230 |          10.9318 |
[32m[20221213 23:57:54 @agent_ppo2.py:185][0m |          -0.0128 |          38.6100 |          10.9218 |
[32m[20221213 23:57:54 @agent_ppo2.py:185][0m |          -0.0142 |          38.4526 |          10.8854 |
[32m[20221213 23:57:54 @agent_ppo2.py:185][0m |          -0.0219 |          38.1352 |          10.8978 |
[32m[20221213 23:57:54 @agent_ppo2.py:185][0m |          -0.0119 |          38.7123 |          10.8939 |
[32m[20221213 23:57:54 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:57:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 409.63
[32m[20221213 23:57:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 482.63
[32m[20221213 23:57:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 429.97
[32m[20221213 23:57:55 @agent_ppo2.py:143][0m Total time:       0.40 min
[32m[20221213 23:57:55 @agent_ppo2.py:145][0m 38912 total steps have happened
[32m[20221213 23:57:55 @agent_ppo2.py:121][0m #------------------------ Iteration 4019 --------------------------#
[32m[20221213 23:57:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:57:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:57:55 @agent_ppo2.py:185][0m |          -0.0046 |          68.7064 |          10.8353 |
[32m[20221213 23:57:55 @agent_ppo2.py:185][0m |          -0.0047 |          65.3299 |          10.8535 |
[32m[20221213 23:57:55 @agent_ppo2.py:185][0m |          -0.0060 |          63.8324 |          10.8871 |
[32m[20221213 23:57:55 @agent_ppo2.py:185][0m |          -0.0007 |          65.0465 |          10.8925 |
[32m[20221213 23:57:55 @agent_ppo2.py:185][0m |          -0.0168 |          61.8050 |          10.9544 |
[32m[20221213 23:57:55 @agent_ppo2.py:185][0m |          -0.0133 |          60.8819 |          10.9582 |
[32m[20221213 23:57:55 @agent_ppo2.py:185][0m |          -0.0155 |          60.3605 |          10.9374 |
[32m[20221213 23:57:56 @agent_ppo2.py:185][0m |          -0.0197 |          59.6016 |          10.9751 |
[32m[20221213 23:57:56 @agent_ppo2.py:185][0m |          -0.0173 |          59.2744 |          11.0064 |
[32m[20221213 23:57:56 @agent_ppo2.py:185][0m |          -0.0173 |          59.5899 |          11.0168 |
[32m[20221213 23:57:56 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:57:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 385.31
[32m[20221213 23:57:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 443.38
[32m[20221213 23:57:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 485.30
[32m[20221213 23:57:56 @agent_ppo2.py:143][0m Total time:       0.42 min
[32m[20221213 23:57:56 @agent_ppo2.py:145][0m 40960 total steps have happened
[32m[20221213 23:57:56 @agent_ppo2.py:121][0m #------------------------ Iteration 4020 --------------------------#
[32m[20221213 23:57:56 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:57:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:57:56 @agent_ppo2.py:185][0m |           0.0061 |          67.8381 |          11.0238 |
[32m[20221213 23:57:56 @agent_ppo2.py:185][0m |           0.0096 |          70.3379 |          11.0430 |
[32m[20221213 23:57:56 @agent_ppo2.py:185][0m |           0.0009 |          67.9423 |          11.0920 |
[32m[20221213 23:57:56 @agent_ppo2.py:185][0m |          -0.0072 |          61.3328 |          11.1356 |
[32m[20221213 23:57:57 @agent_ppo2.py:185][0m |          -0.0124 |          60.7680 |          11.1356 |
[32m[20221213 23:57:57 @agent_ppo2.py:185][0m |          -0.0143 |          60.0739 |          11.1246 |
[32m[20221213 23:57:57 @agent_ppo2.py:185][0m |          -0.0126 |          59.7757 |          11.1175 |
[32m[20221213 23:57:57 @agent_ppo2.py:185][0m |          -0.0135 |          59.5132 |          11.1168 |
[32m[20221213 23:57:57 @agent_ppo2.py:185][0m |          -0.0071 |          66.8866 |          11.1576 |
[32m[20221213 23:57:57 @agent_ppo2.py:185][0m |          -0.0167 |          58.8954 |          11.1118 |
[32m[20221213 23:57:57 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 23:57:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 411.74
[32m[20221213 23:57:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 457.54
[32m[20221213 23:57:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 485.85
[32m[20221213 23:57:57 @agent_ppo2.py:143][0m Total time:       0.45 min
[32m[20221213 23:57:57 @agent_ppo2.py:145][0m 43008 total steps have happened
[32m[20221213 23:57:57 @agent_ppo2.py:121][0m #------------------------ Iteration 4021 --------------------------#
[32m[20221213 23:57:57 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:57:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:57:58 @agent_ppo2.py:185][0m |          -0.0001 |          60.6558 |          11.3222 |
[32m[20221213 23:57:58 @agent_ppo2.py:185][0m |          -0.0040 |          56.9979 |          11.4239 |
[32m[20221213 23:57:58 @agent_ppo2.py:185][0m |          -0.0038 |          56.0995 |          11.3917 |
[32m[20221213 23:57:58 @agent_ppo2.py:185][0m |          -0.0056 |          55.5113 |          11.3678 |
[32m[20221213 23:57:58 @agent_ppo2.py:185][0m |          -0.0093 |          55.1807 |          11.3824 |
[32m[20221213 23:57:58 @agent_ppo2.py:185][0m |          -0.0060 |          54.6433 |          11.4032 |
[32m[20221213 23:57:58 @agent_ppo2.py:185][0m |          -0.0076 |          54.3882 |          11.3954 |
[32m[20221213 23:57:58 @agent_ppo2.py:185][0m |          -0.0083 |          54.3040 |          11.4492 |
[32m[20221213 23:57:58 @agent_ppo2.py:185][0m |          -0.0063 |          53.9143 |          11.4578 |
[32m[20221213 23:57:58 @agent_ppo2.py:185][0m |          -0.0113 |          53.6404 |          11.4142 |
[32m[20221213 23:57:58 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221213 23:57:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 439.97
[32m[20221213 23:57:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 457.25
[32m[20221213 23:57:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 503.19
[32m[20221213 23:57:59 @agent_ppo2.py:143][0m Total time:       0.47 min
[32m[20221213 23:57:59 @agent_ppo2.py:145][0m 45056 total steps have happened
[32m[20221213 23:57:59 @agent_ppo2.py:121][0m #------------------------ Iteration 4022 --------------------------#
[32m[20221213 23:57:59 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:57:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:57:59 @agent_ppo2.py:185][0m |          -0.0002 |          66.0551 |          11.3011 |
[32m[20221213 23:57:59 @agent_ppo2.py:185][0m |          -0.0067 |          64.1830 |          11.3369 |
[32m[20221213 23:57:59 @agent_ppo2.py:185][0m |          -0.0081 |          63.3974 |          11.3314 |
[32m[20221213 23:57:59 @agent_ppo2.py:185][0m |          -0.0070 |          62.9031 |          11.2303 |
[32m[20221213 23:58:00 @agent_ppo2.py:185][0m |          -0.0095 |          62.5942 |          11.2701 |
[32m[20221213 23:58:00 @agent_ppo2.py:185][0m |          -0.0081 |          62.8090 |          11.2863 |
[32m[20221213 23:58:00 @agent_ppo2.py:185][0m |          -0.0103 |          62.1341 |          11.2360 |
[32m[20221213 23:58:00 @agent_ppo2.py:185][0m |          -0.0130 |          62.0714 |          11.2509 |
[32m[20221213 23:58:00 @agent_ppo2.py:185][0m |          -0.0148 |          61.9343 |          11.2421 |
[32m[20221213 23:58:00 @agent_ppo2.py:185][0m |          -0.0120 |          61.8905 |          11.2282 |
[32m[20221213 23:58:00 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 23:58:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 441.04
[32m[20221213 23:58:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 466.65
[32m[20221213 23:58:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 502.64
[32m[20221213 23:58:00 @agent_ppo2.py:143][0m Total time:       0.50 min
[32m[20221213 23:58:00 @agent_ppo2.py:145][0m 47104 total steps have happened
[32m[20221213 23:58:00 @agent_ppo2.py:121][0m #------------------------ Iteration 4023 --------------------------#
[32m[20221213 23:58:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:58:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:58:01 @agent_ppo2.py:185][0m |          -0.0007 |          78.7683 |          11.0821 |
[32m[20221213 23:58:01 @agent_ppo2.py:185][0m |          -0.0034 |          74.5944 |          11.1363 |
[32m[20221213 23:58:01 @agent_ppo2.py:185][0m |          -0.0005 |          77.9344 |          11.1091 |
[32m[20221213 23:58:01 @agent_ppo2.py:185][0m |          -0.0140 |          72.6311 |          11.1219 |
[32m[20221213 23:58:01 @agent_ppo2.py:185][0m |          -0.0157 |          72.0823 |          11.0687 |
[32m[20221213 23:58:01 @agent_ppo2.py:185][0m |          -0.0123 |          71.6452 |          11.0990 |
[32m[20221213 23:58:01 @agent_ppo2.py:185][0m |          -0.0159 |          71.5172 |          11.0750 |
[32m[20221213 23:58:01 @agent_ppo2.py:185][0m |          -0.0170 |          71.1492 |          11.1201 |
[32m[20221213 23:58:01 @agent_ppo2.py:185][0m |          -0.0173 |          70.9237 |          11.0951 |
[32m[20221213 23:58:01 @agent_ppo2.py:185][0m |          -0.0196 |          70.8396 |          11.0841 |
[32m[20221213 23:58:01 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:58:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 392.60
[32m[20221213 23:58:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 447.75
[32m[20221213 23:58:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 471.69
[32m[20221213 23:58:01 @agent_ppo2.py:143][0m Total time:       0.52 min
[32m[20221213 23:58:01 @agent_ppo2.py:145][0m 49152 total steps have happened
[32m[20221213 23:58:01 @agent_ppo2.py:121][0m #------------------------ Iteration 4024 --------------------------#
[32m[20221213 23:58:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:58:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:58:02 @agent_ppo2.py:185][0m |           0.0013 |          59.3636 |          11.2187 |
[32m[20221213 23:58:02 @agent_ppo2.py:185][0m |          -0.0047 |          56.8014 |          11.1864 |
[32m[20221213 23:58:02 @agent_ppo2.py:185][0m |          -0.0075 |          55.4658 |          11.1827 |
[32m[20221213 23:58:02 @agent_ppo2.py:185][0m |          -0.0080 |          54.8106 |          11.1809 |
[32m[20221213 23:58:02 @agent_ppo2.py:185][0m |          -0.0045 |          54.6497 |          11.1763 |
[32m[20221213 23:58:02 @agent_ppo2.py:185][0m |          -0.0120 |          53.4878 |          11.2420 |
[32m[20221213 23:58:02 @agent_ppo2.py:185][0m |          -0.0102 |          53.0776 |          11.2230 |
[32m[20221213 23:58:02 @agent_ppo2.py:185][0m |          -0.0137 |          52.7052 |          11.2647 |
[32m[20221213 23:58:03 @agent_ppo2.py:185][0m |          -0.0101 |          52.4563 |          11.2870 |
[32m[20221213 23:58:03 @agent_ppo2.py:185][0m |          -0.0060 |          53.5582 |          11.2595 |
[32m[20221213 23:58:03 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:58:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 457.68
[32m[20221213 23:58:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 480.72
[32m[20221213 23:58:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 450.06
[32m[20221213 23:58:03 @agent_ppo2.py:143][0m Total time:       0.54 min
[32m[20221213 23:58:03 @agent_ppo2.py:145][0m 51200 total steps have happened
[32m[20221213 23:58:03 @agent_ppo2.py:121][0m #------------------------ Iteration 4025 --------------------------#
[32m[20221213 23:58:03 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:58:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:58:03 @agent_ppo2.py:185][0m |           0.0057 |          54.4948 |          11.1122 |
[32m[20221213 23:58:03 @agent_ppo2.py:185][0m |          -0.0127 |          49.0186 |          11.1514 |
[32m[20221213 23:58:03 @agent_ppo2.py:185][0m |          -0.0114 |          46.7519 |          11.1609 |
[32m[20221213 23:58:03 @agent_ppo2.py:185][0m |          -0.0172 |          45.5550 |          11.1654 |
[32m[20221213 23:58:03 @agent_ppo2.py:185][0m |          -0.0149 |          44.4932 |          11.1649 |
[32m[20221213 23:58:04 @agent_ppo2.py:185][0m |          -0.0197 |          43.6916 |          11.1709 |
[32m[20221213 23:58:04 @agent_ppo2.py:185][0m |          -0.0164 |          43.2831 |          11.1219 |
[32m[20221213 23:58:04 @agent_ppo2.py:185][0m |          -0.0235 |          42.4843 |          11.1820 |
[32m[20221213 23:58:04 @agent_ppo2.py:185][0m |          -0.0233 |          42.3614 |          11.1874 |
[32m[20221213 23:58:04 @agent_ppo2.py:185][0m |          -0.0172 |          42.3183 |          11.1894 |
[32m[20221213 23:58:04 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:58:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 356.65
[32m[20221213 23:58:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 530.37
[32m[20221213 23:58:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 461.84
[32m[20221213 23:58:04 @agent_ppo2.py:143][0m Total time:       0.56 min
[32m[20221213 23:58:04 @agent_ppo2.py:145][0m 53248 total steps have happened
[32m[20221213 23:58:04 @agent_ppo2.py:121][0m #------------------------ Iteration 4026 --------------------------#
[32m[20221213 23:58:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:58:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:58:04 @agent_ppo2.py:185][0m |           0.0010 |          56.9971 |          11.0796 |
[32m[20221213 23:58:04 @agent_ppo2.py:185][0m |          -0.0086 |          53.9619 |          11.0361 |
[32m[20221213 23:58:04 @agent_ppo2.py:185][0m |          -0.0103 |          52.4759 |          11.0514 |
[32m[20221213 23:58:05 @agent_ppo2.py:185][0m |          -0.0109 |          51.5137 |          11.0452 |
[32m[20221213 23:58:05 @agent_ppo2.py:185][0m |          -0.0096 |          51.0364 |          11.0464 |
[32m[20221213 23:58:05 @agent_ppo2.py:185][0m |           0.0011 |          58.3863 |          11.0558 |
[32m[20221213 23:58:05 @agent_ppo2.py:185][0m |          -0.0142 |          50.1401 |          11.0324 |
[32m[20221213 23:58:05 @agent_ppo2.py:185][0m |          -0.0127 |          49.4986 |          11.0402 |
[32m[20221213 23:58:05 @agent_ppo2.py:185][0m |          -0.0154 |          49.2471 |          11.0147 |
[32m[20221213 23:58:05 @agent_ppo2.py:185][0m |          -0.0131 |          49.3774 |          11.0338 |
[32m[20221213 23:58:05 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:58:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 411.90
[32m[20221213 23:58:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 446.53
[32m[20221213 23:58:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 449.00
[32m[20221213 23:58:05 @agent_ppo2.py:143][0m Total time:       0.58 min
[32m[20221213 23:58:05 @agent_ppo2.py:145][0m 55296 total steps have happened
[32m[20221213 23:58:05 @agent_ppo2.py:121][0m #------------------------ Iteration 4027 --------------------------#
[32m[20221213 23:58:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:58:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:58:06 @agent_ppo2.py:185][0m |           0.0006 |          58.3623 |          11.0726 |
[32m[20221213 23:58:06 @agent_ppo2.py:185][0m |           0.0002 |          56.5632 |          11.1341 |
[32m[20221213 23:58:06 @agent_ppo2.py:185][0m |          -0.0061 |          55.0983 |          11.0818 |
[32m[20221213 23:58:06 @agent_ppo2.py:185][0m |          -0.0033 |          56.5560 |          11.1099 |
[32m[20221213 23:58:06 @agent_ppo2.py:185][0m |          -0.0096 |          53.9586 |          11.0854 |
[32m[20221213 23:58:06 @agent_ppo2.py:185][0m |          -0.0017 |          57.6803 |          11.0923 |
[32m[20221213 23:58:06 @agent_ppo2.py:185][0m |          -0.0104 |          53.7363 |          11.1085 |
[32m[20221213 23:58:06 @agent_ppo2.py:185][0m |          -0.0116 |          53.1371 |          11.1023 |
[32m[20221213 23:58:06 @agent_ppo2.py:185][0m |          -0.0162 |          53.1063 |          11.0491 |
[32m[20221213 23:58:06 @agent_ppo2.py:185][0m |          -0.0201 |          52.9663 |          11.0999 |
[32m[20221213 23:58:06 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:58:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 423.97
[32m[20221213 23:58:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 452.12
[32m[20221213 23:58:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 351.62
[32m[20221213 23:58:06 @agent_ppo2.py:143][0m Total time:       0.60 min
[32m[20221213 23:58:06 @agent_ppo2.py:145][0m 57344 total steps have happened
[32m[20221213 23:58:06 @agent_ppo2.py:121][0m #------------------------ Iteration 4028 --------------------------#
[32m[20221213 23:58:07 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:58:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:58:07 @agent_ppo2.py:185][0m |          -0.0024 |          70.1982 |          10.8193 |
[32m[20221213 23:58:07 @agent_ppo2.py:185][0m |          -0.0024 |          68.7360 |          10.8087 |
[32m[20221213 23:58:07 @agent_ppo2.py:185][0m |          -0.0047 |          66.7855 |          10.8109 |
[32m[20221213 23:58:07 @agent_ppo2.py:185][0m |          -0.0071 |          66.0831 |          10.8012 |
[32m[20221213 23:58:07 @agent_ppo2.py:185][0m |          -0.0096 |          65.6496 |          10.7918 |
[32m[20221213 23:58:07 @agent_ppo2.py:185][0m |          -0.0126 |          65.0158 |          10.8275 |
[32m[20221213 23:58:07 @agent_ppo2.py:185][0m |          -0.0033 |          66.1632 |          10.8336 |
[32m[20221213 23:58:07 @agent_ppo2.py:185][0m |          -0.0125 |          64.9728 |          10.7984 |
[32m[20221213 23:58:08 @agent_ppo2.py:185][0m |          -0.0133 |          64.2179 |          10.8108 |
[32m[20221213 23:58:08 @agent_ppo2.py:185][0m |          -0.0132 |          64.1433 |          10.7810 |
[32m[20221213 23:58:08 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:58:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 430.88
[32m[20221213 23:58:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 455.21
[32m[20221213 23:58:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 419.57
[32m[20221213 23:58:08 @agent_ppo2.py:143][0m Total time:       0.62 min
[32m[20221213 23:58:08 @agent_ppo2.py:145][0m 59392 total steps have happened
[32m[20221213 23:58:08 @agent_ppo2.py:121][0m #------------------------ Iteration 4029 --------------------------#
[32m[20221213 23:58:08 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:58:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:58:08 @agent_ppo2.py:185][0m |           0.0015 |          63.2995 |          10.7845 |
[32m[20221213 23:58:08 @agent_ppo2.py:185][0m |           0.0049 |          66.6866 |          10.8058 |
[32m[20221213 23:58:08 @agent_ppo2.py:185][0m |          -0.0073 |          60.1303 |          10.8262 |
[32m[20221213 23:58:08 @agent_ppo2.py:185][0m |          -0.0118 |          59.5907 |          10.8467 |
[32m[20221213 23:58:08 @agent_ppo2.py:185][0m |          -0.0058 |          60.6873 |          10.8345 |
[32m[20221213 23:58:09 @agent_ppo2.py:185][0m |          -0.0143 |          58.9250 |          10.8809 |
[32m[20221213 23:58:09 @agent_ppo2.py:185][0m |          -0.0145 |          58.4770 |          10.8391 |
[32m[20221213 23:58:09 @agent_ppo2.py:185][0m |          -0.0142 |          58.2729 |          10.8826 |
[32m[20221213 23:58:09 @agent_ppo2.py:185][0m |          -0.0158 |          58.0704 |          10.8717 |
[32m[20221213 23:58:09 @agent_ppo2.py:185][0m |          -0.0165 |          58.1034 |          10.8407 |
[32m[20221213 23:58:09 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:58:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 420.18
[32m[20221213 23:58:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 451.05
[32m[20221213 23:58:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 451.73
[32m[20221213 23:58:09 @agent_ppo2.py:143][0m Total time:       0.64 min
[32m[20221213 23:58:09 @agent_ppo2.py:145][0m 61440 total steps have happened
[32m[20221213 23:58:09 @agent_ppo2.py:121][0m #------------------------ Iteration 4030 --------------------------#
[32m[20221213 23:58:09 @agent_ppo2.py:127][0m Sampling time: 0.26 s by 5 slaves
[32m[20221213 23:58:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:58:09 @agent_ppo2.py:185][0m |          -0.0006 |          60.7572 |          11.0819 |
[32m[20221213 23:58:10 @agent_ppo2.py:185][0m |          -0.0047 |          58.5250 |          11.0684 |
[32m[20221213 23:58:10 @agent_ppo2.py:185][0m |          -0.0055 |          58.0182 |          11.1771 |
[32m[20221213 23:58:10 @agent_ppo2.py:185][0m |          -0.0130 |          57.3119 |          11.1478 |
[32m[20221213 23:58:10 @agent_ppo2.py:185][0m |          -0.0121 |          57.1825 |          11.0924 |
[32m[20221213 23:58:10 @agent_ppo2.py:185][0m |          -0.0112 |          57.0002 |          11.1529 |
[32m[20221213 23:58:10 @agent_ppo2.py:185][0m |          -0.0120 |          56.6818 |          11.1345 |
[32m[20221213 23:58:10 @agent_ppo2.py:185][0m |          -0.0144 |          56.5210 |          11.1263 |
[32m[20221213 23:58:10 @agent_ppo2.py:185][0m |          -0.0147 |          56.5129 |          11.1409 |
[32m[20221213 23:58:10 @agent_ppo2.py:185][0m |          -0.0131 |          56.3693 |          11.1333 |
[32m[20221213 23:58:10 @agent_ppo2.py:130][0m Policy update time: 1.20 s
[32m[20221213 23:58:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 477.93
[32m[20221213 23:58:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 504.21
[32m[20221213 23:58:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 393.81
[32m[20221213 23:58:11 @agent_ppo2.py:143][0m Total time:       0.67 min
[32m[20221213 23:58:11 @agent_ppo2.py:145][0m 63488 total steps have happened
[32m[20221213 23:58:11 @agent_ppo2.py:121][0m #------------------------ Iteration 4031 --------------------------#
[32m[20221213 23:58:11 @agent_ppo2.py:127][0m Sampling time: 0.31 s by 5 slaves
[32m[20221213 23:58:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:58:11 @agent_ppo2.py:185][0m |          -0.0024 |          52.5467 |          10.7223 |
[32m[20221213 23:58:11 @agent_ppo2.py:185][0m |          -0.0075 |          48.8125 |          10.7010 |
[32m[20221213 23:58:12 @agent_ppo2.py:185][0m |          -0.0126 |          46.5991 |          10.7833 |
[32m[20221213 23:58:12 @agent_ppo2.py:185][0m |          -0.0040 |          46.4217 |          10.7704 |
[32m[20221213 23:58:12 @agent_ppo2.py:185][0m |           0.0052 |          50.9684 |          10.7838 |
[32m[20221213 23:58:12 @agent_ppo2.py:185][0m |          -0.0128 |          44.4114 |          10.7742 |
[32m[20221213 23:58:12 @agent_ppo2.py:185][0m |          -0.0128 |          43.7551 |          10.7777 |
[32m[20221213 23:58:12 @agent_ppo2.py:185][0m |          -0.0173 |          43.5726 |          10.8299 |
[32m[20221213 23:58:12 @agent_ppo2.py:185][0m |          -0.0139 |          43.4064 |          10.7803 |
[32m[20221213 23:58:12 @agent_ppo2.py:185][0m |          -0.0064 |          46.5041 |          10.7900 |
[32m[20221213 23:58:12 @agent_ppo2.py:130][0m Policy update time: 1.31 s
[32m[20221213 23:58:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 363.74
[32m[20221213 23:58:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 488.34
[32m[20221213 23:58:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 431.08
[32m[20221213 23:58:12 @agent_ppo2.py:143][0m Total time:       0.70 min
[32m[20221213 23:58:12 @agent_ppo2.py:145][0m 65536 total steps have happened
[32m[20221213 23:58:12 @agent_ppo2.py:121][0m #------------------------ Iteration 4032 --------------------------#
[32m[20221213 23:58:13 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 23:58:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:58:13 @agent_ppo2.py:185][0m |           0.0000 |          53.9660 |          11.0346 |
[32m[20221213 23:58:13 @agent_ppo2.py:185][0m |          -0.0084 |          48.7800 |          11.0728 |
[32m[20221213 23:58:13 @agent_ppo2.py:185][0m |          -0.0075 |          47.1676 |          11.0674 |
[32m[20221213 23:58:13 @agent_ppo2.py:185][0m |          -0.0045 |          46.2102 |          11.0918 |
[32m[20221213 23:58:13 @agent_ppo2.py:185][0m |          -0.0163 |          45.4819 |          11.1195 |
[32m[20221213 23:58:13 @agent_ppo2.py:185][0m |          -0.0098 |          44.8627 |          11.0976 |
[32m[20221213 23:58:14 @agent_ppo2.py:185][0m |          -0.0120 |          44.3833 |          11.1039 |
[32m[20221213 23:58:14 @agent_ppo2.py:185][0m |          -0.0103 |          43.6553 |          11.1081 |
[32m[20221213 23:58:14 @agent_ppo2.py:185][0m |          -0.0143 |          43.2910 |          11.1525 |
[32m[20221213 23:58:14 @agent_ppo2.py:185][0m |          -0.0121 |          43.0178 |          11.1624 |
[32m[20221213 23:58:14 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 23:58:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 468.65
[32m[20221213 23:58:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 520.86
[32m[20221213 23:58:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 480.76
[32m[20221213 23:58:14 @agent_ppo2.py:143][0m Total time:       0.73 min
[32m[20221213 23:58:14 @agent_ppo2.py:145][0m 67584 total steps have happened
[32m[20221213 23:58:14 @agent_ppo2.py:121][0m #------------------------ Iteration 4033 --------------------------#
[32m[20221213 23:58:14 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 23:58:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:58:14 @agent_ppo2.py:185][0m |           0.0013 |          53.9135 |          11.0168 |
[32m[20221213 23:58:14 @agent_ppo2.py:185][0m |          -0.0086 |          47.2703 |          11.0372 |
[32m[20221213 23:58:15 @agent_ppo2.py:185][0m |          -0.0004 |          44.9286 |          11.0308 |
[32m[20221213 23:58:15 @agent_ppo2.py:185][0m |          -0.0033 |          44.4200 |          11.0607 |
[32m[20221213 23:58:15 @agent_ppo2.py:185][0m |          -0.0056 |          42.3462 |          11.0571 |
[32m[20221213 23:58:15 @agent_ppo2.py:185][0m |          -0.0148 |          41.8040 |          11.0770 |
[32m[20221213 23:58:15 @agent_ppo2.py:185][0m |           0.0031 |          41.6970 |          11.1223 |
[32m[20221213 23:58:15 @agent_ppo2.py:185][0m |          -0.0026 |          41.4998 |          11.1324 |
[32m[20221213 23:58:15 @agent_ppo2.py:185][0m |          -0.0146 |          40.2706 |          11.1482 |
[32m[20221213 23:58:15 @agent_ppo2.py:185][0m |           0.0003 |          43.2147 |          11.1542 |
[32m[20221213 23:58:15 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221213 23:58:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 333.70
[32m[20221213 23:58:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 463.95
[32m[20221213 23:58:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 448.53
[32m[20221213 23:58:16 @agent_ppo2.py:143][0m Total time:       0.75 min
[32m[20221213 23:58:16 @agent_ppo2.py:145][0m 69632 total steps have happened
[32m[20221213 23:58:16 @agent_ppo2.py:121][0m #------------------------ Iteration 4034 --------------------------#
[32m[20221213 23:58:16 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 23:58:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:58:16 @agent_ppo2.py:185][0m |           0.0005 |          46.9258 |          11.2388 |
[32m[20221213 23:58:16 @agent_ppo2.py:185][0m |          -0.0085 |          43.1684 |          11.2389 |
[32m[20221213 23:58:16 @agent_ppo2.py:185][0m |          -0.0109 |          41.7401 |          11.2326 |
[32m[20221213 23:58:16 @agent_ppo2.py:185][0m |          -0.0089 |          40.8216 |          11.2284 |
[32m[20221213 23:58:16 @agent_ppo2.py:185][0m |          -0.0088 |          40.6336 |          11.2550 |
[32m[20221213 23:58:16 @agent_ppo2.py:185][0m |          -0.0153 |          39.3954 |          11.2662 |
[32m[20221213 23:58:17 @agent_ppo2.py:185][0m |          -0.0165 |          38.7959 |          11.2854 |
[32m[20221213 23:58:17 @agent_ppo2.py:185][0m |          -0.0179 |          38.3454 |          11.2802 |
[32m[20221213 23:58:17 @agent_ppo2.py:185][0m |          -0.0225 |          38.1006 |          11.3019 |
[32m[20221213 23:58:17 @agent_ppo2.py:185][0m |          -0.0150 |          37.7461 |          11.3096 |
[32m[20221213 23:58:17 @agent_ppo2.py:130][0m Policy update time: 1.17 s
[32m[20221213 23:58:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 435.71
[32m[20221213 23:58:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 474.02
[32m[20221213 23:58:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 561.93
[32m[20221213 23:58:17 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 561.93
[32m[20221213 23:58:17 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 561.93
[32m[20221213 23:58:17 @agent_ppo2.py:143][0m Total time:       0.78 min
[32m[20221213 23:58:17 @agent_ppo2.py:145][0m 71680 total steps have happened
[32m[20221213 23:58:17 @agent_ppo2.py:121][0m #------------------------ Iteration 4035 --------------------------#
[32m[20221213 23:58:17 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 23:58:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:58:17 @agent_ppo2.py:185][0m |          -0.0016 |          57.7663 |          11.6170 |
[32m[20221213 23:58:18 @agent_ppo2.py:185][0m |          -0.0042 |          53.3168 |          11.5999 |
[32m[20221213 23:58:18 @agent_ppo2.py:185][0m |          -0.0109 |          51.4128 |          11.5715 |
[32m[20221213 23:58:18 @agent_ppo2.py:185][0m |          -0.0042 |          50.0291 |          11.5630 |
[32m[20221213 23:58:18 @agent_ppo2.py:185][0m |          -0.0099 |          49.3060 |          11.5817 |
[32m[20221213 23:58:18 @agent_ppo2.py:185][0m |          -0.0166 |          48.5875 |          11.5777 |
[32m[20221213 23:58:18 @agent_ppo2.py:185][0m |          -0.0153 |          47.7784 |          11.5740 |
[32m[20221213 23:58:18 @agent_ppo2.py:185][0m |          -0.0161 |          49.0895 |          11.5710 |
[32m[20221213 23:58:18 @agent_ppo2.py:185][0m |          -0.0213 |          46.8387 |          11.5791 |
[32m[20221213 23:58:18 @agent_ppo2.py:185][0m |          -0.0154 |          46.5445 |          11.5816 |
[32m[20221213 23:58:18 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221213 23:58:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 406.39
[32m[20221213 23:58:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 490.81
[32m[20221213 23:58:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 527.94
[32m[20221213 23:58:19 @agent_ppo2.py:143][0m Total time:       0.80 min
[32m[20221213 23:58:19 @agent_ppo2.py:145][0m 73728 total steps have happened
[32m[20221213 23:58:19 @agent_ppo2.py:121][0m #------------------------ Iteration 4036 --------------------------#
[32m[20221213 23:58:19 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 23:58:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:58:19 @agent_ppo2.py:185][0m |           0.0132 |          69.5868 |          11.4994 |
[32m[20221213 23:58:19 @agent_ppo2.py:185][0m |          -0.0013 |          63.9227 |          11.4724 |
[32m[20221213 23:58:19 @agent_ppo2.py:185][0m |          -0.0067 |          61.8216 |          11.4870 |
[32m[20221213 23:58:19 @agent_ppo2.py:185][0m |          -0.0111 |          60.6570 |          11.4276 |
[32m[20221213 23:58:19 @agent_ppo2.py:185][0m |          -0.0104 |          59.6419 |          11.4217 |
[32m[20221213 23:58:19 @agent_ppo2.py:185][0m |          -0.0088 |          59.6317 |          11.3469 |
[32m[20221213 23:58:20 @agent_ppo2.py:185][0m |          -0.0125 |          57.9570 |          11.3605 |
[32m[20221213 23:58:20 @agent_ppo2.py:185][0m |          -0.0143 |          57.2858 |          11.3165 |
[32m[20221213 23:58:20 @agent_ppo2.py:185][0m |          -0.0162 |          56.9323 |          11.3075 |
[32m[20221213 23:58:20 @agent_ppo2.py:185][0m |          -0.0100 |          57.8079 |          11.3109 |
[32m[20221213 23:58:20 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 23:58:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 431.76
[32m[20221213 23:58:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 493.47
[32m[20221213 23:58:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 563.66
[32m[20221213 23:58:20 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 563.66
[32m[20221213 23:58:20 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 563.66
[32m[20221213 23:58:20 @agent_ppo2.py:143][0m Total time:       0.83 min
[32m[20221213 23:58:20 @agent_ppo2.py:145][0m 75776 total steps have happened
[32m[20221213 23:58:20 @agent_ppo2.py:121][0m #------------------------ Iteration 4037 --------------------------#
[32m[20221213 23:58:20 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 23:58:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:58:20 @agent_ppo2.py:185][0m |           0.0037 |          62.9040 |          11.2366 |
[32m[20221213 23:58:21 @agent_ppo2.py:185][0m |          -0.0049 |          59.9392 |          11.2192 |
[32m[20221213 23:58:21 @agent_ppo2.py:185][0m |          -0.0048 |          58.9112 |          11.2042 |
[32m[20221213 23:58:21 @agent_ppo2.py:185][0m |          -0.0077 |          58.5052 |          11.2357 |
[32m[20221213 23:58:21 @agent_ppo2.py:185][0m |          -0.0087 |          57.8934 |          11.2007 |
[32m[20221213 23:58:21 @agent_ppo2.py:185][0m |          -0.0104 |          57.4588 |          11.2180 |
[32m[20221213 23:58:21 @agent_ppo2.py:185][0m |          -0.0156 |          57.1742 |          11.2377 |
[32m[20221213 23:58:21 @agent_ppo2.py:185][0m |          -0.0029 |          58.4175 |          11.2044 |
[32m[20221213 23:58:21 @agent_ppo2.py:185][0m |          -0.0063 |          57.4462 |          11.2426 |
[32m[20221213 23:58:21 @agent_ppo2.py:185][0m |          -0.0148 |          56.2016 |          11.2258 |
[32m[20221213 23:58:21 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 23:58:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 454.93
[32m[20221213 23:58:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 494.23
[32m[20221213 23:58:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 372.17
[32m[20221213 23:58:22 @agent_ppo2.py:143][0m Total time:       0.85 min
[32m[20221213 23:58:22 @agent_ppo2.py:145][0m 77824 total steps have happened
[32m[20221213 23:58:22 @agent_ppo2.py:121][0m #------------------------ Iteration 4038 --------------------------#
[32m[20221213 23:58:22 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 23:58:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:58:22 @agent_ppo2.py:185][0m |           0.0005 |          66.4652 |          11.3153 |
[32m[20221213 23:58:22 @agent_ppo2.py:185][0m |          -0.0031 |          62.4088 |          11.3412 |
[32m[20221213 23:58:22 @agent_ppo2.py:185][0m |          -0.0069 |          60.6867 |          11.2821 |
[32m[20221213 23:58:22 @agent_ppo2.py:185][0m |          -0.0119 |          59.4850 |          11.2917 |
[32m[20221213 23:58:22 @agent_ppo2.py:185][0m |          -0.0084 |          58.6781 |          11.2737 |
[32m[20221213 23:58:22 @agent_ppo2.py:185][0m |          -0.0141 |          58.1238 |          11.2966 |
[32m[20221213 23:58:23 @agent_ppo2.py:185][0m |          -0.0129 |          57.1110 |          11.2822 |
[32m[20221213 23:58:23 @agent_ppo2.py:185][0m |          -0.0109 |          57.3164 |          11.2710 |
[32m[20221213 23:58:23 @agent_ppo2.py:185][0m |          -0.0159 |          56.1794 |          11.2697 |
[32m[20221213 23:58:23 @agent_ppo2.py:185][0m |          -0.0169 |          55.6586 |          11.2410 |
[32m[20221213 23:58:23 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 23:58:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 297.55
[32m[20221213 23:58:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 422.40
[32m[20221213 23:58:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 479.06
[32m[20221213 23:58:23 @agent_ppo2.py:143][0m Total time:       0.88 min
[32m[20221213 23:58:23 @agent_ppo2.py:145][0m 79872 total steps have happened
[32m[20221213 23:58:23 @agent_ppo2.py:121][0m #------------------------ Iteration 4039 --------------------------#
[32m[20221213 23:58:23 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 23:58:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:58:23 @agent_ppo2.py:185][0m |          -0.0000 |          64.1584 |          11.0144 |
[32m[20221213 23:58:23 @agent_ppo2.py:185][0m |          -0.0014 |          60.6413 |          11.0155 |
[32m[20221213 23:58:24 @agent_ppo2.py:185][0m |          -0.0102 |          57.8724 |          10.9871 |
[32m[20221213 23:58:24 @agent_ppo2.py:185][0m |          -0.0081 |          57.0382 |          10.9726 |
[32m[20221213 23:58:24 @agent_ppo2.py:185][0m |          -0.0120 |          56.1818 |          10.9770 |
[32m[20221213 23:58:24 @agent_ppo2.py:185][0m |          -0.0120 |          55.7448 |          11.0020 |
[32m[20221213 23:58:24 @agent_ppo2.py:185][0m |          -0.0090 |          55.2251 |          11.0328 |
[32m[20221213 23:58:24 @agent_ppo2.py:185][0m |          -0.0183 |          54.8438 |          11.0271 |
[32m[20221213 23:58:24 @agent_ppo2.py:185][0m |          -0.0167 |          54.6730 |          11.0024 |
[32m[20221213 23:58:24 @agent_ppo2.py:185][0m |          -0.0145 |          54.0812 |          11.0126 |
[32m[20221213 23:58:24 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 23:58:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 366.17
[32m[20221213 23:58:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 496.34
[32m[20221213 23:58:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 495.92
[32m[20221213 23:58:24 @agent_ppo2.py:143][0m Total time:       0.90 min
[32m[20221213 23:58:24 @agent_ppo2.py:145][0m 81920 total steps have happened
[32m[20221213 23:58:24 @agent_ppo2.py:121][0m #------------------------ Iteration 4040 --------------------------#
[32m[20221213 23:58:25 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 23:58:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:58:25 @agent_ppo2.py:185][0m |           0.0037 |          64.9774 |          11.3632 |
[32m[20221213 23:58:25 @agent_ppo2.py:185][0m |          -0.0071 |          60.7570 |          11.3987 |
[32m[20221213 23:58:25 @agent_ppo2.py:185][0m |          -0.0090 |          59.3190 |          11.4200 |
[32m[20221213 23:58:25 @agent_ppo2.py:185][0m |          -0.0078 |          58.0941 |          11.4492 |
[32m[20221213 23:58:25 @agent_ppo2.py:185][0m |          -0.0142 |          57.3638 |          11.4386 |
[32m[20221213 23:58:25 @agent_ppo2.py:185][0m |          -0.0061 |          57.6555 |          11.4346 |
[32m[20221213 23:58:26 @agent_ppo2.py:185][0m |          -0.0125 |          56.5765 |          11.4237 |
[32m[20221213 23:58:26 @agent_ppo2.py:185][0m |          -0.0143 |          56.1329 |          11.4215 |
[32m[20221213 23:58:26 @agent_ppo2.py:185][0m |          -0.0110 |          56.6094 |          11.4510 |
[32m[20221213 23:58:26 @agent_ppo2.py:185][0m |          -0.0175 |          55.8503 |          11.4442 |
[32m[20221213 23:58:26 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 23:58:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 385.51
[32m[20221213 23:58:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 465.85
[32m[20221213 23:58:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 445.55
[32m[20221213 23:58:26 @agent_ppo2.py:143][0m Total time:       0.93 min
[32m[20221213 23:58:26 @agent_ppo2.py:145][0m 83968 total steps have happened
[32m[20221213 23:58:26 @agent_ppo2.py:121][0m #------------------------ Iteration 4041 --------------------------#
[32m[20221213 23:58:26 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 23:58:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:58:26 @agent_ppo2.py:185][0m |          -0.0005 |          67.3977 |          11.2288 |
[32m[20221213 23:58:26 @agent_ppo2.py:185][0m |          -0.0005 |          64.4127 |          11.2425 |
[32m[20221213 23:58:27 @agent_ppo2.py:185][0m |          -0.0085 |          63.4334 |          11.3102 |
[32m[20221213 23:58:27 @agent_ppo2.py:185][0m |          -0.0082 |          62.6107 |          11.3107 |
[32m[20221213 23:58:27 @agent_ppo2.py:185][0m |          -0.0149 |          62.1292 |          11.2705 |
[32m[20221213 23:58:27 @agent_ppo2.py:185][0m |          -0.0139 |          61.6791 |          11.2771 |
[32m[20221213 23:58:27 @agent_ppo2.py:185][0m |          -0.0162 |          61.2622 |          11.3116 |
[32m[20221213 23:58:27 @agent_ppo2.py:185][0m |          -0.0117 |          60.9340 |          11.2823 |
[32m[20221213 23:58:27 @agent_ppo2.py:185][0m |          -0.0114 |          61.1279 |          11.2945 |
[32m[20221213 23:58:27 @agent_ppo2.py:185][0m |          -0.0179 |          60.7117 |          11.2801 |
[32m[20221213 23:58:27 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 23:58:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 487.56
[32m[20221213 23:58:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 524.08
[32m[20221213 23:58:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 23:58:27 @agent_ppo2.py:143][0m Total time:       0.95 min
[32m[20221213 23:58:27 @agent_ppo2.py:145][0m 86016 total steps have happened
[32m[20221213 23:58:27 @agent_ppo2.py:121][0m #------------------------ Iteration 4042 --------------------------#
[32m[20221213 23:58:28 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 23:58:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:58:28 @agent_ppo2.py:185][0m |          -0.0005 |          56.4152 |          11.2371 |
[32m[20221213 23:58:28 @agent_ppo2.py:185][0m |          -0.0008 |          53.3947 |          11.2289 |
[32m[20221213 23:58:28 @agent_ppo2.py:185][0m |          -0.0022 |          53.5154 |          11.2464 |
[32m[20221213 23:58:28 @agent_ppo2.py:185][0m |           0.0017 |          57.5110 |          11.2399 |
[32m[20221213 23:58:28 @agent_ppo2.py:185][0m |          -0.0093 |          52.1010 |          11.1908 |
[32m[20221213 23:58:28 @agent_ppo2.py:185][0m |          -0.0093 |          51.6041 |          11.1962 |
[32m[20221213 23:58:29 @agent_ppo2.py:185][0m |          -0.0128 |          51.5508 |          11.1908 |
[32m[20221213 23:58:29 @agent_ppo2.py:185][0m |          -0.0089 |          51.1360 |          11.2237 |
[32m[20221213 23:58:29 @agent_ppo2.py:185][0m |          -0.0126 |          50.8544 |          11.2239 |
[32m[20221213 23:58:29 @agent_ppo2.py:185][0m |          -0.0133 |          50.7169 |          11.2183 |
[32m[20221213 23:58:29 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221213 23:58:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 438.02
[32m[20221213 23:58:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 483.73
[32m[20221213 23:58:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 463.05
[32m[20221213 23:58:29 @agent_ppo2.py:143][0m Total time:       0.98 min
[32m[20221213 23:58:29 @agent_ppo2.py:145][0m 88064 total steps have happened
[32m[20221213 23:58:29 @agent_ppo2.py:121][0m #------------------------ Iteration 4043 --------------------------#
[32m[20221213 23:58:29 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 23:58:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:58:29 @agent_ppo2.py:185][0m |          -0.0010 |          72.2033 |          10.7836 |
[32m[20221213 23:58:29 @agent_ppo2.py:185][0m |          -0.0054 |          69.3005 |          10.7933 |
[32m[20221213 23:58:30 @agent_ppo2.py:185][0m |          -0.0069 |          67.9696 |          10.7765 |
[32m[20221213 23:58:30 @agent_ppo2.py:185][0m |          -0.0060 |          67.5471 |          10.8231 |
[32m[20221213 23:58:30 @agent_ppo2.py:185][0m |          -0.0117 |          66.3679 |          10.8321 |
[32m[20221213 23:58:30 @agent_ppo2.py:185][0m |          -0.0148 |          65.9093 |          10.7722 |
[32m[20221213 23:58:30 @agent_ppo2.py:185][0m |          -0.0165 |          65.5292 |          10.8359 |
[32m[20221213 23:58:30 @agent_ppo2.py:185][0m |          -0.0118 |          65.0127 |          10.8376 |
[32m[20221213 23:58:30 @agent_ppo2.py:185][0m |          -0.0165 |          64.7775 |          10.8249 |
[32m[20221213 23:58:30 @agent_ppo2.py:185][0m |          -0.0144 |          64.4805 |          10.8333 |
[32m[20221213 23:58:30 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221213 23:58:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 450.36
[32m[20221213 23:58:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 501.89
[32m[20221213 23:58:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 430.44
[32m[20221213 23:58:31 @agent_ppo2.py:143][0m Total time:       1.00 min
[32m[20221213 23:58:31 @agent_ppo2.py:145][0m 90112 total steps have happened
[32m[20221213 23:58:31 @agent_ppo2.py:121][0m #------------------------ Iteration 4044 --------------------------#
[32m[20221213 23:58:31 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 23:58:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:58:31 @agent_ppo2.py:185][0m |          -0.0012 |          61.6894 |          11.5114 |
[32m[20221213 23:58:31 @agent_ppo2.py:185][0m |          -0.0073 |          56.1292 |          11.4981 |
[32m[20221213 23:58:31 @agent_ppo2.py:185][0m |          -0.0046 |          54.9755 |          11.4795 |
[32m[20221213 23:58:31 @agent_ppo2.py:185][0m |          -0.0108 |          53.0162 |          11.4839 |
[32m[20221213 23:58:31 @agent_ppo2.py:185][0m |          -0.0138 |          51.7756 |          11.5074 |
[32m[20221213 23:58:31 @agent_ppo2.py:185][0m |          -0.0151 |          50.8340 |          11.5224 |
[32m[20221213 23:58:32 @agent_ppo2.py:185][0m |          -0.0145 |          49.9563 |          11.5310 |
[32m[20221213 23:58:32 @agent_ppo2.py:185][0m |          -0.0128 |          49.3048 |          11.5079 |
[32m[20221213 23:58:32 @agent_ppo2.py:185][0m |          -0.0185 |          48.7578 |          11.5221 |
[32m[20221213 23:58:32 @agent_ppo2.py:185][0m |          -0.0033 |          49.6233 |          11.5207 |
[32m[20221213 23:58:32 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 23:58:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 361.81
[32m[20221213 23:58:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 441.28
[32m[20221213 23:58:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 505.70
[32m[20221213 23:58:32 @agent_ppo2.py:143][0m Total time:       1.03 min
[32m[20221213 23:58:32 @agent_ppo2.py:145][0m 92160 total steps have happened
[32m[20221213 23:58:32 @agent_ppo2.py:121][0m #------------------------ Iteration 4045 --------------------------#
[32m[20221213 23:58:32 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 23:58:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:58:32 @agent_ppo2.py:185][0m |           0.0009 |          83.0475 |          11.0105 |
[32m[20221213 23:58:32 @agent_ppo2.py:185][0m |          -0.0012 |          80.4322 |          11.0430 |
[32m[20221213 23:58:33 @agent_ppo2.py:185][0m |          -0.0070 |          78.5900 |          11.0925 |
[32m[20221213 23:58:33 @agent_ppo2.py:185][0m |          -0.0102 |          77.4159 |          11.1268 |
[32m[20221213 23:58:33 @agent_ppo2.py:185][0m |          -0.0097 |          76.5860 |          11.1967 |
[32m[20221213 23:58:33 @agent_ppo2.py:185][0m |          -0.0114 |          76.6713 |          11.1834 |
[32m[20221213 23:58:33 @agent_ppo2.py:185][0m |          -0.0139 |          75.7664 |          11.2528 |
[32m[20221213 23:58:33 @agent_ppo2.py:185][0m |          -0.0182 |          75.4612 |          11.2774 |
[32m[20221213 23:58:33 @agent_ppo2.py:185][0m |          -0.0126 |          74.8831 |          11.2740 |
[32m[20221213 23:58:33 @agent_ppo2.py:185][0m |          -0.0146 |          74.5058 |          11.3255 |
[32m[20221213 23:58:33 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 23:58:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 389.80
[32m[20221213 23:58:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 452.98
[32m[20221213 23:58:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 437.13
[32m[20221213 23:58:33 @agent_ppo2.py:143][0m Total time:       1.05 min
[32m[20221213 23:58:33 @agent_ppo2.py:145][0m 94208 total steps have happened
[32m[20221213 23:58:33 @agent_ppo2.py:121][0m #------------------------ Iteration 4046 --------------------------#
[32m[20221213 23:58:34 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 23:58:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:58:34 @agent_ppo2.py:185][0m |           0.0047 |          63.2167 |          11.4137 |
[32m[20221213 23:58:34 @agent_ppo2.py:185][0m |          -0.0054 |          58.9054 |          11.4267 |
[32m[20221213 23:58:34 @agent_ppo2.py:185][0m |          -0.0020 |          58.3370 |          11.4410 |
[32m[20221213 23:58:34 @agent_ppo2.py:185][0m |          -0.0088 |          56.5603 |          11.3897 |
[32m[20221213 23:58:34 @agent_ppo2.py:185][0m |          -0.0113 |          55.9136 |          11.3825 |
[32m[20221213 23:58:34 @agent_ppo2.py:185][0m |          -0.0083 |          57.4778 |          11.4303 |
[32m[20221213 23:58:34 @agent_ppo2.py:185][0m |          -0.0159 |          54.9425 |          11.3615 |
[32m[20221213 23:58:35 @agent_ppo2.py:185][0m |          -0.0196 |          54.7778 |          11.3809 |
[32m[20221213 23:58:35 @agent_ppo2.py:185][0m |          -0.0155 |          54.2820 |          11.3502 |
[32m[20221213 23:58:35 @agent_ppo2.py:185][0m |          -0.0065 |          58.8207 |          11.3204 |
[32m[20221213 23:58:35 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 23:58:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 400.04
[32m[20221213 23:58:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 466.15
[32m[20221213 23:58:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 449.48
[32m[20221213 23:58:35 @agent_ppo2.py:143][0m Total time:       1.08 min
[32m[20221213 23:58:35 @agent_ppo2.py:145][0m 96256 total steps have happened
[32m[20221213 23:58:35 @agent_ppo2.py:121][0m #------------------------ Iteration 4047 --------------------------#
[32m[20221213 23:58:35 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 23:58:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:58:35 @agent_ppo2.py:185][0m |          -0.0017 |          60.5845 |          11.5277 |
[32m[20221213 23:58:35 @agent_ppo2.py:185][0m |          -0.0074 |          57.4391 |          11.5203 |
[32m[20221213 23:58:36 @agent_ppo2.py:185][0m |          -0.0061 |          56.4005 |          11.5514 |
[32m[20221213 23:58:36 @agent_ppo2.py:185][0m |          -0.0081 |          55.8389 |          11.5567 |
[32m[20221213 23:58:36 @agent_ppo2.py:185][0m |          -0.0109 |          55.2092 |          11.5556 |
[32m[20221213 23:58:36 @agent_ppo2.py:185][0m |          -0.0117 |          54.8728 |          11.5591 |
[32m[20221213 23:58:36 @agent_ppo2.py:185][0m |          -0.0112 |          54.7473 |          11.5506 |
[32m[20221213 23:58:36 @agent_ppo2.py:185][0m |          -0.0115 |          54.5782 |          11.5562 |
[32m[20221213 23:58:36 @agent_ppo2.py:185][0m |          -0.0111 |          54.1026 |          11.5546 |
[32m[20221213 23:58:36 @agent_ppo2.py:185][0m |          -0.0135 |          54.2659 |          11.5702 |
[32m[20221213 23:58:36 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 23:58:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 476.42
[32m[20221213 23:58:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 518.09
[32m[20221213 23:58:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 444.24
[32m[20221213 23:58:36 @agent_ppo2.py:143][0m Total time:       1.10 min
[32m[20221213 23:58:36 @agent_ppo2.py:145][0m 98304 total steps have happened
[32m[20221213 23:58:36 @agent_ppo2.py:121][0m #------------------------ Iteration 4048 --------------------------#
[32m[20221213 23:58:37 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 23:58:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:58:37 @agent_ppo2.py:185][0m |           0.0010 |          61.1225 |          11.7953 |
[32m[20221213 23:58:37 @agent_ppo2.py:185][0m |          -0.0087 |          57.8278 |          11.7774 |
[32m[20221213 23:58:37 @agent_ppo2.py:185][0m |          -0.0103 |          56.8472 |          11.8031 |
[32m[20221213 23:58:37 @agent_ppo2.py:185][0m |          -0.0072 |          55.9994 |          11.8789 |
[32m[20221213 23:58:37 @agent_ppo2.py:185][0m |          -0.0115 |          55.1564 |          11.8530 |
[32m[20221213 23:58:37 @agent_ppo2.py:185][0m |          -0.0119 |          54.6760 |          11.9066 |
[32m[20221213 23:58:37 @agent_ppo2.py:185][0m |          -0.0124 |          54.1986 |          11.8818 |
[32m[20221213 23:58:38 @agent_ppo2.py:185][0m |          -0.0175 |          53.9608 |          11.8973 |
[32m[20221213 23:58:38 @agent_ppo2.py:185][0m |          -0.0130 |          53.9770 |          11.9232 |
[32m[20221213 23:58:38 @agent_ppo2.py:185][0m |          -0.0084 |          53.5772 |          11.9436 |
[32m[20221213 23:58:38 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 23:58:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 464.13
[32m[20221213 23:58:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 513.71
[32m[20221213 23:58:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 480.40
[32m[20221213 23:58:38 @agent_ppo2.py:143][0m Total time:       1.13 min
[32m[20221213 23:58:38 @agent_ppo2.py:145][0m 100352 total steps have happened
[32m[20221213 23:58:38 @agent_ppo2.py:121][0m #------------------------ Iteration 4049 --------------------------#
[32m[20221213 23:58:38 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 23:58:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:58:38 @agent_ppo2.py:185][0m |           0.0068 |          61.7400 |          11.7389 |
[32m[20221213 23:58:38 @agent_ppo2.py:185][0m |          -0.0056 |          58.0128 |          11.7315 |
[32m[20221213 23:58:39 @agent_ppo2.py:185][0m |          -0.0091 |          56.3944 |          11.6943 |
[32m[20221213 23:58:39 @agent_ppo2.py:185][0m |          -0.0104 |          55.4787 |          11.7139 |
[32m[20221213 23:58:39 @agent_ppo2.py:185][0m |          -0.0177 |          54.7327 |          11.6837 |
[32m[20221213 23:58:39 @agent_ppo2.py:185][0m |          -0.0029 |          55.5836 |          11.6975 |
[32m[20221213 23:58:39 @agent_ppo2.py:185][0m |          -0.0171 |          53.5467 |          11.6751 |
[32m[20221213 23:58:39 @agent_ppo2.py:185][0m |          -0.0175 |          53.0254 |          11.6794 |
[32m[20221213 23:58:39 @agent_ppo2.py:185][0m |          -0.0187 |          52.8740 |          11.6697 |
[32m[20221213 23:58:39 @agent_ppo2.py:185][0m |          -0.0176 |          52.4724 |          11.6865 |
[32m[20221213 23:58:39 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221213 23:58:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 409.49
[32m[20221213 23:58:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 439.97
[32m[20221213 23:58:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 487.05
[32m[20221213 23:58:39 @agent_ppo2.py:143][0m Total time:       1.15 min
[32m[20221213 23:58:39 @agent_ppo2.py:145][0m 102400 total steps have happened
[32m[20221213 23:58:39 @agent_ppo2.py:121][0m #------------------------ Iteration 4050 --------------------------#
[32m[20221213 23:58:40 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 23:58:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:58:40 @agent_ppo2.py:185][0m |           0.0110 |          68.0059 |          11.2788 |
[32m[20221213 23:58:40 @agent_ppo2.py:185][0m |          -0.0054 |          61.7913 |          11.2987 |
[32m[20221213 23:58:40 @agent_ppo2.py:185][0m |          -0.0111 |          60.0957 |          11.2926 |
[32m[20221213 23:58:40 @agent_ppo2.py:185][0m |          -0.0125 |          58.9944 |          11.3591 |
[32m[20221213 23:58:40 @agent_ppo2.py:185][0m |          -0.0128 |          58.0257 |          11.4314 |
[32m[20221213 23:58:40 @agent_ppo2.py:185][0m |          -0.0114 |          58.0766 |          11.4267 |
[32m[20221213 23:58:40 @agent_ppo2.py:185][0m |          -0.0164 |          57.4074 |          11.4814 |
[32m[20221213 23:58:41 @agent_ppo2.py:185][0m |          -0.0137 |          56.8347 |          11.5270 |
[32m[20221213 23:58:41 @agent_ppo2.py:185][0m |          -0.0196 |          56.4641 |          11.5304 |
[32m[20221213 23:58:41 @agent_ppo2.py:185][0m |          -0.0170 |          56.2693 |          11.5082 |
[32m[20221213 23:58:41 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 23:58:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 376.69
[32m[20221213 23:58:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 444.13
[32m[20221213 23:58:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 487.14
[32m[20221213 23:58:41 @agent_ppo2.py:143][0m Total time:       1.18 min
[32m[20221213 23:58:41 @agent_ppo2.py:145][0m 104448 total steps have happened
[32m[20221213 23:58:41 @agent_ppo2.py:121][0m #------------------------ Iteration 4051 --------------------------#
[32m[20221213 23:58:41 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 23:58:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:58:41 @agent_ppo2.py:185][0m |           0.0022 |          70.2770 |          11.5805 |
[32m[20221213 23:58:41 @agent_ppo2.py:185][0m |          -0.0053 |          67.7230 |          11.6755 |
[32m[20221213 23:58:42 @agent_ppo2.py:185][0m |          -0.0094 |          66.8263 |          11.6737 |
[32m[20221213 23:58:42 @agent_ppo2.py:185][0m |          -0.0092 |          66.6201 |          11.6916 |
[32m[20221213 23:58:42 @agent_ppo2.py:185][0m |           0.0002 |          69.2645 |          11.6190 |
[32m[20221213 23:58:42 @agent_ppo2.py:185][0m |          -0.0099 |          65.7001 |          11.6583 |
[32m[20221213 23:58:42 @agent_ppo2.py:185][0m |          -0.0111 |          65.1077 |          11.6973 |
[32m[20221213 23:58:42 @agent_ppo2.py:185][0m |          -0.0061 |          68.3144 |          11.6993 |
[32m[20221213 23:58:42 @agent_ppo2.py:185][0m |          -0.0132 |          64.7418 |          11.7271 |
[32m[20221213 23:58:42 @agent_ppo2.py:185][0m |          -0.0163 |          64.4811 |          11.7478 |
[32m[20221213 23:58:42 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 23:58:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 413.68
[32m[20221213 23:58:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 470.11
[32m[20221213 23:58:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 494.08
[32m[20221213 23:58:42 @agent_ppo2.py:143][0m Total time:       1.20 min
[32m[20221213 23:58:42 @agent_ppo2.py:145][0m 106496 total steps have happened
[32m[20221213 23:58:42 @agent_ppo2.py:121][0m #------------------------ Iteration 4052 --------------------------#
[32m[20221213 23:58:43 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 23:58:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:58:43 @agent_ppo2.py:185][0m |          -0.0025 |          69.3846 |          12.1245 |
[32m[20221213 23:58:43 @agent_ppo2.py:185][0m |          -0.0093 |          65.6729 |          12.1508 |
[32m[20221213 23:58:43 @agent_ppo2.py:185][0m |          -0.0134 |          64.4837 |          12.1804 |
[32m[20221213 23:58:43 @agent_ppo2.py:185][0m |          -0.0112 |          63.9597 |          12.1757 |
[32m[20221213 23:58:43 @agent_ppo2.py:185][0m |          -0.0140 |          63.1137 |          12.1900 |
[32m[20221213 23:58:43 @agent_ppo2.py:185][0m |          -0.0057 |          69.4096 |          12.1921 |
[32m[20221213 23:58:43 @agent_ppo2.py:185][0m |          -0.0160 |          62.4011 |          12.1849 |
[32m[20221213 23:58:44 @agent_ppo2.py:185][0m |           0.0088 |          75.1549 |          12.1846 |
[32m[20221213 23:58:44 @agent_ppo2.py:185][0m |          -0.0106 |          63.5610 |          12.2137 |
[32m[20221213 23:58:44 @agent_ppo2.py:185][0m |          -0.0170 |          61.3421 |          12.1967 |
[32m[20221213 23:58:44 @agent_ppo2.py:130][0m Policy update time: 1.17 s
[32m[20221213 23:58:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 430.62
[32m[20221213 23:58:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 451.90
[32m[20221213 23:58:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 476.04
[32m[20221213 23:58:44 @agent_ppo2.py:143][0m Total time:       1.23 min
[32m[20221213 23:58:44 @agent_ppo2.py:145][0m 108544 total steps have happened
[32m[20221213 23:58:44 @agent_ppo2.py:121][0m #------------------------ Iteration 4053 --------------------------#
[32m[20221213 23:58:44 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 23:58:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:58:44 @agent_ppo2.py:185][0m |           0.0059 |          68.0381 |          12.1426 |
[32m[20221213 23:58:44 @agent_ppo2.py:185][0m |          -0.0056 |          62.9393 |          12.2248 |
[32m[20221213 23:58:45 @agent_ppo2.py:185][0m |          -0.0059 |          61.3795 |          12.1774 |
[32m[20221213 23:58:45 @agent_ppo2.py:185][0m |          -0.0009 |          60.6466 |          12.1776 |
[32m[20221213 23:58:45 @agent_ppo2.py:185][0m |          -0.0067 |          59.0667 |          12.1822 |
[32m[20221213 23:58:45 @agent_ppo2.py:185][0m |          -0.0095 |          58.7069 |          12.1506 |
[32m[20221213 23:58:45 @agent_ppo2.py:185][0m |          -0.0098 |          57.9501 |          12.1786 |
[32m[20221213 23:58:45 @agent_ppo2.py:185][0m |          -0.0109 |          57.6392 |          12.1744 |
[32m[20221213 23:58:45 @agent_ppo2.py:185][0m |          -0.0169 |          57.2269 |          12.1830 |
[32m[20221213 23:58:45 @agent_ppo2.py:185][0m |          -0.0090 |          56.7608 |          12.1873 |
[32m[20221213 23:58:45 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 23:58:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 435.84
[32m[20221213 23:58:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 514.57
[32m[20221213 23:58:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 440.07
[32m[20221213 23:58:45 @agent_ppo2.py:143][0m Total time:       1.25 min
[32m[20221213 23:58:45 @agent_ppo2.py:145][0m 110592 total steps have happened
[32m[20221213 23:58:45 @agent_ppo2.py:121][0m #------------------------ Iteration 4054 --------------------------#
[32m[20221213 23:58:46 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 23:58:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:58:46 @agent_ppo2.py:185][0m |           0.0012 |          71.1517 |          11.8580 |
[32m[20221213 23:58:46 @agent_ppo2.py:185][0m |          -0.0032 |          66.7712 |          11.9133 |
[32m[20221213 23:58:46 @agent_ppo2.py:185][0m |          -0.0096 |          64.8102 |          11.9133 |
[32m[20221213 23:58:46 @agent_ppo2.py:185][0m |          -0.0071 |          63.5450 |          11.9472 |
[32m[20221213 23:58:46 @agent_ppo2.py:185][0m |          -0.0112 |          62.9809 |          11.9161 |
[32m[20221213 23:58:46 @agent_ppo2.py:185][0m |          -0.0125 |          62.2509 |          11.9538 |
[32m[20221213 23:58:46 @agent_ppo2.py:185][0m |          -0.0129 |          61.7087 |          11.9870 |
[32m[20221213 23:58:47 @agent_ppo2.py:185][0m |          -0.0120 |          61.2882 |          11.9922 |
[32m[20221213 23:58:47 @agent_ppo2.py:185][0m |          -0.0043 |          61.5169 |          11.9966 |
[32m[20221213 23:58:47 @agent_ppo2.py:185][0m |          -0.0136 |          60.6358 |          12.0430 |
[32m[20221213 23:58:47 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221213 23:58:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 378.42
[32m[20221213 23:58:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 456.20
[32m[20221213 23:58:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 496.02
[32m[20221213 23:58:47 @agent_ppo2.py:143][0m Total time:       1.28 min
[32m[20221213 23:58:47 @agent_ppo2.py:145][0m 112640 total steps have happened
[32m[20221213 23:58:47 @agent_ppo2.py:121][0m #------------------------ Iteration 4055 --------------------------#
[32m[20221213 23:58:47 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 23:58:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:58:47 @agent_ppo2.py:185][0m |           0.0129 |          79.9433 |          12.0323 |
[32m[20221213 23:58:47 @agent_ppo2.py:185][0m |          -0.0005 |          68.5714 |          12.0896 |
[32m[20221213 23:58:48 @agent_ppo2.py:185][0m |          -0.0024 |          67.7551 |          12.1161 |
[32m[20221213 23:58:48 @agent_ppo2.py:185][0m |          -0.0042 |          67.5015 |          12.1217 |
[32m[20221213 23:58:48 @agent_ppo2.py:185][0m |          -0.0060 |          67.1863 |          12.1672 |
[32m[20221213 23:58:48 @agent_ppo2.py:185][0m |          -0.0060 |          67.0902 |          12.1928 |
[32m[20221213 23:58:48 @agent_ppo2.py:185][0m |          -0.0104 |          66.9167 |          12.2245 |
[32m[20221213 23:58:48 @agent_ppo2.py:185][0m |          -0.0082 |          66.6531 |          12.2123 |
[32m[20221213 23:58:48 @agent_ppo2.py:185][0m |          -0.0104 |          66.7694 |          12.2260 |
[32m[20221213 23:58:48 @agent_ppo2.py:185][0m |          -0.0066 |          66.6771 |          12.2573 |
[32m[20221213 23:58:48 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 23:58:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 446.05
[32m[20221213 23:58:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 482.48
[32m[20221213 23:58:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 399.23
[32m[20221213 23:58:48 @agent_ppo2.py:143][0m Total time:       1.30 min
[32m[20221213 23:58:48 @agent_ppo2.py:145][0m 114688 total steps have happened
[32m[20221213 23:58:48 @agent_ppo2.py:121][0m #------------------------ Iteration 4056 --------------------------#
[32m[20221213 23:58:49 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 23:58:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:58:49 @agent_ppo2.py:185][0m |           0.0029 |          56.2640 |          12.1690 |
[32m[20221213 23:58:49 @agent_ppo2.py:185][0m |          -0.0032 |          51.0823 |          12.2133 |
[32m[20221213 23:58:49 @agent_ppo2.py:185][0m |          -0.0059 |          49.3878 |          12.2215 |
[32m[20221213 23:58:49 @agent_ppo2.py:185][0m |          -0.0089 |          48.2003 |          12.2326 |
[32m[20221213 23:58:49 @agent_ppo2.py:185][0m |          -0.0082 |          47.5414 |          12.2264 |
[32m[20221213 23:58:49 @agent_ppo2.py:185][0m |          -0.0069 |          46.8021 |          12.2512 |
[32m[20221213 23:58:49 @agent_ppo2.py:185][0m |          -0.0121 |          46.1102 |          12.2345 |
[32m[20221213 23:58:50 @agent_ppo2.py:185][0m |          -0.0084 |          45.7139 |          12.1933 |
[32m[20221213 23:58:50 @agent_ppo2.py:185][0m |          -0.0126 |          45.6188 |          12.2299 |
[32m[20221213 23:58:50 @agent_ppo2.py:185][0m |          -0.0138 |          45.0561 |          12.2346 |
[32m[20221213 23:58:50 @agent_ppo2.py:130][0m Policy update time: 1.17 s
[32m[20221213 23:58:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 477.68
[32m[20221213 23:58:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 512.47
[32m[20221213 23:58:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 391.42
[32m[20221213 23:58:50 @agent_ppo2.py:143][0m Total time:       1.33 min
[32m[20221213 23:58:50 @agent_ppo2.py:145][0m 116736 total steps have happened
[32m[20221213 23:58:50 @agent_ppo2.py:121][0m #------------------------ Iteration 4057 --------------------------#
[32m[20221213 23:58:50 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 23:58:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:58:50 @agent_ppo2.py:185][0m |           0.0011 |          50.9379 |          12.4597 |
[32m[20221213 23:58:50 @agent_ppo2.py:185][0m |          -0.0087 |          47.0125 |          12.4045 |
[32m[20221213 23:58:51 @agent_ppo2.py:185][0m |          -0.0061 |          45.5681 |          12.4049 |
[32m[20221213 23:58:51 @agent_ppo2.py:185][0m |          -0.0141 |          45.0322 |          12.3669 |
[32m[20221213 23:58:51 @agent_ppo2.py:185][0m |          -0.0111 |          44.3086 |          12.3827 |
[32m[20221213 23:58:51 @agent_ppo2.py:185][0m |          -0.0098 |          44.2186 |          12.4016 |
[32m[20221213 23:58:51 @agent_ppo2.py:185][0m |          -0.0123 |          43.7251 |          12.3706 |
[32m[20221213 23:58:51 @agent_ppo2.py:185][0m |          -0.0152 |          43.4677 |          12.3715 |
[32m[20221213 23:58:51 @agent_ppo2.py:185][0m |          -0.0148 |          43.1578 |          12.3488 |
[32m[20221213 23:58:51 @agent_ppo2.py:185][0m |          -0.0141 |          42.8277 |          12.3569 |
[32m[20221213 23:58:51 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 23:58:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 502.81
[32m[20221213 23:58:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 533.87
[32m[20221213 23:58:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 416.88
[32m[20221213 23:58:51 @agent_ppo2.py:143][0m Total time:       1.35 min
[32m[20221213 23:58:51 @agent_ppo2.py:145][0m 118784 total steps have happened
[32m[20221213 23:58:51 @agent_ppo2.py:121][0m #------------------------ Iteration 4058 --------------------------#
[32m[20221213 23:58:52 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 23:58:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:58:52 @agent_ppo2.py:185][0m |           0.0039 |          65.0645 |          12.3440 |
[32m[20221213 23:58:52 @agent_ppo2.py:185][0m |          -0.0057 |          60.0463 |          12.2958 |
[32m[20221213 23:58:52 @agent_ppo2.py:185][0m |          -0.0097 |          58.5552 |          12.2795 |
[32m[20221213 23:58:52 @agent_ppo2.py:185][0m |          -0.0096 |          56.8469 |          12.2888 |
[32m[20221213 23:58:52 @agent_ppo2.py:185][0m |          -0.0105 |          57.0320 |          12.2998 |
[32m[20221213 23:58:52 @agent_ppo2.py:185][0m |          -0.0123 |          55.2898 |          12.3047 |
[32m[20221213 23:58:52 @agent_ppo2.py:185][0m |          -0.0128 |          54.4688 |          12.2670 |
[32m[20221213 23:58:53 @agent_ppo2.py:185][0m |          -0.0142 |          54.3406 |          12.2982 |
[32m[20221213 23:58:53 @agent_ppo2.py:185][0m |          -0.0200 |          53.6396 |          12.3058 |
[32m[20221213 23:58:53 @agent_ppo2.py:185][0m |          -0.0212 |          53.2104 |          12.3006 |
[32m[20221213 23:58:53 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 23:58:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 424.27
[32m[20221213 23:58:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 481.75
[32m[20221213 23:58:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221213 23:58:53 @agent_ppo2.py:143][0m Total time:       1.38 min
[32m[20221213 23:58:53 @agent_ppo2.py:145][0m 120832 total steps have happened
[32m[20221213 23:58:53 @agent_ppo2.py:121][0m #------------------------ Iteration 4059 --------------------------#
[32m[20221213 23:58:53 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 23:58:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:58:53 @agent_ppo2.py:185][0m |          -0.0014 |          61.0700 |          12.4467 |
[32m[20221213 23:58:53 @agent_ppo2.py:185][0m |          -0.0038 |          59.3728 |          12.4610 |
[32m[20221213 23:58:54 @agent_ppo2.py:185][0m |          -0.0087 |          58.6228 |          12.4164 |
[32m[20221213 23:58:54 @agent_ppo2.py:185][0m |          -0.0084 |          58.1923 |          12.3801 |
[32m[20221213 23:58:54 @agent_ppo2.py:185][0m |          -0.0122 |          57.7941 |          12.3923 |
[32m[20221213 23:58:54 @agent_ppo2.py:185][0m |          -0.0134 |          57.6240 |          12.3999 |
[32m[20221213 23:58:54 @agent_ppo2.py:185][0m |          -0.0127 |          57.4744 |          12.3828 |
[32m[20221213 23:58:54 @agent_ppo2.py:185][0m |          -0.0121 |          57.3142 |          12.3483 |
[32m[20221213 23:58:54 @agent_ppo2.py:185][0m |          -0.0118 |          57.3115 |          12.3322 |
[32m[20221213 23:58:54 @agent_ppo2.py:185][0m |          -0.0157 |          57.0316 |          12.3133 |
[32m[20221213 23:58:54 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 23:58:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 482.09
[32m[20221213 23:58:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 527.91
[32m[20221213 23:58:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 508.74
[32m[20221213 23:58:54 @agent_ppo2.py:143][0m Total time:       1.40 min
[32m[20221213 23:58:54 @agent_ppo2.py:145][0m 122880 total steps have happened
[32m[20221213 23:58:54 @agent_ppo2.py:121][0m #------------------------ Iteration 4060 --------------------------#
[32m[20221213 23:58:55 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 23:58:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:58:55 @agent_ppo2.py:185][0m |          -0.0034 |          69.6782 |          12.1012 |
[32m[20221213 23:58:55 @agent_ppo2.py:185][0m |          -0.0085 |          65.0739 |          12.1171 |
[32m[20221213 23:58:55 @agent_ppo2.py:185][0m |          -0.0106 |          62.8634 |          12.1208 |
[32m[20221213 23:58:55 @agent_ppo2.py:185][0m |          -0.0135 |          61.7249 |          12.1389 |
[32m[20221213 23:58:55 @agent_ppo2.py:185][0m |          -0.0125 |          61.0785 |          12.1460 |
[32m[20221213 23:58:55 @agent_ppo2.py:185][0m |          -0.0160 |          60.5113 |          12.1338 |
[32m[20221213 23:58:55 @agent_ppo2.py:185][0m |          -0.0147 |          59.7352 |          12.1334 |
[32m[20221213 23:58:56 @agent_ppo2.py:185][0m |          -0.0157 |          59.3107 |          12.1223 |
[32m[20221213 23:58:56 @agent_ppo2.py:185][0m |          -0.0179 |          58.8033 |          12.1318 |
[32m[20221213 23:58:56 @agent_ppo2.py:185][0m |          -0.0184 |          58.9365 |          12.1555 |
[32m[20221213 23:58:56 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 23:58:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 413.52
[32m[20221213 23:58:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 450.36
[32m[20221213 23:58:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 421.63
[32m[20221213 23:58:56 @agent_ppo2.py:143][0m Total time:       1.43 min
[32m[20221213 23:58:56 @agent_ppo2.py:145][0m 124928 total steps have happened
[32m[20221213 23:58:56 @agent_ppo2.py:121][0m #------------------------ Iteration 4061 --------------------------#
[32m[20221213 23:58:56 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 23:58:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:58:56 @agent_ppo2.py:185][0m |          -0.0037 |          61.8993 |          12.1043 |
[32m[20221213 23:58:56 @agent_ppo2.py:185][0m |          -0.0086 |          57.2848 |          12.1208 |
[32m[20221213 23:58:57 @agent_ppo2.py:185][0m |          -0.0114 |          55.7624 |          12.1107 |
[32m[20221213 23:58:57 @agent_ppo2.py:185][0m |          -0.0108 |          54.7387 |          12.1188 |
[32m[20221213 23:58:57 @agent_ppo2.py:185][0m |           0.0102 |          59.8162 |          12.1172 |
[32m[20221213 23:58:57 @agent_ppo2.py:185][0m |          -0.0032 |          56.6451 |          12.1326 |
[32m[20221213 23:58:57 @agent_ppo2.py:185][0m |          -0.0148 |          52.5122 |          12.1226 |
[32m[20221213 23:58:57 @agent_ppo2.py:185][0m |          -0.0191 |          52.0988 |          12.1157 |
[32m[20221213 23:58:57 @agent_ppo2.py:185][0m |          -0.0145 |          53.4306 |          12.1300 |
[32m[20221213 23:58:57 @agent_ppo2.py:185][0m |          -0.0152 |          51.1873 |          12.1950 |
[32m[20221213 23:58:57 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 23:58:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 427.20
[32m[20221213 23:58:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 484.72
[32m[20221213 23:58:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 557.75
[32m[20221213 23:58:57 @agent_ppo2.py:143][0m Total time:       1.45 min
[32m[20221213 23:58:57 @agent_ppo2.py:145][0m 126976 total steps have happened
[32m[20221213 23:58:57 @agent_ppo2.py:121][0m #------------------------ Iteration 4062 --------------------------#
[32m[20221213 23:58:58 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 23:58:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:58:58 @agent_ppo2.py:185][0m |           0.0014 |          57.3196 |          12.0814 |
[32m[20221213 23:58:58 @agent_ppo2.py:185][0m |           0.0010 |          56.9604 |          12.1039 |
[32m[20221213 23:58:58 @agent_ppo2.py:185][0m |          -0.0130 |          50.4138 |          12.0823 |
[32m[20221213 23:58:58 @agent_ppo2.py:185][0m |          -0.0097 |          48.9934 |          12.1359 |
[32m[20221213 23:58:58 @agent_ppo2.py:185][0m |          -0.0048 |          52.4901 |          12.1134 |
[32m[20221213 23:58:58 @agent_ppo2.py:185][0m |          -0.0150 |          47.5045 |          12.0984 |
[32m[20221213 23:58:58 @agent_ppo2.py:185][0m |          -0.0159 |          46.7320 |          12.1177 |
[32m[20221213 23:58:59 @agent_ppo2.py:185][0m |          -0.0141 |          46.2313 |          12.0970 |
[32m[20221213 23:58:59 @agent_ppo2.py:185][0m |          -0.0193 |          45.9017 |          12.1232 |
[32m[20221213 23:58:59 @agent_ppo2.py:185][0m |          -0.0163 |          45.3453 |          12.1291 |
[32m[20221213 23:58:59 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 23:58:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 408.78
[32m[20221213 23:58:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 433.85
[32m[20221213 23:58:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 497.82
[32m[20221213 23:58:59 @agent_ppo2.py:143][0m Total time:       1.48 min
[32m[20221213 23:58:59 @agent_ppo2.py:145][0m 129024 total steps have happened
[32m[20221213 23:58:59 @agent_ppo2.py:121][0m #------------------------ Iteration 4063 --------------------------#
[32m[20221213 23:58:59 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 23:58:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:58:59 @agent_ppo2.py:185][0m |          -0.0003 |          66.7702 |          12.2443 |
[32m[20221213 23:58:59 @agent_ppo2.py:185][0m |          -0.0093 |          61.1041 |          12.2713 |
[32m[20221213 23:58:59 @agent_ppo2.py:185][0m |          -0.0119 |          59.2006 |          12.2792 |
[32m[20221213 23:59:00 @agent_ppo2.py:185][0m |          -0.0102 |          58.1800 |          12.2781 |
[32m[20221213 23:59:00 @agent_ppo2.py:185][0m |          -0.0123 |          57.2981 |          12.3525 |
[32m[20221213 23:59:00 @agent_ppo2.py:185][0m |          -0.0114 |          56.7432 |          12.3528 |
[32m[20221213 23:59:00 @agent_ppo2.py:185][0m |          -0.0201 |          56.3227 |          12.3263 |
[32m[20221213 23:59:00 @agent_ppo2.py:185][0m |          -0.0139 |          55.9238 |          12.3499 |
[32m[20221213 23:59:00 @agent_ppo2.py:185][0m |          -0.0201 |          55.4612 |          12.3942 |
[32m[20221213 23:59:00 @agent_ppo2.py:185][0m |          -0.0195 |          55.2205 |          12.3556 |
[32m[20221213 23:59:00 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221213 23:59:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 448.10
[32m[20221213 23:59:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 524.04
[32m[20221213 23:59:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 499.94
[32m[20221213 23:59:00 @agent_ppo2.py:143][0m Total time:       1.50 min
[32m[20221213 23:59:00 @agent_ppo2.py:145][0m 131072 total steps have happened
[32m[20221213 23:59:00 @agent_ppo2.py:121][0m #------------------------ Iteration 4064 --------------------------#
[32m[20221213 23:59:01 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 23:59:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:59:01 @agent_ppo2.py:185][0m |           0.0014 |          68.8242 |          12.4652 |
[32m[20221213 23:59:01 @agent_ppo2.py:185][0m |          -0.0017 |          65.4678 |          12.4011 |
[32m[20221213 23:59:01 @agent_ppo2.py:185][0m |          -0.0062 |          65.6955 |          12.3929 |
[32m[20221213 23:59:01 @agent_ppo2.py:185][0m |          -0.0116 |          63.5380 |          12.4042 |
[32m[20221213 23:59:01 @agent_ppo2.py:185][0m |          -0.0135 |          63.2500 |          12.4238 |
[32m[20221213 23:59:01 @agent_ppo2.py:185][0m |          -0.0096 |          62.6228 |          12.4377 |
[32m[20221213 23:59:01 @agent_ppo2.py:185][0m |          -0.0097 |          62.3092 |          12.4223 |
[32m[20221213 23:59:02 @agent_ppo2.py:185][0m |          -0.0132 |          62.0928 |          12.4243 |
[32m[20221213 23:59:02 @agent_ppo2.py:185][0m |          -0.0190 |          61.8993 |          12.3964 |
[32m[20221213 23:59:02 @agent_ppo2.py:185][0m |          -0.0132 |          61.5155 |          12.3800 |
[32m[20221213 23:59:02 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 23:59:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 338.86
[32m[20221213 23:59:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 451.56
[32m[20221213 23:59:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 498.24
[32m[20221213 23:59:02 @agent_ppo2.py:143][0m Total time:       1.53 min
[32m[20221213 23:59:02 @agent_ppo2.py:145][0m 133120 total steps have happened
[32m[20221213 23:59:02 @agent_ppo2.py:121][0m #------------------------ Iteration 4065 --------------------------#
[32m[20221213 23:59:02 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 23:59:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:59:02 @agent_ppo2.py:185][0m |          -0.0014 |          50.2112 |          12.0507 |
[32m[20221213 23:59:02 @agent_ppo2.py:185][0m |           0.0006 |          46.6103 |          12.0408 |
[32m[20221213 23:59:02 @agent_ppo2.py:185][0m |          -0.0099 |          43.5844 |          12.0359 |
[32m[20221213 23:59:03 @agent_ppo2.py:185][0m |          -0.0147 |          42.6641 |          12.0322 |
[32m[20221213 23:59:03 @agent_ppo2.py:185][0m |          -0.0077 |          42.1918 |          12.0367 |
[32m[20221213 23:59:03 @agent_ppo2.py:185][0m |          -0.0142 |          41.3032 |          12.0766 |
[32m[20221213 23:59:03 @agent_ppo2.py:185][0m |          -0.0147 |          40.1136 |          12.0498 |
[32m[20221213 23:59:03 @agent_ppo2.py:185][0m |          -0.0007 |          47.2738 |          12.0472 |
[32m[20221213 23:59:03 @agent_ppo2.py:185][0m |          -0.0144 |          39.5821 |          12.0410 |
[32m[20221213 23:59:03 @agent_ppo2.py:185][0m |          -0.0179 |          38.8230 |          12.0435 |
[32m[20221213 23:59:03 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 23:59:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 393.95
[32m[20221213 23:59:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 446.09
[32m[20221213 23:59:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 565.30
[32m[20221213 23:59:03 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 565.30
[32m[20221213 23:59:03 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 565.30
[32m[20221213 23:59:03 @agent_ppo2.py:143][0m Total time:       1.55 min
[32m[20221213 23:59:03 @agent_ppo2.py:145][0m 135168 total steps have happened
[32m[20221213 23:59:03 @agent_ppo2.py:121][0m #------------------------ Iteration 4066 --------------------------#
[32m[20221213 23:59:04 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 23:59:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:59:04 @agent_ppo2.py:185][0m |          -0.0010 |          72.2411 |          12.3656 |
[32m[20221213 23:59:04 @agent_ppo2.py:185][0m |          -0.0094 |          69.3313 |          12.3746 |
[32m[20221213 23:59:04 @agent_ppo2.py:185][0m |          -0.0017 |          73.7828 |          12.4005 |
[32m[20221213 23:59:04 @agent_ppo2.py:185][0m |          -0.0118 |          67.9300 |          12.3966 |
[32m[20221213 23:59:04 @agent_ppo2.py:185][0m |          -0.0139 |          67.5019 |          12.3854 |
[32m[20221213 23:59:04 @agent_ppo2.py:185][0m |          -0.0018 |          74.7323 |          12.4142 |
[32m[20221213 23:59:04 @agent_ppo2.py:185][0m |          -0.0171 |          67.1976 |          12.4120 |
[32m[20221213 23:59:04 @agent_ppo2.py:185][0m |          -0.0160 |          66.6910 |          12.4320 |
[32m[20221213 23:59:05 @agent_ppo2.py:185][0m |          -0.0170 |          66.5228 |          12.4168 |
[32m[20221213 23:59:05 @agent_ppo2.py:185][0m |          -0.0145 |          66.5906 |          12.4101 |
[32m[20221213 23:59:05 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 23:59:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 423.74
[32m[20221213 23:59:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 455.30
[32m[20221213 23:59:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 451.59
[32m[20221213 23:59:05 @agent_ppo2.py:143][0m Total time:       1.58 min
[32m[20221213 23:59:05 @agent_ppo2.py:145][0m 137216 total steps have happened
[32m[20221213 23:59:05 @agent_ppo2.py:121][0m #------------------------ Iteration 4067 --------------------------#
[32m[20221213 23:59:05 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 23:59:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:59:05 @agent_ppo2.py:185][0m |           0.0020 |          37.5745 |          12.2436 |
[32m[20221213 23:59:05 @agent_ppo2.py:185][0m |          -0.0095 |          33.4904 |          12.2346 |
[32m[20221213 23:59:05 @agent_ppo2.py:185][0m |          -0.0147 |          32.0403 |          12.2801 |
[32m[20221213 23:59:06 @agent_ppo2.py:185][0m |          -0.0129 |          30.9298 |          12.2851 |
[32m[20221213 23:59:06 @agent_ppo2.py:185][0m |          -0.0158 |          30.1539 |          12.3049 |
[32m[20221213 23:59:06 @agent_ppo2.py:185][0m |          -0.0132 |          29.4911 |          12.3096 |
[32m[20221213 23:59:06 @agent_ppo2.py:185][0m |          -0.0137 |          29.1651 |          12.2835 |
[32m[20221213 23:59:06 @agent_ppo2.py:185][0m |          -0.0228 |          28.3070 |          12.2956 |
[32m[20221213 23:59:06 @agent_ppo2.py:185][0m |          -0.0171 |          28.7674 |          12.2836 |
[32m[20221213 23:59:06 @agent_ppo2.py:185][0m |          -0.0222 |          27.6648 |          12.3024 |
[32m[20221213 23:59:06 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221213 23:59:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 350.33
[32m[20221213 23:59:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 431.83
[32m[20221213 23:59:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 497.12
[32m[20221213 23:59:06 @agent_ppo2.py:143][0m Total time:       1.60 min
[32m[20221213 23:59:06 @agent_ppo2.py:145][0m 139264 total steps have happened
[32m[20221213 23:59:06 @agent_ppo2.py:121][0m #------------------------ Iteration 4068 --------------------------#
[32m[20221213 23:59:07 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 23:59:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:59:07 @agent_ppo2.py:185][0m |          -0.0041 |          70.1696 |          12.6341 |
[32m[20221213 23:59:07 @agent_ppo2.py:185][0m |          -0.0064 |          67.3220 |          12.6248 |
[32m[20221213 23:59:07 @agent_ppo2.py:185][0m |          -0.0088 |          65.9987 |          12.6067 |
[32m[20221213 23:59:07 @agent_ppo2.py:185][0m |          -0.0101 |          64.8544 |          12.6101 |
[32m[20221213 23:59:07 @agent_ppo2.py:185][0m |          -0.0101 |          64.6780 |          12.5794 |
[32m[20221213 23:59:07 @agent_ppo2.py:185][0m |          -0.0129 |          63.2244 |          12.5679 |
[32m[20221213 23:59:07 @agent_ppo2.py:185][0m |          -0.0114 |          62.8263 |          12.5394 |
[32m[20221213 23:59:08 @agent_ppo2.py:185][0m |          -0.0149 |          62.3992 |          12.5652 |
[32m[20221213 23:59:08 @agent_ppo2.py:185][0m |          -0.0076 |          66.7525 |          12.5352 |
[32m[20221213 23:59:08 @agent_ppo2.py:185][0m |          -0.0174 |          61.6083 |          12.5508 |
[32m[20221213 23:59:08 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 23:59:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 430.89
[32m[20221213 23:59:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 481.00
[32m[20221213 23:59:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 517.39
[32m[20221213 23:59:08 @agent_ppo2.py:143][0m Total time:       1.63 min
[32m[20221213 23:59:08 @agent_ppo2.py:145][0m 141312 total steps have happened
[32m[20221213 23:59:08 @agent_ppo2.py:121][0m #------------------------ Iteration 4069 --------------------------#
[32m[20221213 23:59:08 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 23:59:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:59:08 @agent_ppo2.py:185][0m |           0.0109 |          51.6297 |          12.1293 |
[32m[20221213 23:59:08 @agent_ppo2.py:185][0m |          -0.0033 |          48.0887 |          12.0728 |
[32m[20221213 23:59:08 @agent_ppo2.py:185][0m |           0.0005 |          49.1051 |          12.1204 |
[32m[20221213 23:59:09 @agent_ppo2.py:185][0m |          -0.0088 |          47.0202 |          12.1703 |
[32m[20221213 23:59:09 @agent_ppo2.py:185][0m |           0.0061 |          51.5331 |          12.1445 |
[32m[20221213 23:59:09 @agent_ppo2.py:185][0m |           0.0000 |          49.0837 |          12.1584 |
[32m[20221213 23:59:09 @agent_ppo2.py:185][0m |          -0.0110 |          46.2572 |          12.1408 |
[32m[20221213 23:59:09 @agent_ppo2.py:185][0m |          -0.0088 |          46.1702 |          12.1396 |
[32m[20221213 23:59:09 @agent_ppo2.py:185][0m |          -0.0127 |          45.7127 |          12.1362 |
[32m[20221213 23:59:09 @agent_ppo2.py:185][0m |          -0.0157 |          45.5001 |          12.1174 |
[32m[20221213 23:59:09 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 23:59:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 473.55
[32m[20221213 23:59:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 522.69
[32m[20221213 23:59:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 392.93
[32m[20221213 23:59:09 @agent_ppo2.py:143][0m Total time:       1.65 min
[32m[20221213 23:59:09 @agent_ppo2.py:145][0m 143360 total steps have happened
[32m[20221213 23:59:09 @agent_ppo2.py:121][0m #------------------------ Iteration 4070 --------------------------#
[32m[20221213 23:59:10 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221213 23:59:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:59:10 @agent_ppo2.py:185][0m |          -0.0016 |          83.4978 |          12.6338 |
[32m[20221213 23:59:10 @agent_ppo2.py:185][0m |          -0.0134 |          79.2223 |          12.6259 |
[32m[20221213 23:59:10 @agent_ppo2.py:185][0m |          -0.0091 |          77.3561 |          12.6208 |
[32m[20221213 23:59:10 @agent_ppo2.py:185][0m |          -0.0118 |          76.3064 |          12.6423 |
[32m[20221213 23:59:10 @agent_ppo2.py:185][0m |          -0.0134 |          75.2519 |          12.6405 |
[32m[20221213 23:59:10 @agent_ppo2.py:185][0m |          -0.0123 |          74.7370 |          12.6546 |
[32m[20221213 23:59:10 @agent_ppo2.py:185][0m |          -0.0170 |          74.0767 |          12.6650 |
[32m[20221213 23:59:10 @agent_ppo2.py:185][0m |          -0.0196 |          73.6875 |          12.6424 |
[32m[20221213 23:59:11 @agent_ppo2.py:185][0m |          -0.0133 |          74.1757 |          12.6478 |
[32m[20221213 23:59:11 @agent_ppo2.py:185][0m |          -0.0179 |          72.8122 |          12.6699 |
[32m[20221213 23:59:11 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 23:59:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 408.98
[32m[20221213 23:59:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 435.92
[32m[20221213 23:59:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 536.27
[32m[20221213 23:59:11 @agent_ppo2.py:143][0m Total time:       1.67 min
[32m[20221213 23:59:11 @agent_ppo2.py:145][0m 145408 total steps have happened
[32m[20221213 23:59:11 @agent_ppo2.py:121][0m #------------------------ Iteration 4071 --------------------------#
[32m[20221213 23:59:11 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 23:59:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:59:11 @agent_ppo2.py:185][0m |           0.0035 |          63.8706 |          12.1694 |
[32m[20221213 23:59:11 @agent_ppo2.py:185][0m |          -0.0041 |          56.5632 |          12.1038 |
[32m[20221213 23:59:11 @agent_ppo2.py:185][0m |          -0.0010 |          53.6859 |          12.1508 |
[32m[20221213 23:59:12 @agent_ppo2.py:185][0m |          -0.0062 |          51.8926 |          12.1565 |
[32m[20221213 23:59:12 @agent_ppo2.py:185][0m |          -0.0103 |          50.5664 |          12.1368 |
[32m[20221213 23:59:12 @agent_ppo2.py:185][0m |          -0.0058 |          50.2541 |          12.1528 |
[32m[20221213 23:59:12 @agent_ppo2.py:185][0m |          -0.0062 |          51.7458 |          12.1431 |
[32m[20221213 23:59:12 @agent_ppo2.py:185][0m |          -0.0109 |          48.2090 |          12.1462 |
[32m[20221213 23:59:12 @agent_ppo2.py:185][0m |          -0.0109 |          47.9469 |          12.1534 |
[32m[20221213 23:59:12 @agent_ppo2.py:185][0m |          -0.0174 |          46.8460 |          12.1802 |
[32m[20221213 23:59:12 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221213 23:59:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 387.00
[32m[20221213 23:59:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 461.89
[32m[20221213 23:59:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 450.72
[32m[20221213 23:59:12 @agent_ppo2.py:143][0m Total time:       1.70 min
[32m[20221213 23:59:12 @agent_ppo2.py:145][0m 147456 total steps have happened
[32m[20221213 23:59:12 @agent_ppo2.py:121][0m #------------------------ Iteration 4072 --------------------------#
[32m[20221213 23:59:13 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 23:59:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:59:13 @agent_ppo2.py:185][0m |           0.0044 |          58.9822 |          12.5767 |
[32m[20221213 23:59:13 @agent_ppo2.py:185][0m |          -0.0026 |          55.6939 |          12.5306 |
[32m[20221213 23:59:13 @agent_ppo2.py:185][0m |           0.0041 |          58.9401 |          12.5323 |
[32m[20221213 23:59:13 @agent_ppo2.py:185][0m |          -0.0081 |          54.3440 |          12.5367 |
[32m[20221213 23:59:13 @agent_ppo2.py:185][0m |          -0.0093 |          53.9783 |          12.5463 |
[32m[20221213 23:59:13 @agent_ppo2.py:185][0m |          -0.0101 |          53.7932 |          12.5718 |
[32m[20221213 23:59:13 @agent_ppo2.py:185][0m |          -0.0113 |          53.3737 |          12.5934 |
[32m[20221213 23:59:13 @agent_ppo2.py:185][0m |          -0.0128 |          53.3225 |          12.5863 |
[32m[20221213 23:59:14 @agent_ppo2.py:185][0m |          -0.0107 |          53.3120 |          12.5688 |
[32m[20221213 23:59:14 @agent_ppo2.py:185][0m |          -0.0106 |          53.0050 |          12.5731 |
[32m[20221213 23:59:14 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221213 23:59:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 482.57
[32m[20221213 23:59:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 506.18
[32m[20221213 23:59:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 284.30
[32m[20221213 23:59:14 @agent_ppo2.py:143][0m Total time:       1.72 min
[32m[20221213 23:59:14 @agent_ppo2.py:145][0m 149504 total steps have happened
[32m[20221213 23:59:14 @agent_ppo2.py:121][0m #------------------------ Iteration 4073 --------------------------#
[32m[20221213 23:59:14 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 23:59:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:59:14 @agent_ppo2.py:185][0m |          -0.0010 |          67.2310 |          12.2724 |
[32m[20221213 23:59:14 @agent_ppo2.py:185][0m |          -0.0070 |          63.0092 |          12.1981 |
[32m[20221213 23:59:14 @agent_ppo2.py:185][0m |          -0.0054 |          61.5019 |          12.2018 |
[32m[20221213 23:59:15 @agent_ppo2.py:185][0m |          -0.0132 |          60.1474 |          12.1441 |
[32m[20221213 23:59:15 @agent_ppo2.py:185][0m |          -0.0135 |          59.8737 |          12.1725 |
[32m[20221213 23:59:15 @agent_ppo2.py:185][0m |          -0.0161 |          58.9731 |          12.1261 |
[32m[20221213 23:59:15 @agent_ppo2.py:185][0m |          -0.0049 |          61.1184 |          12.1484 |
[32m[20221213 23:59:15 @agent_ppo2.py:185][0m |          -0.0132 |          58.6002 |          12.1515 |
[32m[20221213 23:59:15 @agent_ppo2.py:185][0m |          -0.0176 |          58.0864 |          12.1034 |
[32m[20221213 23:59:15 @agent_ppo2.py:185][0m |          -0.0188 |          57.5260 |          12.0925 |
[32m[20221213 23:59:15 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221213 23:59:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 386.65
[32m[20221213 23:59:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 450.16
[32m[20221213 23:59:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 497.32
[32m[20221213 23:59:15 @agent_ppo2.py:143][0m Total time:       1.75 min
[32m[20221213 23:59:15 @agent_ppo2.py:145][0m 151552 total steps have happened
[32m[20221213 23:59:15 @agent_ppo2.py:121][0m #------------------------ Iteration 4074 --------------------------#
[32m[20221213 23:59:15 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:59:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:59:16 @agent_ppo2.py:185][0m |          -0.0016 |          75.2029 |          11.9768 |
[32m[20221213 23:59:16 @agent_ppo2.py:185][0m |          -0.0001 |          71.2431 |          12.0210 |
[32m[20221213 23:59:16 @agent_ppo2.py:185][0m |          -0.0059 |          69.8155 |          11.9573 |
[32m[20221213 23:59:16 @agent_ppo2.py:185][0m |           0.0037 |          74.7827 |          11.9699 |
[32m[20221213 23:59:16 @agent_ppo2.py:185][0m |          -0.0074 |          69.0211 |          11.9711 |
[32m[20221213 23:59:16 @agent_ppo2.py:185][0m |          -0.0112 |          68.6094 |          11.9817 |
[32m[20221213 23:59:16 @agent_ppo2.py:185][0m |          -0.0116 |          68.1856 |          11.9941 |
[32m[20221213 23:59:16 @agent_ppo2.py:185][0m |          -0.0140 |          68.0128 |          12.0116 |
[32m[20221213 23:59:16 @agent_ppo2.py:185][0m |          -0.0148 |          67.7928 |          11.9666 |
[32m[20221213 23:59:17 @agent_ppo2.py:185][0m |          -0.0119 |          67.6637 |          11.9891 |
[32m[20221213 23:59:17 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221213 23:59:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 501.88
[32m[20221213 23:59:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 520.30
[32m[20221213 23:59:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 275.20
[32m[20221213 23:59:17 @agent_ppo2.py:143][0m Total time:       1.77 min
[32m[20221213 23:59:17 @agent_ppo2.py:145][0m 153600 total steps have happened
[32m[20221213 23:59:17 @agent_ppo2.py:121][0m #------------------------ Iteration 4075 --------------------------#
[32m[20221213 23:59:17 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 23:59:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:59:17 @agent_ppo2.py:185][0m |           0.0044 |          50.3082 |          11.9449 |
[32m[20221213 23:59:17 @agent_ppo2.py:185][0m |          -0.0050 |          45.2956 |          11.9157 |
[32m[20221213 23:59:17 @agent_ppo2.py:185][0m |          -0.0012 |          44.5722 |          11.9362 |
[32m[20221213 23:59:17 @agent_ppo2.py:185][0m |          -0.0138 |          43.0977 |          11.9151 |
[32m[20221213 23:59:17 @agent_ppo2.py:185][0m |          -0.0001 |          44.1706 |          11.9031 |
[32m[20221213 23:59:18 @agent_ppo2.py:185][0m |          -0.0103 |          41.8843 |          11.8466 |
[32m[20221213 23:59:18 @agent_ppo2.py:185][0m |          -0.0136 |          41.4349 |          11.8736 |
[32m[20221213 23:59:18 @agent_ppo2.py:185][0m |          -0.0084 |          41.2150 |          11.8234 |
[32m[20221213 23:59:18 @agent_ppo2.py:185][0m |          -0.0105 |          40.6464 |          11.8253 |
[32m[20221213 23:59:18 @agent_ppo2.py:185][0m |          -0.0216 |          40.4092 |          11.8310 |
[32m[20221213 23:59:18 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:59:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 379.84
[32m[20221213 23:59:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 436.48
[32m[20221213 23:59:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 455.90
[32m[20221213 23:59:18 @agent_ppo2.py:143][0m Total time:       1.79 min
[32m[20221213 23:59:18 @agent_ppo2.py:145][0m 155648 total steps have happened
[32m[20221213 23:59:18 @agent_ppo2.py:121][0m #------------------------ Iteration 4076 --------------------------#
[32m[20221213 23:59:18 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:59:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:59:18 @agent_ppo2.py:185][0m |           0.0022 |          68.6411 |          12.3105 |
[32m[20221213 23:59:18 @agent_ppo2.py:185][0m |          -0.0007 |          66.9755 |          12.3011 |
[32m[20221213 23:59:19 @agent_ppo2.py:185][0m |          -0.0051 |          66.7759 |          12.2869 |
[32m[20221213 23:59:19 @agent_ppo2.py:185][0m |          -0.0060 |          66.2648 |          12.2796 |
[32m[20221213 23:59:19 @agent_ppo2.py:185][0m |          -0.0034 |          67.0730 |          12.2840 |
[32m[20221213 23:59:19 @agent_ppo2.py:185][0m |           0.0099 |          72.0940 |          12.2734 |
[32m[20221213 23:59:19 @agent_ppo2.py:185][0m |          -0.0081 |          65.9976 |          12.2837 |
[32m[20221213 23:59:19 @agent_ppo2.py:185][0m |          -0.0042 |          67.1761 |          12.2728 |
[32m[20221213 23:59:19 @agent_ppo2.py:185][0m |          -0.0126 |          65.2716 |          12.2959 |
[32m[20221213 23:59:19 @agent_ppo2.py:185][0m |          -0.0091 |          65.1916 |          12.3087 |
[32m[20221213 23:59:19 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:59:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 462.29
[32m[20221213 23:59:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 496.63
[32m[20221213 23:59:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 515.59
[32m[20221213 23:59:19 @agent_ppo2.py:143][0m Total time:       1.82 min
[32m[20221213 23:59:19 @agent_ppo2.py:145][0m 157696 total steps have happened
[32m[20221213 23:59:19 @agent_ppo2.py:121][0m #------------------------ Iteration 4077 --------------------------#
[32m[20221213 23:59:19 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:59:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:59:20 @agent_ppo2.py:185][0m |           0.0020 |          58.7408 |          12.0662 |
[32m[20221213 23:59:20 @agent_ppo2.py:185][0m |          -0.0055 |          54.9907 |          11.9843 |
[32m[20221213 23:59:20 @agent_ppo2.py:185][0m |          -0.0057 |          54.2508 |          11.9678 |
[32m[20221213 23:59:20 @agent_ppo2.py:185][0m |          -0.0024 |          56.3263 |          11.9621 |
[32m[20221213 23:59:20 @agent_ppo2.py:185][0m |          -0.0116 |          52.7879 |          11.9575 |
[32m[20221213 23:59:20 @agent_ppo2.py:185][0m |          -0.0131 |          52.2350 |          11.9146 |
[32m[20221213 23:59:20 @agent_ppo2.py:185][0m |          -0.0105 |          51.9795 |          11.9300 |
[32m[20221213 23:59:20 @agent_ppo2.py:185][0m |          -0.0121 |          52.2025 |          11.9335 |
[32m[20221213 23:59:20 @agent_ppo2.py:185][0m |          -0.0125 |          51.4522 |          11.9103 |
[32m[20221213 23:59:20 @agent_ppo2.py:185][0m |          -0.0180 |          51.2469 |          11.8685 |
[32m[20221213 23:59:20 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:59:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 411.74
[32m[20221213 23:59:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 461.83
[32m[20221213 23:59:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 468.69
[32m[20221213 23:59:21 @agent_ppo2.py:143][0m Total time:       1.84 min
[32m[20221213 23:59:21 @agent_ppo2.py:145][0m 159744 total steps have happened
[32m[20221213 23:59:21 @agent_ppo2.py:121][0m #------------------------ Iteration 4078 --------------------------#
[32m[20221213 23:59:21 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:59:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:59:21 @agent_ppo2.py:185][0m |          -0.0031 |          56.7457 |          12.2525 |
[32m[20221213 23:59:21 @agent_ppo2.py:185][0m |          -0.0074 |          53.2257 |          12.2965 |
[32m[20221213 23:59:21 @agent_ppo2.py:185][0m |          -0.0083 |          51.8574 |          12.2957 |
[32m[20221213 23:59:21 @agent_ppo2.py:185][0m |          -0.0102 |          50.9690 |          12.3134 |
[32m[20221213 23:59:21 @agent_ppo2.py:185][0m |          -0.0115 |          50.4082 |          12.3601 |
[32m[20221213 23:59:21 @agent_ppo2.py:185][0m |          -0.0127 |          49.9840 |          12.3865 |
[32m[20221213 23:59:21 @agent_ppo2.py:185][0m |          -0.0157 |          49.6578 |          12.3877 |
[32m[20221213 23:59:21 @agent_ppo2.py:185][0m |          -0.0114 |          49.1808 |          12.4110 |
[32m[20221213 23:59:22 @agent_ppo2.py:185][0m |          -0.0188 |          48.8660 |          12.3895 |
[32m[20221213 23:59:22 @agent_ppo2.py:185][0m |          -0.0195 |          48.5091 |          12.4648 |
[32m[20221213 23:59:22 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:59:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 383.98
[32m[20221213 23:59:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 453.54
[32m[20221213 23:59:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 501.51
[32m[20221213 23:59:22 @agent_ppo2.py:143][0m Total time:       1.86 min
[32m[20221213 23:59:22 @agent_ppo2.py:145][0m 161792 total steps have happened
[32m[20221213 23:59:22 @agent_ppo2.py:121][0m #------------------------ Iteration 4079 --------------------------#
[32m[20221213 23:59:22 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:59:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:59:22 @agent_ppo2.py:185][0m |          -0.0001 |          68.7918 |          11.9452 |
[32m[20221213 23:59:22 @agent_ppo2.py:185][0m |          -0.0074 |          63.8874 |          11.9457 |
[32m[20221213 23:59:22 @agent_ppo2.py:185][0m |          -0.0103 |          61.3819 |          12.0031 |
[32m[20221213 23:59:22 @agent_ppo2.py:185][0m |          -0.0087 |          59.4998 |          12.0239 |
[32m[20221213 23:59:22 @agent_ppo2.py:185][0m |          -0.0147 |          58.6821 |          12.0803 |
[32m[20221213 23:59:23 @agent_ppo2.py:185][0m |          -0.0153 |          57.5126 |          12.0802 |
[32m[20221213 23:59:23 @agent_ppo2.py:185][0m |          -0.0113 |          57.6356 |          12.0919 |
[32m[20221213 23:59:23 @agent_ppo2.py:185][0m |          -0.0122 |          56.8515 |          12.1416 |
[32m[20221213 23:59:23 @agent_ppo2.py:185][0m |          -0.0149 |          55.7201 |          12.1689 |
[32m[20221213 23:59:23 @agent_ppo2.py:185][0m |          -0.0161 |          55.2221 |          12.1394 |
[32m[20221213 23:59:23 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:59:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 430.86
[32m[20221213 23:59:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 507.04
[32m[20221213 23:59:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 501.68
[32m[20221213 23:59:23 @agent_ppo2.py:143][0m Total time:       1.88 min
[32m[20221213 23:59:23 @agent_ppo2.py:145][0m 163840 total steps have happened
[32m[20221213 23:59:23 @agent_ppo2.py:121][0m #------------------------ Iteration 4080 --------------------------#
[32m[20221213 23:59:23 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:59:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:59:23 @agent_ppo2.py:185][0m |           0.0009 |          65.1518 |          12.1547 |
[32m[20221213 23:59:24 @agent_ppo2.py:185][0m |          -0.0057 |          59.3947 |          12.1593 |
[32m[20221213 23:59:24 @agent_ppo2.py:185][0m |          -0.0089 |          57.6578 |          12.1471 |
[32m[20221213 23:59:24 @agent_ppo2.py:185][0m |          -0.0097 |          56.6585 |          12.1820 |
[32m[20221213 23:59:24 @agent_ppo2.py:185][0m |          -0.0137 |          55.9495 |          12.1929 |
[32m[20221213 23:59:24 @agent_ppo2.py:185][0m |          -0.0134 |          55.2994 |          12.2440 |
[32m[20221213 23:59:24 @agent_ppo2.py:185][0m |          -0.0195 |          54.9522 |          12.2284 |
[32m[20221213 23:59:24 @agent_ppo2.py:185][0m |          -0.0162 |          54.4812 |          12.2512 |
[32m[20221213 23:59:24 @agent_ppo2.py:185][0m |          -0.0187 |          54.3121 |          12.2469 |
[32m[20221213 23:59:24 @agent_ppo2.py:185][0m |          -0.0187 |          53.9391 |          12.2703 |
[32m[20221213 23:59:24 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:59:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 504.34
[32m[20221213 23:59:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 526.29
[32m[20221213 23:59:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 497.65
[32m[20221213 23:59:24 @agent_ppo2.py:143][0m Total time:       1.90 min
[32m[20221213 23:59:24 @agent_ppo2.py:145][0m 165888 total steps have happened
[32m[20221213 23:59:24 @agent_ppo2.py:121][0m #------------------------ Iteration 4081 --------------------------#
[32m[20221213 23:59:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:59:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:59:25 @agent_ppo2.py:185][0m |           0.0015 |          62.5793 |          12.1930 |
[32m[20221213 23:59:25 @agent_ppo2.py:185][0m |          -0.0045 |          57.1039 |          12.1727 |
[32m[20221213 23:59:25 @agent_ppo2.py:185][0m |          -0.0066 |          55.2967 |          12.1479 |
[32m[20221213 23:59:25 @agent_ppo2.py:185][0m |          -0.0051 |          53.9317 |          12.1284 |
[32m[20221213 23:59:25 @agent_ppo2.py:185][0m |          -0.0109 |          52.8317 |          12.1327 |
[32m[20221213 23:59:25 @agent_ppo2.py:185][0m |          -0.0084 |          53.4778 |          12.1153 |
[32m[20221213 23:59:25 @agent_ppo2.py:185][0m |          -0.0086 |          52.0412 |          12.1129 |
[32m[20221213 23:59:25 @agent_ppo2.py:185][0m |          -0.0121 |          50.4982 |          12.0972 |
[32m[20221213 23:59:25 @agent_ppo2.py:185][0m |          -0.0181 |          50.1242 |          12.0789 |
[32m[20221213 23:59:25 @agent_ppo2.py:185][0m |          -0.0154 |          49.5903 |          12.0901 |
[32m[20221213 23:59:25 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:59:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 435.59
[32m[20221213 23:59:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 489.10
[32m[20221213 23:59:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 292.64
[32m[20221213 23:59:26 @agent_ppo2.py:143][0m Total time:       1.92 min
[32m[20221213 23:59:26 @agent_ppo2.py:145][0m 167936 total steps have happened
[32m[20221213 23:59:26 @agent_ppo2.py:121][0m #------------------------ Iteration 4082 --------------------------#
[32m[20221213 23:59:26 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:59:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:59:26 @agent_ppo2.py:185][0m |          -0.0005 |          68.8085 |          12.3753 |
[32m[20221213 23:59:26 @agent_ppo2.py:185][0m |          -0.0056 |          65.6227 |          12.3805 |
[32m[20221213 23:59:26 @agent_ppo2.py:185][0m |          -0.0116 |          64.6061 |          12.3741 |
[32m[20221213 23:59:26 @agent_ppo2.py:185][0m |          -0.0101 |          63.9748 |          12.4031 |
[32m[20221213 23:59:26 @agent_ppo2.py:185][0m |          -0.0119 |          63.4474 |          12.3521 |
[32m[20221213 23:59:26 @agent_ppo2.py:185][0m |          -0.0160 |          63.0004 |          12.3795 |
[32m[20221213 23:59:26 @agent_ppo2.py:185][0m |          -0.0120 |          62.7668 |          12.3636 |
[32m[20221213 23:59:27 @agent_ppo2.py:185][0m |          -0.0140 |          62.3248 |          12.3790 |
[32m[20221213 23:59:27 @agent_ppo2.py:185][0m |          -0.0166 |          62.0013 |          12.3883 |
[32m[20221213 23:59:27 @agent_ppo2.py:185][0m |          -0.0230 |          61.9783 |          12.3613 |
[32m[20221213 23:59:27 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:59:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 442.81
[32m[20221213 23:59:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 484.43
[32m[20221213 23:59:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 424.60
[32m[20221213 23:59:27 @agent_ppo2.py:143][0m Total time:       1.94 min
[32m[20221213 23:59:27 @agent_ppo2.py:145][0m 169984 total steps have happened
[32m[20221213 23:59:27 @agent_ppo2.py:121][0m #------------------------ Iteration 4083 --------------------------#
[32m[20221213 23:59:27 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 23:59:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:59:27 @agent_ppo2.py:185][0m |           0.0101 |          67.0016 |          12.3511 |
[32m[20221213 23:59:27 @agent_ppo2.py:185][0m |          -0.0046 |          60.2638 |          12.4053 |
[32m[20221213 23:59:27 @agent_ppo2.py:185][0m |          -0.0064 |          59.4370 |          12.3684 |
[32m[20221213 23:59:28 @agent_ppo2.py:185][0m |          -0.0074 |          59.0389 |          12.3282 |
[32m[20221213 23:59:28 @agent_ppo2.py:185][0m |          -0.0121 |          58.9336 |          12.3217 |
[32m[20221213 23:59:28 @agent_ppo2.py:185][0m |          -0.0108 |          58.6732 |          12.3007 |
[32m[20221213 23:59:28 @agent_ppo2.py:185][0m |          -0.0077 |          58.4596 |          12.2953 |
[32m[20221213 23:59:28 @agent_ppo2.py:185][0m |          -0.0173 |          58.2914 |          12.3150 |
[32m[20221213 23:59:28 @agent_ppo2.py:185][0m |          -0.0153 |          57.9813 |          12.3080 |
[32m[20221213 23:59:28 @agent_ppo2.py:185][0m |          -0.0106 |          58.8414 |          12.3039 |
[32m[20221213 23:59:28 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221213 23:59:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 487.14
[32m[20221213 23:59:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 530.42
[32m[20221213 23:59:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 395.87
[32m[20221213 23:59:28 @agent_ppo2.py:143][0m Total time:       1.97 min
[32m[20221213 23:59:28 @agent_ppo2.py:145][0m 172032 total steps have happened
[32m[20221213 23:59:28 @agent_ppo2.py:121][0m #------------------------ Iteration 4084 --------------------------#
[32m[20221213 23:59:29 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:59:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:59:29 @agent_ppo2.py:185][0m |           0.0014 |          66.7252 |          12.0821 |
[32m[20221213 23:59:29 @agent_ppo2.py:185][0m |           0.0010 |          61.1111 |          12.0733 |
[32m[20221213 23:59:29 @agent_ppo2.py:185][0m |          -0.0047 |          59.4293 |          12.1041 |
[32m[20221213 23:59:29 @agent_ppo2.py:185][0m |          -0.0120 |          57.7258 |          12.0860 |
[32m[20221213 23:59:29 @agent_ppo2.py:185][0m |          -0.0107 |          56.6437 |          12.0987 |
[32m[20221213 23:59:29 @agent_ppo2.py:185][0m |          -0.0162 |          55.9578 |          12.1000 |
[32m[20221213 23:59:29 @agent_ppo2.py:185][0m |          -0.0132 |          55.2839 |          12.1377 |
[32m[20221213 23:59:29 @agent_ppo2.py:185][0m |          -0.0201 |          54.8420 |          12.1416 |
[32m[20221213 23:59:29 @agent_ppo2.py:185][0m |          -0.0155 |          54.4740 |          12.1633 |
[32m[20221213 23:59:30 @agent_ppo2.py:185][0m |          -0.0195 |          54.0953 |          12.1902 |
[32m[20221213 23:59:30 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 23:59:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 399.81
[32m[20221213 23:59:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 469.49
[32m[20221213 23:59:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 485.99
[32m[20221213 23:59:30 @agent_ppo2.py:143][0m Total time:       1.99 min
[32m[20221213 23:59:30 @agent_ppo2.py:145][0m 174080 total steps have happened
[32m[20221213 23:59:30 @agent_ppo2.py:121][0m #------------------------ Iteration 4085 --------------------------#
[32m[20221213 23:59:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:59:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:59:30 @agent_ppo2.py:185][0m |           0.0091 |          74.7514 |          12.4498 |
[32m[20221213 23:59:30 @agent_ppo2.py:185][0m |          -0.0002 |          67.0178 |          12.4370 |
[32m[20221213 23:59:30 @agent_ppo2.py:185][0m |          -0.0070 |          65.1887 |          12.4051 |
[32m[20221213 23:59:30 @agent_ppo2.py:185][0m |          -0.0084 |          64.3883 |          12.4154 |
[32m[20221213 23:59:30 @agent_ppo2.py:185][0m |          -0.0103 |          64.0675 |          12.4318 |
[32m[20221213 23:59:30 @agent_ppo2.py:185][0m |          -0.0076 |          63.8511 |          12.4334 |
[32m[20221213 23:59:31 @agent_ppo2.py:185][0m |          -0.0084 |          63.1624 |          12.4159 |
[32m[20221213 23:59:31 @agent_ppo2.py:185][0m |           0.0014 |          68.0091 |          12.4823 |
[32m[20221213 23:59:31 @agent_ppo2.py:185][0m |          -0.0103 |          62.7428 |          12.4669 |
[32m[20221213 23:59:31 @agent_ppo2.py:185][0m |          -0.0079 |          63.5145 |          12.4738 |
[32m[20221213 23:59:31 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:59:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 455.54
[32m[20221213 23:59:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 479.18
[32m[20221213 23:59:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 539.46
[32m[20221213 23:59:31 @agent_ppo2.py:143][0m Total time:       2.01 min
[32m[20221213 23:59:31 @agent_ppo2.py:145][0m 176128 total steps have happened
[32m[20221213 23:59:31 @agent_ppo2.py:121][0m #------------------------ Iteration 4086 --------------------------#
[32m[20221213 23:59:31 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:59:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:59:31 @agent_ppo2.py:185][0m |           0.0005 |          64.9716 |          12.4578 |
[32m[20221213 23:59:31 @agent_ppo2.py:185][0m |          -0.0026 |          61.3845 |          12.5382 |
[32m[20221213 23:59:31 @agent_ppo2.py:185][0m |          -0.0084 |          59.6563 |          12.5463 |
[32m[20221213 23:59:32 @agent_ppo2.py:185][0m |           0.0053 |          64.9644 |          12.5634 |
[32m[20221213 23:59:32 @agent_ppo2.py:185][0m |          -0.0104 |          58.1370 |          12.5924 |
[32m[20221213 23:59:32 @agent_ppo2.py:185][0m |          -0.0100 |          57.6642 |          12.6271 |
[32m[20221213 23:59:32 @agent_ppo2.py:185][0m |          -0.0121 |          57.1239 |          12.5898 |
[32m[20221213 23:59:32 @agent_ppo2.py:185][0m |          -0.0112 |          56.8851 |          12.6024 |
[32m[20221213 23:59:32 @agent_ppo2.py:185][0m |          -0.0125 |          56.5121 |          12.6003 |
[32m[20221213 23:59:32 @agent_ppo2.py:185][0m |          -0.0146 |          56.1762 |          12.6641 |
[32m[20221213 23:59:32 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:59:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 489.28
[32m[20221213 23:59:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 555.12
[32m[20221213 23:59:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 566.30
[32m[20221213 23:59:32 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 566.30
[32m[20221213 23:59:32 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 566.30
[32m[20221213 23:59:32 @agent_ppo2.py:143][0m Total time:       2.03 min
[32m[20221213 23:59:32 @agent_ppo2.py:145][0m 178176 total steps have happened
[32m[20221213 23:59:32 @agent_ppo2.py:121][0m #------------------------ Iteration 4087 --------------------------#
[32m[20221213 23:59:32 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:59:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:59:33 @agent_ppo2.py:185][0m |           0.0030 |          62.3453 |          12.9041 |
[32m[20221213 23:59:33 @agent_ppo2.py:185][0m |          -0.0048 |          59.6767 |          12.9313 |
[32m[20221213 23:59:33 @agent_ppo2.py:185][0m |          -0.0086 |          58.4066 |          12.9469 |
[32m[20221213 23:59:33 @agent_ppo2.py:185][0m |          -0.0071 |          58.0992 |          12.9023 |
[32m[20221213 23:59:33 @agent_ppo2.py:185][0m |          -0.0090 |          57.1367 |          12.9086 |
[32m[20221213 23:59:33 @agent_ppo2.py:185][0m |          -0.0003 |          67.1973 |          12.8956 |
[32m[20221213 23:59:33 @agent_ppo2.py:185][0m |          -0.0130 |          56.9700 |          12.8676 |
[32m[20221213 23:59:33 @agent_ppo2.py:185][0m |          -0.0146 |          56.3812 |          12.8773 |
[32m[20221213 23:59:33 @agent_ppo2.py:185][0m |          -0.0132 |          56.2802 |          12.8637 |
[32m[20221213 23:59:33 @agent_ppo2.py:185][0m |          -0.0126 |          56.0831 |          12.8661 |
[32m[20221213 23:59:33 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:59:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 481.06
[32m[20221213 23:59:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 534.55
[32m[20221213 23:59:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 358.60
[32m[20221213 23:59:34 @agent_ppo2.py:143][0m Total time:       2.05 min
[32m[20221213 23:59:34 @agent_ppo2.py:145][0m 180224 total steps have happened
[32m[20221213 23:59:34 @agent_ppo2.py:121][0m #------------------------ Iteration 4088 --------------------------#
[32m[20221213 23:59:34 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:59:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:59:34 @agent_ppo2.py:185][0m |          -0.0016 |          54.9121 |          12.7127 |
[32m[20221213 23:59:34 @agent_ppo2.py:185][0m |          -0.0094 |          52.6341 |          12.6783 |
[32m[20221213 23:59:34 @agent_ppo2.py:185][0m |          -0.0122 |          51.9075 |          12.7277 |
[32m[20221213 23:59:34 @agent_ppo2.py:185][0m |          -0.0117 |          51.1916 |          12.7239 |
[32m[20221213 23:59:34 @agent_ppo2.py:185][0m |          -0.0105 |          51.0998 |          12.7401 |
[32m[20221213 23:59:34 @agent_ppo2.py:185][0m |          -0.0132 |          50.1744 |          12.7177 |
[32m[20221213 23:59:34 @agent_ppo2.py:185][0m |          -0.0099 |          52.5604 |          12.7135 |
[32m[20221213 23:59:35 @agent_ppo2.py:185][0m |          -0.0191 |          49.8078 |          12.7354 |
[32m[20221213 23:59:35 @agent_ppo2.py:185][0m |          -0.0049 |          56.8503 |          12.7291 |
[32m[20221213 23:59:35 @agent_ppo2.py:185][0m |          -0.0164 |          49.2978 |          12.7262 |
[32m[20221213 23:59:35 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221213 23:59:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 455.76
[32m[20221213 23:59:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 509.56
[32m[20221213 23:59:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 515.92
[32m[20221213 23:59:35 @agent_ppo2.py:143][0m Total time:       2.08 min
[32m[20221213 23:59:35 @agent_ppo2.py:145][0m 182272 total steps have happened
[32m[20221213 23:59:35 @agent_ppo2.py:121][0m #------------------------ Iteration 4089 --------------------------#
[32m[20221213 23:59:35 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:59:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:59:35 @agent_ppo2.py:185][0m |           0.0018 |          71.5660 |          12.6877 |
[32m[20221213 23:59:35 @agent_ppo2.py:185][0m |          -0.0044 |          65.8394 |          12.6661 |
[32m[20221213 23:59:35 @agent_ppo2.py:185][0m |          -0.0124 |          63.4948 |          12.6723 |
[32m[20221213 23:59:36 @agent_ppo2.py:185][0m |          -0.0074 |          63.0258 |          12.6518 |
[32m[20221213 23:59:36 @agent_ppo2.py:185][0m |          -0.0130 |          61.6891 |          12.6667 |
[32m[20221213 23:59:36 @agent_ppo2.py:185][0m |          -0.0157 |          61.6100 |          12.6574 |
[32m[20221213 23:59:36 @agent_ppo2.py:185][0m |          -0.0135 |          61.0308 |          12.6286 |
[32m[20221213 23:59:36 @agent_ppo2.py:185][0m |          -0.0161 |          60.2373 |          12.6469 |
[32m[20221213 23:59:36 @agent_ppo2.py:185][0m |          -0.0106 |          61.4801 |          12.6391 |
[32m[20221213 23:59:36 @agent_ppo2.py:185][0m |          -0.0172 |          60.0029 |          12.6377 |
[32m[20221213 23:59:36 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:59:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 406.05
[32m[20221213 23:59:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 475.33
[32m[20221213 23:59:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 585.42
[32m[20221213 23:59:36 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 585.42
[32m[20221213 23:59:36 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 585.42
[32m[20221213 23:59:36 @agent_ppo2.py:143][0m Total time:       2.10 min
[32m[20221213 23:59:36 @agent_ppo2.py:145][0m 184320 total steps have happened
[32m[20221213 23:59:36 @agent_ppo2.py:121][0m #------------------------ Iteration 4090 --------------------------#
[32m[20221213 23:59:36 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221213 23:59:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:59:37 @agent_ppo2.py:185][0m |          -0.0017 |          64.0884 |          12.5857 |
[32m[20221213 23:59:37 @agent_ppo2.py:185][0m |          -0.0066 |          62.7287 |          12.5834 |
[32m[20221213 23:59:37 @agent_ppo2.py:185][0m |          -0.0084 |          62.0339 |          12.5504 |
[32m[20221213 23:59:37 @agent_ppo2.py:185][0m |          -0.0129 |          61.7273 |          12.5680 |
[32m[20221213 23:59:37 @agent_ppo2.py:185][0m |          -0.0109 |          61.2532 |          12.5423 |
[32m[20221213 23:59:37 @agent_ppo2.py:185][0m |          -0.0125 |          61.3810 |          12.5584 |
[32m[20221213 23:59:37 @agent_ppo2.py:185][0m |           0.0018 |          67.1389 |          12.4960 |
[32m[20221213 23:59:37 @agent_ppo2.py:185][0m |          -0.0106 |          60.8594 |          12.4947 |
[32m[20221213 23:59:37 @agent_ppo2.py:185][0m |          -0.0139 |          60.8385 |          12.4766 |
[32m[20221213 23:59:37 @agent_ppo2.py:185][0m |          -0.0158 |          60.4444 |          12.4813 |
[32m[20221213 23:59:37 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221213 23:59:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 459.82
[32m[20221213 23:59:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 499.04
[32m[20221213 23:59:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 501.01
[32m[20221213 23:59:38 @agent_ppo2.py:143][0m Total time:       2.12 min
[32m[20221213 23:59:38 @agent_ppo2.py:145][0m 186368 total steps have happened
[32m[20221213 23:59:38 @agent_ppo2.py:121][0m #------------------------ Iteration 4091 --------------------------#
[32m[20221213 23:59:38 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:59:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:59:38 @agent_ppo2.py:185][0m |           0.0016 |          68.8086 |          12.2503 |
[32m[20221213 23:59:38 @agent_ppo2.py:185][0m |           0.0173 |          67.9642 |          12.2929 |
[32m[20221213 23:59:38 @agent_ppo2.py:185][0m |          -0.0078 |          63.1820 |          12.3039 |
[32m[20221213 23:59:38 @agent_ppo2.py:185][0m |          -0.0091 |          61.7434 |          12.3069 |
[32m[20221213 23:59:38 @agent_ppo2.py:185][0m |          -0.0129 |          60.9437 |          12.3374 |
[32m[20221213 23:59:38 @agent_ppo2.py:185][0m |          -0.0078 |          61.1127 |          12.3420 |
[32m[20221213 23:59:38 @agent_ppo2.py:185][0m |          -0.0143 |          60.2581 |          12.3590 |
[32m[20221213 23:59:39 @agent_ppo2.py:185][0m |          -0.0130 |          59.7984 |          12.3876 |
[32m[20221213 23:59:39 @agent_ppo2.py:185][0m |          -0.0053 |          63.4188 |          12.3886 |
[32m[20221213 23:59:39 @agent_ppo2.py:185][0m |          -0.0166 |          59.0513 |          12.4308 |
[32m[20221213 23:59:39 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:59:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 435.25
[32m[20221213 23:59:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 483.83
[32m[20221213 23:59:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 492.81
[32m[20221213 23:59:39 @agent_ppo2.py:143][0m Total time:       2.14 min
[32m[20221213 23:59:39 @agent_ppo2.py:145][0m 188416 total steps have happened
[32m[20221213 23:59:39 @agent_ppo2.py:121][0m #------------------------ Iteration 4092 --------------------------#
[32m[20221213 23:59:39 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:59:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:59:39 @agent_ppo2.py:185][0m |           0.0011 |          70.5275 |          12.5888 |
[32m[20221213 23:59:39 @agent_ppo2.py:185][0m |          -0.0096 |          68.0088 |          12.7015 |
[32m[20221213 23:59:39 @agent_ppo2.py:185][0m |          -0.0069 |          67.1740 |          12.6758 |
[32m[20221213 23:59:39 @agent_ppo2.py:185][0m |          -0.0072 |          66.7224 |          12.7030 |
[32m[20221213 23:59:40 @agent_ppo2.py:185][0m |          -0.0114 |          66.0852 |          12.7069 |
[32m[20221213 23:59:40 @agent_ppo2.py:185][0m |          -0.0097 |          65.8718 |          12.6846 |
[32m[20221213 23:59:40 @agent_ppo2.py:185][0m |          -0.0048 |          71.8341 |          12.7335 |
[32m[20221213 23:59:40 @agent_ppo2.py:185][0m |          -0.0128 |          65.8552 |          12.7177 |
[32m[20221213 23:59:40 @agent_ppo2.py:185][0m |          -0.0122 |          65.3566 |          12.7087 |
[32m[20221213 23:59:40 @agent_ppo2.py:185][0m |          -0.0143 |          65.1257 |          12.7206 |
[32m[20221213 23:59:40 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221213 23:59:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 493.50
[32m[20221213 23:59:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 532.66
[32m[20221213 23:59:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 528.35
[32m[20221213 23:59:40 @agent_ppo2.py:143][0m Total time:       2.16 min
[32m[20221213 23:59:40 @agent_ppo2.py:145][0m 190464 total steps have happened
[32m[20221213 23:59:40 @agent_ppo2.py:121][0m #------------------------ Iteration 4093 --------------------------#
[32m[20221213 23:59:40 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:59:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:59:40 @agent_ppo2.py:185][0m |          -0.0015 |          64.1708 |          12.8191 |
[32m[20221213 23:59:41 @agent_ppo2.py:185][0m |          -0.0049 |          61.3741 |          12.8851 |
[32m[20221213 23:59:41 @agent_ppo2.py:185][0m |          -0.0069 |          60.6861 |          12.9185 |
[32m[20221213 23:59:41 @agent_ppo2.py:185][0m |          -0.0085 |          60.0181 |          12.9283 |
[32m[20221213 23:59:41 @agent_ppo2.py:185][0m |          -0.0102 |          59.6625 |          12.9588 |
[32m[20221213 23:59:41 @agent_ppo2.py:185][0m |          -0.0052 |          60.2839 |          12.9807 |
[32m[20221213 23:59:41 @agent_ppo2.py:185][0m |          -0.0139 |          59.1441 |          13.0085 |
[32m[20221213 23:59:41 @agent_ppo2.py:185][0m |          -0.0024 |          61.2863 |          13.0170 |
[32m[20221213 23:59:41 @agent_ppo2.py:185][0m |          -0.0120 |          58.8216 |          13.0537 |
[32m[20221213 23:59:41 @agent_ppo2.py:185][0m |          -0.0141 |          58.4538 |          13.0611 |
[32m[20221213 23:59:41 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221213 23:59:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 474.14
[32m[20221213 23:59:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 513.23
[32m[20221213 23:59:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 507.36
[32m[20221213 23:59:41 @agent_ppo2.py:143][0m Total time:       2.18 min
[32m[20221213 23:59:41 @agent_ppo2.py:145][0m 192512 total steps have happened
[32m[20221213 23:59:41 @agent_ppo2.py:121][0m #------------------------ Iteration 4094 --------------------------#
[32m[20221213 23:59:42 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:59:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:59:42 @agent_ppo2.py:185][0m |          -0.0031 |          70.3919 |          13.0208 |
[32m[20221213 23:59:42 @agent_ppo2.py:185][0m |          -0.0086 |          66.6217 |          13.0120 |
[32m[20221213 23:59:42 @agent_ppo2.py:185][0m |          -0.0104 |          65.3966 |          13.0058 |
[32m[20221213 23:59:42 @agent_ppo2.py:185][0m |          -0.0137 |          64.1506 |          13.0016 |
[32m[20221213 23:59:42 @agent_ppo2.py:185][0m |          -0.0115 |          63.9966 |          13.0130 |
[32m[20221213 23:59:42 @agent_ppo2.py:185][0m |          -0.0175 |          62.9631 |          12.9761 |
[32m[20221213 23:59:42 @agent_ppo2.py:185][0m |          -0.0152 |          62.3145 |          12.9796 |
[32m[20221213 23:59:42 @agent_ppo2.py:185][0m |          -0.0167 |          61.6856 |          12.9775 |
[32m[20221213 23:59:42 @agent_ppo2.py:185][0m |          -0.0221 |          61.6655 |          12.9616 |
[32m[20221213 23:59:43 @agent_ppo2.py:185][0m |          -0.0187 |          61.1679 |          12.9771 |
[32m[20221213 23:59:43 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:59:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 428.26
[32m[20221213 23:59:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 468.23
[32m[20221213 23:59:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 230.56
[32m[20221213 23:59:43 @agent_ppo2.py:143][0m Total time:       2.20 min
[32m[20221213 23:59:43 @agent_ppo2.py:145][0m 194560 total steps have happened
[32m[20221213 23:59:43 @agent_ppo2.py:121][0m #------------------------ Iteration 4095 --------------------------#
[32m[20221213 23:59:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:59:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:59:43 @agent_ppo2.py:185][0m |           0.0019 |          59.1662 |          12.8887 |
[32m[20221213 23:59:43 @agent_ppo2.py:185][0m |           0.0052 |          54.7606 |          12.8615 |
[32m[20221213 23:59:43 @agent_ppo2.py:185][0m |          -0.0065 |          49.9402 |          12.8806 |
[32m[20221213 23:59:43 @agent_ppo2.py:185][0m |          -0.0102 |          48.7605 |          12.8788 |
[32m[20221213 23:59:43 @agent_ppo2.py:185][0m |          -0.0128 |          47.5864 |          12.8932 |
[32m[20221213 23:59:43 @agent_ppo2.py:185][0m |          -0.0096 |          46.8935 |          12.8754 |
[32m[20221213 23:59:44 @agent_ppo2.py:185][0m |          -0.0125 |          46.3797 |          12.8921 |
[32m[20221213 23:59:44 @agent_ppo2.py:185][0m |          -0.0136 |          45.8555 |          12.8616 |
[32m[20221213 23:59:44 @agent_ppo2.py:185][0m |          -0.0178 |          45.8968 |          12.8483 |
[32m[20221213 23:59:44 @agent_ppo2.py:185][0m |          -0.0183 |          45.2900 |          12.8624 |
[32m[20221213 23:59:44 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221213 23:59:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 412.20
[32m[20221213 23:59:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 507.87
[32m[20221213 23:59:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 313.51
[32m[20221213 23:59:44 @agent_ppo2.py:143][0m Total time:       2.23 min
[32m[20221213 23:59:44 @agent_ppo2.py:145][0m 196608 total steps have happened
[32m[20221213 23:59:44 @agent_ppo2.py:121][0m #------------------------ Iteration 4096 --------------------------#
[32m[20221213 23:59:44 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:59:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:59:44 @agent_ppo2.py:185][0m |           0.0031 |          53.5770 |          12.7910 |
[32m[20221213 23:59:44 @agent_ppo2.py:185][0m |          -0.0022 |          48.7329 |          12.8096 |
[32m[20221213 23:59:44 @agent_ppo2.py:185][0m |          -0.0033 |          47.3580 |          12.7554 |
[32m[20221213 23:59:45 @agent_ppo2.py:185][0m |          -0.0095 |          45.9605 |          12.7905 |
[32m[20221213 23:59:45 @agent_ppo2.py:185][0m |          -0.0094 |          45.3131 |          12.8244 |
[32m[20221213 23:59:45 @agent_ppo2.py:185][0m |          -0.0113 |          44.7893 |          12.8379 |
[32m[20221213 23:59:45 @agent_ppo2.py:185][0m |          -0.0071 |          44.9945 |          12.8148 |
[32m[20221213 23:59:45 @agent_ppo2.py:185][0m |          -0.0112 |          44.3257 |          12.7986 |
[32m[20221213 23:59:45 @agent_ppo2.py:185][0m |          -0.0174 |          44.0396 |          12.8032 |
[32m[20221213 23:59:45 @agent_ppo2.py:185][0m |          -0.0137 |          43.9687 |          12.8305 |
[32m[20221213 23:59:45 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221213 23:59:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 422.49
[32m[20221213 23:59:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 489.53
[32m[20221213 23:59:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 509.71
[32m[20221213 23:59:45 @agent_ppo2.py:143][0m Total time:       2.25 min
[32m[20221213 23:59:45 @agent_ppo2.py:145][0m 198656 total steps have happened
[32m[20221213 23:59:45 @agent_ppo2.py:121][0m #------------------------ Iteration 4097 --------------------------#
[32m[20221213 23:59:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:59:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:59:46 @agent_ppo2.py:185][0m |           0.0025 |          72.9979 |          12.4331 |
[32m[20221213 23:59:46 @agent_ppo2.py:185][0m |          -0.0028 |          68.9123 |          12.4995 |
[32m[20221213 23:59:46 @agent_ppo2.py:185][0m |          -0.0099 |          67.0243 |          12.4748 |
[32m[20221213 23:59:46 @agent_ppo2.py:185][0m |          -0.0063 |          65.9876 |          12.4609 |
[32m[20221213 23:59:46 @agent_ppo2.py:185][0m |          -0.0085 |          64.9376 |          12.4377 |
[32m[20221213 23:59:46 @agent_ppo2.py:185][0m |          -0.0122 |          63.2436 |          12.4812 |
[32m[20221213 23:59:46 @agent_ppo2.py:185][0m |          -0.0015 |          69.5992 |          12.4551 |
[32m[20221213 23:59:46 @agent_ppo2.py:185][0m |          -0.0137 |          62.3797 |          12.4398 |
[32m[20221213 23:59:46 @agent_ppo2.py:185][0m |          -0.0151 |          61.6154 |          12.4463 |
[32m[20221213 23:59:46 @agent_ppo2.py:185][0m |          -0.0144 |          61.0274 |          12.4352 |
[32m[20221213 23:59:46 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221213 23:59:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 469.34
[32m[20221213 23:59:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 526.80
[32m[20221213 23:59:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 436.94
[32m[20221213 23:59:46 @agent_ppo2.py:143][0m Total time:       2.27 min
[32m[20221213 23:59:46 @agent_ppo2.py:145][0m 200704 total steps have happened
[32m[20221213 23:59:46 @agent_ppo2.py:121][0m #------------------------ Iteration 4098 --------------------------#
[32m[20221213 23:59:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:59:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:59:47 @agent_ppo2.py:185][0m |           0.0002 |          67.1627 |          13.0404 |
[32m[20221213 23:59:47 @agent_ppo2.py:185][0m |          -0.0037 |          61.6624 |          13.0452 |
[32m[20221213 23:59:47 @agent_ppo2.py:185][0m |          -0.0054 |          60.0475 |          13.0568 |
[32m[20221213 23:59:47 @agent_ppo2.py:185][0m |          -0.0032 |          59.6917 |          13.0823 |
[32m[20221213 23:59:47 @agent_ppo2.py:185][0m |          -0.0091 |          57.7817 |          13.1075 |
[32m[20221213 23:59:47 @agent_ppo2.py:185][0m |          -0.0123 |          57.4499 |          13.1092 |
[32m[20221213 23:59:47 @agent_ppo2.py:185][0m |          -0.0166 |          56.9051 |          13.1016 |
[32m[20221213 23:59:48 @agent_ppo2.py:185][0m |          -0.0117 |          57.0512 |          13.1167 |
[32m[20221213 23:59:48 @agent_ppo2.py:185][0m |          -0.0141 |          56.2758 |          13.1142 |
[32m[20221213 23:59:48 @agent_ppo2.py:185][0m |          -0.0118 |          55.9689 |          13.1145 |
[32m[20221213 23:59:48 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221213 23:59:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 336.71
[32m[20221213 23:59:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 432.73
[32m[20221213 23:59:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 602.65
[32m[20221213 23:59:48 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 602.65
[32m[20221213 23:59:48 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 602.65
[32m[20221213 23:59:48 @agent_ppo2.py:143][0m Total time:       2.29 min
[32m[20221213 23:59:48 @agent_ppo2.py:145][0m 202752 total steps have happened
[32m[20221213 23:59:48 @agent_ppo2.py:121][0m #------------------------ Iteration 4099 --------------------------#
[32m[20221213 23:59:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:59:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:59:48 @agent_ppo2.py:185][0m |           0.0002 |          59.7620 |          12.9026 |
[32m[20221213 23:59:48 @agent_ppo2.py:185][0m |          -0.0049 |          53.8368 |          12.9553 |
[32m[20221213 23:59:48 @agent_ppo2.py:185][0m |          -0.0119 |          51.8176 |          12.9948 |
[32m[20221213 23:59:48 @agent_ppo2.py:185][0m |          -0.0110 |          50.9927 |          13.0074 |
[32m[20221213 23:59:49 @agent_ppo2.py:185][0m |          -0.0183 |          49.7345 |          13.0652 |
[32m[20221213 23:59:49 @agent_ppo2.py:185][0m |          -0.0126 |          49.0166 |          13.0915 |
[32m[20221213 23:59:49 @agent_ppo2.py:185][0m |          -0.0129 |          48.3857 |          13.0858 |
[32m[20221213 23:59:49 @agent_ppo2.py:185][0m |          -0.0142 |          47.8887 |          13.0926 |
[32m[20221213 23:59:49 @agent_ppo2.py:185][0m |          -0.0181 |          47.5331 |          13.1107 |
[32m[20221213 23:59:49 @agent_ppo2.py:185][0m |          -0.0038 |          52.3734 |          13.1240 |
[32m[20221213 23:59:49 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:59:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 444.33
[32m[20221213 23:59:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 546.54
[32m[20221213 23:59:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 246.98
[32m[20221213 23:59:49 @agent_ppo2.py:143][0m Total time:       2.31 min
[32m[20221213 23:59:49 @agent_ppo2.py:145][0m 204800 total steps have happened
[32m[20221213 23:59:49 @agent_ppo2.py:121][0m #------------------------ Iteration 4100 --------------------------#
[32m[20221213 23:59:49 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221213 23:59:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:59:49 @agent_ppo2.py:185][0m |           0.0061 |          43.0015 |          13.0253 |
[32m[20221213 23:59:50 @agent_ppo2.py:185][0m |          -0.0039 |          38.5596 |          13.0259 |
[32m[20221213 23:59:50 @agent_ppo2.py:185][0m |          -0.0113 |          36.5936 |          13.0185 |
[32m[20221213 23:59:50 @agent_ppo2.py:185][0m |          -0.0074 |          35.5101 |          12.9882 |
[32m[20221213 23:59:50 @agent_ppo2.py:185][0m |          -0.0106 |          35.3677 |          13.0050 |
[32m[20221213 23:59:50 @agent_ppo2.py:185][0m |          -0.0167 |          33.9147 |          12.9571 |
[32m[20221213 23:59:50 @agent_ppo2.py:185][0m |          -0.0190 |          33.3221 |          12.9463 |
[32m[20221213 23:59:50 @agent_ppo2.py:185][0m |          -0.0162 |          32.8238 |          12.9486 |
[32m[20221213 23:59:50 @agent_ppo2.py:185][0m |          -0.0186 |          32.6034 |          12.9370 |
[32m[20221213 23:59:50 @agent_ppo2.py:185][0m |          -0.0269 |          32.5278 |          12.9325 |
[32m[20221213 23:59:50 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221213 23:59:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 365.75
[32m[20221213 23:59:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 490.35
[32m[20221213 23:59:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 485.37
[32m[20221213 23:59:50 @agent_ppo2.py:143][0m Total time:       2.33 min
[32m[20221213 23:59:50 @agent_ppo2.py:145][0m 206848 total steps have happened
[32m[20221213 23:59:50 @agent_ppo2.py:121][0m #------------------------ Iteration 4101 --------------------------#
[32m[20221213 23:59:51 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:59:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:59:51 @agent_ppo2.py:185][0m |           0.0013 |          68.4582 |          13.0531 |
[32m[20221213 23:59:51 @agent_ppo2.py:185][0m |          -0.0073 |          63.9644 |          13.0583 |
[32m[20221213 23:59:51 @agent_ppo2.py:185][0m |          -0.0085 |          61.8391 |          13.0677 |
[32m[20221213 23:59:51 @agent_ppo2.py:185][0m |           0.0001 |          64.0527 |          13.0790 |
[32m[20221213 23:59:51 @agent_ppo2.py:185][0m |          -0.0146 |          59.6304 |          13.1029 |
[32m[20221213 23:59:51 @agent_ppo2.py:185][0m |          -0.0156 |          58.5009 |          13.1063 |
[32m[20221213 23:59:51 @agent_ppo2.py:185][0m |          -0.0177 |          58.2596 |          13.1139 |
[32m[20221213 23:59:52 @agent_ppo2.py:185][0m |          -0.0037 |          59.7522 |          13.1213 |
[32m[20221213 23:59:52 @agent_ppo2.py:185][0m |          -0.0129 |          58.6302 |          13.1360 |
[32m[20221213 23:59:52 @agent_ppo2.py:185][0m |          -0.0164 |          56.7736 |          13.1535 |
[32m[20221213 23:59:52 @agent_ppo2.py:130][0m Policy update time: 1.50 s
[32m[20221213 23:59:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 389.13
[32m[20221213 23:59:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 433.44
[32m[20221213 23:59:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 477.33
[32m[20221213 23:59:52 @agent_ppo2.py:143][0m Total time:       2.36 min
[32m[20221213 23:59:52 @agent_ppo2.py:145][0m 208896 total steps have happened
[32m[20221213 23:59:52 @agent_ppo2.py:121][0m #------------------------ Iteration 4102 --------------------------#
[32m[20221213 23:59:52 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:59:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:59:53 @agent_ppo2.py:185][0m |           0.0033 |          49.8764 |          13.1184 |
[32m[20221213 23:59:53 @agent_ppo2.py:185][0m |          -0.0036 |          43.0620 |          13.1091 |
[32m[20221213 23:59:53 @agent_ppo2.py:185][0m |          -0.0062 |          40.7469 |          13.0960 |
[32m[20221213 23:59:53 @agent_ppo2.py:185][0m |          -0.0084 |          38.8172 |          13.1018 |
[32m[20221213 23:59:53 @agent_ppo2.py:185][0m |          -0.0096 |          37.7996 |          13.0783 |
[32m[20221213 23:59:53 @agent_ppo2.py:185][0m |          -0.0070 |          36.9973 |          13.1045 |
[32m[20221213 23:59:53 @agent_ppo2.py:185][0m |          -0.0061 |          36.3965 |          13.0827 |
[32m[20221213 23:59:53 @agent_ppo2.py:185][0m |          -0.0145 |          35.5272 |          13.0805 |
[32m[20221213 23:59:53 @agent_ppo2.py:185][0m |          -0.0138 |          35.1572 |          13.0829 |
[32m[20221213 23:59:53 @agent_ppo2.py:185][0m |          -0.0116 |          34.4403 |          13.0431 |
[32m[20221213 23:59:53 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221213 23:59:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 371.11
[32m[20221213 23:59:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 475.84
[32m[20221213 23:59:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 456.22
[32m[20221213 23:59:54 @agent_ppo2.py:143][0m Total time:       2.39 min
[32m[20221213 23:59:54 @agent_ppo2.py:145][0m 210944 total steps have happened
[32m[20221213 23:59:54 @agent_ppo2.py:121][0m #------------------------ Iteration 4103 --------------------------#
[32m[20221213 23:59:54 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221213 23:59:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:59:54 @agent_ppo2.py:185][0m |          -0.0009 |          58.2409 |          13.0921 |
[32m[20221213 23:59:54 @agent_ppo2.py:185][0m |          -0.0036 |          55.1511 |          13.1141 |
[32m[20221213 23:59:54 @agent_ppo2.py:185][0m |          -0.0047 |          54.3872 |          13.1134 |
[32m[20221213 23:59:54 @agent_ppo2.py:185][0m |          -0.0081 |          54.0019 |          13.1278 |
[32m[20221213 23:59:54 @agent_ppo2.py:185][0m |          -0.0071 |          54.0439 |          13.1024 |
[32m[20221213 23:59:54 @agent_ppo2.py:185][0m |          -0.0088 |          53.4536 |          13.0761 |
[32m[20221213 23:59:55 @agent_ppo2.py:185][0m |          -0.0028 |          54.2500 |          13.0974 |
[32m[20221213 23:59:55 @agent_ppo2.py:185][0m |          -0.0074 |          53.8856 |          13.1233 |
[32m[20221213 23:59:55 @agent_ppo2.py:185][0m |          -0.0147 |          53.0664 |          13.1320 |
[32m[20221213 23:59:55 @agent_ppo2.py:185][0m |          -0.0067 |          53.7074 |          13.0922 |
[32m[20221213 23:59:55 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221213 23:59:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 546.51
[32m[20221213 23:59:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 579.75
[32m[20221213 23:59:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 499.82
[32m[20221213 23:59:55 @agent_ppo2.py:143][0m Total time:       2.41 min
[32m[20221213 23:59:55 @agent_ppo2.py:145][0m 212992 total steps have happened
[32m[20221213 23:59:55 @agent_ppo2.py:121][0m #------------------------ Iteration 4104 --------------------------#
[32m[20221213 23:59:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:59:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:59:55 @agent_ppo2.py:185][0m |           0.0002 |          62.5643 |          12.7877 |
[32m[20221213 23:59:55 @agent_ppo2.py:185][0m |           0.0005 |          61.4919 |          12.7614 |
[32m[20221213 23:59:56 @agent_ppo2.py:185][0m |          -0.0129 |          54.6471 |          12.7729 |
[32m[20221213 23:59:56 @agent_ppo2.py:185][0m |          -0.0150 |          53.0568 |          12.7881 |
[32m[20221213 23:59:56 @agent_ppo2.py:185][0m |          -0.0171 |          51.7373 |          12.7882 |
[32m[20221213 23:59:56 @agent_ppo2.py:185][0m |          -0.0109 |          51.5535 |          12.7844 |
[32m[20221213 23:59:56 @agent_ppo2.py:185][0m |          -0.0136 |          50.3102 |          12.7792 |
[32m[20221213 23:59:56 @agent_ppo2.py:185][0m |          -0.0207 |          49.4541 |          12.7988 |
[32m[20221213 23:59:56 @agent_ppo2.py:185][0m |          -0.0183 |          49.0352 |          12.7925 |
[32m[20221213 23:59:56 @agent_ppo2.py:185][0m |          -0.0211 |          48.5290 |          12.7982 |
[32m[20221213 23:59:56 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:59:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 394.89
[32m[20221213 23:59:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 459.22
[32m[20221213 23:59:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 587.25
[32m[20221213 23:59:56 @agent_ppo2.py:143][0m Total time:       2.43 min
[32m[20221213 23:59:56 @agent_ppo2.py:145][0m 215040 total steps have happened
[32m[20221213 23:59:56 @agent_ppo2.py:121][0m #------------------------ Iteration 4105 --------------------------#
[32m[20221213 23:59:57 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 23:59:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:59:57 @agent_ppo2.py:185][0m |           0.0031 |          71.6248 |          12.9775 |
[32m[20221213 23:59:57 @agent_ppo2.py:185][0m |          -0.0094 |          68.9164 |          12.9300 |
[32m[20221213 23:59:57 @agent_ppo2.py:185][0m |          -0.0099 |          67.5132 |          12.9423 |
[32m[20221213 23:59:57 @agent_ppo2.py:185][0m |          -0.0091 |          67.2130 |          12.9289 |
[32m[20221213 23:59:57 @agent_ppo2.py:185][0m |          -0.0117 |          66.1618 |          12.9548 |
[32m[20221213 23:59:57 @agent_ppo2.py:185][0m |          -0.0129 |          65.9212 |          12.9352 |
[32m[20221213 23:59:57 @agent_ppo2.py:185][0m |          -0.0150 |          65.2641 |          12.9728 |
[32m[20221213 23:59:57 @agent_ppo2.py:185][0m |          -0.0161 |          65.0756 |          12.9380 |
[32m[20221213 23:59:57 @agent_ppo2.py:185][0m |          -0.0157 |          64.5336 |          12.9365 |
[32m[20221213 23:59:58 @agent_ppo2.py:185][0m |          -0.0062 |          67.8711 |          12.9408 |
[32m[20221213 23:59:58 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221213 23:59:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 418.47
[32m[20221213 23:59:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 481.70
[32m[20221213 23:59:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 397.36
[32m[20221213 23:59:58 @agent_ppo2.py:143][0m Total time:       2.46 min
[32m[20221213 23:59:58 @agent_ppo2.py:145][0m 217088 total steps have happened
[32m[20221213 23:59:58 @agent_ppo2.py:121][0m #------------------------ Iteration 4106 --------------------------#
[32m[20221213 23:59:58 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221213 23:59:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:59:58 @agent_ppo2.py:185][0m |           0.0007 |          16.1695 |          12.9218 |
[32m[20221213 23:59:58 @agent_ppo2.py:185][0m |           0.0002 |          14.7829 |          12.8967 |
[32m[20221213 23:59:58 @agent_ppo2.py:185][0m |          -0.0032 |          14.6509 |          12.9001 |
[32m[20221213 23:59:58 @agent_ppo2.py:185][0m |           0.0002 |          14.5776 |          12.8645 |
[32m[20221213 23:59:58 @agent_ppo2.py:185][0m |           0.0037 |          15.3091 |          12.8503 |
[32m[20221213 23:59:58 @agent_ppo2.py:185][0m |           0.0070 |          15.2503 |          12.8415 |
[32m[20221213 23:59:59 @agent_ppo2.py:185][0m |          -0.0067 |          14.5600 |          12.8254 |
[32m[20221213 23:59:59 @agent_ppo2.py:185][0m |           0.0077 |          15.7796 |          12.8283 |
[32m[20221213 23:59:59 @agent_ppo2.py:185][0m |          -0.0069 |          14.5899 |          12.8595 |
[32m[20221213 23:59:59 @agent_ppo2.py:185][0m |          -0.0029 |          14.4736 |          12.8293 |
[32m[20221213 23:59:59 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221213 23:59:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221213 23:59:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221213 23:59:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 432.70
[32m[20221213 23:59:59 @agent_ppo2.py:143][0m Total time:       2.48 min
[32m[20221213 23:59:59 @agent_ppo2.py:145][0m 219136 total steps have happened
[32m[20221213 23:59:59 @agent_ppo2.py:121][0m #------------------------ Iteration 4107 --------------------------#
[32m[20221213 23:59:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221213 23:59:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221213 23:59:59 @agent_ppo2.py:185][0m |           0.0051 |          76.7764 |          12.8727 |
[32m[20221213 23:59:59 @agent_ppo2.py:185][0m |           0.0062 |          75.1365 |          12.8134 |
[32m[20221214 00:00:00 @agent_ppo2.py:185][0m |          -0.0070 |          71.2683 |          12.7967 |
[32m[20221214 00:00:00 @agent_ppo2.py:185][0m |          -0.0093 |          70.5204 |          12.8056 |
[32m[20221214 00:00:00 @agent_ppo2.py:185][0m |          -0.0097 |          70.0170 |          12.8278 |
[32m[20221214 00:00:00 @agent_ppo2.py:185][0m |          -0.0116 |          69.6922 |          12.8096 |
[32m[20221214 00:00:00 @agent_ppo2.py:185][0m |          -0.0117 |          69.0776 |          12.7864 |
[32m[20221214 00:00:00 @agent_ppo2.py:185][0m |          -0.0106 |          68.9600 |          12.7808 |
[32m[20221214 00:00:00 @agent_ppo2.py:185][0m |          -0.0106 |          69.1633 |          12.7876 |
[32m[20221214 00:00:00 @agent_ppo2.py:185][0m |          -0.0113 |          68.3820 |          12.7962 |
[32m[20221214 00:00:00 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:00:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 499.42
[32m[20221214 00:00:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 519.36
[32m[20221214 00:00:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 439.00
[32m[20221214 00:00:00 @agent_ppo2.py:143][0m Total time:       2.50 min
[32m[20221214 00:00:00 @agent_ppo2.py:145][0m 221184 total steps have happened
[32m[20221214 00:00:00 @agent_ppo2.py:121][0m #------------------------ Iteration 4108 --------------------------#
[32m[20221214 00:00:01 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:00:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:00:01 @agent_ppo2.py:185][0m |           0.0010 |          72.2529 |          12.6411 |
[32m[20221214 00:00:01 @agent_ppo2.py:185][0m |          -0.0042 |          69.1495 |          12.6330 |
[32m[20221214 00:00:01 @agent_ppo2.py:185][0m |          -0.0126 |          68.5113 |          12.6368 |
[32m[20221214 00:00:01 @agent_ppo2.py:185][0m |          -0.0149 |          67.4151 |          12.6029 |
[32m[20221214 00:00:01 @agent_ppo2.py:185][0m |          -0.0123 |          66.8503 |          12.6301 |
[32m[20221214 00:00:01 @agent_ppo2.py:185][0m |          -0.0119 |          67.1446 |          12.6624 |
[32m[20221214 00:00:01 @agent_ppo2.py:185][0m |          -0.0132 |          65.9577 |          12.6328 |
[32m[20221214 00:00:02 @agent_ppo2.py:185][0m |          -0.0158 |          65.7355 |          12.6759 |
[32m[20221214 00:00:02 @agent_ppo2.py:185][0m |          -0.0179 |          65.5042 |          12.6730 |
[32m[20221214 00:00:02 @agent_ppo2.py:185][0m |          -0.0205 |          65.1415 |          12.6822 |
[32m[20221214 00:00:02 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:00:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 382.52
[32m[20221214 00:00:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 418.22
[32m[20221214 00:00:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 518.80
[32m[20221214 00:00:02 @agent_ppo2.py:143][0m Total time:       2.52 min
[32m[20221214 00:00:02 @agent_ppo2.py:145][0m 223232 total steps have happened
[32m[20221214 00:00:02 @agent_ppo2.py:121][0m #------------------------ Iteration 4109 --------------------------#
[32m[20221214 00:00:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:00:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:00:02 @agent_ppo2.py:185][0m |          -0.0003 |          60.9219 |          12.9443 |
[32m[20221214 00:00:02 @agent_ppo2.py:185][0m |          -0.0065 |          56.3936 |          12.9172 |
[32m[20221214 00:00:02 @agent_ppo2.py:185][0m |          -0.0023 |          54.9120 |          12.9269 |
[32m[20221214 00:00:02 @agent_ppo2.py:185][0m |          -0.0074 |          54.2062 |          12.9230 |
[32m[20221214 00:00:03 @agent_ppo2.py:185][0m |          -0.0070 |          53.7217 |          12.9416 |
[32m[20221214 00:00:03 @agent_ppo2.py:185][0m |          -0.0142 |          53.3378 |          12.9053 |
[32m[20221214 00:00:03 @agent_ppo2.py:185][0m |          -0.0078 |          54.2018 |          12.9176 |
[32m[20221214 00:00:03 @agent_ppo2.py:185][0m |          -0.0147 |          52.7131 |          12.9119 |
[32m[20221214 00:00:03 @agent_ppo2.py:185][0m |          -0.0169 |          52.2756 |          12.9055 |
[32m[20221214 00:00:03 @agent_ppo2.py:185][0m |          -0.0162 |          52.0228 |          12.8873 |
[32m[20221214 00:00:03 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:00:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 427.10
[32m[20221214 00:00:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 485.46
[32m[20221214 00:00:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 436.09
[32m[20221214 00:00:03 @agent_ppo2.py:143][0m Total time:       2.55 min
[32m[20221214 00:00:03 @agent_ppo2.py:145][0m 225280 total steps have happened
[32m[20221214 00:00:03 @agent_ppo2.py:121][0m #------------------------ Iteration 4110 --------------------------#
[32m[20221214 00:00:03 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:00:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:00:04 @agent_ppo2.py:185][0m |           0.0030 |          68.1667 |          12.8297 |
[32m[20221214 00:00:04 @agent_ppo2.py:185][0m |           0.0011 |          64.0192 |          12.7918 |
[32m[20221214 00:00:04 @agent_ppo2.py:185][0m |          -0.0058 |          61.9639 |          12.7128 |
[32m[20221214 00:00:04 @agent_ppo2.py:185][0m |          -0.0074 |          61.5385 |          12.7291 |
[32m[20221214 00:00:04 @agent_ppo2.py:185][0m |          -0.0115 |          60.5419 |          12.6980 |
[32m[20221214 00:00:04 @agent_ppo2.py:185][0m |          -0.0142 |          59.8993 |          12.7154 |
[32m[20221214 00:00:04 @agent_ppo2.py:185][0m |          -0.0109 |          59.7170 |          12.6838 |
[32m[20221214 00:00:04 @agent_ppo2.py:185][0m |          -0.0123 |          59.7172 |          12.6838 |
[32m[20221214 00:00:04 @agent_ppo2.py:185][0m |          -0.0152 |          58.8602 |          12.6914 |
[32m[20221214 00:00:04 @agent_ppo2.py:185][0m |          -0.0155 |          58.5191 |          12.6737 |
[32m[20221214 00:00:04 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:00:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 390.83
[32m[20221214 00:00:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 456.03
[32m[20221214 00:00:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 542.91
[32m[20221214 00:00:05 @agent_ppo2.py:143][0m Total time:       2.57 min
[32m[20221214 00:00:05 @agent_ppo2.py:145][0m 227328 total steps have happened
[32m[20221214 00:00:05 @agent_ppo2.py:121][0m #------------------------ Iteration 4111 --------------------------#
[32m[20221214 00:00:05 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:00:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:00:05 @agent_ppo2.py:185][0m |          -0.0003 |          48.3711 |          12.5219 |
[32m[20221214 00:00:05 @agent_ppo2.py:185][0m |          -0.0042 |          45.4429 |          12.5067 |
[32m[20221214 00:00:05 @agent_ppo2.py:185][0m |          -0.0116 |          44.2975 |          12.4996 |
[32m[20221214 00:00:05 @agent_ppo2.py:185][0m |          -0.0120 |          43.4174 |          12.4992 |
[32m[20221214 00:00:05 @agent_ppo2.py:185][0m |          -0.0018 |          48.8503 |          12.5113 |
[32m[20221214 00:00:05 @agent_ppo2.py:185][0m |          -0.0172 |          42.6565 |          12.4919 |
[32m[20221214 00:00:06 @agent_ppo2.py:185][0m |          -0.0169 |          42.1717 |          12.4840 |
[32m[20221214 00:00:06 @agent_ppo2.py:185][0m |          -0.0172 |          41.7500 |          12.5028 |
[32m[20221214 00:00:06 @agent_ppo2.py:185][0m |          -0.0157 |          41.5474 |          12.4664 |
[32m[20221214 00:00:06 @agent_ppo2.py:185][0m |          -0.0155 |          41.3693 |          12.5200 |
[32m[20221214 00:00:06 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221214 00:00:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 474.11
[32m[20221214 00:00:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 527.24
[32m[20221214 00:00:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 525.17
[32m[20221214 00:00:06 @agent_ppo2.py:143][0m Total time:       2.60 min
[32m[20221214 00:00:06 @agent_ppo2.py:145][0m 229376 total steps have happened
[32m[20221214 00:00:06 @agent_ppo2.py:121][0m #------------------------ Iteration 4112 --------------------------#
[32m[20221214 00:00:06 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:00:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:00:06 @agent_ppo2.py:185][0m |          -0.0013 |          66.7651 |          12.6198 |
[32m[20221214 00:00:07 @agent_ppo2.py:185][0m |          -0.0072 |          63.6470 |          12.6220 |
[32m[20221214 00:00:07 @agent_ppo2.py:185][0m |          -0.0113 |          62.7751 |          12.6178 |
[32m[20221214 00:00:07 @agent_ppo2.py:185][0m |          -0.0150 |          62.0284 |          12.6446 |
[32m[20221214 00:00:07 @agent_ppo2.py:185][0m |          -0.0049 |          63.4711 |          12.6430 |
[32m[20221214 00:00:07 @agent_ppo2.py:185][0m |          -0.0122 |          60.9395 |          12.6192 |
[32m[20221214 00:00:07 @agent_ppo2.py:185][0m |          -0.0152 |          61.0788 |          12.6151 |
[32m[20221214 00:00:07 @agent_ppo2.py:185][0m |          -0.0087 |          60.5794 |          12.6454 |
[32m[20221214 00:00:07 @agent_ppo2.py:185][0m |          -0.0149 |          60.1971 |          12.6394 |
[32m[20221214 00:00:07 @agent_ppo2.py:185][0m |          -0.0136 |          60.5843 |          12.6522 |
[32m[20221214 00:00:07 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:00:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 473.12
[32m[20221214 00:00:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 529.47
[32m[20221214 00:00:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 531.59
[32m[20221214 00:00:07 @agent_ppo2.py:143][0m Total time:       2.62 min
[32m[20221214 00:00:07 @agent_ppo2.py:145][0m 231424 total steps have happened
[32m[20221214 00:00:07 @agent_ppo2.py:121][0m #------------------------ Iteration 4113 --------------------------#
[32m[20221214 00:00:08 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:00:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:00:08 @agent_ppo2.py:185][0m |           0.0024 |          66.4536 |          12.6759 |
[32m[20221214 00:00:08 @agent_ppo2.py:185][0m |          -0.0053 |          63.4950 |          12.6340 |
[32m[20221214 00:00:08 @agent_ppo2.py:185][0m |          -0.0092 |          62.3095 |          12.6382 |
[32m[20221214 00:00:08 @agent_ppo2.py:185][0m |          -0.0099 |          61.6516 |          12.6780 |
[32m[20221214 00:00:08 @agent_ppo2.py:185][0m |          -0.0075 |          61.1988 |          12.6286 |
[32m[20221214 00:00:08 @agent_ppo2.py:185][0m |          -0.0123 |          60.8752 |          12.6525 |
[32m[20221214 00:00:08 @agent_ppo2.py:185][0m |          -0.0107 |          60.6051 |          12.6535 |
[32m[20221214 00:00:09 @agent_ppo2.py:185][0m |          -0.0162 |          60.3774 |          12.6332 |
[32m[20221214 00:00:09 @agent_ppo2.py:185][0m |          -0.0133 |          60.0710 |          12.6432 |
[32m[20221214 00:00:09 @agent_ppo2.py:185][0m |          -0.0131 |          59.6135 |          12.6410 |
[32m[20221214 00:00:09 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:00:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 468.30
[32m[20221214 00:00:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 515.79
[32m[20221214 00:00:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 500.93
[32m[20221214 00:00:09 @agent_ppo2.py:143][0m Total time:       2.64 min
[32m[20221214 00:00:09 @agent_ppo2.py:145][0m 233472 total steps have happened
[32m[20221214 00:00:09 @agent_ppo2.py:121][0m #------------------------ Iteration 4114 --------------------------#
[32m[20221214 00:00:09 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:00:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:00:09 @agent_ppo2.py:185][0m |          -0.0006 |          52.4285 |          12.4054 |
[32m[20221214 00:00:09 @agent_ppo2.py:185][0m |          -0.0059 |          44.6537 |          12.3374 |
[32m[20221214 00:00:09 @agent_ppo2.py:185][0m |          -0.0045 |          42.2624 |          12.3465 |
[32m[20221214 00:00:10 @agent_ppo2.py:185][0m |          -0.0137 |          40.9674 |          12.3384 |
[32m[20221214 00:00:10 @agent_ppo2.py:185][0m |          -0.0125 |          40.0124 |          12.3162 |
[32m[20221214 00:00:10 @agent_ppo2.py:185][0m |          -0.0094 |          39.7801 |          12.2791 |
[32m[20221214 00:00:10 @agent_ppo2.py:185][0m |          -0.0111 |          39.4401 |          12.2597 |
[32m[20221214 00:00:10 @agent_ppo2.py:185][0m |          -0.0166 |          38.2942 |          12.2654 |
[32m[20221214 00:00:10 @agent_ppo2.py:185][0m |          -0.0101 |          38.0258 |          12.2589 |
[32m[20221214 00:00:10 @agent_ppo2.py:185][0m |          -0.0156 |          37.6238 |          12.2504 |
[32m[20221214 00:00:10 @agent_ppo2.py:130][0m Policy update time: 1.18 s
[32m[20221214 00:00:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 472.40
[32m[20221214 00:00:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 542.69
[32m[20221214 00:00:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 469.80
[32m[20221214 00:00:10 @agent_ppo2.py:143][0m Total time:       2.67 min
[32m[20221214 00:00:10 @agent_ppo2.py:145][0m 235520 total steps have happened
[32m[20221214 00:00:10 @agent_ppo2.py:121][0m #------------------------ Iteration 4115 --------------------------#
[32m[20221214 00:00:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:00:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:00:11 @agent_ppo2.py:185][0m |          -0.0015 |          42.7544 |          12.2574 |
[32m[20221214 00:00:11 @agent_ppo2.py:185][0m |          -0.0083 |          38.3526 |          12.2756 |
[32m[20221214 00:00:11 @agent_ppo2.py:185][0m |          -0.0057 |          36.8668 |          12.2703 |
[32m[20221214 00:00:11 @agent_ppo2.py:185][0m |          -0.0084 |          35.5536 |          12.2684 |
[32m[20221214 00:00:11 @agent_ppo2.py:185][0m |          -0.0084 |          34.4537 |          12.2274 |
[32m[20221214 00:00:11 @agent_ppo2.py:185][0m |          -0.0167 |          33.6903 |          12.1857 |
[32m[20221214 00:00:11 @agent_ppo2.py:185][0m |          -0.0044 |          35.8835 |          12.2010 |
[32m[20221214 00:00:11 @agent_ppo2.py:185][0m |          -0.0144 |          32.3941 |          12.1559 |
[32m[20221214 00:00:12 @agent_ppo2.py:185][0m |          -0.0178 |          31.8887 |          12.1431 |
[32m[20221214 00:00:12 @agent_ppo2.py:185][0m |          -0.0138 |          32.7273 |          12.1312 |
[32m[20221214 00:00:12 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:00:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 466.00
[32m[20221214 00:00:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 532.78
[32m[20221214 00:00:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 369.92
[32m[20221214 00:00:12 @agent_ppo2.py:143][0m Total time:       2.69 min
[32m[20221214 00:00:12 @agent_ppo2.py:145][0m 237568 total steps have happened
[32m[20221214 00:00:12 @agent_ppo2.py:121][0m #------------------------ Iteration 4116 --------------------------#
[32m[20221214 00:00:12 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:00:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:00:12 @agent_ppo2.py:185][0m |          -0.0028 |          62.7749 |          12.4719 |
[32m[20221214 00:00:12 @agent_ppo2.py:185][0m |           0.0045 |          64.0245 |          12.5121 |
[32m[20221214 00:00:12 @agent_ppo2.py:185][0m |          -0.0072 |          59.5584 |          12.5360 |
[32m[20221214 00:00:12 @agent_ppo2.py:185][0m |          -0.0010 |          64.9450 |          12.5035 |
[32m[20221214 00:00:13 @agent_ppo2.py:185][0m |          -0.0116 |          59.1299 |          12.5119 |
[32m[20221214 00:00:13 @agent_ppo2.py:185][0m |          -0.0136 |          58.2363 |          12.5047 |
[32m[20221214 00:00:13 @agent_ppo2.py:185][0m |          -0.0004 |          66.1386 |          12.5155 |
[32m[20221214 00:00:13 @agent_ppo2.py:185][0m |          -0.0136 |          57.9867 |          12.5677 |
[32m[20221214 00:00:13 @agent_ppo2.py:185][0m |          -0.0180 |          57.7703 |          12.5801 |
[32m[20221214 00:00:13 @agent_ppo2.py:185][0m |          -0.0155 |          57.7197 |          12.5775 |
[32m[20221214 00:00:13 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:00:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 456.21
[32m[20221214 00:00:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 482.26
[32m[20221214 00:00:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 342.11
[32m[20221214 00:00:13 @agent_ppo2.py:143][0m Total time:       2.71 min
[32m[20221214 00:00:13 @agent_ppo2.py:145][0m 239616 total steps have happened
[32m[20221214 00:00:13 @agent_ppo2.py:121][0m #------------------------ Iteration 4117 --------------------------#
[32m[20221214 00:00:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:00:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:00:13 @agent_ppo2.py:185][0m |           0.0034 |          43.6147 |          12.5199 |
[32m[20221214 00:00:14 @agent_ppo2.py:185][0m |          -0.0035 |          37.3668 |          12.4824 |
[32m[20221214 00:00:14 @agent_ppo2.py:185][0m |          -0.0073 |          34.8896 |          12.4834 |
[32m[20221214 00:00:14 @agent_ppo2.py:185][0m |          -0.0108 |          33.8909 |          12.4909 |
[32m[20221214 00:00:14 @agent_ppo2.py:185][0m |          -0.0111 |          33.0083 |          12.4523 |
[32m[20221214 00:00:14 @agent_ppo2.py:185][0m |          -0.0180 |          32.2611 |          12.4816 |
[32m[20221214 00:00:14 @agent_ppo2.py:185][0m |          -0.0016 |          33.7511 |          12.4636 |
[32m[20221214 00:00:14 @agent_ppo2.py:185][0m |          -0.0173 |          31.2838 |          12.4648 |
[32m[20221214 00:00:14 @agent_ppo2.py:185][0m |          -0.0172 |          30.3890 |          12.4641 |
[32m[20221214 00:00:14 @agent_ppo2.py:185][0m |          -0.0172 |          30.0117 |          12.4639 |
[32m[20221214 00:00:14 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:00:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 402.58
[32m[20221214 00:00:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 459.59
[32m[20221214 00:00:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 449.24
[32m[20221214 00:00:14 @agent_ppo2.py:143][0m Total time:       2.74 min
[32m[20221214 00:00:14 @agent_ppo2.py:145][0m 241664 total steps have happened
[32m[20221214 00:00:14 @agent_ppo2.py:121][0m #------------------------ Iteration 4118 --------------------------#
[32m[20221214 00:00:15 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:00:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:00:15 @agent_ppo2.py:185][0m |           0.0023 |          73.1967 |          12.4145 |
[32m[20221214 00:00:15 @agent_ppo2.py:185][0m |           0.0023 |          70.5279 |          12.4272 |
[32m[20221214 00:00:15 @agent_ppo2.py:185][0m |           0.0003 |          69.7205 |          12.4625 |
[32m[20221214 00:00:15 @agent_ppo2.py:185][0m |          -0.0143 |          62.9767 |          12.4430 |
[32m[20221214 00:00:15 @agent_ppo2.py:185][0m |          -0.0121 |          61.6952 |          12.4114 |
[32m[20221214 00:00:15 @agent_ppo2.py:185][0m |          -0.0096 |          61.6507 |          12.4204 |
[32m[20221214 00:00:15 @agent_ppo2.py:185][0m |          -0.0147 |          60.4504 |          12.4337 |
[32m[20221214 00:00:16 @agent_ppo2.py:185][0m |          -0.0194 |          59.8951 |          12.4183 |
[32m[20221214 00:00:16 @agent_ppo2.py:185][0m |          -0.0154 |          59.5737 |          12.4136 |
[32m[20221214 00:00:16 @agent_ppo2.py:185][0m |          -0.0176 |          59.2279 |          12.3937 |
[32m[20221214 00:00:16 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:00:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 485.59
[32m[20221214 00:00:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 561.74
[32m[20221214 00:00:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 355.67
[32m[20221214 00:00:16 @agent_ppo2.py:143][0m Total time:       2.76 min
[32m[20221214 00:00:16 @agent_ppo2.py:145][0m 243712 total steps have happened
[32m[20221214 00:00:16 @agent_ppo2.py:121][0m #------------------------ Iteration 4119 --------------------------#
[32m[20221214 00:00:16 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:00:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:00:16 @agent_ppo2.py:185][0m |          -0.0010 |          65.1273 |          12.4534 |
[32m[20221214 00:00:16 @agent_ppo2.py:185][0m |          -0.0060 |          63.1362 |          12.4404 |
[32m[20221214 00:00:16 @agent_ppo2.py:185][0m |          -0.0076 |          62.3511 |          12.4267 |
[32m[20221214 00:00:16 @agent_ppo2.py:185][0m |          -0.0081 |          61.7853 |          12.4602 |
[32m[20221214 00:00:17 @agent_ppo2.py:185][0m |          -0.0134 |          61.2818 |          12.4551 |
[32m[20221214 00:00:17 @agent_ppo2.py:185][0m |          -0.0104 |          61.1734 |          12.4650 |
[32m[20221214 00:00:17 @agent_ppo2.py:185][0m |          -0.0008 |          66.6597 |          12.4527 |
[32m[20221214 00:00:17 @agent_ppo2.py:185][0m |          -0.0086 |          60.8736 |          12.4475 |
[32m[20221214 00:00:17 @agent_ppo2.py:185][0m |          -0.0104 |          60.3839 |          12.4416 |
[32m[20221214 00:00:17 @agent_ppo2.py:185][0m |          -0.0045 |          62.9460 |          12.4510 |
[32m[20221214 00:00:17 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:00:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 498.12
[32m[20221214 00:00:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 528.48
[32m[20221214 00:00:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 437.73
[32m[20221214 00:00:17 @agent_ppo2.py:143][0m Total time:       2.78 min
[32m[20221214 00:00:17 @agent_ppo2.py:145][0m 245760 total steps have happened
[32m[20221214 00:00:17 @agent_ppo2.py:121][0m #------------------------ Iteration 4120 --------------------------#
[32m[20221214 00:00:17 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:00:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:00:18 @agent_ppo2.py:185][0m |          -0.0054 |          43.9112 |          12.6561 |
[32m[20221214 00:00:18 @agent_ppo2.py:185][0m |          -0.0010 |          34.4568 |          12.6654 |
[32m[20221214 00:00:18 @agent_ppo2.py:185][0m |          -0.0045 |          32.1304 |          12.6766 |
[32m[20221214 00:00:18 @agent_ppo2.py:185][0m |          -0.0129 |          30.6451 |          12.6787 |
[32m[20221214 00:00:18 @agent_ppo2.py:185][0m |          -0.0119 |          29.5864 |          12.6717 |
[32m[20221214 00:00:18 @agent_ppo2.py:185][0m |          -0.0176 |          28.8925 |          12.6624 |
[32m[20221214 00:00:18 @agent_ppo2.py:185][0m |          -0.0173 |          28.2509 |          12.6786 |
[32m[20221214 00:00:18 @agent_ppo2.py:185][0m |          -0.0139 |          28.2795 |          12.6579 |
[32m[20221214 00:00:18 @agent_ppo2.py:185][0m |          -0.0119 |          27.0444 |          12.6471 |
[32m[20221214 00:00:18 @agent_ppo2.py:185][0m |          -0.0203 |          26.5491 |          12.6828 |
[32m[20221214 00:00:18 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:00:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 368.29
[32m[20221214 00:00:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 486.48
[32m[20221214 00:00:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 316.04
[32m[20221214 00:00:19 @agent_ppo2.py:143][0m Total time:       2.80 min
[32m[20221214 00:00:19 @agent_ppo2.py:145][0m 247808 total steps have happened
[32m[20221214 00:00:19 @agent_ppo2.py:121][0m #------------------------ Iteration 4121 --------------------------#
[32m[20221214 00:00:19 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:00:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:00:19 @agent_ppo2.py:185][0m |          -0.0030 |          37.7181 |          12.4553 |
[32m[20221214 00:00:19 @agent_ppo2.py:185][0m |           0.0021 |          33.6834 |          12.5461 |
[32m[20221214 00:00:19 @agent_ppo2.py:185][0m |          -0.0160 |          30.5270 |          12.5209 |
[32m[20221214 00:00:19 @agent_ppo2.py:185][0m |          -0.0133 |          29.2174 |          12.5383 |
[32m[20221214 00:00:19 @agent_ppo2.py:185][0m |          -0.0163 |          28.4553 |          12.5171 |
[32m[20221214 00:00:19 @agent_ppo2.py:185][0m |          -0.0109 |          28.3898 |          12.5316 |
[32m[20221214 00:00:19 @agent_ppo2.py:185][0m |          -0.0167 |          27.1890 |          12.5175 |
[32m[20221214 00:00:20 @agent_ppo2.py:185][0m |          -0.0174 |          26.8386 |          12.5092 |
[32m[20221214 00:00:20 @agent_ppo2.py:185][0m |          -0.0171 |          26.4987 |          12.5060 |
[32m[20221214 00:00:20 @agent_ppo2.py:185][0m |          -0.0197 |          26.1348 |          12.4844 |
[32m[20221214 00:00:20 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:00:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 420.55
[32m[20221214 00:00:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 470.40
[32m[20221214 00:00:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 243.22
[32m[20221214 00:00:20 @agent_ppo2.py:143][0m Total time:       2.83 min
[32m[20221214 00:00:20 @agent_ppo2.py:145][0m 249856 total steps have happened
[32m[20221214 00:00:20 @agent_ppo2.py:121][0m #------------------------ Iteration 4122 --------------------------#
[32m[20221214 00:00:20 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:00:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:00:20 @agent_ppo2.py:185][0m |          -0.0026 |          66.2776 |          12.3726 |
[32m[20221214 00:00:20 @agent_ppo2.py:185][0m |          -0.0041 |          58.1943 |          12.3798 |
[32m[20221214 00:00:20 @agent_ppo2.py:185][0m |          -0.0107 |          55.6553 |          12.3950 |
[32m[20221214 00:00:21 @agent_ppo2.py:185][0m |          -0.0139 |          54.6094 |          12.3693 |
[32m[20221214 00:00:21 @agent_ppo2.py:185][0m |           0.0069 |          61.6517 |          12.3568 |
[32m[20221214 00:00:21 @agent_ppo2.py:185][0m |          -0.0177 |          53.8193 |          12.3647 |
[32m[20221214 00:00:21 @agent_ppo2.py:185][0m |          -0.0135 |          52.3835 |          12.3448 |
[32m[20221214 00:00:21 @agent_ppo2.py:185][0m |          -0.0138 |          51.7163 |          12.3522 |
[32m[20221214 00:00:21 @agent_ppo2.py:185][0m |          -0.0169 |          51.6577 |          12.3629 |
[32m[20221214 00:00:21 @agent_ppo2.py:185][0m |          -0.0176 |          51.1404 |          12.3415 |
[32m[20221214 00:00:21 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:00:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 377.08
[32m[20221214 00:00:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 413.06
[32m[20221214 00:00:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 269.95
[32m[20221214 00:00:21 @agent_ppo2.py:143][0m Total time:       2.85 min
[32m[20221214 00:00:21 @agent_ppo2.py:145][0m 251904 total steps have happened
[32m[20221214 00:00:21 @agent_ppo2.py:121][0m #------------------------ Iteration 4123 --------------------------#
[32m[20221214 00:00:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:00:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:00:22 @agent_ppo2.py:185][0m |          -0.0007 |          46.1281 |          12.2161 |
[32m[20221214 00:00:22 @agent_ppo2.py:185][0m |          -0.0000 |          40.9845 |          12.1957 |
[32m[20221214 00:00:22 @agent_ppo2.py:185][0m |          -0.0072 |          39.1342 |          12.2320 |
[32m[20221214 00:00:22 @agent_ppo2.py:185][0m |          -0.0077 |          37.8401 |          12.2341 |
[32m[20221214 00:00:22 @agent_ppo2.py:185][0m |          -0.0064 |          37.5392 |          12.2674 |
[32m[20221214 00:00:22 @agent_ppo2.py:185][0m |          -0.0165 |          36.1667 |          12.2807 |
[32m[20221214 00:00:22 @agent_ppo2.py:185][0m |          -0.0182 |          35.4317 |          12.2818 |
[32m[20221214 00:00:22 @agent_ppo2.py:185][0m |          -0.0237 |          34.7598 |          12.2675 |
[32m[20221214 00:00:22 @agent_ppo2.py:185][0m |          -0.0101 |          34.8746 |          12.3105 |
[32m[20221214 00:00:22 @agent_ppo2.py:185][0m |          -0.0195 |          34.3284 |          12.3035 |
[32m[20221214 00:00:22 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:00:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 376.39
[32m[20221214 00:00:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 410.75
[32m[20221214 00:00:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 421.47
[32m[20221214 00:00:23 @agent_ppo2.py:143][0m Total time:       2.87 min
[32m[20221214 00:00:23 @agent_ppo2.py:145][0m 253952 total steps have happened
[32m[20221214 00:00:23 @agent_ppo2.py:121][0m #------------------------ Iteration 4124 --------------------------#
[32m[20221214 00:00:23 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:00:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:00:23 @agent_ppo2.py:185][0m |           0.0057 |          45.9161 |          12.2987 |
[32m[20221214 00:00:23 @agent_ppo2.py:185][0m |          -0.0062 |          37.6054 |          12.3311 |
[32m[20221214 00:00:23 @agent_ppo2.py:185][0m |          -0.0075 |          36.0179 |          12.3254 |
[32m[20221214 00:00:23 @agent_ppo2.py:185][0m |          -0.0156 |          34.9642 |          12.3326 |
[32m[20221214 00:00:23 @agent_ppo2.py:185][0m |          -0.0125 |          34.1515 |          12.3353 |
[32m[20221214 00:00:23 @agent_ppo2.py:185][0m |          -0.0120 |          33.4121 |          12.3232 |
[32m[20221214 00:00:23 @agent_ppo2.py:185][0m |          -0.0188 |          32.9079 |          12.3268 |
[32m[20221214 00:00:24 @agent_ppo2.py:185][0m |          -0.0158 |          32.8213 |          12.3605 |
[32m[20221214 00:00:24 @agent_ppo2.py:185][0m |          -0.0177 |          32.1912 |          12.3763 |
[32m[20221214 00:00:24 @agent_ppo2.py:185][0m |          -0.0182 |          31.5790 |          12.3463 |
[32m[20221214 00:00:24 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:00:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 305.73
[32m[20221214 00:00:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 452.79
[32m[20221214 00:00:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 497.85
[32m[20221214 00:00:24 @agent_ppo2.py:143][0m Total time:       2.89 min
[32m[20221214 00:00:24 @agent_ppo2.py:145][0m 256000 total steps have happened
[32m[20221214 00:00:24 @agent_ppo2.py:121][0m #------------------------ Iteration 4125 --------------------------#
[32m[20221214 00:00:24 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:00:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:00:24 @agent_ppo2.py:185][0m |           0.0005 |          59.3793 |          12.6252 |
[32m[20221214 00:00:24 @agent_ppo2.py:185][0m |          -0.0072 |          55.9244 |          12.6538 |
[32m[20221214 00:00:24 @agent_ppo2.py:185][0m |           0.0031 |          55.7395 |          12.6591 |
[32m[20221214 00:00:25 @agent_ppo2.py:185][0m |          -0.0055 |          53.5448 |          12.6804 |
[32m[20221214 00:00:25 @agent_ppo2.py:185][0m |          -0.0098 |          52.9390 |          12.6872 |
[32m[20221214 00:00:25 @agent_ppo2.py:185][0m |          -0.0102 |          52.8191 |          12.7172 |
[32m[20221214 00:00:25 @agent_ppo2.py:185][0m |          -0.0123 |          52.4967 |          12.6982 |
[32m[20221214 00:00:25 @agent_ppo2.py:185][0m |          -0.0096 |          52.1490 |          12.7135 |
[32m[20221214 00:00:25 @agent_ppo2.py:185][0m |           0.0043 |          59.4997 |          12.7344 |
[32m[20221214 00:00:25 @agent_ppo2.py:185][0m |          -0.0158 |          52.0185 |          12.7599 |
[32m[20221214 00:00:25 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:00:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 487.03
[32m[20221214 00:00:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 520.98
[32m[20221214 00:00:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 346.14
[32m[20221214 00:00:25 @agent_ppo2.py:143][0m Total time:       2.91 min
[32m[20221214 00:00:25 @agent_ppo2.py:145][0m 258048 total steps have happened
[32m[20221214 00:00:25 @agent_ppo2.py:121][0m #------------------------ Iteration 4126 --------------------------#
[32m[20221214 00:00:25 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:00:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:00:26 @agent_ppo2.py:185][0m |           0.0021 |          73.5546 |          12.6205 |
[32m[20221214 00:00:26 @agent_ppo2.py:185][0m |          -0.0086 |          67.8961 |          12.6911 |
[32m[20221214 00:00:26 @agent_ppo2.py:185][0m |          -0.0113 |          65.8305 |          12.6997 |
[32m[20221214 00:00:26 @agent_ppo2.py:185][0m |          -0.0099 |          64.8490 |          12.6758 |
[32m[20221214 00:00:26 @agent_ppo2.py:185][0m |          -0.0117 |          64.0465 |          12.6875 |
[32m[20221214 00:00:26 @agent_ppo2.py:185][0m |          -0.0134 |          63.0942 |          12.7027 |
[32m[20221214 00:00:26 @agent_ppo2.py:185][0m |          -0.0124 |          63.3939 |          12.6976 |
[32m[20221214 00:00:26 @agent_ppo2.py:185][0m |          -0.0165 |          62.1265 |          12.7026 |
[32m[20221214 00:00:26 @agent_ppo2.py:185][0m |          -0.0119 |          62.8494 |          12.6914 |
[32m[20221214 00:00:26 @agent_ppo2.py:185][0m |          -0.0171 |          61.4870 |          12.6864 |
[32m[20221214 00:00:26 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:00:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 411.61
[32m[20221214 00:00:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 491.21
[32m[20221214 00:00:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 388.48
[32m[20221214 00:00:27 @agent_ppo2.py:143][0m Total time:       2.94 min
[32m[20221214 00:00:27 @agent_ppo2.py:145][0m 260096 total steps have happened
[32m[20221214 00:00:27 @agent_ppo2.py:121][0m #------------------------ Iteration 4127 --------------------------#
[32m[20221214 00:00:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:00:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:00:27 @agent_ppo2.py:185][0m |          -0.0013 |          46.3755 |          12.6162 |
[32m[20221214 00:00:27 @agent_ppo2.py:185][0m |          -0.0101 |          41.8304 |          12.6377 |
[32m[20221214 00:00:27 @agent_ppo2.py:185][0m |          -0.0085 |          40.2859 |          12.5966 |
[32m[20221214 00:00:27 @agent_ppo2.py:185][0m |          -0.0064 |          40.0960 |          12.6545 |
[32m[20221214 00:00:27 @agent_ppo2.py:185][0m |          -0.0152 |          38.9505 |          12.6160 |
[32m[20221214 00:00:27 @agent_ppo2.py:185][0m |          -0.0185 |          38.2723 |          12.6096 |
[32m[20221214 00:00:28 @agent_ppo2.py:185][0m |          -0.0173 |          37.7142 |          12.6457 |
[32m[20221214 00:00:28 @agent_ppo2.py:185][0m |          -0.0209 |          37.2524 |          12.6261 |
[32m[20221214 00:00:28 @agent_ppo2.py:185][0m |          -0.0180 |          36.9387 |          12.6102 |
[32m[20221214 00:00:28 @agent_ppo2.py:185][0m |          -0.0199 |          36.5595 |          12.6336 |
[32m[20221214 00:00:28 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:00:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 367.05
[32m[20221214 00:00:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 446.49
[32m[20221214 00:00:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 497.33
[32m[20221214 00:00:28 @agent_ppo2.py:143][0m Total time:       2.96 min
[32m[20221214 00:00:28 @agent_ppo2.py:145][0m 262144 total steps have happened
[32m[20221214 00:00:28 @agent_ppo2.py:121][0m #------------------------ Iteration 4128 --------------------------#
[32m[20221214 00:00:28 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221214 00:00:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:00:28 @agent_ppo2.py:185][0m |          -0.0023 |          57.3738 |          12.6726 |
[32m[20221214 00:00:29 @agent_ppo2.py:185][0m |          -0.0077 |          53.5270 |          12.6474 |
[32m[20221214 00:00:29 @agent_ppo2.py:185][0m |          -0.0142 |          52.1370 |          12.6057 |
[32m[20221214 00:00:29 @agent_ppo2.py:185][0m |          -0.0147 |          50.7969 |          12.5964 |
[32m[20221214 00:00:29 @agent_ppo2.py:185][0m |          -0.0133 |          49.7594 |          12.5856 |
[32m[20221214 00:00:29 @agent_ppo2.py:185][0m |          -0.0140 |          49.4078 |          12.6317 |
[32m[20221214 00:00:29 @agent_ppo2.py:185][0m |          -0.0169 |          48.6497 |          12.5967 |
[32m[20221214 00:00:29 @agent_ppo2.py:185][0m |          -0.0072 |          53.1898 |          12.5696 |
[32m[20221214 00:00:29 @agent_ppo2.py:185][0m |          -0.0174 |          48.0730 |          12.6056 |
[32m[20221214 00:00:29 @agent_ppo2.py:185][0m |          -0.0169 |          47.5234 |          12.5405 |
[32m[20221214 00:00:29 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:00:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 395.32
[32m[20221214 00:00:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 457.45
[32m[20221214 00:00:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 540.09
[32m[20221214 00:00:29 @agent_ppo2.py:143][0m Total time:       2.99 min
[32m[20221214 00:00:29 @agent_ppo2.py:145][0m 264192 total steps have happened
[32m[20221214 00:00:29 @agent_ppo2.py:121][0m #------------------------ Iteration 4129 --------------------------#
[32m[20221214 00:00:30 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:00:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:00:30 @agent_ppo2.py:185][0m |           0.0051 |          55.3097 |          12.6234 |
[32m[20221214 00:00:30 @agent_ppo2.py:185][0m |          -0.0040 |          50.8951 |          12.6048 |
[32m[20221214 00:00:30 @agent_ppo2.py:185][0m |          -0.0087 |          49.3087 |          12.6464 |
[32m[20221214 00:00:30 @agent_ppo2.py:185][0m |          -0.0117 |          48.1115 |          12.6547 |
[32m[20221214 00:00:30 @agent_ppo2.py:185][0m |          -0.0134 |          47.1829 |          12.6612 |
[32m[20221214 00:00:30 @agent_ppo2.py:185][0m |          -0.0148 |          46.5854 |          12.6667 |
[32m[20221214 00:00:31 @agent_ppo2.py:185][0m |          -0.0080 |          48.5762 |          12.6447 |
[32m[20221214 00:00:31 @agent_ppo2.py:185][0m |          -0.0169 |          45.5302 |          12.6432 |
[32m[20221214 00:00:31 @agent_ppo2.py:185][0m |          -0.0139 |          44.9049 |          12.6559 |
[32m[20221214 00:00:31 @agent_ppo2.py:185][0m |          -0.0158 |          44.4794 |          12.6786 |
[32m[20221214 00:00:31 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221214 00:00:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 412.29
[32m[20221214 00:00:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 472.46
[32m[20221214 00:00:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 503.98
[32m[20221214 00:00:31 @agent_ppo2.py:143][0m Total time:       3.01 min
[32m[20221214 00:00:31 @agent_ppo2.py:145][0m 266240 total steps have happened
[32m[20221214 00:00:31 @agent_ppo2.py:121][0m #------------------------ Iteration 4130 --------------------------#
[32m[20221214 00:00:31 @agent_ppo2.py:127][0m Sampling time: 0.24 s by 5 slaves
[32m[20221214 00:00:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:00:31 @agent_ppo2.py:185][0m |           0.0019 |          62.1754 |          12.4994 |
[32m[20221214 00:00:31 @agent_ppo2.py:185][0m |          -0.0064 |          60.4183 |          12.4387 |
[32m[20221214 00:00:32 @agent_ppo2.py:185][0m |          -0.0070 |          59.9414 |          12.4392 |
[32m[20221214 00:00:32 @agent_ppo2.py:185][0m |          -0.0069 |          59.8356 |          12.4687 |
[32m[20221214 00:00:32 @agent_ppo2.py:185][0m |           0.0054 |          67.0522 |          12.4791 |
[32m[20221214 00:00:32 @agent_ppo2.py:185][0m |          -0.0000 |          61.7487 |          12.4750 |
[32m[20221214 00:00:32 @agent_ppo2.py:185][0m |          -0.0079 |          59.0766 |          12.4815 |
[32m[20221214 00:00:32 @agent_ppo2.py:185][0m |          -0.0055 |          60.1126 |          12.4768 |
[32m[20221214 00:00:32 @agent_ppo2.py:185][0m |           0.0091 |          64.4571 |          12.4998 |
[32m[20221214 00:00:32 @agent_ppo2.py:185][0m |          -0.0072 |          59.8157 |          12.4169 |
[32m[20221214 00:00:32 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:00:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 483.18
[32m[20221214 00:00:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 495.10
[32m[20221214 00:00:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 514.63
[32m[20221214 00:00:32 @agent_ppo2.py:143][0m Total time:       3.03 min
[32m[20221214 00:00:32 @agent_ppo2.py:145][0m 268288 total steps have happened
[32m[20221214 00:00:32 @agent_ppo2.py:121][0m #------------------------ Iteration 4131 --------------------------#
[32m[20221214 00:00:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:00:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:00:33 @agent_ppo2.py:185][0m |           0.0069 |          62.5366 |          12.7564 |
[32m[20221214 00:00:33 @agent_ppo2.py:185][0m |          -0.0085 |          54.2293 |          12.7162 |
[32m[20221214 00:00:33 @agent_ppo2.py:185][0m |          -0.0113 |          53.0203 |          12.7182 |
[32m[20221214 00:00:33 @agent_ppo2.py:185][0m |          -0.0108 |          52.2159 |          12.6862 |
[32m[20221214 00:00:33 @agent_ppo2.py:185][0m |          -0.0142 |          51.4917 |          12.6914 |
[32m[20221214 00:00:33 @agent_ppo2.py:185][0m |          -0.0135 |          51.0946 |          12.7121 |
[32m[20221214 00:00:33 @agent_ppo2.py:185][0m |          -0.0182 |          50.4491 |          12.6989 |
[32m[20221214 00:00:33 @agent_ppo2.py:185][0m |          -0.0171 |          50.0164 |          12.7211 |
[32m[20221214 00:00:33 @agent_ppo2.py:185][0m |          -0.0176 |          49.4872 |          12.6918 |
[32m[20221214 00:00:34 @agent_ppo2.py:185][0m |          -0.0109 |          49.5816 |          12.7139 |
[32m[20221214 00:00:34 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:00:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 354.93
[32m[20221214 00:00:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 392.79
[32m[20221214 00:00:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 507.17
[32m[20221214 00:00:34 @agent_ppo2.py:143][0m Total time:       3.06 min
[32m[20221214 00:00:34 @agent_ppo2.py:145][0m 270336 total steps have happened
[32m[20221214 00:00:34 @agent_ppo2.py:121][0m #------------------------ Iteration 4132 --------------------------#
[32m[20221214 00:00:34 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:00:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:00:34 @agent_ppo2.py:185][0m |           0.0013 |          13.6393 |          12.7684 |
[32m[20221214 00:00:34 @agent_ppo2.py:185][0m |           0.0002 |          12.5010 |          12.8323 |
[32m[20221214 00:00:34 @agent_ppo2.py:185][0m |          -0.0018 |          12.4466 |          12.7755 |
[32m[20221214 00:00:34 @agent_ppo2.py:185][0m |           0.0003 |          12.3635 |          12.7901 |
[32m[20221214 00:00:34 @agent_ppo2.py:185][0m |          -0.0010 |          12.3554 |          12.8261 |
[32m[20221214 00:00:35 @agent_ppo2.py:185][0m |          -0.0009 |          12.3332 |          12.8397 |
[32m[20221214 00:00:35 @agent_ppo2.py:185][0m |          -0.0047 |          12.2966 |          12.8233 |
[32m[20221214 00:00:35 @agent_ppo2.py:185][0m |           0.0025 |          12.4558 |          12.7914 |
[32m[20221214 00:00:35 @agent_ppo2.py:185][0m |          -0.0024 |          12.2674 |          12.8263 |
[32m[20221214 00:00:35 @agent_ppo2.py:185][0m |          -0.0038 |          12.2283 |          12.7868 |
[32m[20221214 00:00:35 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:00:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221214 00:00:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221214 00:00:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 474.75
[32m[20221214 00:00:35 @agent_ppo2.py:143][0m Total time:       3.08 min
[32m[20221214 00:00:35 @agent_ppo2.py:145][0m 272384 total steps have happened
[32m[20221214 00:00:35 @agent_ppo2.py:121][0m #------------------------ Iteration 4133 --------------------------#
[32m[20221214 00:00:35 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:00:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:00:36 @agent_ppo2.py:185][0m |          -0.0016 |          50.2166 |          12.7277 |
[32m[20221214 00:00:36 @agent_ppo2.py:185][0m |          -0.0050 |          47.4854 |          12.7180 |
[32m[20221214 00:00:36 @agent_ppo2.py:185][0m |          -0.0068 |          46.0165 |          12.7589 |
[32m[20221214 00:00:36 @agent_ppo2.py:185][0m |          -0.0091 |          45.1893 |          12.7257 |
[32m[20221214 00:00:36 @agent_ppo2.py:185][0m |          -0.0069 |          44.6448 |          12.7575 |
[32m[20221214 00:00:36 @agent_ppo2.py:185][0m |          -0.0157 |          44.1064 |          12.7576 |
[32m[20221214 00:00:36 @agent_ppo2.py:185][0m |          -0.0071 |          47.4141 |          12.7440 |
[32m[20221214 00:00:36 @agent_ppo2.py:185][0m |          -0.0114 |          43.6259 |          12.7528 |
[32m[20221214 00:00:36 @agent_ppo2.py:185][0m |          -0.0124 |          43.5387 |          12.7553 |
[32m[20221214 00:00:36 @agent_ppo2.py:185][0m |          -0.0154 |          42.9801 |          12.7610 |
[32m[20221214 00:00:36 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221214 00:00:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 437.34
[32m[20221214 00:00:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 486.36
[32m[20221214 00:00:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 424.86
[32m[20221214 00:00:37 @agent_ppo2.py:143][0m Total time:       3.10 min
[32m[20221214 00:00:37 @agent_ppo2.py:145][0m 274432 total steps have happened
[32m[20221214 00:00:37 @agent_ppo2.py:121][0m #------------------------ Iteration 4134 --------------------------#
[32m[20221214 00:00:37 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:00:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:00:37 @agent_ppo2.py:185][0m |           0.0048 |          68.4265 |          12.5090 |
[32m[20221214 00:00:37 @agent_ppo2.py:185][0m |          -0.0065 |          66.5325 |          12.4584 |
[32m[20221214 00:00:37 @agent_ppo2.py:185][0m |          -0.0093 |          65.6343 |          12.4473 |
[32m[20221214 00:00:37 @agent_ppo2.py:185][0m |          -0.0059 |          65.2482 |          12.4038 |
[32m[20221214 00:00:37 @agent_ppo2.py:185][0m |          -0.0138 |          64.1366 |          12.4465 |
[32m[20221214 00:00:38 @agent_ppo2.py:185][0m |          -0.0153 |          64.0117 |          12.4376 |
[32m[20221214 00:00:38 @agent_ppo2.py:185][0m |          -0.0154 |          63.5342 |          12.4194 |
[32m[20221214 00:00:38 @agent_ppo2.py:185][0m |          -0.0167 |          63.3998 |          12.4366 |
[32m[20221214 00:00:38 @agent_ppo2.py:185][0m |          -0.0176 |          63.1197 |          12.4561 |
[32m[20221214 00:00:38 @agent_ppo2.py:185][0m |          -0.0122 |          64.9133 |          12.4236 |
[32m[20221214 00:00:38 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221214 00:00:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 443.64
[32m[20221214 00:00:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 491.48
[32m[20221214 00:00:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 515.89
[32m[20221214 00:00:38 @agent_ppo2.py:143][0m Total time:       3.13 min
[32m[20221214 00:00:38 @agent_ppo2.py:145][0m 276480 total steps have happened
[32m[20221214 00:00:38 @agent_ppo2.py:121][0m #------------------------ Iteration 4135 --------------------------#
[32m[20221214 00:00:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:00:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:00:38 @agent_ppo2.py:185][0m |           0.0015 |          71.0998 |          12.7713 |
[32m[20221214 00:00:39 @agent_ppo2.py:185][0m |          -0.0065 |          66.6087 |          12.7514 |
[32m[20221214 00:00:39 @agent_ppo2.py:185][0m |          -0.0103 |          64.7042 |          12.7687 |
[32m[20221214 00:00:39 @agent_ppo2.py:185][0m |          -0.0053 |          65.5188 |          12.7879 |
[32m[20221214 00:00:39 @agent_ppo2.py:185][0m |          -0.0080 |          64.3434 |          12.7677 |
[32m[20221214 00:00:39 @agent_ppo2.py:185][0m |          -0.0037 |          69.6454 |          12.7593 |
[32m[20221214 00:00:39 @agent_ppo2.py:185][0m |          -0.0129 |          61.9092 |          12.7806 |
[32m[20221214 00:00:39 @agent_ppo2.py:185][0m |          -0.0101 |          63.4685 |          12.7701 |
[32m[20221214 00:00:39 @agent_ppo2.py:185][0m |          -0.0117 |          61.4721 |          12.7668 |
[32m[20221214 00:00:39 @agent_ppo2.py:185][0m |          -0.0144 |          61.3792 |          12.7826 |
[32m[20221214 00:00:39 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:00:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 414.88
[32m[20221214 00:00:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 468.86
[32m[20221214 00:00:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 610.54
[32m[20221214 00:00:39 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 610.54
[32m[20221214 00:00:39 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 610.54
[32m[20221214 00:00:39 @agent_ppo2.py:143][0m Total time:       3.15 min
[32m[20221214 00:00:39 @agent_ppo2.py:145][0m 278528 total steps have happened
[32m[20221214 00:00:39 @agent_ppo2.py:121][0m #------------------------ Iteration 4136 --------------------------#
[32m[20221214 00:00:40 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:00:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:00:40 @agent_ppo2.py:185][0m |          -0.0027 |          61.8854 |          12.7540 |
[32m[20221214 00:00:40 @agent_ppo2.py:185][0m |          -0.0048 |          59.1543 |          12.7636 |
[32m[20221214 00:00:40 @agent_ppo2.py:185][0m |          -0.0079 |          57.9392 |          12.7255 |
[32m[20221214 00:00:40 @agent_ppo2.py:185][0m |          -0.0129 |          57.8421 |          12.7239 |
[32m[20221214 00:00:40 @agent_ppo2.py:185][0m |          -0.0134 |          57.2975 |          12.6810 |
[32m[20221214 00:00:40 @agent_ppo2.py:185][0m |          -0.0145 |          56.8980 |          12.6918 |
[32m[20221214 00:00:40 @agent_ppo2.py:185][0m |          -0.0136 |          56.7670 |          12.6868 |
[32m[20221214 00:00:40 @agent_ppo2.py:185][0m |          -0.0068 |          58.7780 |          12.6836 |
[32m[20221214 00:00:41 @agent_ppo2.py:185][0m |          -0.0187 |          56.8212 |          12.6765 |
[32m[20221214 00:00:41 @agent_ppo2.py:185][0m |          -0.0175 |          56.3770 |          12.6623 |
[32m[20221214 00:00:41 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:00:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 458.26
[32m[20221214 00:00:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 498.69
[32m[20221214 00:00:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 497.07
[32m[20221214 00:00:41 @agent_ppo2.py:143][0m Total time:       3.17 min
[32m[20221214 00:00:41 @agent_ppo2.py:145][0m 280576 total steps have happened
[32m[20221214 00:00:41 @agent_ppo2.py:121][0m #------------------------ Iteration 4137 --------------------------#
[32m[20221214 00:00:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:00:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:00:41 @agent_ppo2.py:185][0m |           0.0073 |          57.4085 |          12.9067 |
[32m[20221214 00:00:41 @agent_ppo2.py:185][0m |           0.0006 |          54.4156 |          12.9450 |
[32m[20221214 00:00:41 @agent_ppo2.py:185][0m |          -0.0080 |          53.5172 |          12.9602 |
[32m[20221214 00:00:41 @agent_ppo2.py:185][0m |          -0.0094 |          53.2497 |          12.9367 |
[32m[20221214 00:00:41 @agent_ppo2.py:185][0m |          -0.0085 |          52.7937 |          12.9639 |
[32m[20221214 00:00:42 @agent_ppo2.py:185][0m |          -0.0079 |          52.6164 |          12.9831 |
[32m[20221214 00:00:42 @agent_ppo2.py:185][0m |          -0.0081 |          52.3892 |          12.9903 |
[32m[20221214 00:00:42 @agent_ppo2.py:185][0m |          -0.0081 |          52.3958 |          12.9925 |
[32m[20221214 00:00:42 @agent_ppo2.py:185][0m |          -0.0077 |          52.0756 |          13.0311 |
[32m[20221214 00:00:42 @agent_ppo2.py:185][0m |          -0.0129 |          52.0861 |          13.0352 |
[32m[20221214 00:00:42 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:00:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 462.19
[32m[20221214 00:00:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 491.30
[32m[20221214 00:00:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 530.48
[32m[20221214 00:00:42 @agent_ppo2.py:143][0m Total time:       3.20 min
[32m[20221214 00:00:42 @agent_ppo2.py:145][0m 282624 total steps have happened
[32m[20221214 00:00:42 @agent_ppo2.py:121][0m #------------------------ Iteration 4138 --------------------------#
[32m[20221214 00:00:42 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:00:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:00:42 @agent_ppo2.py:185][0m |          -0.0005 |          56.3549 |          12.8857 |
[32m[20221214 00:00:42 @agent_ppo2.py:185][0m |          -0.0081 |          53.4514 |          12.9199 |
[32m[20221214 00:00:43 @agent_ppo2.py:185][0m |          -0.0015 |          53.1910 |          12.9253 |
[32m[20221214 00:00:43 @agent_ppo2.py:185][0m |          -0.0085 |          51.1517 |          12.8837 |
[32m[20221214 00:00:43 @agent_ppo2.py:185][0m |          -0.0086 |          50.5797 |          12.8969 |
[32m[20221214 00:00:43 @agent_ppo2.py:185][0m |          -0.0107 |          50.0754 |          12.9098 |
[32m[20221214 00:00:43 @agent_ppo2.py:185][0m |          -0.0113 |          49.8373 |          12.8880 |
[32m[20221214 00:00:43 @agent_ppo2.py:185][0m |          -0.0022 |          51.1785 |          12.8862 |
[32m[20221214 00:00:43 @agent_ppo2.py:185][0m |          -0.0051 |          50.5312 |          12.8718 |
[32m[20221214 00:00:43 @agent_ppo2.py:185][0m |          -0.0113 |          49.1261 |          12.8413 |
[32m[20221214 00:00:43 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:00:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 438.99
[32m[20221214 00:00:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 453.84
[32m[20221214 00:00:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 368.87
[32m[20221214 00:00:43 @agent_ppo2.py:143][0m Total time:       3.22 min
[32m[20221214 00:00:43 @agent_ppo2.py:145][0m 284672 total steps have happened
[32m[20221214 00:00:43 @agent_ppo2.py:121][0m #------------------------ Iteration 4139 --------------------------#
[32m[20221214 00:00:44 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:00:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:00:44 @agent_ppo2.py:185][0m |          -0.0011 |          40.4538 |          12.6810 |
[32m[20221214 00:00:44 @agent_ppo2.py:185][0m |          -0.0055 |          34.5792 |          12.7011 |
[32m[20221214 00:00:44 @agent_ppo2.py:185][0m |          -0.0055 |          32.8539 |          12.6821 |
[32m[20221214 00:00:44 @agent_ppo2.py:185][0m |          -0.0065 |          31.7907 |          12.6623 |
[32m[20221214 00:00:44 @agent_ppo2.py:185][0m |          -0.0107 |          30.4607 |          12.6554 |
[32m[20221214 00:00:44 @agent_ppo2.py:185][0m |          -0.0130 |          29.5124 |          12.6264 |
[32m[20221214 00:00:44 @agent_ppo2.py:185][0m |          -0.0111 |          29.6690 |          12.6170 |
[32m[20221214 00:00:44 @agent_ppo2.py:185][0m |          -0.0048 |          29.7651 |          12.6204 |
[32m[20221214 00:00:44 @agent_ppo2.py:185][0m |          -0.0219 |          28.3485 |          12.6155 |
[32m[20221214 00:00:45 @agent_ppo2.py:185][0m |          -0.0201 |          27.2738 |          12.5963 |
[32m[20221214 00:00:45 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:00:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 360.51
[32m[20221214 00:00:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 473.50
[32m[20221214 00:00:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 430.46
[32m[20221214 00:00:45 @agent_ppo2.py:143][0m Total time:       3.24 min
[32m[20221214 00:00:45 @agent_ppo2.py:145][0m 286720 total steps have happened
[32m[20221214 00:00:45 @agent_ppo2.py:121][0m #------------------------ Iteration 4140 --------------------------#
[32m[20221214 00:00:45 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:00:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:00:45 @agent_ppo2.py:185][0m |           0.0025 |          54.3740 |          12.6785 |
[32m[20221214 00:00:45 @agent_ppo2.py:185][0m |          -0.0038 |          45.0479 |          12.6906 |
[32m[20221214 00:00:45 @agent_ppo2.py:185][0m |          -0.0067 |          41.3949 |          12.7319 |
[32m[20221214 00:00:45 @agent_ppo2.py:185][0m |          -0.0043 |          39.2394 |          12.7333 |
[32m[20221214 00:00:45 @agent_ppo2.py:185][0m |          -0.0038 |          37.7782 |          12.7097 |
[32m[20221214 00:00:46 @agent_ppo2.py:185][0m |          -0.0131 |          36.3613 |          12.6978 |
[32m[20221214 00:00:46 @agent_ppo2.py:185][0m |          -0.0113 |          35.5358 |          12.7385 |
[32m[20221214 00:00:46 @agent_ppo2.py:185][0m |          -0.0123 |          34.7975 |          12.7302 |
[32m[20221214 00:00:46 @agent_ppo2.py:185][0m |          -0.0116 |          34.1602 |          12.7822 |
[32m[20221214 00:00:46 @agent_ppo2.py:185][0m |          -0.0177 |          33.5947 |          12.7353 |
[32m[20221214 00:00:46 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:00:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 347.38
[32m[20221214 00:00:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 423.97
[32m[20221214 00:00:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 427.14
[32m[20221214 00:00:46 @agent_ppo2.py:143][0m Total time:       3.26 min
[32m[20221214 00:00:46 @agent_ppo2.py:145][0m 288768 total steps have happened
[32m[20221214 00:00:46 @agent_ppo2.py:121][0m #------------------------ Iteration 4141 --------------------------#
[32m[20221214 00:00:46 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:00:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:00:46 @agent_ppo2.py:185][0m |          -0.0010 |          57.4929 |          12.1086 |
[32m[20221214 00:00:46 @agent_ppo2.py:185][0m |          -0.0132 |          49.8577 |          12.1250 |
[32m[20221214 00:00:47 @agent_ppo2.py:185][0m |          -0.0111 |          47.5180 |          12.0840 |
[32m[20221214 00:00:47 @agent_ppo2.py:185][0m |          -0.0158 |          46.0293 |          12.1308 |
[32m[20221214 00:00:47 @agent_ppo2.py:185][0m |          -0.0084 |          45.6348 |          12.1493 |
[32m[20221214 00:00:47 @agent_ppo2.py:185][0m |          -0.0152 |          44.0565 |          12.1386 |
[32m[20221214 00:00:47 @agent_ppo2.py:185][0m |          -0.0148 |          43.8239 |          12.1546 |
[32m[20221214 00:00:47 @agent_ppo2.py:185][0m |          -0.0182 |          43.1122 |          12.1963 |
[32m[20221214 00:00:47 @agent_ppo2.py:185][0m |          -0.0215 |          42.6805 |          12.1879 |
[32m[20221214 00:00:47 @agent_ppo2.py:185][0m |          -0.0176 |          42.2121 |          12.1749 |
[32m[20221214 00:00:47 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:00:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 375.65
[32m[20221214 00:00:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 465.56
[32m[20221214 00:00:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 477.45
[32m[20221214 00:00:47 @agent_ppo2.py:143][0m Total time:       3.28 min
[32m[20221214 00:00:47 @agent_ppo2.py:145][0m 290816 total steps have happened
[32m[20221214 00:00:47 @agent_ppo2.py:121][0m #------------------------ Iteration 4142 --------------------------#
[32m[20221214 00:00:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:00:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:00:48 @agent_ppo2.py:185][0m |          -0.0074 |          38.2910 |          12.4213 |
[32m[20221214 00:00:48 @agent_ppo2.py:185][0m |          -0.0029 |          32.2162 |          12.4121 |
[32m[20221214 00:00:48 @agent_ppo2.py:185][0m |          -0.0090 |          30.0562 |          12.3790 |
[32m[20221214 00:00:48 @agent_ppo2.py:185][0m |          -0.0122 |          28.9542 |          12.3851 |
[32m[20221214 00:00:48 @agent_ppo2.py:185][0m |          -0.0113 |          28.0879 |          12.3514 |
[32m[20221214 00:00:48 @agent_ppo2.py:185][0m |          -0.0165 |          27.2220 |          12.3580 |
[32m[20221214 00:00:48 @agent_ppo2.py:185][0m |          -0.0057 |          29.2231 |          12.3230 |
[32m[20221214 00:00:48 @agent_ppo2.py:185][0m |          -0.0180 |          26.1643 |          12.2960 |
[32m[20221214 00:00:48 @agent_ppo2.py:185][0m |          -0.0181 |          25.5874 |          12.3025 |
[32m[20221214 00:00:49 @agent_ppo2.py:185][0m |          -0.0091 |          28.1376 |          12.2901 |
[32m[20221214 00:00:49 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:00:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 372.76
[32m[20221214 00:00:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 517.00
[32m[20221214 00:00:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 556.71
[32m[20221214 00:00:49 @agent_ppo2.py:143][0m Total time:       3.31 min
[32m[20221214 00:00:49 @agent_ppo2.py:145][0m 292864 total steps have happened
[32m[20221214 00:00:49 @agent_ppo2.py:121][0m #------------------------ Iteration 4143 --------------------------#
[32m[20221214 00:00:49 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:00:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:00:49 @agent_ppo2.py:185][0m |           0.0028 |          63.8277 |          12.4703 |
[32m[20221214 00:00:49 @agent_ppo2.py:185][0m |          -0.0050 |          58.7246 |          12.4550 |
[32m[20221214 00:00:49 @agent_ppo2.py:185][0m |          -0.0068 |          55.8330 |          12.4170 |
[32m[20221214 00:00:49 @agent_ppo2.py:185][0m |          -0.0063 |          55.0805 |          12.4463 |
[32m[20221214 00:00:49 @agent_ppo2.py:185][0m |          -0.0063 |          54.2997 |          12.4249 |
[32m[20221214 00:00:49 @agent_ppo2.py:185][0m |          -0.0124 |          52.6911 |          12.4108 |
[32m[20221214 00:00:50 @agent_ppo2.py:185][0m |          -0.0153 |          51.1141 |          12.4330 |
[32m[20221214 00:00:50 @agent_ppo2.py:185][0m |          -0.0153 |          50.2373 |          12.4498 |
[32m[20221214 00:00:50 @agent_ppo2.py:185][0m |          -0.0177 |          49.7265 |          12.4461 |
[32m[20221214 00:00:50 @agent_ppo2.py:185][0m |           0.0015 |          58.4961 |          12.4439 |
[32m[20221214 00:00:50 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:00:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 402.12
[32m[20221214 00:00:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 467.91
[32m[20221214 00:00:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 506.81
[32m[20221214 00:00:50 @agent_ppo2.py:143][0m Total time:       3.33 min
[32m[20221214 00:00:50 @agent_ppo2.py:145][0m 294912 total steps have happened
[32m[20221214 00:00:50 @agent_ppo2.py:121][0m #------------------------ Iteration 4144 --------------------------#
[32m[20221214 00:00:50 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:00:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:00:50 @agent_ppo2.py:185][0m |           0.0095 |          48.5380 |          12.0699 |
[32m[20221214 00:00:50 @agent_ppo2.py:185][0m |          -0.0053 |          38.3453 |          11.9997 |
[32m[20221214 00:00:51 @agent_ppo2.py:185][0m |          -0.0129 |          35.6595 |          12.0132 |
[32m[20221214 00:00:51 @agent_ppo2.py:185][0m |          -0.0210 |          34.3974 |          12.0006 |
[32m[20221214 00:00:51 @agent_ppo2.py:185][0m |          -0.0045 |          33.1537 |          12.0033 |
[32m[20221214 00:00:51 @agent_ppo2.py:185][0m |          -0.0120 |          31.8347 |          11.9802 |
[32m[20221214 00:00:51 @agent_ppo2.py:185][0m |          -0.0121 |          33.3907 |          11.9167 |
[32m[20221214 00:00:51 @agent_ppo2.py:185][0m |          -0.0144 |          31.2951 |          11.9210 |
[32m[20221214 00:00:51 @agent_ppo2.py:185][0m |          -0.0226 |          30.2179 |          11.9218 |
[32m[20221214 00:00:51 @agent_ppo2.py:185][0m |          -0.0189 |          29.6454 |          11.9183 |
[32m[20221214 00:00:51 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:00:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 374.22
[32m[20221214 00:00:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 523.84
[32m[20221214 00:00:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 488.72
[32m[20221214 00:00:51 @agent_ppo2.py:143][0m Total time:       3.35 min
[32m[20221214 00:00:51 @agent_ppo2.py:145][0m 296960 total steps have happened
[32m[20221214 00:00:51 @agent_ppo2.py:121][0m #------------------------ Iteration 4145 --------------------------#
[32m[20221214 00:00:51 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:00:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:00:52 @agent_ppo2.py:185][0m |           0.0045 |          56.1257 |          12.3470 |
[32m[20221214 00:00:52 @agent_ppo2.py:185][0m |          -0.0038 |          50.7611 |          12.3763 |
[32m[20221214 00:00:52 @agent_ppo2.py:185][0m |          -0.0026 |          49.5743 |          12.3960 |
[32m[20221214 00:00:52 @agent_ppo2.py:185][0m |          -0.0091 |          48.3948 |          12.3745 |
[32m[20221214 00:00:52 @agent_ppo2.py:185][0m |          -0.0101 |          47.5363 |          12.3486 |
[32m[20221214 00:00:52 @agent_ppo2.py:185][0m |          -0.0156 |          47.2835 |          12.3710 |
[32m[20221214 00:00:52 @agent_ppo2.py:185][0m |          -0.0092 |          46.8434 |          12.3694 |
[32m[20221214 00:00:52 @agent_ppo2.py:185][0m |          -0.0150 |          46.3846 |          12.3768 |
[32m[20221214 00:00:52 @agent_ppo2.py:185][0m |          -0.0133 |          46.2754 |          12.3526 |
[32m[20221214 00:00:53 @agent_ppo2.py:185][0m |          -0.0170 |          46.0983 |          12.3187 |
[32m[20221214 00:00:53 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:00:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 417.77
[32m[20221214 00:00:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 478.38
[32m[20221214 00:00:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 529.50
[32m[20221214 00:00:53 @agent_ppo2.py:143][0m Total time:       3.37 min
[32m[20221214 00:00:53 @agent_ppo2.py:145][0m 299008 total steps have happened
[32m[20221214 00:00:53 @agent_ppo2.py:121][0m #------------------------ Iteration 4146 --------------------------#
[32m[20221214 00:00:53 @agent_ppo2.py:127][0m Sampling time: 0.27 s by 5 slaves
[32m[20221214 00:00:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:00:53 @agent_ppo2.py:185][0m |          -0.0021 |          59.5057 |          12.1668 |
[32m[20221214 00:00:53 @agent_ppo2.py:185][0m |          -0.0067 |          57.3103 |          12.1590 |
[32m[20221214 00:00:53 @agent_ppo2.py:185][0m |          -0.0129 |          56.4706 |          12.2278 |
[32m[20221214 00:00:54 @agent_ppo2.py:185][0m |          -0.0110 |          56.0288 |          12.2026 |
[32m[20221214 00:00:54 @agent_ppo2.py:185][0m |          -0.0100 |          55.5433 |          12.2458 |
[32m[20221214 00:00:54 @agent_ppo2.py:185][0m |          -0.0103 |          55.3878 |          12.2629 |
[32m[20221214 00:00:54 @agent_ppo2.py:185][0m |          -0.0105 |          55.0637 |          12.2653 |
[32m[20221214 00:00:54 @agent_ppo2.py:185][0m |          -0.0139 |          54.8400 |          12.2404 |
[32m[20221214 00:00:54 @agent_ppo2.py:185][0m |          -0.0131 |          54.6053 |          12.2444 |
[32m[20221214 00:00:54 @agent_ppo2.py:185][0m |          -0.0151 |          54.4097 |          12.2817 |
[32m[20221214 00:00:54 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 00:00:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 466.47
[32m[20221214 00:00:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 492.23
[32m[20221214 00:00:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 516.76
[32m[20221214 00:00:54 @agent_ppo2.py:143][0m Total time:       3.40 min
[32m[20221214 00:00:54 @agent_ppo2.py:145][0m 301056 total steps have happened
[32m[20221214 00:00:54 @agent_ppo2.py:121][0m #------------------------ Iteration 4147 --------------------------#
[32m[20221214 00:00:54 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:00:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:00:55 @agent_ppo2.py:185][0m |           0.0003 |          55.6206 |          12.2314 |
[32m[20221214 00:00:55 @agent_ppo2.py:185][0m |          -0.0059 |          50.1720 |          12.2722 |
[32m[20221214 00:00:55 @agent_ppo2.py:185][0m |          -0.0106 |          48.3354 |          12.2837 |
[32m[20221214 00:00:55 @agent_ppo2.py:185][0m |          -0.0106 |          47.3747 |          12.2801 |
[32m[20221214 00:00:55 @agent_ppo2.py:185][0m |          -0.0138 |          46.7226 |          12.3492 |
[32m[20221214 00:00:55 @agent_ppo2.py:185][0m |          -0.0189 |          46.0171 |          12.3075 |
[32m[20221214 00:00:55 @agent_ppo2.py:185][0m |          -0.0139 |          46.1936 |          12.3449 |
[32m[20221214 00:00:55 @agent_ppo2.py:185][0m |          -0.0197 |          45.5400 |          12.3437 |
[32m[20221214 00:00:55 @agent_ppo2.py:185][0m |          -0.0122 |          46.7202 |          12.3498 |
[32m[20221214 00:00:55 @agent_ppo2.py:185][0m |          -0.0224 |          44.9550 |          12.3214 |
[32m[20221214 00:00:55 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:00:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 443.21
[32m[20221214 00:00:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 506.51
[32m[20221214 00:00:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 156.23
[32m[20221214 00:00:56 @agent_ppo2.py:143][0m Total time:       3.42 min
[32m[20221214 00:00:56 @agent_ppo2.py:145][0m 303104 total steps have happened
[32m[20221214 00:00:56 @agent_ppo2.py:121][0m #------------------------ Iteration 4148 --------------------------#
[32m[20221214 00:00:56 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:00:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:00:56 @agent_ppo2.py:185][0m |           0.0034 |          72.8320 |          12.3964 |
[32m[20221214 00:00:56 @agent_ppo2.py:185][0m |          -0.0055 |          63.6054 |          12.3815 |
[32m[20221214 00:00:56 @agent_ppo2.py:185][0m |          -0.0070 |          60.6101 |          12.3821 |
[32m[20221214 00:00:56 @agent_ppo2.py:185][0m |          -0.0081 |          58.9075 |          12.3627 |
[32m[20221214 00:00:56 @agent_ppo2.py:185][0m |          -0.0117 |          57.7147 |          12.3805 |
[32m[20221214 00:00:56 @agent_ppo2.py:185][0m |          -0.0127 |          56.8693 |          12.3360 |
[32m[20221214 00:00:57 @agent_ppo2.py:185][0m |          -0.0109 |          56.1341 |          12.3832 |
[32m[20221214 00:00:57 @agent_ppo2.py:185][0m |          -0.0156 |          55.6372 |          12.3522 |
[32m[20221214 00:00:57 @agent_ppo2.py:185][0m |          -0.0148 |          55.1150 |          12.3643 |
[32m[20221214 00:00:57 @agent_ppo2.py:185][0m |          -0.0094 |          54.8179 |          12.3636 |
[32m[20221214 00:00:57 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:00:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 467.54
[32m[20221214 00:00:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 513.97
[32m[20221214 00:00:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 452.89
[32m[20221214 00:00:57 @agent_ppo2.py:143][0m Total time:       3.44 min
[32m[20221214 00:00:57 @agent_ppo2.py:145][0m 305152 total steps have happened
[32m[20221214 00:00:57 @agent_ppo2.py:121][0m #------------------------ Iteration 4149 --------------------------#
[32m[20221214 00:00:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:00:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:00:57 @agent_ppo2.py:185][0m |           0.0017 |          35.7323 |          12.1564 |
[32m[20221214 00:00:57 @agent_ppo2.py:185][0m |          -0.0079 |          30.5059 |          12.1247 |
[32m[20221214 00:00:58 @agent_ppo2.py:185][0m |          -0.0065 |          28.5708 |          12.0895 |
[32m[20221214 00:00:58 @agent_ppo2.py:185][0m |          -0.0145 |          27.4091 |          12.0896 |
[32m[20221214 00:00:58 @agent_ppo2.py:185][0m |          -0.0145 |          26.6536 |          12.0515 |
[32m[20221214 00:00:58 @agent_ppo2.py:185][0m |          -0.0132 |          26.6572 |          12.0644 |
[32m[20221214 00:00:58 @agent_ppo2.py:185][0m |          -0.0139 |          25.6623 |          12.0474 |
[32m[20221214 00:00:58 @agent_ppo2.py:185][0m |          -0.0211 |          25.2368 |          12.0373 |
[32m[20221214 00:00:58 @agent_ppo2.py:185][0m |          -0.0241 |          24.7181 |          12.0307 |
[32m[20221214 00:00:58 @agent_ppo2.py:185][0m |          -0.0154 |          24.4314 |          12.0080 |
[32m[20221214 00:00:58 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:00:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 329.34
[32m[20221214 00:00:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 389.47
[32m[20221214 00:00:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 283.99
[32m[20221214 00:00:58 @agent_ppo2.py:143][0m Total time:       3.47 min
[32m[20221214 00:00:58 @agent_ppo2.py:145][0m 307200 total steps have happened
[32m[20221214 00:00:58 @agent_ppo2.py:121][0m #------------------------ Iteration 4150 --------------------------#
[32m[20221214 00:00:58 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:00:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:00:59 @agent_ppo2.py:185][0m |           0.0100 |          40.8075 |          12.3039 |
[32m[20221214 00:00:59 @agent_ppo2.py:185][0m |          -0.0054 |          34.3132 |          12.3190 |
[32m[20221214 00:00:59 @agent_ppo2.py:185][0m |          -0.0045 |          32.2035 |          12.3124 |
[32m[20221214 00:00:59 @agent_ppo2.py:185][0m |          -0.0099 |          31.2071 |          12.3420 |
[32m[20221214 00:00:59 @agent_ppo2.py:185][0m |          -0.0082 |          30.4252 |          12.3603 |
[32m[20221214 00:00:59 @agent_ppo2.py:185][0m |          -0.0127 |          30.2751 |          12.3662 |
[32m[20221214 00:00:59 @agent_ppo2.py:185][0m |          -0.0150 |          29.0872 |          12.3975 |
[32m[20221214 00:00:59 @agent_ppo2.py:185][0m |          -0.0068 |          28.8546 |          12.4034 |
[32m[20221214 00:00:59 @agent_ppo2.py:185][0m |          -0.0125 |          28.1849 |          12.4124 |
[32m[20221214 00:01:00 @agent_ppo2.py:185][0m |          -0.0146 |          27.8020 |          12.4602 |
[32m[20221214 00:01:00 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:01:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 368.33
[32m[20221214 00:01:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 462.28
[32m[20221214 00:01:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 529.02
[32m[20221214 00:01:00 @agent_ppo2.py:143][0m Total time:       3.49 min
[32m[20221214 00:01:00 @agent_ppo2.py:145][0m 309248 total steps have happened
[32m[20221214 00:01:00 @agent_ppo2.py:121][0m #------------------------ Iteration 4151 --------------------------#
[32m[20221214 00:01:00 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:01:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:01:00 @agent_ppo2.py:185][0m |          -0.0001 |          40.5595 |          12.1527 |
[32m[20221214 00:01:00 @agent_ppo2.py:185][0m |           0.0109 |          43.4669 |          12.1465 |
[32m[20221214 00:01:00 @agent_ppo2.py:185][0m |          -0.0064 |          34.5874 |          12.1322 |
[32m[20221214 00:01:00 @agent_ppo2.py:185][0m |          -0.0103 |          32.5803 |          12.1161 |
[32m[20221214 00:01:00 @agent_ppo2.py:185][0m |          -0.0084 |          31.4689 |          12.0874 |
[32m[20221214 00:01:00 @agent_ppo2.py:185][0m |          -0.0139 |          31.6193 |          12.0628 |
[32m[20221214 00:01:01 @agent_ppo2.py:185][0m |          -0.0179 |          30.6138 |          12.0558 |
[32m[20221214 00:01:01 @agent_ppo2.py:185][0m |          -0.0151 |          29.8330 |          12.0215 |
[32m[20221214 00:01:01 @agent_ppo2.py:185][0m |          -0.0152 |          29.3670 |          12.0062 |
[32m[20221214 00:01:01 @agent_ppo2.py:185][0m |          -0.0250 |          28.9545 |          11.9784 |
[32m[20221214 00:01:01 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:01:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 355.68
[32m[20221214 00:01:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 539.60
[32m[20221214 00:01:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 501.36
[32m[20221214 00:01:01 @agent_ppo2.py:143][0m Total time:       3.51 min
[32m[20221214 00:01:01 @agent_ppo2.py:145][0m 311296 total steps have happened
[32m[20221214 00:01:01 @agent_ppo2.py:121][0m #------------------------ Iteration 4152 --------------------------#
[32m[20221214 00:01:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:01:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:01:01 @agent_ppo2.py:185][0m |          -0.0018 |          57.2432 |          12.0760 |
[32m[20221214 00:01:01 @agent_ppo2.py:185][0m |          -0.0047 |          54.5883 |          12.0981 |
[32m[20221214 00:01:02 @agent_ppo2.py:185][0m |          -0.0054 |          53.7298 |          12.0690 |
[32m[20221214 00:01:02 @agent_ppo2.py:185][0m |          -0.0057 |          53.0758 |          12.0518 |
[32m[20221214 00:01:02 @agent_ppo2.py:185][0m |          -0.0066 |          52.9944 |          12.0966 |
[32m[20221214 00:01:02 @agent_ppo2.py:185][0m |          -0.0074 |          52.2654 |          12.0570 |
[32m[20221214 00:01:02 @agent_ppo2.py:185][0m |           0.0050 |          57.7521 |          12.0675 |
[32m[20221214 00:01:02 @agent_ppo2.py:185][0m |           0.0047 |          57.8716 |          12.1006 |
[32m[20221214 00:01:02 @agent_ppo2.py:185][0m |          -0.0074 |          51.1515 |          12.0737 |
[32m[20221214 00:01:02 @agent_ppo2.py:185][0m |          -0.0110 |          50.8399 |          12.1109 |
[32m[20221214 00:01:02 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:01:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 481.88
[32m[20221214 00:01:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 498.39
[32m[20221214 00:01:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 466.98
[32m[20221214 00:01:02 @agent_ppo2.py:143][0m Total time:       3.53 min
[32m[20221214 00:01:02 @agent_ppo2.py:145][0m 313344 total steps have happened
[32m[20221214 00:01:02 @agent_ppo2.py:121][0m #------------------------ Iteration 4153 --------------------------#
[32m[20221214 00:01:02 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:01:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:01:03 @agent_ppo2.py:185][0m |          -0.0012 |          39.6122 |          12.3019 |
[32m[20221214 00:01:03 @agent_ppo2.py:185][0m |          -0.0064 |          32.8408 |          12.2624 |
[32m[20221214 00:01:03 @agent_ppo2.py:185][0m |          -0.0105 |          30.3729 |          12.3125 |
[32m[20221214 00:01:03 @agent_ppo2.py:185][0m |          -0.0106 |          28.9386 |          12.3054 |
[32m[20221214 00:01:03 @agent_ppo2.py:185][0m |          -0.0092 |          28.1166 |          12.2828 |
[32m[20221214 00:01:03 @agent_ppo2.py:185][0m |          -0.0088 |          28.1682 |          12.2901 |
[32m[20221214 00:01:03 @agent_ppo2.py:185][0m |          -0.0156 |          26.8464 |          12.2686 |
[32m[20221214 00:01:03 @agent_ppo2.py:185][0m |          -0.0151 |          25.8311 |          12.2505 |
[32m[20221214 00:01:03 @agent_ppo2.py:185][0m |          -0.0176 |          25.4465 |          12.2598 |
[32m[20221214 00:01:03 @agent_ppo2.py:185][0m |          -0.0206 |          25.1657 |          12.2588 |
[32m[20221214 00:01:03 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:01:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 337.26
[32m[20221214 00:01:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 450.50
[32m[20221214 00:01:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 462.03
[32m[20221214 00:01:04 @agent_ppo2.py:143][0m Total time:       3.55 min
[32m[20221214 00:01:04 @agent_ppo2.py:145][0m 315392 total steps have happened
[32m[20221214 00:01:04 @agent_ppo2.py:121][0m #------------------------ Iteration 4154 --------------------------#
[32m[20221214 00:01:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:01:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:01:04 @agent_ppo2.py:185][0m |           0.0027 |          48.7658 |          11.9769 |
[32m[20221214 00:01:04 @agent_ppo2.py:185][0m |          -0.0080 |          43.8469 |          12.0360 |
[32m[20221214 00:01:04 @agent_ppo2.py:185][0m |          -0.0005 |          42.0971 |          12.0489 |
[32m[20221214 00:01:04 @agent_ppo2.py:185][0m |          -0.0039 |          40.1138 |          12.0910 |
[32m[20221214 00:01:04 @agent_ppo2.py:185][0m |          -0.0072 |          39.6545 |          12.0494 |
[32m[20221214 00:01:04 @agent_ppo2.py:185][0m |          -0.0069 |          38.3735 |          12.1128 |
[32m[20221214 00:01:05 @agent_ppo2.py:185][0m |          -0.0095 |          37.9065 |          12.0457 |
[32m[20221214 00:01:05 @agent_ppo2.py:185][0m |          -0.0126 |          37.1708 |          12.0822 |
[32m[20221214 00:01:05 @agent_ppo2.py:185][0m |          -0.0165 |          36.9078 |          12.1008 |
[32m[20221214 00:01:05 @agent_ppo2.py:185][0m |          -0.0165 |          36.2409 |          12.0930 |
[32m[20221214 00:01:05 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:01:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 400.09
[32m[20221214 00:01:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 484.99
[32m[20221214 00:01:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 481.31
[32m[20221214 00:01:05 @agent_ppo2.py:143][0m Total time:       3.58 min
[32m[20221214 00:01:05 @agent_ppo2.py:145][0m 317440 total steps have happened
[32m[20221214 00:01:05 @agent_ppo2.py:121][0m #------------------------ Iteration 4155 --------------------------#
[32m[20221214 00:01:05 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:01:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:01:05 @agent_ppo2.py:185][0m |           0.0003 |          63.8236 |          11.7216 |
[32m[20221214 00:01:05 @agent_ppo2.py:185][0m |          -0.0039 |          61.0406 |          11.6811 |
[32m[20221214 00:01:05 @agent_ppo2.py:185][0m |          -0.0074 |          60.3554 |          11.6558 |
[32m[20221214 00:01:06 @agent_ppo2.py:185][0m |           0.0053 |          67.0806 |          11.6857 |
[32m[20221214 00:01:06 @agent_ppo2.py:185][0m |          -0.0080 |          59.5565 |          11.7099 |
[32m[20221214 00:01:06 @agent_ppo2.py:185][0m |          -0.0080 |          59.1822 |          11.6585 |
[32m[20221214 00:01:06 @agent_ppo2.py:185][0m |          -0.0085 |          58.8264 |          11.6843 |
[32m[20221214 00:01:06 @agent_ppo2.py:185][0m |          -0.0128 |          58.8417 |          11.7242 |
[32m[20221214 00:01:06 @agent_ppo2.py:185][0m |          -0.0133 |          58.7651 |          11.6893 |
[32m[20221214 00:01:06 @agent_ppo2.py:185][0m |          -0.0101 |          58.9984 |          11.7227 |
[32m[20221214 00:01:06 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:01:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 450.62
[32m[20221214 00:01:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 478.21
[32m[20221214 00:01:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 523.05
[32m[20221214 00:01:06 @agent_ppo2.py:143][0m Total time:       3.60 min
[32m[20221214 00:01:06 @agent_ppo2.py:145][0m 319488 total steps have happened
[32m[20221214 00:01:06 @agent_ppo2.py:121][0m #------------------------ Iteration 4156 --------------------------#
[32m[20221214 00:01:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:01:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:01:07 @agent_ppo2.py:185][0m |           0.0020 |          59.9667 |          12.1676 |
[32m[20221214 00:01:07 @agent_ppo2.py:185][0m |          -0.0074 |          52.2667 |          12.1922 |
[32m[20221214 00:01:07 @agent_ppo2.py:185][0m |          -0.0112 |          50.0182 |          12.1826 |
[32m[20221214 00:01:07 @agent_ppo2.py:185][0m |          -0.0184 |          48.9027 |          12.2224 |
[32m[20221214 00:01:07 @agent_ppo2.py:185][0m |          -0.0136 |          47.9095 |          12.1777 |
[32m[20221214 00:01:07 @agent_ppo2.py:185][0m |          -0.0200 |          47.5170 |          12.2027 |
[32m[20221214 00:01:07 @agent_ppo2.py:185][0m |          -0.0195 |          46.4750 |          12.1966 |
[32m[20221214 00:01:07 @agent_ppo2.py:185][0m |          -0.0183 |          45.9416 |          12.1693 |
[32m[20221214 00:01:07 @agent_ppo2.py:185][0m |          -0.0223 |          45.6836 |          12.1374 |
[32m[20221214 00:01:07 @agent_ppo2.py:185][0m |          -0.0179 |          45.4448 |          12.1339 |
[32m[20221214 00:01:07 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:01:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 358.25
[32m[20221214 00:01:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 450.76
[32m[20221214 00:01:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 390.71
[32m[20221214 00:01:08 @agent_ppo2.py:143][0m Total time:       3.62 min
[32m[20221214 00:01:08 @agent_ppo2.py:145][0m 321536 total steps have happened
[32m[20221214 00:01:08 @agent_ppo2.py:121][0m #------------------------ Iteration 4157 --------------------------#
[32m[20221214 00:01:08 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:01:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:01:08 @agent_ppo2.py:185][0m |           0.0069 |          67.0889 |          12.2414 |
[32m[20221214 00:01:08 @agent_ppo2.py:185][0m |          -0.0038 |          60.1893 |          12.2182 |
[32m[20221214 00:01:08 @agent_ppo2.py:185][0m |          -0.0099 |          58.3361 |          12.2018 |
[32m[20221214 00:01:08 @agent_ppo2.py:185][0m |          -0.0056 |          57.6521 |          12.1995 |
[32m[20221214 00:01:08 @agent_ppo2.py:185][0m |          -0.0100 |          56.1589 |          12.2115 |
[32m[20221214 00:01:08 @agent_ppo2.py:185][0m |          -0.0148 |          55.4942 |          12.1571 |
[32m[20221214 00:01:09 @agent_ppo2.py:185][0m |          -0.0139 |          54.8452 |          12.1773 |
[32m[20221214 00:01:09 @agent_ppo2.py:185][0m |          -0.0133 |          54.5451 |          12.2242 |
[32m[20221214 00:01:09 @agent_ppo2.py:185][0m |          -0.0126 |          54.0138 |          12.1660 |
[32m[20221214 00:01:09 @agent_ppo2.py:185][0m |          -0.0184 |          53.5441 |          12.1310 |
[32m[20221214 00:01:09 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:01:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 428.24
[32m[20221214 00:01:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 486.68
[32m[20221214 00:01:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 493.12
[32m[20221214 00:01:09 @agent_ppo2.py:143][0m Total time:       3.64 min
[32m[20221214 00:01:09 @agent_ppo2.py:145][0m 323584 total steps have happened
[32m[20221214 00:01:09 @agent_ppo2.py:121][0m #------------------------ Iteration 4158 --------------------------#
[32m[20221214 00:01:09 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:01:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:01:09 @agent_ppo2.py:185][0m |          -0.0026 |          60.2231 |          11.9281 |
[32m[20221214 00:01:09 @agent_ppo2.py:185][0m |          -0.0099 |          56.7296 |          11.9497 |
[32m[20221214 00:01:09 @agent_ppo2.py:185][0m |          -0.0139 |          54.8510 |          11.9983 |
[32m[20221214 00:01:10 @agent_ppo2.py:185][0m |          -0.0132 |          53.9235 |          12.0303 |
[32m[20221214 00:01:10 @agent_ppo2.py:185][0m |          -0.0082 |          55.4587 |          12.0892 |
[32m[20221214 00:01:10 @agent_ppo2.py:185][0m |          -0.0103 |          52.7301 |          12.1094 |
[32m[20221214 00:01:10 @agent_ppo2.py:185][0m |          -0.0127 |          51.9068 |          12.1205 |
[32m[20221214 00:01:10 @agent_ppo2.py:185][0m |          -0.0054 |          60.0457 |          12.1442 |
[32m[20221214 00:01:10 @agent_ppo2.py:185][0m |          -0.0158 |          51.2999 |          12.1787 |
[32m[20221214 00:01:10 @agent_ppo2.py:185][0m |          -0.0127 |          50.7122 |          12.2226 |
[32m[20221214 00:01:10 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:01:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 478.86
[32m[20221214 00:01:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 507.79
[32m[20221214 00:01:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 521.87
[32m[20221214 00:01:10 @agent_ppo2.py:143][0m Total time:       3.66 min
[32m[20221214 00:01:10 @agent_ppo2.py:145][0m 325632 total steps have happened
[32m[20221214 00:01:10 @agent_ppo2.py:121][0m #------------------------ Iteration 4159 --------------------------#
[32m[20221214 00:01:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:01:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:01:11 @agent_ppo2.py:185][0m |           0.0047 |          65.3293 |          12.4292 |
[32m[20221214 00:01:11 @agent_ppo2.py:185][0m |          -0.0024 |          63.2563 |          12.4033 |
[32m[20221214 00:01:11 @agent_ppo2.py:185][0m |           0.0142 |          70.1870 |          12.4263 |
[32m[20221214 00:01:11 @agent_ppo2.py:185][0m |          -0.0031 |          61.8481 |          12.4543 |
[32m[20221214 00:01:11 @agent_ppo2.py:185][0m |          -0.0093 |          61.0336 |          12.4993 |
[32m[20221214 00:01:11 @agent_ppo2.py:185][0m |          -0.0102 |          60.5784 |          12.4686 |
[32m[20221214 00:01:11 @agent_ppo2.py:185][0m |          -0.0103 |          60.3177 |          12.4969 |
[32m[20221214 00:01:11 @agent_ppo2.py:185][0m |          -0.0070 |          61.6795 |          12.4296 |
[32m[20221214 00:01:11 @agent_ppo2.py:185][0m |          -0.0106 |          59.7246 |          12.4986 |
[32m[20221214 00:01:11 @agent_ppo2.py:185][0m |          -0.0122 |          59.5365 |          12.5078 |
[32m[20221214 00:01:11 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:01:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 456.45
[32m[20221214 00:01:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 488.77
[32m[20221214 00:01:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 496.59
[32m[20221214 00:01:12 @agent_ppo2.py:143][0m Total time:       3.69 min
[32m[20221214 00:01:12 @agent_ppo2.py:145][0m 327680 total steps have happened
[32m[20221214 00:01:12 @agent_ppo2.py:121][0m #------------------------ Iteration 4160 --------------------------#
[32m[20221214 00:01:12 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:01:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:01:12 @agent_ppo2.py:185][0m |          -0.0001 |          59.8911 |          12.5193 |
[32m[20221214 00:01:12 @agent_ppo2.py:185][0m |          -0.0048 |          58.7457 |          12.5489 |
[32m[20221214 00:01:12 @agent_ppo2.py:185][0m |          -0.0078 |          58.1554 |          12.5679 |
[32m[20221214 00:01:12 @agent_ppo2.py:185][0m |          -0.0078 |          57.9727 |          12.5556 |
[32m[20221214 00:01:12 @agent_ppo2.py:185][0m |          -0.0099 |          57.6268 |          12.5683 |
[32m[20221214 00:01:12 @agent_ppo2.py:185][0m |          -0.0109 |          57.4826 |          12.5917 |
[32m[20221214 00:01:12 @agent_ppo2.py:185][0m |          -0.0109 |          57.2636 |          12.6426 |
[32m[20221214 00:01:13 @agent_ppo2.py:185][0m |          -0.0137 |          57.1213 |          12.6284 |
[32m[20221214 00:01:13 @agent_ppo2.py:185][0m |          -0.0124 |          56.9422 |          12.6503 |
[32m[20221214 00:01:13 @agent_ppo2.py:185][0m |          -0.0086 |          57.4276 |          12.6399 |
[32m[20221214 00:01:13 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:01:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 463.80
[32m[20221214 00:01:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 489.70
[32m[20221214 00:01:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 447.51
[32m[20221214 00:01:13 @agent_ppo2.py:143][0m Total time:       3.71 min
[32m[20221214 00:01:13 @agent_ppo2.py:145][0m 329728 total steps have happened
[32m[20221214 00:01:13 @agent_ppo2.py:121][0m #------------------------ Iteration 4161 --------------------------#
[32m[20221214 00:01:13 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:01:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:01:13 @agent_ppo2.py:185][0m |           0.0034 |          61.3374 |          12.6535 |
[32m[20221214 00:01:13 @agent_ppo2.py:185][0m |           0.0025 |          60.3137 |          12.6028 |
[32m[20221214 00:01:13 @agent_ppo2.py:185][0m |          -0.0055 |          59.4656 |          12.6091 |
[32m[20221214 00:01:14 @agent_ppo2.py:185][0m |          -0.0064 |          59.4762 |          12.6180 |
[32m[20221214 00:01:14 @agent_ppo2.py:185][0m |          -0.0102 |          59.2202 |          12.6200 |
[32m[20221214 00:01:14 @agent_ppo2.py:185][0m |          -0.0120 |          59.2483 |          12.6202 |
[32m[20221214 00:01:14 @agent_ppo2.py:185][0m |          -0.0102 |          59.1294 |          12.6072 |
[32m[20221214 00:01:14 @agent_ppo2.py:185][0m |          -0.0122 |          58.6652 |          12.6060 |
[32m[20221214 00:01:14 @agent_ppo2.py:185][0m |          -0.0100 |          59.4494 |          12.6467 |
[32m[20221214 00:01:14 @agent_ppo2.py:185][0m |          -0.0149 |          58.5100 |          12.6431 |
[32m[20221214 00:01:14 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:01:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 488.30
[32m[20221214 00:01:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 513.85
[32m[20221214 00:01:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 491.21
[32m[20221214 00:01:14 @agent_ppo2.py:143][0m Total time:       3.73 min
[32m[20221214 00:01:14 @agent_ppo2.py:145][0m 331776 total steps have happened
[32m[20221214 00:01:14 @agent_ppo2.py:121][0m #------------------------ Iteration 4162 --------------------------#
[32m[20221214 00:01:14 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:01:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:01:15 @agent_ppo2.py:185][0m |          -0.0048 |          54.4038 |          12.4631 |
[32m[20221214 00:01:15 @agent_ppo2.py:185][0m |          -0.0067 |          49.3836 |          12.4673 |
[32m[20221214 00:01:15 @agent_ppo2.py:185][0m |          -0.0060 |          48.0062 |          12.4557 |
[32m[20221214 00:01:15 @agent_ppo2.py:185][0m |          -0.0122 |          46.9795 |          12.4388 |
[32m[20221214 00:01:15 @agent_ppo2.py:185][0m |          -0.0120 |          46.3431 |          12.4270 |
[32m[20221214 00:01:15 @agent_ppo2.py:185][0m |          -0.0119 |          46.1115 |          12.4517 |
[32m[20221214 00:01:15 @agent_ppo2.py:185][0m |          -0.0201 |          45.5933 |          12.4130 |
[32m[20221214 00:01:15 @agent_ppo2.py:185][0m |          -0.0187 |          44.9536 |          12.4451 |
[32m[20221214 00:01:15 @agent_ppo2.py:185][0m |          -0.0183 |          44.9776 |          12.4129 |
[32m[20221214 00:01:15 @agent_ppo2.py:185][0m |          -0.0222 |          44.3426 |          12.4367 |
[32m[20221214 00:01:15 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:01:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 415.30
[32m[20221214 00:01:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 487.22
[32m[20221214 00:01:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 487.21
[32m[20221214 00:01:16 @agent_ppo2.py:143][0m Total time:       3.75 min
[32m[20221214 00:01:16 @agent_ppo2.py:145][0m 333824 total steps have happened
[32m[20221214 00:01:16 @agent_ppo2.py:121][0m #------------------------ Iteration 4163 --------------------------#
[32m[20221214 00:01:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:01:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:01:16 @agent_ppo2.py:185][0m |          -0.0060 |          66.2630 |          12.6959 |
[32m[20221214 00:01:16 @agent_ppo2.py:185][0m |          -0.0075 |          60.9749 |          12.7117 |
[32m[20221214 00:01:16 @agent_ppo2.py:185][0m |          -0.0113 |          58.8246 |          12.6714 |
[32m[20221214 00:01:16 @agent_ppo2.py:185][0m |          -0.0117 |          57.4524 |          12.6849 |
[32m[20221214 00:01:16 @agent_ppo2.py:185][0m |          -0.0137 |          56.0420 |          12.6540 |
[32m[20221214 00:01:16 @agent_ppo2.py:185][0m |          -0.0166 |          55.7191 |          12.6232 |
[32m[20221214 00:01:16 @agent_ppo2.py:185][0m |          -0.0173 |          54.5330 |          12.6488 |
[32m[20221214 00:01:17 @agent_ppo2.py:185][0m |          -0.0192 |          54.3614 |          12.6121 |
[32m[20221214 00:01:17 @agent_ppo2.py:185][0m |           0.0119 |          77.5775 |          12.6019 |
[32m[20221214 00:01:17 @agent_ppo2.py:185][0m |          -0.0171 |          54.4223 |          12.5836 |
[32m[20221214 00:01:17 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:01:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 390.95
[32m[20221214 00:01:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 450.02
[32m[20221214 00:01:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 473.91
[32m[20221214 00:01:17 @agent_ppo2.py:143][0m Total time:       3.78 min
[32m[20221214 00:01:17 @agent_ppo2.py:145][0m 335872 total steps have happened
[32m[20221214 00:01:17 @agent_ppo2.py:121][0m #------------------------ Iteration 4164 --------------------------#
[32m[20221214 00:01:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:01:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:01:17 @agent_ppo2.py:185][0m |           0.0014 |          72.4766 |          12.4452 |
[32m[20221214 00:01:17 @agent_ppo2.py:185][0m |          -0.0110 |          68.6380 |          12.4181 |
[32m[20221214 00:01:17 @agent_ppo2.py:185][0m |          -0.0110 |          67.6270 |          12.3889 |
[32m[20221214 00:01:18 @agent_ppo2.py:185][0m |          -0.0162 |          66.7798 |          12.4165 |
[32m[20221214 00:01:18 @agent_ppo2.py:185][0m |          -0.0086 |          68.1942 |          12.4141 |
[32m[20221214 00:01:18 @agent_ppo2.py:185][0m |          -0.0171 |          65.8768 |          12.4359 |
[32m[20221214 00:01:18 @agent_ppo2.py:185][0m |          -0.0166 |          65.0511 |          12.4057 |
[32m[20221214 00:01:18 @agent_ppo2.py:185][0m |          -0.0110 |          65.3718 |          12.3995 |
[32m[20221214 00:01:18 @agent_ppo2.py:185][0m |          -0.0147 |          65.1635 |          12.3927 |
[32m[20221214 00:01:18 @agent_ppo2.py:185][0m |          -0.0187 |          64.3347 |          12.4083 |
[32m[20221214 00:01:18 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:01:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 441.97
[32m[20221214 00:01:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 470.49
[32m[20221214 00:01:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 550.14
[32m[20221214 00:01:18 @agent_ppo2.py:143][0m Total time:       3.80 min
[32m[20221214 00:01:18 @agent_ppo2.py:145][0m 337920 total steps have happened
[32m[20221214 00:01:18 @agent_ppo2.py:121][0m #------------------------ Iteration 4165 --------------------------#
[32m[20221214 00:01:18 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:01:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:01:19 @agent_ppo2.py:185][0m |           0.0086 |          71.9954 |          12.0231 |
[32m[20221214 00:01:19 @agent_ppo2.py:185][0m |          -0.0033 |          64.7715 |          12.0439 |
[32m[20221214 00:01:19 @agent_ppo2.py:185][0m |          -0.0109 |          63.4206 |          12.0248 |
[32m[20221214 00:01:19 @agent_ppo2.py:185][0m |          -0.0041 |          63.4762 |          12.0230 |
[32m[20221214 00:01:19 @agent_ppo2.py:185][0m |          -0.0090 |          62.3038 |          12.0458 |
[32m[20221214 00:01:19 @agent_ppo2.py:185][0m |          -0.0072 |          61.6441 |          11.9737 |
[32m[20221214 00:01:19 @agent_ppo2.py:185][0m |          -0.0124 |          61.3008 |          11.9829 |
[32m[20221214 00:01:19 @agent_ppo2.py:185][0m |          -0.0134 |          60.8322 |          11.9981 |
[32m[20221214 00:01:19 @agent_ppo2.py:185][0m |          -0.0173 |          60.6588 |          12.0145 |
[32m[20221214 00:01:19 @agent_ppo2.py:185][0m |          -0.0137 |          60.6111 |          11.9904 |
[32m[20221214 00:01:19 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:01:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 479.48
[32m[20221214 00:01:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 540.28
[32m[20221214 00:01:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 493.33
[32m[20221214 00:01:20 @agent_ppo2.py:143][0m Total time:       3.82 min
[32m[20221214 00:01:20 @agent_ppo2.py:145][0m 339968 total steps have happened
[32m[20221214 00:01:20 @agent_ppo2.py:121][0m #------------------------ Iteration 4166 --------------------------#
[32m[20221214 00:01:20 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:01:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:01:20 @agent_ppo2.py:185][0m |           0.0191 |          78.7245 |          12.0762 |
[32m[20221214 00:01:20 @agent_ppo2.py:185][0m |          -0.0032 |          64.0707 |          12.0304 |
[32m[20221214 00:01:20 @agent_ppo2.py:185][0m |          -0.0094 |          61.8351 |          12.0627 |
[32m[20221214 00:01:20 @agent_ppo2.py:185][0m |          -0.0069 |          61.5145 |          12.0241 |
[32m[20221214 00:01:20 @agent_ppo2.py:185][0m |          -0.0102 |          59.5047 |          12.0563 |
[32m[20221214 00:01:20 @agent_ppo2.py:185][0m |          -0.0175 |          58.6940 |          11.9948 |
[32m[20221214 00:01:20 @agent_ppo2.py:185][0m |          -0.0101 |          58.7404 |          11.9867 |
[32m[20221214 00:01:21 @agent_ppo2.py:185][0m |          -0.0167 |          57.8723 |          11.9902 |
[32m[20221214 00:01:21 @agent_ppo2.py:185][0m |          -0.0167 |          57.3138 |          11.9774 |
[32m[20221214 00:01:21 @agent_ppo2.py:185][0m |          -0.0195 |          57.0839 |          11.9735 |
[32m[20221214 00:01:21 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:01:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 434.93
[32m[20221214 00:01:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 510.38
[32m[20221214 00:01:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 315.92
[32m[20221214 00:01:21 @agent_ppo2.py:143][0m Total time:       3.84 min
[32m[20221214 00:01:21 @agent_ppo2.py:145][0m 342016 total steps have happened
[32m[20221214 00:01:21 @agent_ppo2.py:121][0m #------------------------ Iteration 4167 --------------------------#
[32m[20221214 00:01:21 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:01:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:01:21 @agent_ppo2.py:185][0m |          -0.0008 |          63.2550 |          12.5246 |
[32m[20221214 00:01:21 @agent_ppo2.py:185][0m |          -0.0054 |          61.2594 |          12.5303 |
[32m[20221214 00:01:21 @agent_ppo2.py:185][0m |          -0.0050 |          60.5524 |          12.5991 |
[32m[20221214 00:01:21 @agent_ppo2.py:185][0m |          -0.0055 |          60.0951 |          12.5731 |
[32m[20221214 00:01:22 @agent_ppo2.py:185][0m |          -0.0062 |          59.6926 |          12.6190 |
[32m[20221214 00:01:22 @agent_ppo2.py:185][0m |          -0.0070 |          59.4211 |          12.6224 |
[32m[20221214 00:01:22 @agent_ppo2.py:185][0m |          -0.0072 |          58.9716 |          12.6560 |
[32m[20221214 00:01:22 @agent_ppo2.py:185][0m |          -0.0089 |          58.8587 |          12.6561 |
[32m[20221214 00:01:22 @agent_ppo2.py:185][0m |          -0.0083 |          58.5776 |          12.6539 |
[32m[20221214 00:01:22 @agent_ppo2.py:185][0m |           0.0003 |          63.2808 |          12.6908 |
[32m[20221214 00:01:22 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:01:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 475.87
[32m[20221214 00:01:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 509.40
[32m[20221214 00:01:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 544.98
[32m[20221214 00:01:22 @agent_ppo2.py:143][0m Total time:       3.86 min
[32m[20221214 00:01:22 @agent_ppo2.py:145][0m 344064 total steps have happened
[32m[20221214 00:01:22 @agent_ppo2.py:121][0m #------------------------ Iteration 4168 --------------------------#
[32m[20221214 00:01:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:01:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:01:23 @agent_ppo2.py:185][0m |          -0.0015 |          73.2636 |          12.2375 |
[32m[20221214 00:01:23 @agent_ppo2.py:185][0m |          -0.0033 |          70.7860 |          12.1821 |
[32m[20221214 00:01:23 @agent_ppo2.py:185][0m |          -0.0014 |          70.6207 |          12.1480 |
[32m[20221214 00:01:23 @agent_ppo2.py:185][0m |          -0.0093 |          69.1766 |          12.2256 |
[32m[20221214 00:01:23 @agent_ppo2.py:185][0m |          -0.0057 |          68.6316 |          12.1565 |
[32m[20221214 00:01:23 @agent_ppo2.py:185][0m |          -0.0083 |          68.4556 |          12.1967 |
[32m[20221214 00:01:23 @agent_ppo2.py:185][0m |          -0.0096 |          68.0849 |          12.1732 |
[32m[20221214 00:01:23 @agent_ppo2.py:185][0m |          -0.0141 |          67.8135 |          12.1569 |
[32m[20221214 00:01:23 @agent_ppo2.py:185][0m |          -0.0137 |          67.7486 |          12.1738 |
[32m[20221214 00:01:23 @agent_ppo2.py:185][0m |          -0.0127 |          67.3980 |          12.1831 |
[32m[20221214 00:01:23 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:01:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 465.20
[32m[20221214 00:01:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 487.13
[32m[20221214 00:01:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 376.17
[32m[20221214 00:01:24 @agent_ppo2.py:143][0m Total time:       3.89 min
[32m[20221214 00:01:24 @agent_ppo2.py:145][0m 346112 total steps have happened
[32m[20221214 00:01:24 @agent_ppo2.py:121][0m #------------------------ Iteration 4169 --------------------------#
[32m[20221214 00:01:24 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:01:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:01:24 @agent_ppo2.py:185][0m |           0.0060 |          70.5726 |          12.7039 |
[32m[20221214 00:01:24 @agent_ppo2.py:185][0m |          -0.0070 |          64.2472 |          12.7116 |
[32m[20221214 00:01:24 @agent_ppo2.py:185][0m |          -0.0063 |          62.2698 |          12.6705 |
[32m[20221214 00:01:24 @agent_ppo2.py:185][0m |          -0.0099 |          60.8626 |          12.6726 |
[32m[20221214 00:01:24 @agent_ppo2.py:185][0m |          -0.0115 |          60.1843 |          12.7031 |
[32m[20221214 00:01:24 @agent_ppo2.py:185][0m |          -0.0071 |          60.7321 |          12.7139 |
[32m[20221214 00:01:24 @agent_ppo2.py:185][0m |          -0.0136 |          58.7741 |          12.7048 |
[32m[20221214 00:01:24 @agent_ppo2.py:185][0m |          -0.0122 |          58.3906 |          12.7016 |
[32m[20221214 00:01:25 @agent_ppo2.py:185][0m |          -0.0115 |          57.9866 |          12.7443 |
[32m[20221214 00:01:25 @agent_ppo2.py:185][0m |          -0.0170 |          57.3104 |          12.7468 |
[32m[20221214 00:01:25 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:01:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 370.78
[32m[20221214 00:01:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 455.06
[32m[20221214 00:01:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 552.48
[32m[20221214 00:01:25 @agent_ppo2.py:143][0m Total time:       3.91 min
[32m[20221214 00:01:25 @agent_ppo2.py:145][0m 348160 total steps have happened
[32m[20221214 00:01:25 @agent_ppo2.py:121][0m #------------------------ Iteration 4170 --------------------------#
[32m[20221214 00:01:25 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:01:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:01:25 @agent_ppo2.py:185][0m |          -0.0021 |          58.7876 |          12.1507 |
[32m[20221214 00:01:25 @agent_ppo2.py:185][0m |          -0.0059 |          55.3181 |          12.0934 |
[32m[20221214 00:01:25 @agent_ppo2.py:185][0m |          -0.0060 |          54.2737 |          12.1172 |
[32m[20221214 00:01:25 @agent_ppo2.py:185][0m |          -0.0095 |          53.5101 |          12.1158 |
[32m[20221214 00:01:26 @agent_ppo2.py:185][0m |          -0.0075 |          53.4806 |          12.1439 |
[32m[20221214 00:01:26 @agent_ppo2.py:185][0m |          -0.0134 |          52.6524 |          12.1227 |
[32m[20221214 00:01:26 @agent_ppo2.py:185][0m |          -0.0088 |          52.8417 |          12.1302 |
[32m[20221214 00:01:26 @agent_ppo2.py:185][0m |          -0.0115 |          51.9932 |          12.0901 |
[32m[20221214 00:01:26 @agent_ppo2.py:185][0m |          -0.0160 |          51.4706 |          12.1523 |
[32m[20221214 00:01:26 @agent_ppo2.py:185][0m |          -0.0141 |          51.0499 |          12.1278 |
[32m[20221214 00:01:26 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:01:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 470.63
[32m[20221214 00:01:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 530.05
[32m[20221214 00:01:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 419.09
[32m[20221214 00:01:26 @agent_ppo2.py:143][0m Total time:       3.93 min
[32m[20221214 00:01:26 @agent_ppo2.py:145][0m 350208 total steps have happened
[32m[20221214 00:01:26 @agent_ppo2.py:121][0m #------------------------ Iteration 4171 --------------------------#
[32m[20221214 00:01:26 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:01:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:01:26 @agent_ppo2.py:185][0m |          -0.0003 |          50.1359 |          12.0875 |
[32m[20221214 00:01:27 @agent_ppo2.py:185][0m |           0.0006 |          43.3598 |          12.0539 |
[32m[20221214 00:01:27 @agent_ppo2.py:185][0m |          -0.0113 |          40.9883 |          12.0350 |
[32m[20221214 00:01:27 @agent_ppo2.py:185][0m |          -0.0085 |          39.9186 |          11.9839 |
[32m[20221214 00:01:27 @agent_ppo2.py:185][0m |          -0.0121 |          38.8406 |          11.9156 |
[32m[20221214 00:01:27 @agent_ppo2.py:185][0m |          -0.0163 |          38.1437 |          11.9134 |
[32m[20221214 00:01:27 @agent_ppo2.py:185][0m |          -0.0152 |          37.8463 |          11.8848 |
[32m[20221214 00:01:27 @agent_ppo2.py:185][0m |          -0.0131 |          37.1433 |          11.8253 |
[32m[20221214 00:01:27 @agent_ppo2.py:185][0m |          -0.0239 |          36.6413 |          11.8015 |
[32m[20221214 00:01:27 @agent_ppo2.py:185][0m |          -0.0205 |          36.4231 |          11.7287 |
[32m[20221214 00:01:27 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:01:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 328.28
[32m[20221214 00:01:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 425.42
[32m[20221214 00:01:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 302.53
[32m[20221214 00:01:27 @agent_ppo2.py:143][0m Total time:       3.95 min
[32m[20221214 00:01:27 @agent_ppo2.py:145][0m 352256 total steps have happened
[32m[20221214 00:01:27 @agent_ppo2.py:121][0m #------------------------ Iteration 4172 --------------------------#
[32m[20221214 00:01:28 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:01:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:01:28 @agent_ppo2.py:185][0m |          -0.0004 |          72.2550 |          11.8657 |
[32m[20221214 00:01:28 @agent_ppo2.py:185][0m |          -0.0005 |          67.9549 |          11.9106 |
[32m[20221214 00:01:28 @agent_ppo2.py:185][0m |           0.0028 |          68.5450 |          11.9534 |
[32m[20221214 00:01:28 @agent_ppo2.py:185][0m |          -0.0058 |          66.5675 |          11.9759 |
[32m[20221214 00:01:28 @agent_ppo2.py:185][0m |          -0.0091 |          66.2624 |          11.9824 |
[32m[20221214 00:01:28 @agent_ppo2.py:185][0m |          -0.0060 |          65.8042 |          11.9870 |
[32m[20221214 00:01:28 @agent_ppo2.py:185][0m |          -0.0094 |          65.7149 |          12.0140 |
[32m[20221214 00:01:28 @agent_ppo2.py:185][0m |          -0.0114 |          65.3765 |          12.0417 |
[32m[20221214 00:01:29 @agent_ppo2.py:185][0m |          -0.0100 |          65.1001 |          12.0356 |
[32m[20221214 00:01:29 @agent_ppo2.py:185][0m |          -0.0109 |          65.1903 |          12.0734 |
[32m[20221214 00:01:29 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:01:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 501.69
[32m[20221214 00:01:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 530.33
[32m[20221214 00:01:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 557.81
[32m[20221214 00:01:29 @agent_ppo2.py:143][0m Total time:       3.97 min
[32m[20221214 00:01:29 @agent_ppo2.py:145][0m 354304 total steps have happened
[32m[20221214 00:01:29 @agent_ppo2.py:121][0m #------------------------ Iteration 4173 --------------------------#
[32m[20221214 00:01:29 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:01:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:01:29 @agent_ppo2.py:185][0m |           0.0014 |          45.3706 |          11.8884 |
[32m[20221214 00:01:29 @agent_ppo2.py:185][0m |          -0.0024 |          40.8203 |          11.8318 |
[32m[20221214 00:01:29 @agent_ppo2.py:185][0m |          -0.0068 |          39.1015 |          11.8650 |
[32m[20221214 00:01:29 @agent_ppo2.py:185][0m |          -0.0130 |          37.9627 |          11.8370 |
[32m[20221214 00:01:30 @agent_ppo2.py:185][0m |          -0.0138 |          37.0508 |          11.8763 |
[32m[20221214 00:01:30 @agent_ppo2.py:185][0m |          -0.0146 |          35.9942 |          11.8425 |
[32m[20221214 00:01:30 @agent_ppo2.py:185][0m |          -0.0152 |          35.3224 |          11.8415 |
[32m[20221214 00:01:30 @agent_ppo2.py:185][0m |          -0.0167 |          34.7441 |          11.8663 |
[32m[20221214 00:01:30 @agent_ppo2.py:185][0m |          -0.0129 |          34.3669 |          11.8175 |
[32m[20221214 00:01:30 @agent_ppo2.py:185][0m |          -0.0090 |          35.1421 |          11.8187 |
[32m[20221214 00:01:30 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:01:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 405.92
[32m[20221214 00:01:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 502.70
[32m[20221214 00:01:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 410.42
[32m[20221214 00:01:30 @agent_ppo2.py:143][0m Total time:       4.00 min
[32m[20221214 00:01:30 @agent_ppo2.py:145][0m 356352 total steps have happened
[32m[20221214 00:01:30 @agent_ppo2.py:121][0m #------------------------ Iteration 4174 --------------------------#
[32m[20221214 00:01:30 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:01:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:01:31 @agent_ppo2.py:185][0m |           0.0043 |          63.5242 |          11.6273 |
[32m[20221214 00:01:31 @agent_ppo2.py:185][0m |           0.0062 |          63.2715 |          11.6635 |
[32m[20221214 00:01:31 @agent_ppo2.py:185][0m |          -0.0007 |          59.3110 |          11.6095 |
[32m[20221214 00:01:31 @agent_ppo2.py:185][0m |          -0.0054 |          58.6424 |          11.6390 |
[32m[20221214 00:01:31 @agent_ppo2.py:185][0m |          -0.0033 |          58.1593 |          11.5500 |
[32m[20221214 00:01:31 @agent_ppo2.py:185][0m |          -0.0047 |          57.9326 |          11.5773 |
[32m[20221214 00:01:31 @agent_ppo2.py:185][0m |          -0.0082 |          57.6280 |          11.5665 |
[32m[20221214 00:01:31 @agent_ppo2.py:185][0m |          -0.0091 |          57.4264 |          11.5435 |
[32m[20221214 00:01:31 @agent_ppo2.py:185][0m |          -0.0079 |          57.4327 |          11.5615 |
[32m[20221214 00:01:31 @agent_ppo2.py:185][0m |          -0.0123 |          57.1866 |          11.5452 |
[32m[20221214 00:01:31 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:01:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 496.30
[32m[20221214 00:01:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 512.53
[32m[20221214 00:01:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221214 00:01:32 @agent_ppo2.py:143][0m Total time:       4.02 min
[32m[20221214 00:01:32 @agent_ppo2.py:145][0m 358400 total steps have happened
[32m[20221214 00:01:32 @agent_ppo2.py:121][0m #------------------------ Iteration 4175 --------------------------#
[32m[20221214 00:01:32 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:01:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:01:32 @agent_ppo2.py:185][0m |          -0.0014 |          64.8316 |          11.7090 |
[32m[20221214 00:01:32 @agent_ppo2.py:185][0m |           0.0065 |          69.9625 |          11.7593 |
[32m[20221214 00:01:32 @agent_ppo2.py:185][0m |          -0.0038 |          62.3081 |          11.7836 |
[32m[20221214 00:01:32 @agent_ppo2.py:185][0m |          -0.0118 |          61.8832 |          11.7436 |
[32m[20221214 00:01:32 @agent_ppo2.py:185][0m |          -0.0094 |          61.7278 |          11.8171 |
[32m[20221214 00:01:32 @agent_ppo2.py:185][0m |          -0.0099 |          61.7879 |          11.8501 |
[32m[20221214 00:01:32 @agent_ppo2.py:185][0m |          -0.0143 |          61.2436 |          11.7961 |
[32m[20221214 00:01:33 @agent_ppo2.py:185][0m |          -0.0133 |          61.0762 |          11.7928 |
[32m[20221214 00:01:33 @agent_ppo2.py:185][0m |          -0.0156 |          61.0503 |          11.8347 |
[32m[20221214 00:01:33 @agent_ppo2.py:185][0m |          -0.0156 |          60.9544 |          11.8096 |
[32m[20221214 00:01:33 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:01:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 507.30
[32m[20221214 00:01:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 524.79
[32m[20221214 00:01:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 464.67
[32m[20221214 00:01:33 @agent_ppo2.py:143][0m Total time:       4.04 min
[32m[20221214 00:01:33 @agent_ppo2.py:145][0m 360448 total steps have happened
[32m[20221214 00:01:33 @agent_ppo2.py:121][0m #------------------------ Iteration 4176 --------------------------#
[32m[20221214 00:01:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:01:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:01:33 @agent_ppo2.py:185][0m |           0.0077 |          50.6273 |          12.1257 |
[32m[20221214 00:01:33 @agent_ppo2.py:185][0m |          -0.0094 |          42.4115 |          12.1274 |
[32m[20221214 00:01:33 @agent_ppo2.py:185][0m |          -0.0070 |          40.0893 |          12.1642 |
[32m[20221214 00:01:34 @agent_ppo2.py:185][0m |          -0.0018 |          39.8137 |          12.1860 |
[32m[20221214 00:01:34 @agent_ppo2.py:185][0m |          -0.0128 |          37.6978 |          12.1685 |
[32m[20221214 00:01:34 @agent_ppo2.py:185][0m |          -0.0194 |          36.5664 |          12.1562 |
[32m[20221214 00:01:34 @agent_ppo2.py:185][0m |          -0.0176 |          36.2613 |          12.1960 |
[32m[20221214 00:01:34 @agent_ppo2.py:185][0m |          -0.0130 |          36.4105 |          12.2218 |
[32m[20221214 00:01:34 @agent_ppo2.py:185][0m |          -0.0198 |          34.9084 |          12.2100 |
[32m[20221214 00:01:34 @agent_ppo2.py:185][0m |          -0.0037 |          42.6571 |          12.1767 |
[32m[20221214 00:01:34 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:01:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 335.54
[32m[20221214 00:01:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 422.75
[32m[20221214 00:01:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 424.03
[32m[20221214 00:01:34 @agent_ppo2.py:143][0m Total time:       4.06 min
[32m[20221214 00:01:34 @agent_ppo2.py:145][0m 362496 total steps have happened
[32m[20221214 00:01:34 @agent_ppo2.py:121][0m #------------------------ Iteration 4177 --------------------------#
[32m[20221214 00:01:34 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:01:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:01:35 @agent_ppo2.py:185][0m |           0.0008 |          69.8345 |          12.2917 |
[32m[20221214 00:01:35 @agent_ppo2.py:185][0m |          -0.0075 |          67.4847 |          12.2098 |
[32m[20221214 00:01:35 @agent_ppo2.py:185][0m |          -0.0086 |          66.5842 |          12.2276 |
[32m[20221214 00:01:35 @agent_ppo2.py:185][0m |          -0.0016 |          68.2464 |          12.2228 |
[32m[20221214 00:01:35 @agent_ppo2.py:185][0m |          -0.0014 |          69.4460 |          12.2383 |
[32m[20221214 00:01:35 @agent_ppo2.py:185][0m |          -0.0059 |          66.8139 |          12.2369 |
[32m[20221214 00:01:35 @agent_ppo2.py:185][0m |          -0.0094 |          65.5661 |          12.1985 |
[32m[20221214 00:01:35 @agent_ppo2.py:185][0m |          -0.0152 |          65.3967 |          12.2017 |
[32m[20221214 00:01:35 @agent_ppo2.py:185][0m |          -0.0125 |          65.0883 |          12.2194 |
[32m[20221214 00:01:35 @agent_ppo2.py:185][0m |          -0.0134 |          65.0307 |          12.2131 |
[32m[20221214 00:01:35 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:01:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 485.88
[32m[20221214 00:01:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 526.72
[32m[20221214 00:01:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 329.23
[32m[20221214 00:01:36 @agent_ppo2.py:143][0m Total time:       4.09 min
[32m[20221214 00:01:36 @agent_ppo2.py:145][0m 364544 total steps have happened
[32m[20221214 00:01:36 @agent_ppo2.py:121][0m #------------------------ Iteration 4178 --------------------------#
[32m[20221214 00:01:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:01:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:01:36 @agent_ppo2.py:185][0m |          -0.0001 |          62.2902 |          11.9265 |
[32m[20221214 00:01:36 @agent_ppo2.py:185][0m |          -0.0035 |          60.6497 |          11.9084 |
[32m[20221214 00:01:36 @agent_ppo2.py:185][0m |          -0.0032 |          61.1042 |          11.8935 |
[32m[20221214 00:01:36 @agent_ppo2.py:185][0m |          -0.0043 |          60.0878 |          11.9153 |
[32m[20221214 00:01:36 @agent_ppo2.py:185][0m |          -0.0061 |          59.7410 |          11.8598 |
[32m[20221214 00:01:36 @agent_ppo2.py:185][0m |          -0.0066 |          59.5504 |          11.8414 |
[32m[20221214 00:01:36 @agent_ppo2.py:185][0m |          -0.0080 |          59.3619 |          11.8140 |
[32m[20221214 00:01:37 @agent_ppo2.py:185][0m |          -0.0094 |          59.2250 |          11.7942 |
[32m[20221214 00:01:37 @agent_ppo2.py:185][0m |          -0.0101 |          59.0151 |          11.7717 |
[32m[20221214 00:01:37 @agent_ppo2.py:185][0m |          -0.0098 |          58.8897 |          11.7418 |
[32m[20221214 00:01:37 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:01:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 477.65
[32m[20221214 00:01:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 498.60
[32m[20221214 00:01:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 406.40
[32m[20221214 00:01:37 @agent_ppo2.py:143][0m Total time:       4.11 min
[32m[20221214 00:01:37 @agent_ppo2.py:145][0m 366592 total steps have happened
[32m[20221214 00:01:37 @agent_ppo2.py:121][0m #------------------------ Iteration 4179 --------------------------#
[32m[20221214 00:01:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:01:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:01:37 @agent_ppo2.py:185][0m |          -0.0010 |          56.8028 |          11.6743 |
[32m[20221214 00:01:37 @agent_ppo2.py:185][0m |          -0.0011 |          53.5800 |          11.6434 |
[32m[20221214 00:01:37 @agent_ppo2.py:185][0m |          -0.0133 |          50.5680 |          11.6399 |
[32m[20221214 00:01:37 @agent_ppo2.py:185][0m |          -0.0125 |          49.3914 |          11.6338 |
[32m[20221214 00:01:38 @agent_ppo2.py:185][0m |          -0.0037 |          49.4652 |          11.6774 |
[32m[20221214 00:01:38 @agent_ppo2.py:185][0m |          -0.0134 |          48.0539 |          11.6269 |
[32m[20221214 00:01:38 @agent_ppo2.py:185][0m |          -0.0151 |          47.6910 |          11.6478 |
[32m[20221214 00:01:38 @agent_ppo2.py:185][0m |          -0.0147 |          47.3339 |          11.6899 |
[32m[20221214 00:01:38 @agent_ppo2.py:185][0m |          -0.0123 |          48.7999 |          11.6602 |
[32m[20221214 00:01:38 @agent_ppo2.py:185][0m |          -0.0164 |          46.8961 |          11.6904 |
[32m[20221214 00:01:38 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:01:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 481.00
[32m[20221214 00:01:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 549.24
[32m[20221214 00:01:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 536.58
[32m[20221214 00:01:38 @agent_ppo2.py:143][0m Total time:       4.13 min
[32m[20221214 00:01:38 @agent_ppo2.py:145][0m 368640 total steps have happened
[32m[20221214 00:01:38 @agent_ppo2.py:121][0m #------------------------ Iteration 4180 --------------------------#
[32m[20221214 00:01:38 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:01:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:01:39 @agent_ppo2.py:185][0m |           0.0064 |          69.4453 |          11.8628 |
[32m[20221214 00:01:39 @agent_ppo2.py:185][0m |          -0.0030 |          63.9898 |          11.8759 |
[32m[20221214 00:01:39 @agent_ppo2.py:185][0m |          -0.0047 |          63.2685 |          11.8763 |
[32m[20221214 00:01:39 @agent_ppo2.py:185][0m |          -0.0073 |          62.0100 |          11.8615 |
[32m[20221214 00:01:39 @agent_ppo2.py:185][0m |          -0.0113 |          61.6829 |          11.8369 |
[32m[20221214 00:01:39 @agent_ppo2.py:185][0m |          -0.0157 |          61.1800 |          11.8358 |
[32m[20221214 00:01:39 @agent_ppo2.py:185][0m |          -0.0124 |          61.0538 |          11.8083 |
[32m[20221214 00:01:39 @agent_ppo2.py:185][0m |          -0.0125 |          60.8373 |          11.8047 |
[32m[20221214 00:01:39 @agent_ppo2.py:185][0m |          -0.0174 |          60.6422 |          11.7872 |
[32m[20221214 00:01:39 @agent_ppo2.py:185][0m |          -0.0142 |          60.2424 |          11.8152 |
[32m[20221214 00:01:39 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:01:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 490.88
[32m[20221214 00:01:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 550.00
[32m[20221214 00:01:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 441.42
[32m[20221214 00:01:39 @agent_ppo2.py:143][0m Total time:       4.15 min
[32m[20221214 00:01:39 @agent_ppo2.py:145][0m 370688 total steps have happened
[32m[20221214 00:01:39 @agent_ppo2.py:121][0m #------------------------ Iteration 4181 --------------------------#
[32m[20221214 00:01:40 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:01:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:01:40 @agent_ppo2.py:185][0m |          -0.0013 |          65.2596 |          11.5671 |
[32m[20221214 00:01:40 @agent_ppo2.py:185][0m |          -0.0061 |          61.5357 |          11.5246 |
[32m[20221214 00:01:40 @agent_ppo2.py:185][0m |          -0.0065 |          60.2416 |          11.5035 |
[32m[20221214 00:01:40 @agent_ppo2.py:185][0m |          -0.0087 |          59.5794 |          11.5316 |
[32m[20221214 00:01:40 @agent_ppo2.py:185][0m |          -0.0096 |          58.9819 |          11.4728 |
[32m[20221214 00:01:40 @agent_ppo2.py:185][0m |          -0.0106 |          58.4744 |          11.4833 |
[32m[20221214 00:01:40 @agent_ppo2.py:185][0m |          -0.0131 |          58.2323 |          11.5103 |
[32m[20221214 00:01:40 @agent_ppo2.py:185][0m |          -0.0130 |          58.1373 |          11.5579 |
[32m[20221214 00:01:40 @agent_ppo2.py:185][0m |          -0.0160 |          57.6681 |          11.4937 |
[32m[20221214 00:01:41 @agent_ppo2.py:185][0m |          -0.0144 |          57.5500 |          11.5059 |
[32m[20221214 00:01:41 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:01:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 499.62
[32m[20221214 00:01:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 519.92
[32m[20221214 00:01:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 464.57
[32m[20221214 00:01:41 @agent_ppo2.py:143][0m Total time:       4.17 min
[32m[20221214 00:01:41 @agent_ppo2.py:145][0m 372736 total steps have happened
[32m[20221214 00:01:41 @agent_ppo2.py:121][0m #------------------------ Iteration 4182 --------------------------#
[32m[20221214 00:01:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:01:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:01:41 @agent_ppo2.py:185][0m |          -0.0002 |          63.1849 |          11.6106 |
[32m[20221214 00:01:41 @agent_ppo2.py:185][0m |          -0.0072 |          57.6679 |          11.6433 |
[32m[20221214 00:01:41 @agent_ppo2.py:185][0m |          -0.0027 |          57.5356 |          11.6033 |
[32m[20221214 00:01:41 @agent_ppo2.py:185][0m |          -0.0086 |          53.7041 |          11.5840 |
[32m[20221214 00:01:41 @agent_ppo2.py:185][0m |          -0.0118 |          52.7712 |          11.5550 |
[32m[20221214 00:01:41 @agent_ppo2.py:185][0m |          -0.0125 |          51.9719 |          11.5768 |
[32m[20221214 00:01:42 @agent_ppo2.py:185][0m |          -0.0131 |          51.6703 |          11.5232 |
[32m[20221214 00:01:42 @agent_ppo2.py:185][0m |          -0.0133 |          51.3164 |          11.5180 |
[32m[20221214 00:01:42 @agent_ppo2.py:185][0m |          -0.0146 |          50.9098 |          11.5178 |
[32m[20221214 00:01:42 @agent_ppo2.py:185][0m |          -0.0155 |          50.9353 |          11.5003 |
[32m[20221214 00:01:42 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:01:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 484.06
[32m[20221214 00:01:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 501.04
[32m[20221214 00:01:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 539.23
[32m[20221214 00:01:42 @agent_ppo2.py:143][0m Total time:       4.19 min
[32m[20221214 00:01:42 @agent_ppo2.py:145][0m 374784 total steps have happened
[32m[20221214 00:01:42 @agent_ppo2.py:121][0m #------------------------ Iteration 4183 --------------------------#
[32m[20221214 00:01:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:01:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:01:42 @agent_ppo2.py:185][0m |           0.0056 |          51.6355 |          11.3065 |
[32m[20221214 00:01:42 @agent_ppo2.py:185][0m |          -0.0016 |          32.7419 |          11.2971 |
[32m[20221214 00:01:42 @agent_ppo2.py:185][0m |          -0.0054 |          30.0129 |          11.2954 |
[32m[20221214 00:01:43 @agent_ppo2.py:185][0m |          -0.0144 |          28.0640 |          11.3125 |
[32m[20221214 00:01:43 @agent_ppo2.py:185][0m |          -0.0185 |          26.8162 |          11.2537 |
[32m[20221214 00:01:43 @agent_ppo2.py:185][0m |          -0.0121 |          25.7001 |          11.2590 |
[32m[20221214 00:01:43 @agent_ppo2.py:185][0m |          -0.0130 |          25.0495 |          11.3045 |
[32m[20221214 00:01:43 @agent_ppo2.py:185][0m |          -0.0179 |          24.2113 |          11.2439 |
[32m[20221214 00:01:43 @agent_ppo2.py:185][0m |          -0.0117 |          23.9738 |          11.2988 |
[32m[20221214 00:01:43 @agent_ppo2.py:185][0m |          -0.0121 |          23.4142 |          11.2921 |
[32m[20221214 00:01:43 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:01:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 330.05
[32m[20221214 00:01:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 431.10
[32m[20221214 00:01:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 400.80
[32m[20221214 00:01:43 @agent_ppo2.py:143][0m Total time:       4.22 min
[32m[20221214 00:01:43 @agent_ppo2.py:145][0m 376832 total steps have happened
[32m[20221214 00:01:43 @agent_ppo2.py:121][0m #------------------------ Iteration 4184 --------------------------#
[32m[20221214 00:01:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:01:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:01:44 @agent_ppo2.py:185][0m |           0.0046 |          67.9266 |          11.0866 |
[32m[20221214 00:01:44 @agent_ppo2.py:185][0m |          -0.0039 |          63.1564 |          11.0707 |
[32m[20221214 00:01:44 @agent_ppo2.py:185][0m |          -0.0073 |          61.0938 |          11.0652 |
[32m[20221214 00:01:44 @agent_ppo2.py:185][0m |          -0.0061 |          60.1029 |          11.0084 |
[32m[20221214 00:01:44 @agent_ppo2.py:185][0m |          -0.0098 |          59.1708 |          10.9985 |
[32m[20221214 00:01:44 @agent_ppo2.py:185][0m |          -0.0117 |          58.3938 |          11.0165 |
[32m[20221214 00:01:44 @agent_ppo2.py:185][0m |          -0.0133 |          58.0174 |          11.0364 |
[32m[20221214 00:01:44 @agent_ppo2.py:185][0m |          -0.0135 |          57.6259 |          11.0125 |
[32m[20221214 00:01:44 @agent_ppo2.py:185][0m |          -0.0169 |          57.8933 |          11.0221 |
[32m[20221214 00:01:44 @agent_ppo2.py:185][0m |          -0.0055 |          60.2900 |          10.9794 |
[32m[20221214 00:01:44 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:01:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 489.30
[32m[20221214 00:01:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 524.64
[32m[20221214 00:01:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 505.38
[32m[20221214 00:01:45 @agent_ppo2.py:143][0m Total time:       4.24 min
[32m[20221214 00:01:45 @agent_ppo2.py:145][0m 378880 total steps have happened
[32m[20221214 00:01:45 @agent_ppo2.py:121][0m #------------------------ Iteration 4185 --------------------------#
[32m[20221214 00:01:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:01:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:01:45 @agent_ppo2.py:185][0m |           0.0080 |          52.1434 |          11.4077 |
[32m[20221214 00:01:45 @agent_ppo2.py:185][0m |           0.0049 |          36.3327 |          11.3979 |
[32m[20221214 00:01:45 @agent_ppo2.py:185][0m |          -0.0041 |          33.5431 |          11.3661 |
[32m[20221214 00:01:45 @agent_ppo2.py:185][0m |          -0.0095 |          32.2909 |          11.3663 |
[32m[20221214 00:01:45 @agent_ppo2.py:185][0m |          -0.0058 |          31.6345 |          11.3548 |
[32m[20221214 00:01:45 @agent_ppo2.py:185][0m |          -0.0124 |          31.0845 |          11.3378 |
[32m[20221214 00:01:45 @agent_ppo2.py:185][0m |          -0.0108 |          30.4422 |          11.3403 |
[32m[20221214 00:01:45 @agent_ppo2.py:185][0m |          -0.0118 |          30.1580 |          11.3021 |
[32m[20221214 00:01:46 @agent_ppo2.py:185][0m |          -0.0086 |          30.0183 |          11.3309 |
[32m[20221214 00:01:46 @agent_ppo2.py:185][0m |          -0.0132 |          29.6061 |          11.2725 |
[32m[20221214 00:01:46 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:01:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 432.23
[32m[20221214 00:01:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 521.86
[32m[20221214 00:01:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 369.93
[32m[20221214 00:01:46 @agent_ppo2.py:143][0m Total time:       4.26 min
[32m[20221214 00:01:46 @agent_ppo2.py:145][0m 380928 total steps have happened
[32m[20221214 00:01:46 @agent_ppo2.py:121][0m #------------------------ Iteration 4186 --------------------------#
[32m[20221214 00:01:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:01:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:01:46 @agent_ppo2.py:185][0m |          -0.0015 |          59.5539 |          11.3503 |
[32m[20221214 00:01:46 @agent_ppo2.py:185][0m |          -0.0057 |          56.8776 |          11.4118 |
[32m[20221214 00:01:46 @agent_ppo2.py:185][0m |          -0.0101 |          55.8608 |          11.4160 |
[32m[20221214 00:01:46 @agent_ppo2.py:185][0m |          -0.0115 |          55.2099 |          11.4800 |
[32m[20221214 00:01:46 @agent_ppo2.py:185][0m |          -0.0018 |          57.6339 |          11.4878 |
[32m[20221214 00:01:47 @agent_ppo2.py:185][0m |          -0.0085 |          54.5578 |          11.5576 |
[32m[20221214 00:01:47 @agent_ppo2.py:185][0m |          -0.0138 |          53.6894 |          11.5520 |
[32m[20221214 00:01:47 @agent_ppo2.py:185][0m |          -0.0149 |          53.2443 |          11.5893 |
[32m[20221214 00:01:47 @agent_ppo2.py:185][0m |          -0.0148 |          53.1055 |          11.6039 |
[32m[20221214 00:01:47 @agent_ppo2.py:185][0m |          -0.0171 |          52.7178 |          11.6510 |
[32m[20221214 00:01:47 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:01:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 490.83
[32m[20221214 00:01:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 501.19
[32m[20221214 00:01:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 316.72
[32m[20221214 00:01:47 @agent_ppo2.py:143][0m Total time:       4.28 min
[32m[20221214 00:01:47 @agent_ppo2.py:145][0m 382976 total steps have happened
[32m[20221214 00:01:47 @agent_ppo2.py:121][0m #------------------------ Iteration 4187 --------------------------#
[32m[20221214 00:01:47 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:01:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:01:47 @agent_ppo2.py:185][0m |          -0.0016 |          64.3851 |          11.3488 |
[32m[20221214 00:01:48 @agent_ppo2.py:185][0m |          -0.0073 |          59.6032 |          11.3447 |
[32m[20221214 00:01:48 @agent_ppo2.py:185][0m |          -0.0142 |          57.8552 |          11.4323 |
[32m[20221214 00:01:48 @agent_ppo2.py:185][0m |          -0.0090 |          56.7249 |          11.4637 |
[32m[20221214 00:01:48 @agent_ppo2.py:185][0m |          -0.0115 |          55.9897 |          11.4521 |
[32m[20221214 00:01:48 @agent_ppo2.py:185][0m |          -0.0034 |          59.2786 |          11.4620 |
[32m[20221214 00:01:48 @agent_ppo2.py:185][0m |          -0.0100 |          55.0925 |          11.4342 |
[32m[20221214 00:01:48 @agent_ppo2.py:185][0m |          -0.0148 |          54.5372 |          11.5136 |
[32m[20221214 00:01:48 @agent_ppo2.py:185][0m |          -0.0142 |          54.3373 |          11.5018 |
[32m[20221214 00:01:48 @agent_ppo2.py:185][0m |          -0.0168 |          54.0757 |          11.5552 |
[32m[20221214 00:01:48 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:01:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 499.78
[32m[20221214 00:01:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 548.62
[32m[20221214 00:01:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 443.17
[32m[20221214 00:01:48 @agent_ppo2.py:143][0m Total time:       4.30 min
[32m[20221214 00:01:48 @agent_ppo2.py:145][0m 385024 total steps have happened
[32m[20221214 00:01:48 @agent_ppo2.py:121][0m #------------------------ Iteration 4188 --------------------------#
[32m[20221214 00:01:49 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:01:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:01:49 @agent_ppo2.py:185][0m |           0.0036 |          73.4164 |          11.4373 |
[32m[20221214 00:01:49 @agent_ppo2.py:185][0m |           0.0048 |          70.2940 |          11.4878 |
[32m[20221214 00:01:49 @agent_ppo2.py:185][0m |          -0.0067 |          66.5426 |          11.4801 |
[32m[20221214 00:01:49 @agent_ppo2.py:185][0m |          -0.0040 |          65.4567 |          11.4289 |
[32m[20221214 00:01:49 @agent_ppo2.py:185][0m |          -0.0069 |          64.8435 |          11.4983 |
[32m[20221214 00:01:49 @agent_ppo2.py:185][0m |           0.0007 |          70.9675 |          11.4825 |
[32m[20221214 00:01:49 @agent_ppo2.py:185][0m |          -0.0007 |          68.0367 |          11.5522 |
[32m[20221214 00:01:49 @agent_ppo2.py:185][0m |          -0.0092 |          63.9939 |          11.4770 |
[32m[20221214 00:01:49 @agent_ppo2.py:185][0m |          -0.0144 |          63.5291 |          11.5185 |
[32m[20221214 00:01:50 @agent_ppo2.py:185][0m |          -0.0102 |          63.4381 |          11.4987 |
[32m[20221214 00:01:50 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:01:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 475.47
[32m[20221214 00:01:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 508.76
[32m[20221214 00:01:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 284.77
[32m[20221214 00:01:50 @agent_ppo2.py:143][0m Total time:       4.32 min
[32m[20221214 00:01:50 @agent_ppo2.py:145][0m 387072 total steps have happened
[32m[20221214 00:01:50 @agent_ppo2.py:121][0m #------------------------ Iteration 4189 --------------------------#
[32m[20221214 00:01:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:01:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:01:50 @agent_ppo2.py:185][0m |           0.0026 |          59.3293 |          11.9835 |
[32m[20221214 00:01:50 @agent_ppo2.py:185][0m |          -0.0069 |          54.2513 |          12.0044 |
[32m[20221214 00:01:50 @agent_ppo2.py:185][0m |          -0.0130 |          52.3222 |          12.0458 |
[32m[20221214 00:01:50 @agent_ppo2.py:185][0m |          -0.0054 |          53.1204 |          12.0864 |
[32m[20221214 00:01:50 @agent_ppo2.py:185][0m |          -0.0134 |          50.2996 |          12.0901 |
[32m[20221214 00:01:50 @agent_ppo2.py:185][0m |          -0.0142 |          49.5970 |          12.1186 |
[32m[20221214 00:01:51 @agent_ppo2.py:185][0m |          -0.0162 |          49.0159 |          12.1403 |
[32m[20221214 00:01:51 @agent_ppo2.py:185][0m |          -0.0166 |          48.6285 |          12.1805 |
[32m[20221214 00:01:51 @agent_ppo2.py:185][0m |          -0.0188 |          48.1983 |          12.1886 |
[32m[20221214 00:01:51 @agent_ppo2.py:185][0m |          -0.0162 |          47.8801 |          12.1720 |
[32m[20221214 00:01:51 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:01:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 502.27
[32m[20221214 00:01:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 540.85
[32m[20221214 00:01:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 498.83
[32m[20221214 00:01:51 @agent_ppo2.py:143][0m Total time:       4.34 min
[32m[20221214 00:01:51 @agent_ppo2.py:145][0m 389120 total steps have happened
[32m[20221214 00:01:51 @agent_ppo2.py:121][0m #------------------------ Iteration 4190 --------------------------#
[32m[20221214 00:01:51 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:01:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:01:51 @agent_ppo2.py:185][0m |          -0.0039 |          73.6373 |          12.0237 |
[32m[20221214 00:01:51 @agent_ppo2.py:185][0m |          -0.0103 |          70.7758 |          12.0613 |
[32m[20221214 00:01:51 @agent_ppo2.py:185][0m |          -0.0104 |          69.8319 |          12.0762 |
[32m[20221214 00:01:52 @agent_ppo2.py:185][0m |          -0.0129 |          68.8837 |          12.0837 |
[32m[20221214 00:01:52 @agent_ppo2.py:185][0m |          -0.0054 |          69.6741 |          12.1541 |
[32m[20221214 00:01:52 @agent_ppo2.py:185][0m |          -0.0137 |          67.7880 |          12.1319 |
[32m[20221214 00:01:52 @agent_ppo2.py:185][0m |          -0.0162 |          67.5284 |          12.1569 |
[32m[20221214 00:01:52 @agent_ppo2.py:185][0m |          -0.0152 |          66.9237 |          12.1723 |
[32m[20221214 00:01:52 @agent_ppo2.py:185][0m |          -0.0108 |          68.7076 |          12.2013 |
[32m[20221214 00:01:52 @agent_ppo2.py:185][0m |          -0.0186 |          66.7405 |          12.2132 |
[32m[20221214 00:01:52 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:01:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 484.09
[32m[20221214 00:01:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 511.95
[32m[20221214 00:01:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 489.53
[32m[20221214 00:01:52 @agent_ppo2.py:143][0m Total time:       4.36 min
[32m[20221214 00:01:52 @agent_ppo2.py:145][0m 391168 total steps have happened
[32m[20221214 00:01:52 @agent_ppo2.py:121][0m #------------------------ Iteration 4191 --------------------------#
[32m[20221214 00:01:52 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:01:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:01:53 @agent_ppo2.py:185][0m |           0.0063 |          22.1404 |          12.4125 |
[32m[20221214 00:01:53 @agent_ppo2.py:185][0m |          -0.0074 |          14.9667 |          12.4494 |
[32m[20221214 00:01:53 @agent_ppo2.py:185][0m |          -0.0223 |          13.6848 |          12.4498 |
[32m[20221214 00:01:53 @agent_ppo2.py:185][0m |          -0.0098 |          13.2321 |          12.4000 |
[32m[20221214 00:01:53 @agent_ppo2.py:185][0m |          -0.0139 |          12.7361 |          12.3990 |
[32m[20221214 00:01:53 @agent_ppo2.py:185][0m |          -0.0171 |          12.3050 |          12.4120 |
[32m[20221214 00:01:53 @agent_ppo2.py:185][0m |          -0.0207 |          12.0919 |          12.4078 |
[32m[20221214 00:01:53 @agent_ppo2.py:185][0m |          -0.0192 |          11.8365 |          12.3946 |
[32m[20221214 00:01:53 @agent_ppo2.py:185][0m |          -0.0208 |          11.6959 |          12.3858 |
[32m[20221214 00:01:53 @agent_ppo2.py:185][0m |          -0.0113 |          12.1889 |          12.3623 |
[32m[20221214 00:01:53 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:01:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 408.84
[32m[20221214 00:01:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 531.51
[32m[20221214 00:01:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 263.67
[32m[20221214 00:01:53 @agent_ppo2.py:143][0m Total time:       4.38 min
[32m[20221214 00:01:53 @agent_ppo2.py:145][0m 393216 total steps have happened
[32m[20221214 00:01:53 @agent_ppo2.py:121][0m #------------------------ Iteration 4192 --------------------------#
[32m[20221214 00:01:54 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:01:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:01:54 @agent_ppo2.py:185][0m |          -0.0028 |          27.2019 |          12.2560 |
[32m[20221214 00:01:54 @agent_ppo2.py:185][0m |          -0.0044 |          19.3330 |          12.2214 |
[32m[20221214 00:01:54 @agent_ppo2.py:185][0m |          -0.0079 |          17.6372 |          12.2085 |
[32m[20221214 00:01:54 @agent_ppo2.py:185][0m |          -0.0104 |          16.7193 |          12.2036 |
[32m[20221214 00:01:54 @agent_ppo2.py:185][0m |          -0.0176 |          16.2404 |          12.1754 |
[32m[20221214 00:01:54 @agent_ppo2.py:185][0m |          -0.0154 |          15.5399 |          12.1739 |
[32m[20221214 00:01:54 @agent_ppo2.py:185][0m |          -0.0200 |          15.0175 |          12.1472 |
[32m[20221214 00:01:54 @agent_ppo2.py:185][0m |          -0.0185 |          14.8410 |          12.1305 |
[32m[20221214 00:01:54 @agent_ppo2.py:185][0m |          -0.0235 |          14.4466 |          12.1476 |
[32m[20221214 00:01:55 @agent_ppo2.py:185][0m |          -0.0228 |          14.2353 |          12.1262 |
[32m[20221214 00:01:55 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:01:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 353.65
[32m[20221214 00:01:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 398.04
[32m[20221214 00:01:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 576.56
[32m[20221214 00:01:55 @agent_ppo2.py:143][0m Total time:       4.41 min
[32m[20221214 00:01:55 @agent_ppo2.py:145][0m 395264 total steps have happened
[32m[20221214 00:01:55 @agent_ppo2.py:121][0m #------------------------ Iteration 4193 --------------------------#
[32m[20221214 00:01:55 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:01:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:01:55 @agent_ppo2.py:185][0m |          -0.0032 |          36.4375 |          11.5109 |
[32m[20221214 00:01:55 @agent_ppo2.py:185][0m |          -0.0022 |          32.9317 |          11.4626 |
[32m[20221214 00:01:55 @agent_ppo2.py:185][0m |          -0.0117 |          31.6187 |          11.4858 |
[32m[20221214 00:01:55 @agent_ppo2.py:185][0m |          -0.0134 |          30.6785 |          11.4869 |
[32m[20221214 00:01:55 @agent_ppo2.py:185][0m |          -0.0117 |          30.1124 |          11.5119 |
[32m[20221214 00:01:56 @agent_ppo2.py:185][0m |          -0.0165 |          29.7005 |          11.5241 |
[32m[20221214 00:01:56 @agent_ppo2.py:185][0m |          -0.0126 |          29.8730 |          11.4892 |
[32m[20221214 00:01:56 @agent_ppo2.py:185][0m |          -0.0118 |          28.8937 |          11.5140 |
[32m[20221214 00:01:56 @agent_ppo2.py:185][0m |          -0.0177 |          28.7522 |          11.5175 |
[32m[20221214 00:01:56 @agent_ppo2.py:185][0m |          -0.0197 |          28.4157 |          11.5247 |
[32m[20221214 00:01:56 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:01:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 325.71
[32m[20221214 00:01:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 417.20
[32m[20221214 00:01:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 537.44
[32m[20221214 00:01:56 @agent_ppo2.py:143][0m Total time:       4.43 min
[32m[20221214 00:01:56 @agent_ppo2.py:145][0m 397312 total steps have happened
[32m[20221214 00:01:56 @agent_ppo2.py:121][0m #------------------------ Iteration 4194 --------------------------#
[32m[20221214 00:01:56 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:01:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:01:56 @agent_ppo2.py:185][0m |          -0.0017 |          65.5739 |          12.3180 |
[32m[20221214 00:01:56 @agent_ppo2.py:185][0m |           0.0029 |          67.4081 |          12.2918 |
[32m[20221214 00:01:57 @agent_ppo2.py:185][0m |          -0.0072 |          61.8435 |          12.2964 |
[32m[20221214 00:01:57 @agent_ppo2.py:185][0m |          -0.0126 |          59.5654 |          12.2781 |
[32m[20221214 00:01:57 @agent_ppo2.py:185][0m |          -0.0128 |          58.8162 |          12.2499 |
[32m[20221214 00:01:57 @agent_ppo2.py:185][0m |          -0.0138 |          58.1970 |          12.2691 |
[32m[20221214 00:01:57 @agent_ppo2.py:185][0m |          -0.0113 |          57.8257 |          12.2771 |
[32m[20221214 00:01:57 @agent_ppo2.py:185][0m |          -0.0143 |          57.5158 |          12.2689 |
[32m[20221214 00:01:57 @agent_ppo2.py:185][0m |          -0.0166 |          56.9484 |          12.2471 |
[32m[20221214 00:01:57 @agent_ppo2.py:185][0m |          -0.0164 |          56.6471 |          12.2694 |
[32m[20221214 00:01:57 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:01:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 488.15
[32m[20221214 00:01:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 561.14
[32m[20221214 00:01:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221214 00:01:57 @agent_ppo2.py:143][0m Total time:       4.45 min
[32m[20221214 00:01:57 @agent_ppo2.py:145][0m 399360 total steps have happened
[32m[20221214 00:01:57 @agent_ppo2.py:121][0m #------------------------ Iteration 4195 --------------------------#
[32m[20221214 00:01:58 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:01:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:01:58 @agent_ppo2.py:185][0m |           0.0091 |          69.6827 |          11.6495 |
[32m[20221214 00:01:58 @agent_ppo2.py:185][0m |          -0.0090 |          59.2759 |          11.6252 |
[32m[20221214 00:01:58 @agent_ppo2.py:185][0m |          -0.0082 |          56.0788 |          11.6557 |
[32m[20221214 00:01:58 @agent_ppo2.py:185][0m |          -0.0110 |          54.4574 |          11.6587 |
[32m[20221214 00:01:58 @agent_ppo2.py:185][0m |          -0.0125 |          52.7554 |          11.6022 |
[32m[20221214 00:01:58 @agent_ppo2.py:185][0m |          -0.0152 |          51.5684 |          11.6392 |
[32m[20221214 00:01:58 @agent_ppo2.py:185][0m |          -0.0158 |          50.7945 |          11.6430 |
[32m[20221214 00:01:58 @agent_ppo2.py:185][0m |          -0.0154 |          49.8706 |          11.6469 |
[32m[20221214 00:01:58 @agent_ppo2.py:185][0m |          -0.0083 |          52.6010 |          11.6460 |
[32m[20221214 00:01:59 @agent_ppo2.py:185][0m |          -0.0119 |          49.0734 |          11.6314 |
[32m[20221214 00:01:59 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:01:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 492.97
[32m[20221214 00:01:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 532.47
[32m[20221214 00:01:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 507.22
[32m[20221214 00:01:59 @agent_ppo2.py:143][0m Total time:       4.47 min
[32m[20221214 00:01:59 @agent_ppo2.py:145][0m 401408 total steps have happened
[32m[20221214 00:01:59 @agent_ppo2.py:121][0m #------------------------ Iteration 4196 --------------------------#
[32m[20221214 00:01:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:01:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:01:59 @agent_ppo2.py:185][0m |           0.0006 |          35.7215 |          11.8069 |
[32m[20221214 00:01:59 @agent_ppo2.py:185][0m |           0.0006 |          25.5881 |          11.8551 |
[32m[20221214 00:01:59 @agent_ppo2.py:185][0m |          -0.0156 |          22.8092 |          11.8313 |
[32m[20221214 00:01:59 @agent_ppo2.py:185][0m |          -0.0090 |          22.3532 |          11.8524 |
[32m[20221214 00:01:59 @agent_ppo2.py:185][0m |          -0.0109 |          21.6689 |          11.8421 |
[32m[20221214 00:01:59 @agent_ppo2.py:185][0m |          -0.0063 |          22.8974 |          11.8578 |
[32m[20221214 00:02:00 @agent_ppo2.py:185][0m |          -0.0131 |          20.5563 |          11.8808 |
[32m[20221214 00:02:00 @agent_ppo2.py:185][0m |          -0.0195 |          19.8612 |          11.8657 |
[32m[20221214 00:02:00 @agent_ppo2.py:185][0m |          -0.0266 |          19.4645 |          11.8795 |
[32m[20221214 00:02:00 @agent_ppo2.py:185][0m |          -0.0153 |          19.2651 |          11.8974 |
[32m[20221214 00:02:00 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:02:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 395.01
[32m[20221214 00:02:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 460.72
[32m[20221214 00:02:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 372.32
[32m[20221214 00:02:00 @agent_ppo2.py:143][0m Total time:       4.49 min
[32m[20221214 00:02:00 @agent_ppo2.py:145][0m 403456 total steps have happened
[32m[20221214 00:02:00 @agent_ppo2.py:121][0m #------------------------ Iteration 4197 --------------------------#
[32m[20221214 00:02:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:02:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:02:00 @agent_ppo2.py:185][0m |           0.0048 |          63.6187 |          11.7927 |
[32m[20221214 00:02:00 @agent_ppo2.py:185][0m |          -0.0037 |          58.8336 |          11.7675 |
[32m[20221214 00:02:01 @agent_ppo2.py:185][0m |          -0.0061 |          56.9926 |          11.7071 |
[32m[20221214 00:02:01 @agent_ppo2.py:185][0m |          -0.0067 |          56.6933 |          11.6724 |
[32m[20221214 00:02:01 @agent_ppo2.py:185][0m |          -0.0094 |          55.5177 |          11.6591 |
[32m[20221214 00:02:01 @agent_ppo2.py:185][0m |          -0.0111 |          55.3065 |          11.6357 |
[32m[20221214 00:02:01 @agent_ppo2.py:185][0m |          -0.0147 |          54.7113 |          11.6202 |
[32m[20221214 00:02:01 @agent_ppo2.py:185][0m |          -0.0129 |          54.1844 |          11.6088 |
[32m[20221214 00:02:01 @agent_ppo2.py:185][0m |          -0.0136 |          53.8143 |          11.5728 |
[32m[20221214 00:02:01 @agent_ppo2.py:185][0m |          -0.0136 |          53.6645 |          11.5581 |
[32m[20221214 00:02:01 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:02:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 525.97
[32m[20221214 00:02:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 578.71
[32m[20221214 00:02:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 504.12
[32m[20221214 00:02:01 @agent_ppo2.py:143][0m Total time:       4.52 min
[32m[20221214 00:02:01 @agent_ppo2.py:145][0m 405504 total steps have happened
[32m[20221214 00:02:01 @agent_ppo2.py:121][0m #------------------------ Iteration 4198 --------------------------#
[32m[20221214 00:02:01 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:02:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:02:02 @agent_ppo2.py:185][0m |          -0.0010 |          67.1717 |          11.9052 |
[32m[20221214 00:02:02 @agent_ppo2.py:185][0m |          -0.0094 |          64.8319 |          11.8736 |
[32m[20221214 00:02:02 @agent_ppo2.py:185][0m |          -0.0049 |          66.5811 |          11.8319 |
[32m[20221214 00:02:02 @agent_ppo2.py:185][0m |          -0.0120 |          63.5049 |          11.8554 |
[32m[20221214 00:02:02 @agent_ppo2.py:185][0m |          -0.0124 |          63.1942 |          11.8730 |
[32m[20221214 00:02:02 @agent_ppo2.py:185][0m |          -0.0131 |          62.8056 |          11.8656 |
[32m[20221214 00:02:02 @agent_ppo2.py:185][0m |          -0.0139 |          62.4890 |          11.8401 |
[32m[20221214 00:02:02 @agent_ppo2.py:185][0m |          -0.0149 |          62.4681 |          11.8882 |
[32m[20221214 00:02:02 @agent_ppo2.py:185][0m |          -0.0118 |          62.5378 |          11.9189 |
[32m[20221214 00:02:02 @agent_ppo2.py:185][0m |          -0.0149 |          61.9604 |          11.9236 |
[32m[20221214 00:02:02 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:02:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 516.50
[32m[20221214 00:02:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 564.86
[32m[20221214 00:02:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 319.79
[32m[20221214 00:02:03 @agent_ppo2.py:143][0m Total time:       4.54 min
[32m[20221214 00:02:03 @agent_ppo2.py:145][0m 407552 total steps have happened
[32m[20221214 00:02:03 @agent_ppo2.py:121][0m #------------------------ Iteration 4199 --------------------------#
[32m[20221214 00:02:03 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:02:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:02:03 @agent_ppo2.py:185][0m |           0.0006 |          38.2539 |          12.0048 |
[32m[20221214 00:02:03 @agent_ppo2.py:185][0m |          -0.0092 |          30.9057 |          11.9751 |
[32m[20221214 00:02:03 @agent_ppo2.py:185][0m |          -0.0138 |          29.3279 |          11.9842 |
[32m[20221214 00:02:03 @agent_ppo2.py:185][0m |          -0.0157 |          28.4637 |          11.9737 |
[32m[20221214 00:02:03 @agent_ppo2.py:185][0m |          -0.0058 |          28.0641 |          11.9757 |
[32m[20221214 00:02:03 @agent_ppo2.py:185][0m |          -0.0174 |          27.2014 |          11.9419 |
[32m[20221214 00:02:03 @agent_ppo2.py:185][0m |          -0.0212 |          27.5125 |          11.9126 |
[32m[20221214 00:02:04 @agent_ppo2.py:185][0m |          -0.0136 |          27.5244 |          11.9453 |
[32m[20221214 00:02:04 @agent_ppo2.py:185][0m |          -0.0154 |          26.4077 |          11.9409 |
[32m[20221214 00:02:04 @agent_ppo2.py:185][0m |          -0.0173 |          26.6249 |          11.9404 |
[32m[20221214 00:02:04 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:02:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 428.26
[32m[20221214 00:02:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 506.68
[32m[20221214 00:02:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 573.51
[32m[20221214 00:02:04 @agent_ppo2.py:143][0m Total time:       4.56 min
[32m[20221214 00:02:04 @agent_ppo2.py:145][0m 409600 total steps have happened
[32m[20221214 00:02:04 @agent_ppo2.py:121][0m #------------------------ Iteration 4200 --------------------------#
[32m[20221214 00:02:04 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:02:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:02:04 @agent_ppo2.py:185][0m |           0.0086 |          75.5286 |          11.7960 |
[32m[20221214 00:02:04 @agent_ppo2.py:185][0m |          -0.0029 |          64.5489 |          11.7921 |
[32m[20221214 00:02:04 @agent_ppo2.py:185][0m |           0.0003 |          64.0397 |          11.8252 |
[32m[20221214 00:02:04 @agent_ppo2.py:185][0m |          -0.0074 |          62.3668 |          11.8336 |
[32m[20221214 00:02:05 @agent_ppo2.py:185][0m |          -0.0105 |          61.9132 |          11.8956 |
[32m[20221214 00:02:05 @agent_ppo2.py:185][0m |          -0.0100 |          61.4138 |          11.8882 |
[32m[20221214 00:02:05 @agent_ppo2.py:185][0m |          -0.0094 |          60.9606 |          11.9388 |
[32m[20221214 00:02:05 @agent_ppo2.py:185][0m |          -0.0113 |          60.5965 |          11.9533 |
[32m[20221214 00:02:05 @agent_ppo2.py:185][0m |          -0.0118 |          60.2888 |          11.9663 |
[32m[20221214 00:02:05 @agent_ppo2.py:185][0m |          -0.0123 |          59.8941 |          11.9689 |
[32m[20221214 00:02:05 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:02:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 509.62
[32m[20221214 00:02:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 537.01
[32m[20221214 00:02:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 337.42
[32m[20221214 00:02:05 @agent_ppo2.py:143][0m Total time:       4.58 min
[32m[20221214 00:02:05 @agent_ppo2.py:145][0m 411648 total steps have happened
[32m[20221214 00:02:05 @agent_ppo2.py:121][0m #------------------------ Iteration 4201 --------------------------#
[32m[20221214 00:02:05 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:02:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:02:06 @agent_ppo2.py:185][0m |           0.0002 |          28.0935 |          11.9823 |
[32m[20221214 00:02:06 @agent_ppo2.py:185][0m |          -0.0064 |          23.5463 |          11.9844 |
[32m[20221214 00:02:06 @agent_ppo2.py:185][0m |          -0.0085 |          22.2609 |          11.9365 |
[32m[20221214 00:02:06 @agent_ppo2.py:185][0m |          -0.0104 |          21.3015 |          11.9249 |
[32m[20221214 00:02:06 @agent_ppo2.py:185][0m |          -0.0210 |          20.7990 |          11.8808 |
[32m[20221214 00:02:06 @agent_ppo2.py:185][0m |          -0.0175 |          20.1961 |          11.8161 |
[32m[20221214 00:02:06 @agent_ppo2.py:185][0m |          -0.0225 |          20.2899 |          11.8283 |
[32m[20221214 00:02:06 @agent_ppo2.py:185][0m |          -0.0193 |          19.6194 |          11.7904 |
[32m[20221214 00:02:06 @agent_ppo2.py:185][0m |          -0.0168 |          19.3500 |          11.7940 |
[32m[20221214 00:02:06 @agent_ppo2.py:185][0m |          -0.0272 |          19.0939 |          11.7295 |
[32m[20221214 00:02:06 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:02:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 349.88
[32m[20221214 00:02:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 486.31
[32m[20221214 00:02:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 337.98
[32m[20221214 00:02:06 @agent_ppo2.py:143][0m Total time:       4.60 min
[32m[20221214 00:02:06 @agent_ppo2.py:145][0m 413696 total steps have happened
[32m[20221214 00:02:06 @agent_ppo2.py:121][0m #------------------------ Iteration 4202 --------------------------#
[32m[20221214 00:02:07 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:02:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:02:07 @agent_ppo2.py:185][0m |           0.0081 |          30.8760 |          11.3628 |
[32m[20221214 00:02:07 @agent_ppo2.py:185][0m |          -0.0031 |          26.3349 |          11.3533 |
[32m[20221214 00:02:07 @agent_ppo2.py:185][0m |          -0.0078 |          25.4541 |          11.3422 |
[32m[20221214 00:02:07 @agent_ppo2.py:185][0m |          -0.0055 |          24.6833 |          11.3464 |
[32m[20221214 00:02:07 @agent_ppo2.py:185][0m |          -0.0101 |          24.2279 |          11.3557 |
[32m[20221214 00:02:07 @agent_ppo2.py:185][0m |          -0.0155 |          23.7462 |          11.3261 |
[32m[20221214 00:02:07 @agent_ppo2.py:185][0m |          -0.0104 |          23.5684 |          11.3077 |
[32m[20221214 00:02:07 @agent_ppo2.py:185][0m |          -0.0165 |          23.3272 |          11.3129 |
[32m[20221214 00:02:08 @agent_ppo2.py:185][0m |          -0.0118 |          23.0084 |          11.3157 |
[32m[20221214 00:02:08 @agent_ppo2.py:185][0m |          -0.0149 |          22.7500 |          11.2861 |
[32m[20221214 00:02:08 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:02:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 394.55
[32m[20221214 00:02:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 539.66
[32m[20221214 00:02:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 443.64
[32m[20221214 00:02:08 @agent_ppo2.py:143][0m Total time:       4.62 min
[32m[20221214 00:02:08 @agent_ppo2.py:145][0m 415744 total steps have happened
[32m[20221214 00:02:08 @agent_ppo2.py:121][0m #------------------------ Iteration 4203 --------------------------#
[32m[20221214 00:02:08 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:02:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:02:08 @agent_ppo2.py:185][0m |          -0.0005 |          42.6271 |          11.5129 |
[32m[20221214 00:02:08 @agent_ppo2.py:185][0m |          -0.0049 |          39.4550 |          11.4493 |
[32m[20221214 00:02:08 @agent_ppo2.py:185][0m |          -0.0101 |          38.1720 |          11.4675 |
[32m[20221214 00:02:08 @agent_ppo2.py:185][0m |          -0.0099 |          37.1346 |          11.4561 |
[32m[20221214 00:02:08 @agent_ppo2.py:185][0m |          -0.0121 |          37.2214 |          11.4387 |
[32m[20221214 00:02:09 @agent_ppo2.py:185][0m |          -0.0117 |          36.3773 |          11.4060 |
[32m[20221214 00:02:09 @agent_ppo2.py:185][0m |          -0.0193 |          36.0873 |          11.4022 |
[32m[20221214 00:02:09 @agent_ppo2.py:185][0m |          -0.0113 |          35.7791 |          11.3696 |
[32m[20221214 00:02:09 @agent_ppo2.py:185][0m |          -0.0072 |          35.6204 |          11.3772 |
[32m[20221214 00:02:09 @agent_ppo2.py:185][0m |          -0.0157 |          35.3942 |          11.3920 |
[32m[20221214 00:02:09 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:02:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 444.96
[32m[20221214 00:02:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 467.05
[32m[20221214 00:02:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 543.35
[32m[20221214 00:02:09 @agent_ppo2.py:143][0m Total time:       4.65 min
[32m[20221214 00:02:09 @agent_ppo2.py:145][0m 417792 total steps have happened
[32m[20221214 00:02:09 @agent_ppo2.py:121][0m #------------------------ Iteration 4204 --------------------------#
[32m[20221214 00:02:09 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:02:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:02:09 @agent_ppo2.py:185][0m |          -0.0027 |          63.2351 |          11.3967 |
[32m[20221214 00:02:10 @agent_ppo2.py:185][0m |          -0.0052 |          57.7786 |          11.4009 |
[32m[20221214 00:02:10 @agent_ppo2.py:185][0m |          -0.0089 |          56.1832 |          11.4340 |
[32m[20221214 00:02:10 @agent_ppo2.py:185][0m |          -0.0071 |          56.6928 |          11.4163 |
[32m[20221214 00:02:10 @agent_ppo2.py:185][0m |          -0.0128 |          55.0280 |          11.4311 |
[32m[20221214 00:02:10 @agent_ppo2.py:185][0m |          -0.0148 |          53.8874 |          11.4323 |
[32m[20221214 00:02:10 @agent_ppo2.py:185][0m |          -0.0160 |          53.7386 |          11.4740 |
[32m[20221214 00:02:10 @agent_ppo2.py:185][0m |          -0.0144 |          53.4008 |          11.4547 |
[32m[20221214 00:02:10 @agent_ppo2.py:185][0m |          -0.0088 |          55.4957 |          11.4496 |
[32m[20221214 00:02:10 @agent_ppo2.py:185][0m |          -0.0170 |          53.0791 |          11.4934 |
[32m[20221214 00:02:10 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:02:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 429.70
[32m[20221214 00:02:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 475.51
[32m[20221214 00:02:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 518.77
[32m[20221214 00:02:10 @agent_ppo2.py:143][0m Total time:       4.67 min
[32m[20221214 00:02:10 @agent_ppo2.py:145][0m 419840 total steps have happened
[32m[20221214 00:02:10 @agent_ppo2.py:121][0m #------------------------ Iteration 4205 --------------------------#
[32m[20221214 00:02:11 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:02:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:02:11 @agent_ppo2.py:185][0m |          -0.0009 |          80.0505 |          11.6073 |
[32m[20221214 00:02:11 @agent_ppo2.py:185][0m |          -0.0080 |          77.1715 |          11.6359 |
[32m[20221214 00:02:11 @agent_ppo2.py:185][0m |          -0.0088 |          76.1691 |          11.5787 |
[32m[20221214 00:02:11 @agent_ppo2.py:185][0m |          -0.0114 |          75.4913 |          11.6044 |
[32m[20221214 00:02:11 @agent_ppo2.py:185][0m |          -0.0113 |          75.0998 |          11.6367 |
[32m[20221214 00:02:11 @agent_ppo2.py:185][0m |          -0.0130 |          74.4977 |          11.6115 |
[32m[20221214 00:02:11 @agent_ppo2.py:185][0m |          -0.0156 |          74.5617 |          11.5938 |
[32m[20221214 00:02:11 @agent_ppo2.py:185][0m |          -0.0176 |          74.1334 |          11.6165 |
[32m[20221214 00:02:11 @agent_ppo2.py:185][0m |          -0.0155 |          73.9323 |          11.6271 |
[32m[20221214 00:02:12 @agent_ppo2.py:185][0m |          -0.0145 |          73.6529 |          11.6166 |
[32m[20221214 00:02:12 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:02:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 425.62
[32m[20221214 00:02:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 453.74
[32m[20221214 00:02:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 495.87
[32m[20221214 00:02:12 @agent_ppo2.py:143][0m Total time:       4.69 min
[32m[20221214 00:02:12 @agent_ppo2.py:145][0m 421888 total steps have happened
[32m[20221214 00:02:12 @agent_ppo2.py:121][0m #------------------------ Iteration 4206 --------------------------#
[32m[20221214 00:02:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:02:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:02:12 @agent_ppo2.py:185][0m |           0.0016 |          56.5815 |          11.6250 |
[32m[20221214 00:02:12 @agent_ppo2.py:185][0m |          -0.0079 |          52.6832 |          11.7092 |
[32m[20221214 00:02:12 @agent_ppo2.py:185][0m |          -0.0085 |          51.6857 |          11.6997 |
[32m[20221214 00:02:12 @agent_ppo2.py:185][0m |          -0.0077 |          51.7231 |          11.6863 |
[32m[20221214 00:02:12 @agent_ppo2.py:185][0m |          -0.0115 |          50.5501 |          11.6933 |
[32m[20221214 00:02:12 @agent_ppo2.py:185][0m |          -0.0144 |          50.2028 |          11.6729 |
[32m[20221214 00:02:13 @agent_ppo2.py:185][0m |          -0.0156 |          49.9351 |          11.6677 |
[32m[20221214 00:02:13 @agent_ppo2.py:185][0m |          -0.0174 |          49.8572 |          11.6710 |
[32m[20221214 00:02:13 @agent_ppo2.py:185][0m |          -0.0116 |          50.2921 |          11.6611 |
[32m[20221214 00:02:13 @agent_ppo2.py:185][0m |          -0.0080 |          50.9958 |          11.6621 |
[32m[20221214 00:02:13 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:02:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 407.41
[32m[20221214 00:02:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 444.01
[32m[20221214 00:02:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 390.20
[32m[20221214 00:02:13 @agent_ppo2.py:143][0m Total time:       4.71 min
[32m[20221214 00:02:13 @agent_ppo2.py:145][0m 423936 total steps have happened
[32m[20221214 00:02:13 @agent_ppo2.py:121][0m #------------------------ Iteration 4207 --------------------------#
[32m[20221214 00:02:13 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:02:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:02:13 @agent_ppo2.py:185][0m |          -0.0030 |          54.5840 |          11.5006 |
[32m[20221214 00:02:13 @agent_ppo2.py:185][0m |          -0.0039 |          51.0607 |          11.4367 |
[32m[20221214 00:02:13 @agent_ppo2.py:185][0m |          -0.0085 |          49.8081 |          11.4603 |
[32m[20221214 00:02:14 @agent_ppo2.py:185][0m |          -0.0101 |          49.0938 |          11.4418 |
[32m[20221214 00:02:14 @agent_ppo2.py:185][0m |          -0.0067 |          49.0384 |          11.4265 |
[32m[20221214 00:02:14 @agent_ppo2.py:185][0m |          -0.0136 |          48.4300 |          11.4225 |
[32m[20221214 00:02:14 @agent_ppo2.py:185][0m |          -0.0107 |          48.1317 |          11.4137 |
[32m[20221214 00:02:14 @agent_ppo2.py:185][0m |          -0.0174 |          47.5704 |          11.4034 |
[32m[20221214 00:02:14 @agent_ppo2.py:185][0m |          -0.0139 |          47.4020 |          11.3788 |
[32m[20221214 00:02:14 @agent_ppo2.py:185][0m |          -0.0069 |          53.0846 |          11.3860 |
[32m[20221214 00:02:14 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:02:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 435.11
[32m[20221214 00:02:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 472.59
[32m[20221214 00:02:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 432.62
[32m[20221214 00:02:14 @agent_ppo2.py:143][0m Total time:       4.73 min
[32m[20221214 00:02:14 @agent_ppo2.py:145][0m 425984 total steps have happened
[32m[20221214 00:02:14 @agent_ppo2.py:121][0m #------------------------ Iteration 4208 --------------------------#
[32m[20221214 00:02:15 @agent_ppo2.py:127][0m Sampling time: 0.34 s by 5 slaves
[32m[20221214 00:02:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:02:15 @agent_ppo2.py:185][0m |          -0.0013 |          51.5677 |          11.6531 |
[32m[20221214 00:02:15 @agent_ppo2.py:185][0m |          -0.0039 |          46.9300 |          11.6178 |
[32m[20221214 00:02:15 @agent_ppo2.py:185][0m |          -0.0110 |          44.6111 |          11.6115 |
[32m[20221214 00:02:15 @agent_ppo2.py:185][0m |          -0.0034 |          43.3160 |          11.6211 |
[32m[20221214 00:02:15 @agent_ppo2.py:185][0m |          -0.0109 |          42.5572 |          11.6517 |
[32m[20221214 00:02:15 @agent_ppo2.py:185][0m |          -0.0155 |          41.8986 |          11.6764 |
[32m[20221214 00:02:16 @agent_ppo2.py:185][0m |          -0.0140 |          41.2959 |          11.7312 |
[32m[20221214 00:02:16 @agent_ppo2.py:185][0m |          -0.0163 |          40.9077 |          11.7271 |
[32m[20221214 00:02:16 @agent_ppo2.py:185][0m |          -0.0154 |          40.3534 |          11.7027 |
[32m[20221214 00:02:16 @agent_ppo2.py:185][0m |          -0.0118 |          40.0253 |          11.7286 |
[32m[20221214 00:02:16 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:02:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 462.29
[32m[20221214 00:02:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 506.36
[32m[20221214 00:02:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 494.47
[32m[20221214 00:02:16 @agent_ppo2.py:143][0m Total time:       4.76 min
[32m[20221214 00:02:16 @agent_ppo2.py:145][0m 428032 total steps have happened
[32m[20221214 00:02:16 @agent_ppo2.py:121][0m #------------------------ Iteration 4209 --------------------------#
[32m[20221214 00:02:16 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:02:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:02:16 @agent_ppo2.py:185][0m |           0.0046 |          47.7776 |          11.4629 |
[32m[20221214 00:02:16 @agent_ppo2.py:185][0m |           0.0176 |          46.0524 |          11.4470 |
[32m[20221214 00:02:17 @agent_ppo2.py:185][0m |          -0.0058 |          42.4507 |          11.4436 |
[32m[20221214 00:02:17 @agent_ppo2.py:185][0m |          -0.0077 |          41.6211 |          11.4271 |
[32m[20221214 00:02:17 @agent_ppo2.py:185][0m |          -0.0071 |          40.9242 |          11.4221 |
[32m[20221214 00:02:17 @agent_ppo2.py:185][0m |          -0.0162 |          40.6468 |          11.4385 |
[32m[20221214 00:02:17 @agent_ppo2.py:185][0m |          -0.0088 |          40.3450 |          11.4259 |
[32m[20221214 00:02:17 @agent_ppo2.py:185][0m |          -0.0161 |          40.2378 |          11.4178 |
[32m[20221214 00:02:17 @agent_ppo2.py:185][0m |          -0.0195 |          39.9836 |          11.4015 |
[32m[20221214 00:02:17 @agent_ppo2.py:185][0m |          -0.0189 |          39.7201 |          11.3678 |
[32m[20221214 00:02:17 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:02:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 366.23
[32m[20221214 00:02:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 459.45
[32m[20221214 00:02:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 512.29
[32m[20221214 00:02:17 @agent_ppo2.py:143][0m Total time:       4.78 min
[32m[20221214 00:02:17 @agent_ppo2.py:145][0m 430080 total steps have happened
[32m[20221214 00:02:17 @agent_ppo2.py:121][0m #------------------------ Iteration 4210 --------------------------#
[32m[20221214 00:02:18 @agent_ppo2.py:127][0m Sampling time: 0.23 s by 5 slaves
[32m[20221214 00:02:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:02:18 @agent_ppo2.py:185][0m |          -0.0047 |          56.8815 |          11.2655 |
[32m[20221214 00:02:18 @agent_ppo2.py:185][0m |          -0.0089 |          51.3921 |          11.2182 |
[32m[20221214 00:02:18 @agent_ppo2.py:185][0m |          -0.0102 |          49.4240 |          11.1950 |
[32m[20221214 00:02:18 @agent_ppo2.py:185][0m |          -0.0127 |          48.1605 |          11.1731 |
[32m[20221214 00:02:18 @agent_ppo2.py:185][0m |          -0.0137 |          47.2159 |          11.1724 |
[32m[20221214 00:02:18 @agent_ppo2.py:185][0m |          -0.0128 |          46.7805 |          11.1363 |
[32m[20221214 00:02:18 @agent_ppo2.py:185][0m |          -0.0201 |          45.9493 |          11.1379 |
[32m[20221214 00:02:18 @agent_ppo2.py:185][0m |          -0.0049 |          52.5846 |          11.1615 |
[32m[20221214 00:02:19 @agent_ppo2.py:185][0m |          -0.0219 |          45.0870 |          11.1456 |
[32m[20221214 00:02:19 @agent_ppo2.py:185][0m |          -0.0194 |          44.7005 |          11.1056 |
[32m[20221214 00:02:19 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:02:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 331.72
[32m[20221214 00:02:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 443.86
[32m[20221214 00:02:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 501.89
[32m[20221214 00:02:19 @agent_ppo2.py:143][0m Total time:       4.81 min
[32m[20221214 00:02:19 @agent_ppo2.py:145][0m 432128 total steps have happened
[32m[20221214 00:02:19 @agent_ppo2.py:121][0m #------------------------ Iteration 4211 --------------------------#
[32m[20221214 00:02:19 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:02:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:02:19 @agent_ppo2.py:185][0m |          -0.0047 |          51.7626 |          11.1525 |
[32m[20221214 00:02:19 @agent_ppo2.py:185][0m |           0.0037 |          53.2840 |          11.1394 |
[32m[20221214 00:02:19 @agent_ppo2.py:185][0m |          -0.0053 |          45.6855 |          11.1629 |
[32m[20221214 00:02:19 @agent_ppo2.py:185][0m |          -0.0106 |          44.4335 |          11.1453 |
[32m[20221214 00:02:19 @agent_ppo2.py:185][0m |          -0.0123 |          44.1984 |          11.1832 |
[32m[20221214 00:02:20 @agent_ppo2.py:185][0m |          -0.0114 |          43.4621 |          11.1731 |
[32m[20221214 00:02:20 @agent_ppo2.py:185][0m |          -0.0113 |          43.1699 |          11.1849 |
[32m[20221214 00:02:20 @agent_ppo2.py:185][0m |          -0.0153 |          42.7871 |          11.1926 |
[32m[20221214 00:02:20 @agent_ppo2.py:185][0m |          -0.0110 |          42.7000 |          11.1874 |
[32m[20221214 00:02:20 @agent_ppo2.py:185][0m |          -0.0164 |          42.1695 |          11.1667 |
[32m[20221214 00:02:20 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:02:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 450.25
[32m[20221214 00:02:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 507.71
[32m[20221214 00:02:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 540.23
[32m[20221214 00:02:20 @agent_ppo2.py:143][0m Total time:       4.83 min
[32m[20221214 00:02:20 @agent_ppo2.py:145][0m 434176 total steps have happened
[32m[20221214 00:02:20 @agent_ppo2.py:121][0m #------------------------ Iteration 4212 --------------------------#
[32m[20221214 00:02:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:02:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:02:20 @agent_ppo2.py:185][0m |           0.0125 |          59.4115 |          11.0107 |
[32m[20221214 00:02:21 @agent_ppo2.py:185][0m |          -0.0085 |          49.2188 |          11.0733 |
[32m[20221214 00:02:21 @agent_ppo2.py:185][0m |          -0.0077 |          47.3319 |          11.0966 |
[32m[20221214 00:02:21 @agent_ppo2.py:185][0m |          -0.0086 |          45.8117 |          11.1186 |
[32m[20221214 00:02:21 @agent_ppo2.py:185][0m |          -0.0104 |          45.1094 |          11.1550 |
[32m[20221214 00:02:21 @agent_ppo2.py:185][0m |          -0.0135 |          44.2948 |          11.1531 |
[32m[20221214 00:02:21 @agent_ppo2.py:185][0m |          -0.0172 |          43.8420 |          11.2040 |
[32m[20221214 00:02:21 @agent_ppo2.py:185][0m |          -0.0101 |          44.7585 |          11.2124 |
[32m[20221214 00:02:21 @agent_ppo2.py:185][0m |          -0.0194 |          42.8243 |          11.2673 |
[32m[20221214 00:02:21 @agent_ppo2.py:185][0m |          -0.0255 |          42.6464 |          11.2584 |
[32m[20221214 00:02:21 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:02:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 359.62
[32m[20221214 00:02:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 443.95
[32m[20221214 00:02:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 490.58
[32m[20221214 00:02:21 @agent_ppo2.py:143][0m Total time:       4.85 min
[32m[20221214 00:02:21 @agent_ppo2.py:145][0m 436224 total steps have happened
[32m[20221214 00:02:21 @agent_ppo2.py:121][0m #------------------------ Iteration 4213 --------------------------#
[32m[20221214 00:02:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:02:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:02:22 @agent_ppo2.py:185][0m |           0.0001 |          54.8381 |          11.3365 |
[32m[20221214 00:02:22 @agent_ppo2.py:185][0m |          -0.0053 |          47.8822 |          11.3422 |
[32m[20221214 00:02:22 @agent_ppo2.py:185][0m |          -0.0036 |          45.7731 |          11.3060 |
[32m[20221214 00:02:22 @agent_ppo2.py:185][0m |          -0.0095 |          44.4801 |          11.3065 |
[32m[20221214 00:02:22 @agent_ppo2.py:185][0m |          -0.0144 |          43.7412 |          11.3420 |
[32m[20221214 00:02:22 @agent_ppo2.py:185][0m |          -0.0137 |          43.1396 |          11.2660 |
[32m[20221214 00:02:22 @agent_ppo2.py:185][0m |          -0.0172 |          42.5181 |          11.2640 |
[32m[20221214 00:02:22 @agent_ppo2.py:185][0m |          -0.0097 |          42.2928 |          11.2758 |
[32m[20221214 00:02:23 @agent_ppo2.py:185][0m |          -0.0138 |          41.9209 |          11.2719 |
[32m[20221214 00:02:23 @agent_ppo2.py:185][0m |          -0.0149 |          41.3449 |          11.2737 |
[32m[20221214 00:02:23 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:02:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 385.52
[32m[20221214 00:02:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 495.26
[32m[20221214 00:02:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 499.11
[32m[20221214 00:02:23 @agent_ppo2.py:143][0m Total time:       4.87 min
[32m[20221214 00:02:23 @agent_ppo2.py:145][0m 438272 total steps have happened
[32m[20221214 00:02:23 @agent_ppo2.py:121][0m #------------------------ Iteration 4214 --------------------------#
[32m[20221214 00:02:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:02:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:02:23 @agent_ppo2.py:185][0m |          -0.0014 |          62.0012 |          11.4322 |
[32m[20221214 00:02:23 @agent_ppo2.py:185][0m |          -0.0038 |          57.7587 |          11.4359 |
[32m[20221214 00:02:23 @agent_ppo2.py:185][0m |          -0.0051 |          56.9668 |          11.4388 |
[32m[20221214 00:02:23 @agent_ppo2.py:185][0m |          -0.0099 |          56.4312 |          11.4312 |
[32m[20221214 00:02:24 @agent_ppo2.py:185][0m |          -0.0086 |          56.2548 |          11.4678 |
[32m[20221214 00:02:24 @agent_ppo2.py:185][0m |          -0.0096 |          55.8161 |          11.4637 |
[32m[20221214 00:02:24 @agent_ppo2.py:185][0m |          -0.0102 |          55.5888 |          11.4644 |
[32m[20221214 00:02:24 @agent_ppo2.py:185][0m |          -0.0085 |          55.6797 |          11.5158 |
[32m[20221214 00:02:24 @agent_ppo2.py:185][0m |          -0.0170 |          55.1885 |          11.4758 |
[32m[20221214 00:02:24 @agent_ppo2.py:185][0m |          -0.0223 |          54.8516 |          11.4728 |
[32m[20221214 00:02:24 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:02:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 478.50
[32m[20221214 00:02:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 489.86
[32m[20221214 00:02:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 462.55
[32m[20221214 00:02:24 @agent_ppo2.py:143][0m Total time:       4.90 min
[32m[20221214 00:02:24 @agent_ppo2.py:145][0m 440320 total steps have happened
[32m[20221214 00:02:24 @agent_ppo2.py:121][0m #------------------------ Iteration 4215 --------------------------#
[32m[20221214 00:02:24 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:02:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:02:25 @agent_ppo2.py:185][0m |           0.0063 |          69.3530 |          11.3747 |
[32m[20221214 00:02:25 @agent_ppo2.py:185][0m |          -0.0073 |          64.8485 |          11.3605 |
[32m[20221214 00:02:25 @agent_ppo2.py:185][0m |          -0.0122 |          63.7436 |          11.3212 |
[32m[20221214 00:02:25 @agent_ppo2.py:185][0m |          -0.0071 |          63.6584 |          11.3377 |
[32m[20221214 00:02:25 @agent_ppo2.py:185][0m |          -0.0098 |          62.1543 |          11.2843 |
[32m[20221214 00:02:25 @agent_ppo2.py:185][0m |           0.0023 |          71.2536 |          11.3198 |
[32m[20221214 00:02:25 @agent_ppo2.py:185][0m |          -0.0158 |          61.5136 |          11.3189 |
[32m[20221214 00:02:25 @agent_ppo2.py:185][0m |          -0.0158 |          60.8814 |          11.3280 |
[32m[20221214 00:02:25 @agent_ppo2.py:185][0m |          -0.0128 |          60.6162 |          11.2875 |
[32m[20221214 00:02:25 @agent_ppo2.py:185][0m |          -0.0153 |          60.3762 |          11.2700 |
[32m[20221214 00:02:25 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:02:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 485.32
[32m[20221214 00:02:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 500.95
[32m[20221214 00:02:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 500.51
[32m[20221214 00:02:25 @agent_ppo2.py:143][0m Total time:       4.92 min
[32m[20221214 00:02:25 @agent_ppo2.py:145][0m 442368 total steps have happened
[32m[20221214 00:02:25 @agent_ppo2.py:121][0m #------------------------ Iteration 4216 --------------------------#
[32m[20221214 00:02:26 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:02:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:02:26 @agent_ppo2.py:185][0m |           0.0030 |          65.0838 |          11.2470 |
[32m[20221214 00:02:26 @agent_ppo2.py:185][0m |          -0.0103 |          57.7493 |          11.3019 |
[32m[20221214 00:02:26 @agent_ppo2.py:185][0m |          -0.0153 |          54.3685 |          11.3315 |
[32m[20221214 00:02:26 @agent_ppo2.py:185][0m |          -0.0142 |          52.5897 |          11.3117 |
[32m[20221214 00:02:26 @agent_ppo2.py:185][0m |          -0.0184 |          51.5408 |          11.3089 |
[32m[20221214 00:02:26 @agent_ppo2.py:185][0m |          -0.0101 |          50.1801 |          11.2953 |
[32m[20221214 00:02:26 @agent_ppo2.py:185][0m |          -0.0133 |          49.6201 |          11.3334 |
[32m[20221214 00:02:26 @agent_ppo2.py:185][0m |          -0.0184 |          48.9555 |          11.3664 |
[32m[20221214 00:02:26 @agent_ppo2.py:185][0m |          -0.0182 |          48.3800 |          11.3411 |
[32m[20221214 00:02:27 @agent_ppo2.py:185][0m |          -0.0168 |          47.8452 |          11.3483 |
[32m[20221214 00:02:27 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:02:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 352.45
[32m[20221214 00:02:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 434.78
[32m[20221214 00:02:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 505.99
[32m[20221214 00:02:27 @agent_ppo2.py:143][0m Total time:       4.94 min
[32m[20221214 00:02:27 @agent_ppo2.py:145][0m 444416 total steps have happened
[32m[20221214 00:02:27 @agent_ppo2.py:121][0m #------------------------ Iteration 4217 --------------------------#
[32m[20221214 00:02:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:02:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:02:27 @agent_ppo2.py:185][0m |           0.0037 |          73.4884 |          11.8343 |
[32m[20221214 00:02:27 @agent_ppo2.py:185][0m |          -0.0049 |          69.0350 |          11.8537 |
[32m[20221214 00:02:27 @agent_ppo2.py:185][0m |           0.0021 |          69.3907 |          11.8812 |
[32m[20221214 00:02:27 @agent_ppo2.py:185][0m |          -0.0017 |          68.1967 |          11.8900 |
[32m[20221214 00:02:27 @agent_ppo2.py:185][0m |          -0.0106 |          67.0167 |          11.8237 |
[32m[20221214 00:02:27 @agent_ppo2.py:185][0m |          -0.0085 |          67.2727 |          11.8501 |
[32m[20221214 00:02:28 @agent_ppo2.py:185][0m |          -0.0155 |          66.1624 |          11.8534 |
[32m[20221214 00:02:28 @agent_ppo2.py:185][0m |          -0.0123 |          65.9684 |          11.8670 |
[32m[20221214 00:02:28 @agent_ppo2.py:185][0m |          -0.0121 |          65.7626 |          11.8506 |
[32m[20221214 00:02:28 @agent_ppo2.py:185][0m |          -0.0133 |          65.7116 |          11.8725 |
[32m[20221214 00:02:28 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:02:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 435.69
[32m[20221214 00:02:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 458.96
[32m[20221214 00:02:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 511.07
[32m[20221214 00:02:28 @agent_ppo2.py:143][0m Total time:       4.96 min
[32m[20221214 00:02:28 @agent_ppo2.py:145][0m 446464 total steps have happened
[32m[20221214 00:02:28 @agent_ppo2.py:121][0m #------------------------ Iteration 4218 --------------------------#
[32m[20221214 00:02:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:02:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:02:28 @agent_ppo2.py:185][0m |          -0.0020 |          72.9437 |          11.0165 |
[32m[20221214 00:02:28 @agent_ppo2.py:185][0m |          -0.0061 |          70.9037 |          11.0001 |
[32m[20221214 00:02:28 @agent_ppo2.py:185][0m |          -0.0082 |          70.3379 |          11.0559 |
[32m[20221214 00:02:29 @agent_ppo2.py:185][0m |          -0.0059 |          71.4252 |          11.0572 |
[32m[20221214 00:02:29 @agent_ppo2.py:185][0m |          -0.0129 |          69.4069 |          11.0655 |
[32m[20221214 00:02:29 @agent_ppo2.py:185][0m |          -0.0040 |          76.3047 |          11.1165 |
[32m[20221214 00:02:29 @agent_ppo2.py:185][0m |          -0.0175 |          68.8397 |          11.1200 |
[32m[20221214 00:02:29 @agent_ppo2.py:185][0m |          -0.0147 |          68.5513 |          11.1342 |
[32m[20221214 00:02:29 @agent_ppo2.py:185][0m |          -0.0170 |          68.5005 |          11.1196 |
[32m[20221214 00:02:29 @agent_ppo2.py:185][0m |          -0.0154 |          68.0894 |          11.1314 |
[32m[20221214 00:02:29 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:02:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 483.92
[32m[20221214 00:02:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 510.68
[32m[20221214 00:02:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 428.10
[32m[20221214 00:02:29 @agent_ppo2.py:143][0m Total time:       4.98 min
[32m[20221214 00:02:29 @agent_ppo2.py:145][0m 448512 total steps have happened
[32m[20221214 00:02:29 @agent_ppo2.py:121][0m #------------------------ Iteration 4219 --------------------------#
[32m[20221214 00:02:29 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:02:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:02:30 @agent_ppo2.py:185][0m |          -0.0027 |          67.1782 |          11.7874 |
[32m[20221214 00:02:30 @agent_ppo2.py:185][0m |          -0.0088 |          64.3564 |          11.7577 |
[32m[20221214 00:02:30 @agent_ppo2.py:185][0m |          -0.0095 |          62.9541 |          11.8102 |
[32m[20221214 00:02:30 @agent_ppo2.py:185][0m |          -0.0077 |          64.4203 |          11.7788 |
[32m[20221214 00:02:30 @agent_ppo2.py:185][0m |          -0.0132 |          61.7698 |          11.7860 |
[32m[20221214 00:02:30 @agent_ppo2.py:185][0m |          -0.0182 |          61.1569 |          11.8351 |
[32m[20221214 00:02:30 @agent_ppo2.py:185][0m |          -0.0109 |          61.0745 |          11.8334 |
[32m[20221214 00:02:30 @agent_ppo2.py:185][0m |          -0.0184 |          60.2133 |          11.8348 |
[32m[20221214 00:02:30 @agent_ppo2.py:185][0m |          -0.0191 |          59.9911 |          11.8477 |
[32m[20221214 00:02:30 @agent_ppo2.py:185][0m |          -0.0130 |          59.8134 |          11.8505 |
[32m[20221214 00:02:30 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:02:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 384.09
[32m[20221214 00:02:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 428.74
[32m[20221214 00:02:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 543.03
[32m[20221214 00:02:30 @agent_ppo2.py:143][0m Total time:       5.00 min
[32m[20221214 00:02:30 @agent_ppo2.py:145][0m 450560 total steps have happened
[32m[20221214 00:02:30 @agent_ppo2.py:121][0m #------------------------ Iteration 4220 --------------------------#
[32m[20221214 00:02:31 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:02:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:02:31 @agent_ppo2.py:185][0m |           0.0059 |          62.5064 |          11.4242 |
[32m[20221214 00:02:31 @agent_ppo2.py:185][0m |          -0.0115 |          56.1440 |          11.4071 |
[32m[20221214 00:02:31 @agent_ppo2.py:185][0m |          -0.0002 |          56.1875 |          11.4295 |
[32m[20221214 00:02:31 @agent_ppo2.py:185][0m |          -0.0118 |          54.6481 |          11.4376 |
[32m[20221214 00:02:31 @agent_ppo2.py:185][0m |          -0.0024 |          59.6000 |          11.4232 |
[32m[20221214 00:02:31 @agent_ppo2.py:185][0m |          -0.0120 |          53.7288 |          11.4391 |
[32m[20221214 00:02:31 @agent_ppo2.py:185][0m |           0.0068 |          61.3694 |          11.4399 |
[32m[20221214 00:02:31 @agent_ppo2.py:185][0m |          -0.0162 |          53.2540 |          11.4447 |
[32m[20221214 00:02:32 @agent_ppo2.py:185][0m |          -0.0075 |          56.0784 |          11.4664 |
[32m[20221214 00:02:32 @agent_ppo2.py:185][0m |          -0.0182 |          52.6798 |          11.4505 |
[32m[20221214 00:02:32 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:02:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 480.13
[32m[20221214 00:02:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 537.92
[32m[20221214 00:02:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 486.83
[32m[20221214 00:02:32 @agent_ppo2.py:143][0m Total time:       5.02 min
[32m[20221214 00:02:32 @agent_ppo2.py:145][0m 452608 total steps have happened
[32m[20221214 00:02:32 @agent_ppo2.py:121][0m #------------------------ Iteration 4221 --------------------------#
[32m[20221214 00:02:32 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:02:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:02:32 @agent_ppo2.py:185][0m |          -0.0008 |          64.4426 |          11.3109 |
[32m[20221214 00:02:32 @agent_ppo2.py:185][0m |          -0.0079 |          62.8504 |          11.2841 |
[32m[20221214 00:02:32 @agent_ppo2.py:185][0m |          -0.0088 |          62.2424 |          11.3171 |
[32m[20221214 00:02:32 @agent_ppo2.py:185][0m |          -0.0080 |          61.7617 |          11.3119 |
[32m[20221214 00:02:32 @agent_ppo2.py:185][0m |          -0.0107 |          61.4118 |          11.3445 |
[32m[20221214 00:02:33 @agent_ppo2.py:185][0m |          -0.0102 |          61.3781 |          11.3319 |
[32m[20221214 00:02:33 @agent_ppo2.py:185][0m |          -0.0092 |          61.6500 |          11.3936 |
[32m[20221214 00:02:33 @agent_ppo2.py:185][0m |          -0.0118 |          60.8196 |          11.3864 |
[32m[20221214 00:02:33 @agent_ppo2.py:185][0m |          -0.0122 |          60.5437 |          11.3649 |
[32m[20221214 00:02:33 @agent_ppo2.py:185][0m |          -0.0138 |          60.4538 |          11.4434 |
[32m[20221214 00:02:33 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:02:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 485.54
[32m[20221214 00:02:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 500.56
[32m[20221214 00:02:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 539.60
[32m[20221214 00:02:33 @agent_ppo2.py:143][0m Total time:       5.04 min
[32m[20221214 00:02:33 @agent_ppo2.py:145][0m 454656 total steps have happened
[32m[20221214 00:02:33 @agent_ppo2.py:121][0m #------------------------ Iteration 4222 --------------------------#
[32m[20221214 00:02:33 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:02:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:02:33 @agent_ppo2.py:185][0m |           0.0023 |          70.8437 |          11.2958 |
[32m[20221214 00:02:33 @agent_ppo2.py:185][0m |          -0.0078 |          66.3763 |          11.2979 |
[32m[20221214 00:02:33 @agent_ppo2.py:185][0m |          -0.0022 |          67.0903 |          11.2677 |
[32m[20221214 00:02:34 @agent_ppo2.py:185][0m |          -0.0105 |          64.3156 |          11.2816 |
[32m[20221214 00:02:34 @agent_ppo2.py:185][0m |          -0.0120 |          63.7753 |          11.2755 |
[32m[20221214 00:02:34 @agent_ppo2.py:185][0m |          -0.0175 |          63.2426 |          11.2975 |
[32m[20221214 00:02:34 @agent_ppo2.py:185][0m |          -0.0166 |          62.8307 |          11.2913 |
[32m[20221214 00:02:34 @agent_ppo2.py:185][0m |          -0.0157 |          62.5720 |          11.3185 |
[32m[20221214 00:02:34 @agent_ppo2.py:185][0m |          -0.0174 |          62.1650 |          11.3033 |
[32m[20221214 00:02:34 @agent_ppo2.py:185][0m |          -0.0204 |          61.8844 |          11.3214 |
[32m[20221214 00:02:34 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:02:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 393.09
[32m[20221214 00:02:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 434.23
[32m[20221214 00:02:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 403.07
[32m[20221214 00:02:34 @agent_ppo2.py:143][0m Total time:       5.06 min
[32m[20221214 00:02:34 @agent_ppo2.py:145][0m 456704 total steps have happened
[32m[20221214 00:02:34 @agent_ppo2.py:121][0m #------------------------ Iteration 4223 --------------------------#
[32m[20221214 00:02:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:02:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:02:35 @agent_ppo2.py:185][0m |          -0.0005 |          76.3678 |          11.4736 |
[32m[20221214 00:02:35 @agent_ppo2.py:185][0m |          -0.0070 |          70.6509 |          11.4432 |
[32m[20221214 00:02:35 @agent_ppo2.py:185][0m |          -0.0100 |          69.6241 |          11.4755 |
[32m[20221214 00:02:35 @agent_ppo2.py:185][0m |          -0.0134 |          69.1116 |          11.4835 |
[32m[20221214 00:02:35 @agent_ppo2.py:185][0m |          -0.0106 |          68.6580 |          11.4699 |
[32m[20221214 00:02:35 @agent_ppo2.py:185][0m |          -0.0114 |          68.4154 |          11.4670 |
[32m[20221214 00:02:35 @agent_ppo2.py:185][0m |          -0.0051 |          75.6945 |          11.4252 |
[32m[20221214 00:02:35 @agent_ppo2.py:185][0m |          -0.0173 |          67.9095 |          11.4569 |
[32m[20221214 00:02:35 @agent_ppo2.py:185][0m |          -0.0106 |          68.5717 |          11.4175 |
[32m[20221214 00:02:35 @agent_ppo2.py:185][0m |          -0.0163 |          67.4626 |          11.4473 |
[32m[20221214 00:02:35 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:02:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 475.35
[32m[20221214 00:02:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 498.91
[32m[20221214 00:02:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 538.59
[32m[20221214 00:02:36 @agent_ppo2.py:143][0m Total time:       5.09 min
[32m[20221214 00:02:36 @agent_ppo2.py:145][0m 458752 total steps have happened
[32m[20221214 00:02:36 @agent_ppo2.py:121][0m #------------------------ Iteration 4224 --------------------------#
[32m[20221214 00:02:36 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:02:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:02:36 @agent_ppo2.py:185][0m |           0.0018 |          58.9819 |          11.5986 |
[32m[20221214 00:02:36 @agent_ppo2.py:185][0m |          -0.0039 |          56.7242 |          11.6752 |
[32m[20221214 00:02:36 @agent_ppo2.py:185][0m |          -0.0055 |          56.2105 |          11.6153 |
[32m[20221214 00:02:36 @agent_ppo2.py:185][0m |          -0.0043 |          56.3254 |          11.6602 |
[32m[20221214 00:02:36 @agent_ppo2.py:185][0m |          -0.0042 |          55.9510 |          11.6541 |
[32m[20221214 00:02:36 @agent_ppo2.py:185][0m |           0.0078 |          60.4426 |          11.6491 |
[32m[20221214 00:02:36 @agent_ppo2.py:185][0m |          -0.0005 |          56.1668 |          11.6397 |
[32m[20221214 00:02:36 @agent_ppo2.py:185][0m |          -0.0077 |          55.3309 |          11.6933 |
[32m[20221214 00:02:37 @agent_ppo2.py:185][0m |          -0.0081 |          55.6932 |          11.6565 |
[32m[20221214 00:02:37 @agent_ppo2.py:185][0m |          -0.0026 |          55.1975 |          11.5857 |
[32m[20221214 00:02:37 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:02:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 494.33
[32m[20221214 00:02:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 504.72
[32m[20221214 00:02:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 414.17
[32m[20221214 00:02:37 @agent_ppo2.py:143][0m Total time:       5.11 min
[32m[20221214 00:02:37 @agent_ppo2.py:145][0m 460800 total steps have happened
[32m[20221214 00:02:37 @agent_ppo2.py:121][0m #------------------------ Iteration 4225 --------------------------#
[32m[20221214 00:02:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:02:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:02:37 @agent_ppo2.py:185][0m |           0.0012 |          73.6859 |          11.7023 |
[32m[20221214 00:02:37 @agent_ppo2.py:185][0m |          -0.0073 |          71.5697 |          11.7343 |
[32m[20221214 00:02:37 @agent_ppo2.py:185][0m |          -0.0069 |          70.8171 |          11.6778 |
[32m[20221214 00:02:37 @agent_ppo2.py:185][0m |          -0.0088 |          70.1890 |          11.7022 |
[32m[20221214 00:02:37 @agent_ppo2.py:185][0m |          -0.0011 |          72.0330 |          11.6844 |
[32m[20221214 00:02:38 @agent_ppo2.py:185][0m |          -0.0108 |          69.6851 |          11.6994 |
[32m[20221214 00:02:38 @agent_ppo2.py:185][0m |          -0.0125 |          69.3093 |          11.7359 |
[32m[20221214 00:02:38 @agent_ppo2.py:185][0m |          -0.0152 |          68.9107 |          11.7197 |
[32m[20221214 00:02:38 @agent_ppo2.py:185][0m |          -0.0129 |          68.6621 |          11.7552 |
[32m[20221214 00:02:38 @agent_ppo2.py:185][0m |          -0.0129 |          68.5518 |          11.7660 |
[32m[20221214 00:02:38 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:02:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 434.98
[32m[20221214 00:02:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 461.67
[32m[20221214 00:02:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 449.69
[32m[20221214 00:02:38 @agent_ppo2.py:143][0m Total time:       5.13 min
[32m[20221214 00:02:38 @agent_ppo2.py:145][0m 462848 total steps have happened
[32m[20221214 00:02:38 @agent_ppo2.py:121][0m #------------------------ Iteration 4226 --------------------------#
[32m[20221214 00:02:38 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:02:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:02:38 @agent_ppo2.py:185][0m |          -0.0029 |          62.0231 |          11.9019 |
[32m[20221214 00:02:38 @agent_ppo2.py:185][0m |          -0.0101 |          59.8418 |          11.9449 |
[32m[20221214 00:02:39 @agent_ppo2.py:185][0m |          -0.0034 |          59.7874 |          11.9200 |
[32m[20221214 00:02:39 @agent_ppo2.py:185][0m |          -0.0121 |          59.1514 |          11.8991 |
[32m[20221214 00:02:39 @agent_ppo2.py:185][0m |          -0.0111 |          59.0477 |          11.9558 |
[32m[20221214 00:02:39 @agent_ppo2.py:185][0m |          -0.0020 |          61.8215 |          11.9372 |
[32m[20221214 00:02:39 @agent_ppo2.py:185][0m |          -0.0117 |          58.6016 |          11.9924 |
[32m[20221214 00:02:39 @agent_ppo2.py:185][0m |          -0.0130 |          58.4947 |          12.0017 |
[32m[20221214 00:02:39 @agent_ppo2.py:185][0m |          -0.0062 |          62.3384 |          11.9365 |
[32m[20221214 00:02:39 @agent_ppo2.py:185][0m |          -0.0093 |          58.7498 |          11.9667 |
[32m[20221214 00:02:39 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:02:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 513.18
[32m[20221214 00:02:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 531.99
[32m[20221214 00:02:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 509.29
[32m[20221214 00:02:39 @agent_ppo2.py:143][0m Total time:       5.15 min
[32m[20221214 00:02:39 @agent_ppo2.py:145][0m 464896 total steps have happened
[32m[20221214 00:02:39 @agent_ppo2.py:121][0m #------------------------ Iteration 4227 --------------------------#
[32m[20221214 00:02:39 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:02:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:02:40 @agent_ppo2.py:185][0m |           0.0038 |          70.3357 |          11.9997 |
[32m[20221214 00:02:40 @agent_ppo2.py:185][0m |          -0.0053 |          66.7379 |          12.0450 |
[32m[20221214 00:02:40 @agent_ppo2.py:185][0m |          -0.0018 |          71.1325 |          12.0531 |
[32m[20221214 00:02:40 @agent_ppo2.py:185][0m |          -0.0125 |          65.6281 |          12.0814 |
[32m[20221214 00:02:40 @agent_ppo2.py:185][0m |          -0.0112 |          65.0239 |          12.0885 |
[32m[20221214 00:02:40 @agent_ppo2.py:185][0m |          -0.0155 |          64.7162 |          12.1239 |
[32m[20221214 00:02:40 @agent_ppo2.py:185][0m |          -0.0160 |          64.5336 |          12.1258 |
[32m[20221214 00:02:40 @agent_ppo2.py:185][0m |          -0.0143 |          64.2364 |          12.1620 |
[32m[20221214 00:02:40 @agent_ppo2.py:185][0m |          -0.0166 |          64.3239 |          12.1877 |
[32m[20221214 00:02:40 @agent_ppo2.py:185][0m |          -0.0191 |          64.0603 |          12.2033 |
[32m[20221214 00:02:40 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:02:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 428.61
[32m[20221214 00:02:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 454.45
[32m[20221214 00:02:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 522.82
[32m[20221214 00:02:41 @agent_ppo2.py:143][0m Total time:       5.17 min
[32m[20221214 00:02:41 @agent_ppo2.py:145][0m 466944 total steps have happened
[32m[20221214 00:02:41 @agent_ppo2.py:121][0m #------------------------ Iteration 4228 --------------------------#
[32m[20221214 00:02:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:02:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:02:41 @agent_ppo2.py:185][0m |          -0.0001 |          67.5169 |          12.1616 |
[32m[20221214 00:02:41 @agent_ppo2.py:185][0m |          -0.0059 |          65.4472 |          12.1591 |
[32m[20221214 00:02:41 @agent_ppo2.py:185][0m |          -0.0065 |          64.4347 |          12.1931 |
[32m[20221214 00:02:41 @agent_ppo2.py:185][0m |          -0.0118 |          64.0807 |          12.2004 |
[32m[20221214 00:02:41 @agent_ppo2.py:185][0m |          -0.0124 |          63.8394 |          12.2312 |
[32m[20221214 00:02:41 @agent_ppo2.py:185][0m |          -0.0115 |          63.7252 |          12.2460 |
[32m[20221214 00:02:41 @agent_ppo2.py:185][0m |          -0.0116 |          63.8369 |          12.2780 |
[32m[20221214 00:02:41 @agent_ppo2.py:185][0m |          -0.0152 |          63.3100 |          12.2965 |
[32m[20221214 00:02:42 @agent_ppo2.py:185][0m |          -0.0142 |          63.0437 |          12.2968 |
[32m[20221214 00:02:42 @agent_ppo2.py:185][0m |          -0.0157 |          62.8885 |          12.3108 |
[32m[20221214 00:02:42 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:02:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 489.12
[32m[20221214 00:02:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 514.77
[32m[20221214 00:02:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 499.57
[32m[20221214 00:02:42 @agent_ppo2.py:143][0m Total time:       5.19 min
[32m[20221214 00:02:42 @agent_ppo2.py:145][0m 468992 total steps have happened
[32m[20221214 00:02:42 @agent_ppo2.py:121][0m #------------------------ Iteration 4229 --------------------------#
[32m[20221214 00:02:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:02:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:02:42 @agent_ppo2.py:185][0m |           0.0023 |          61.9426 |          12.2829 |
[32m[20221214 00:02:42 @agent_ppo2.py:185][0m |          -0.0046 |          59.4170 |          12.2252 |
[32m[20221214 00:02:42 @agent_ppo2.py:185][0m |          -0.0064 |          58.8720 |          12.2370 |
[32m[20221214 00:02:42 @agent_ppo2.py:185][0m |          -0.0088 |          57.9380 |          12.2006 |
[32m[20221214 00:02:42 @agent_ppo2.py:185][0m |          -0.0079 |          57.3613 |          12.1921 |
[32m[20221214 00:02:43 @agent_ppo2.py:185][0m |          -0.0110 |          57.5303 |          12.1875 |
[32m[20221214 00:02:43 @agent_ppo2.py:185][0m |          -0.0045 |          58.3903 |          12.2169 |
[32m[20221214 00:02:43 @agent_ppo2.py:185][0m |          -0.0125 |          56.6356 |          12.2232 |
[32m[20221214 00:02:43 @agent_ppo2.py:185][0m |          -0.0139 |          56.7144 |          12.2104 |
[32m[20221214 00:02:43 @agent_ppo2.py:185][0m |          -0.0083 |          56.4918 |          12.2135 |
[32m[20221214 00:02:43 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:02:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 426.45
[32m[20221214 00:02:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 460.00
[32m[20221214 00:02:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 532.94
[32m[20221214 00:02:43 @agent_ppo2.py:143][0m Total time:       5.21 min
[32m[20221214 00:02:43 @agent_ppo2.py:145][0m 471040 total steps have happened
[32m[20221214 00:02:43 @agent_ppo2.py:121][0m #------------------------ Iteration 4230 --------------------------#
[32m[20221214 00:02:43 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:02:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:02:43 @agent_ppo2.py:185][0m |          -0.0012 |          57.8446 |          12.3351 |
[32m[20221214 00:02:43 @agent_ppo2.py:185][0m |          -0.0057 |          56.0938 |          12.3381 |
[32m[20221214 00:02:44 @agent_ppo2.py:185][0m |          -0.0105 |          55.7561 |          12.3511 |
[32m[20221214 00:02:44 @agent_ppo2.py:185][0m |           0.0021 |          58.9546 |          12.3472 |
[32m[20221214 00:02:44 @agent_ppo2.py:185][0m |          -0.0083 |          55.3469 |          12.4086 |
[32m[20221214 00:02:44 @agent_ppo2.py:185][0m |          -0.0111 |          55.0216 |          12.3774 |
[32m[20221214 00:02:44 @agent_ppo2.py:185][0m |          -0.0138 |          54.8523 |          12.3878 |
[32m[20221214 00:02:44 @agent_ppo2.py:185][0m |          -0.0108 |          54.9493 |          12.4047 |
[32m[20221214 00:02:44 @agent_ppo2.py:185][0m |          -0.0155 |          54.7425 |          12.3915 |
[32m[20221214 00:02:44 @agent_ppo2.py:185][0m |          -0.0141 |          54.6321 |          12.4201 |
[32m[20221214 00:02:44 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:02:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 490.25
[32m[20221214 00:02:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 515.13
[32m[20221214 00:02:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 506.96
[32m[20221214 00:02:44 @agent_ppo2.py:143][0m Total time:       5.23 min
[32m[20221214 00:02:44 @agent_ppo2.py:145][0m 473088 total steps have happened
[32m[20221214 00:02:44 @agent_ppo2.py:121][0m #------------------------ Iteration 4231 --------------------------#
[32m[20221214 00:02:44 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:02:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:02:45 @agent_ppo2.py:185][0m |          -0.0015 |          75.9047 |          12.4326 |
[32m[20221214 00:02:45 @agent_ppo2.py:185][0m |          -0.0045 |          72.6757 |          12.4264 |
[32m[20221214 00:02:45 @agent_ppo2.py:185][0m |          -0.0048 |          71.3796 |          12.4903 |
[32m[20221214 00:02:45 @agent_ppo2.py:185][0m |          -0.0065 |          71.0005 |          12.5084 |
[32m[20221214 00:02:45 @agent_ppo2.py:185][0m |          -0.0081 |          70.4358 |          12.5453 |
[32m[20221214 00:02:45 @agent_ppo2.py:185][0m |          -0.0102 |          69.7209 |          12.5557 |
[32m[20221214 00:02:45 @agent_ppo2.py:185][0m |          -0.0136 |          69.6407 |          12.5711 |
[32m[20221214 00:02:45 @agent_ppo2.py:185][0m |          -0.0070 |          69.1316 |          12.5858 |
[32m[20221214 00:02:45 @agent_ppo2.py:185][0m |          -0.0047 |          71.6888 |          12.6420 |
[32m[20221214 00:02:45 @agent_ppo2.py:185][0m |          -0.0096 |          68.8271 |          12.6055 |
[32m[20221214 00:02:45 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:02:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 492.92
[32m[20221214 00:02:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 541.33
[32m[20221214 00:02:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 436.97
[32m[20221214 00:02:46 @agent_ppo2.py:143][0m Total time:       5.25 min
[32m[20221214 00:02:46 @agent_ppo2.py:145][0m 475136 total steps have happened
[32m[20221214 00:02:46 @agent_ppo2.py:121][0m #------------------------ Iteration 4232 --------------------------#
[32m[20221214 00:02:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:02:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:02:46 @agent_ppo2.py:185][0m |          -0.0006 |          67.6949 |          12.1328 |
[32m[20221214 00:02:46 @agent_ppo2.py:185][0m |          -0.0077 |          63.2958 |          12.1554 |
[32m[20221214 00:02:46 @agent_ppo2.py:185][0m |          -0.0113 |          61.6302 |          12.1665 |
[32m[20221214 00:02:46 @agent_ppo2.py:185][0m |          -0.0078 |          60.9450 |          12.2003 |
[32m[20221214 00:02:46 @agent_ppo2.py:185][0m |          -0.0125 |          59.5902 |          12.2064 |
[32m[20221214 00:02:46 @agent_ppo2.py:185][0m |          -0.0151 |          59.0429 |          12.2147 |
[32m[20221214 00:02:46 @agent_ppo2.py:185][0m |          -0.0130 |          58.6193 |          12.1988 |
[32m[20221214 00:02:46 @agent_ppo2.py:185][0m |          -0.0131 |          58.2031 |          12.2225 |
[32m[20221214 00:02:47 @agent_ppo2.py:185][0m |          -0.0147 |          58.0397 |          12.2034 |
[32m[20221214 00:02:47 @agent_ppo2.py:185][0m |          -0.0144 |          57.6496 |          12.2101 |
[32m[20221214 00:02:47 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:02:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 473.74
[32m[20221214 00:02:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 494.35
[32m[20221214 00:02:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 320.77
[32m[20221214 00:02:47 @agent_ppo2.py:143][0m Total time:       5.27 min
[32m[20221214 00:02:47 @agent_ppo2.py:145][0m 477184 total steps have happened
[32m[20221214 00:02:47 @agent_ppo2.py:121][0m #------------------------ Iteration 4233 --------------------------#
[32m[20221214 00:02:47 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:02:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:02:47 @agent_ppo2.py:185][0m |           0.0013 |          78.9394 |          12.4953 |
[32m[20221214 00:02:47 @agent_ppo2.py:185][0m |          -0.0070 |          59.0631 |          12.5067 |
[32m[20221214 00:02:47 @agent_ppo2.py:185][0m |          -0.0033 |          56.9077 |          12.4541 |
[32m[20221214 00:02:47 @agent_ppo2.py:185][0m |          -0.0081 |          56.4858 |          12.4910 |
[32m[20221214 00:02:48 @agent_ppo2.py:185][0m |          -0.0102 |          55.7629 |          12.4646 |
[32m[20221214 00:02:48 @agent_ppo2.py:185][0m |          -0.0150 |          55.0597 |          12.4616 |
[32m[20221214 00:02:48 @agent_ppo2.py:185][0m |          -0.0186 |          54.8913 |          12.4479 |
[32m[20221214 00:02:48 @agent_ppo2.py:185][0m |          -0.0111 |          54.5387 |          12.4069 |
[32m[20221214 00:02:48 @agent_ppo2.py:185][0m |          -0.0144 |          54.1646 |          12.4086 |
[32m[20221214 00:02:48 @agent_ppo2.py:185][0m |          -0.0128 |          54.1039 |          12.4211 |
[32m[20221214 00:02:48 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:02:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 397.68
[32m[20221214 00:02:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 497.49
[32m[20221214 00:02:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 239.40
[32m[20221214 00:02:48 @agent_ppo2.py:143][0m Total time:       5.30 min
[32m[20221214 00:02:48 @agent_ppo2.py:145][0m 479232 total steps have happened
[32m[20221214 00:02:48 @agent_ppo2.py:121][0m #------------------------ Iteration 4234 --------------------------#
[32m[20221214 00:02:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:02:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:02:48 @agent_ppo2.py:185][0m |           0.0018 |          66.5528 |          12.4066 |
[32m[20221214 00:02:49 @agent_ppo2.py:185][0m |          -0.0117 |          59.9083 |          12.4205 |
[32m[20221214 00:02:49 @agent_ppo2.py:185][0m |          -0.0069 |          59.3507 |          12.4293 |
[32m[20221214 00:02:49 @agent_ppo2.py:185][0m |          -0.0069 |          57.8805 |          12.4410 |
[32m[20221214 00:02:49 @agent_ppo2.py:185][0m |          -0.0121 |          57.2081 |          12.4414 |
[32m[20221214 00:02:49 @agent_ppo2.py:185][0m |          -0.0118 |          56.9281 |          12.4514 |
[32m[20221214 00:02:49 @agent_ppo2.py:185][0m |          -0.0020 |          60.3040 |          12.4716 |
[32m[20221214 00:02:49 @agent_ppo2.py:185][0m |          -0.0137 |          55.9905 |          12.4582 |
[32m[20221214 00:02:49 @agent_ppo2.py:185][0m |          -0.0111 |          55.5928 |          12.4626 |
[32m[20221214 00:02:49 @agent_ppo2.py:185][0m |          -0.0131 |          55.4563 |          12.4967 |
[32m[20221214 00:02:49 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:02:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 447.07
[32m[20221214 00:02:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 498.07
[32m[20221214 00:02:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 492.30
[32m[20221214 00:02:49 @agent_ppo2.py:143][0m Total time:       5.32 min
[32m[20221214 00:02:49 @agent_ppo2.py:145][0m 481280 total steps have happened
[32m[20221214 00:02:49 @agent_ppo2.py:121][0m #------------------------ Iteration 4235 --------------------------#
[32m[20221214 00:02:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:02:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:02:50 @agent_ppo2.py:185][0m |          -0.0004 |          64.5284 |          12.6115 |
[32m[20221214 00:02:50 @agent_ppo2.py:185][0m |          -0.0086 |          56.8084 |          12.5681 |
[32m[20221214 00:02:50 @agent_ppo2.py:185][0m |          -0.0097 |          55.6370 |          12.5989 |
[32m[20221214 00:02:50 @agent_ppo2.py:185][0m |          -0.0142 |          54.4109 |          12.5986 |
[32m[20221214 00:02:50 @agent_ppo2.py:185][0m |          -0.0162 |          53.9558 |          12.6021 |
[32m[20221214 00:02:50 @agent_ppo2.py:185][0m |          -0.0133 |          53.1865 |          12.6007 |
[32m[20221214 00:02:50 @agent_ppo2.py:185][0m |          -0.0145 |          52.9104 |          12.5690 |
[32m[20221214 00:02:50 @agent_ppo2.py:185][0m |          -0.0123 |          52.9824 |          12.6034 |
[32m[20221214 00:02:50 @agent_ppo2.py:185][0m |          -0.0164 |          52.3617 |          12.5878 |
[32m[20221214 00:02:50 @agent_ppo2.py:185][0m |          -0.0183 |          52.1435 |          12.5994 |
[32m[20221214 00:02:50 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:02:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 458.06
[32m[20221214 00:02:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 504.92
[32m[20221214 00:02:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 416.63
[32m[20221214 00:02:51 @agent_ppo2.py:143][0m Total time:       5.34 min
[32m[20221214 00:02:51 @agent_ppo2.py:145][0m 483328 total steps have happened
[32m[20221214 00:02:51 @agent_ppo2.py:121][0m #------------------------ Iteration 4236 --------------------------#
[32m[20221214 00:02:51 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:02:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:02:51 @agent_ppo2.py:185][0m |           0.0093 |          36.5420 |          12.4850 |
[32m[20221214 00:02:51 @agent_ppo2.py:185][0m |           0.0025 |          29.6712 |          12.5106 |
[32m[20221214 00:02:51 @agent_ppo2.py:185][0m |          -0.0024 |          29.2307 |          12.5214 |
[32m[20221214 00:02:51 @agent_ppo2.py:185][0m |          -0.0141 |          26.9839 |          12.5606 |
[32m[20221214 00:02:51 @agent_ppo2.py:185][0m |          -0.0129 |          25.9866 |          12.5636 |
[32m[20221214 00:02:51 @agent_ppo2.py:185][0m |          -0.0142 |          25.5501 |          12.5806 |
[32m[20221214 00:02:51 @agent_ppo2.py:185][0m |          -0.0180 |          24.7960 |          12.5746 |
[32m[20221214 00:02:52 @agent_ppo2.py:185][0m |          -0.0175 |          24.6909 |          12.5736 |
[32m[20221214 00:02:52 @agent_ppo2.py:185][0m |          -0.0241 |          24.2498 |          12.5911 |
[32m[20221214 00:02:52 @agent_ppo2.py:185][0m |          -0.0173 |          23.7932 |          12.6118 |
[32m[20221214 00:02:52 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:02:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 344.62
[32m[20221214 00:02:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 452.98
[32m[20221214 00:02:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 441.76
[32m[20221214 00:02:52 @agent_ppo2.py:143][0m Total time:       5.36 min
[32m[20221214 00:02:52 @agent_ppo2.py:145][0m 485376 total steps have happened
[32m[20221214 00:02:52 @agent_ppo2.py:121][0m #------------------------ Iteration 4237 --------------------------#
[32m[20221214 00:02:52 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:02:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:02:52 @agent_ppo2.py:185][0m |          -0.0015 |          54.8019 |          12.5830 |
[32m[20221214 00:02:52 @agent_ppo2.py:185][0m |          -0.0067 |          51.7161 |          12.5892 |
[32m[20221214 00:02:52 @agent_ppo2.py:185][0m |           0.0097 |          55.1564 |          12.6020 |
[32m[20221214 00:02:52 @agent_ppo2.py:185][0m |          -0.0079 |          49.5977 |          12.6194 |
[32m[20221214 00:02:53 @agent_ppo2.py:185][0m |          -0.0148 |          48.4644 |          12.5707 |
[32m[20221214 00:02:53 @agent_ppo2.py:185][0m |          -0.0126 |          47.8736 |          12.5725 |
[32m[20221214 00:02:53 @agent_ppo2.py:185][0m |          -0.0087 |          48.2878 |          12.5700 |
[32m[20221214 00:02:53 @agent_ppo2.py:185][0m |          -0.0129 |          47.1059 |          12.5865 |
[32m[20221214 00:02:53 @agent_ppo2.py:185][0m |          -0.0155 |          47.0637 |          12.5930 |
[32m[20221214 00:02:53 @agent_ppo2.py:185][0m |          -0.0197 |          46.6945 |          12.5906 |
[32m[20221214 00:02:53 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:02:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 375.26
[32m[20221214 00:02:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 478.76
[32m[20221214 00:02:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 447.94
[32m[20221214 00:02:53 @agent_ppo2.py:143][0m Total time:       5.38 min
[32m[20221214 00:02:53 @agent_ppo2.py:145][0m 487424 total steps have happened
[32m[20221214 00:02:53 @agent_ppo2.py:121][0m #------------------------ Iteration 4238 --------------------------#
[32m[20221214 00:02:53 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:02:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:02:54 @agent_ppo2.py:185][0m |           0.0005 |          30.0735 |          12.6097 |
[32m[20221214 00:02:54 @agent_ppo2.py:185][0m |          -0.0056 |          27.0953 |          12.6094 |
[32m[20221214 00:02:54 @agent_ppo2.py:185][0m |          -0.0038 |          26.1326 |          12.6111 |
[32m[20221214 00:02:54 @agent_ppo2.py:185][0m |          -0.0081 |          25.7231 |          12.6034 |
[32m[20221214 00:02:54 @agent_ppo2.py:185][0m |           0.0054 |          29.4967 |          12.6415 |
[32m[20221214 00:02:54 @agent_ppo2.py:185][0m |          -0.0158 |          25.0843 |          12.6522 |
[32m[20221214 00:02:54 @agent_ppo2.py:185][0m |          -0.0082 |          24.8453 |          12.6374 |
[32m[20221214 00:02:54 @agent_ppo2.py:185][0m |          -0.0158 |          24.6332 |          12.6818 |
[32m[20221214 00:02:54 @agent_ppo2.py:185][0m |          -0.0166 |          24.6802 |          12.6618 |
[32m[20221214 00:02:54 @agent_ppo2.py:185][0m |          -0.0142 |          24.3445 |          12.6751 |
[32m[20221214 00:02:54 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:02:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 466.75
[32m[20221214 00:02:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 523.02
[32m[20221214 00:02:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 379.76
[32m[20221214 00:02:54 @agent_ppo2.py:143][0m Total time:       5.40 min
[32m[20221214 00:02:54 @agent_ppo2.py:145][0m 489472 total steps have happened
[32m[20221214 00:02:54 @agent_ppo2.py:121][0m #------------------------ Iteration 4239 --------------------------#
[32m[20221214 00:02:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:02:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:02:55 @agent_ppo2.py:185][0m |          -0.0013 |          80.0364 |          12.6463 |
[32m[20221214 00:02:55 @agent_ppo2.py:185][0m |          -0.0021 |          74.9696 |          12.6706 |
[32m[20221214 00:02:55 @agent_ppo2.py:185][0m |          -0.0078 |          72.9564 |          12.6552 |
[32m[20221214 00:02:55 @agent_ppo2.py:185][0m |          -0.0102 |          71.4831 |          12.6642 |
[32m[20221214 00:02:55 @agent_ppo2.py:185][0m |          -0.0056 |          71.3195 |          12.7005 |
[32m[20221214 00:02:55 @agent_ppo2.py:185][0m |          -0.0086 |          68.8399 |          12.7273 |
[32m[20221214 00:02:55 @agent_ppo2.py:185][0m |          -0.0139 |          68.1388 |          12.7445 |
[32m[20221214 00:02:55 @agent_ppo2.py:185][0m |          -0.0164 |          67.9933 |          12.7591 |
[32m[20221214 00:02:55 @agent_ppo2.py:185][0m |          -0.0146 |          67.3368 |          12.7751 |
[32m[20221214 00:02:56 @agent_ppo2.py:185][0m |          -0.0184 |          67.0192 |          12.7847 |
[32m[20221214 00:02:56 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:02:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 476.60
[32m[20221214 00:02:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 500.73
[32m[20221214 00:02:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 452.20
[32m[20221214 00:02:56 @agent_ppo2.py:143][0m Total time:       5.42 min
[32m[20221214 00:02:56 @agent_ppo2.py:145][0m 491520 total steps have happened
[32m[20221214 00:02:56 @agent_ppo2.py:121][0m #------------------------ Iteration 4240 --------------------------#
[32m[20221214 00:02:56 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:02:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:02:56 @agent_ppo2.py:185][0m |          -0.0008 |          71.8886 |          13.0390 |
[32m[20221214 00:02:56 @agent_ppo2.py:185][0m |           0.0033 |          71.7640 |          13.0651 |
[32m[20221214 00:02:56 @agent_ppo2.py:185][0m |          -0.0062 |          68.2060 |          13.0793 |
[32m[20221214 00:02:56 @agent_ppo2.py:185][0m |          -0.0083 |          67.4391 |          13.0370 |
[32m[20221214 00:02:56 @agent_ppo2.py:185][0m |          -0.0061 |          66.9172 |          13.0575 |
[32m[20221214 00:02:56 @agent_ppo2.py:185][0m |          -0.0006 |          67.3122 |          13.0673 |
[32m[20221214 00:02:57 @agent_ppo2.py:185][0m |          -0.0095 |          66.1080 |          13.0545 |
[32m[20221214 00:02:57 @agent_ppo2.py:185][0m |          -0.0051 |          65.9429 |          13.0873 |
[32m[20221214 00:02:57 @agent_ppo2.py:185][0m |          -0.0083 |          65.2900 |          13.0870 |
[32m[20221214 00:02:57 @agent_ppo2.py:185][0m |          -0.0117 |          64.9015 |          13.0698 |
[32m[20221214 00:02:57 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:02:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 507.89
[32m[20221214 00:02:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 518.33
[32m[20221214 00:02:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 513.17
[32m[20221214 00:02:57 @agent_ppo2.py:143][0m Total time:       5.44 min
[32m[20221214 00:02:57 @agent_ppo2.py:145][0m 493568 total steps have happened
[32m[20221214 00:02:57 @agent_ppo2.py:121][0m #------------------------ Iteration 4241 --------------------------#
[32m[20221214 00:02:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:02:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:02:57 @agent_ppo2.py:185][0m |           0.0056 |          70.9186 |          12.8242 |
[32m[20221214 00:02:57 @agent_ppo2.py:185][0m |          -0.0078 |          64.0061 |          12.8349 |
[32m[20221214 00:02:57 @agent_ppo2.py:185][0m |          -0.0049 |          62.0233 |          12.7759 |
[32m[20221214 00:02:58 @agent_ppo2.py:185][0m |          -0.0036 |          62.0224 |          12.7944 |
[32m[20221214 00:02:58 @agent_ppo2.py:185][0m |           0.0071 |          72.5439 |          12.7826 |
[32m[20221214 00:02:58 @agent_ppo2.py:185][0m |           0.0006 |          69.7239 |          12.7831 |
[32m[20221214 00:02:58 @agent_ppo2.py:185][0m |          -0.0123 |          59.7479 |          12.7655 |
[32m[20221214 00:02:58 @agent_ppo2.py:185][0m |          -0.0111 |          59.5323 |          12.7486 |
[32m[20221214 00:02:58 @agent_ppo2.py:185][0m |          -0.0079 |          61.3759 |          12.7187 |
[32m[20221214 00:02:58 @agent_ppo2.py:185][0m |          -0.0141 |          58.5919 |          12.7122 |
[32m[20221214 00:02:58 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:02:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 526.70
[32m[20221214 00:02:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 571.21
[32m[20221214 00:02:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 558.30
[32m[20221214 00:02:58 @agent_ppo2.py:143][0m Total time:       5.46 min
[32m[20221214 00:02:58 @agent_ppo2.py:145][0m 495616 total steps have happened
[32m[20221214 00:02:58 @agent_ppo2.py:121][0m #------------------------ Iteration 4242 --------------------------#
[32m[20221214 00:02:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:02:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:02:59 @agent_ppo2.py:185][0m |           0.0037 |          48.9630 |          12.7013 |
[32m[20221214 00:02:59 @agent_ppo2.py:185][0m |          -0.0075 |          42.3452 |          12.7038 |
[32m[20221214 00:02:59 @agent_ppo2.py:185][0m |          -0.0086 |          40.4348 |          12.7153 |
[32m[20221214 00:02:59 @agent_ppo2.py:185][0m |          -0.0148 |          39.3397 |          12.7214 |
[32m[20221214 00:02:59 @agent_ppo2.py:185][0m |          -0.0043 |          40.2716 |          12.7042 |
[32m[20221214 00:02:59 @agent_ppo2.py:185][0m |          -0.0148 |          37.8967 |          12.6788 |
[32m[20221214 00:02:59 @agent_ppo2.py:185][0m |          -0.0172 |          37.3432 |          12.6879 |
[32m[20221214 00:02:59 @agent_ppo2.py:185][0m |          -0.0134 |          36.9132 |          12.6831 |
[32m[20221214 00:02:59 @agent_ppo2.py:185][0m |          -0.0192 |          36.5332 |          12.6915 |
[32m[20221214 00:02:59 @agent_ppo2.py:185][0m |          -0.0200 |          36.0774 |          12.6905 |
[32m[20221214 00:02:59 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:02:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 443.15
[32m[20221214 00:02:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 500.12
[32m[20221214 00:02:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 606.48
[32m[20221214 00:02:59 @agent_ppo2.py:143][0m Total time:       5.49 min
[32m[20221214 00:02:59 @agent_ppo2.py:145][0m 497664 total steps have happened
[32m[20221214 00:02:59 @agent_ppo2.py:121][0m #------------------------ Iteration 4243 --------------------------#
[32m[20221214 00:03:00 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:03:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:03:00 @agent_ppo2.py:185][0m |          -0.0003 |          65.4143 |          12.7937 |
[32m[20221214 00:03:00 @agent_ppo2.py:185][0m |          -0.0065 |          55.5470 |          12.7867 |
[32m[20221214 00:03:00 @agent_ppo2.py:185][0m |          -0.0067 |          52.8758 |          12.7806 |
[32m[20221214 00:03:00 @agent_ppo2.py:185][0m |          -0.0075 |          52.1199 |          12.7857 |
[32m[20221214 00:03:00 @agent_ppo2.py:185][0m |          -0.0127 |          51.0876 |          12.7860 |
[32m[20221214 00:03:00 @agent_ppo2.py:185][0m |          -0.0121 |          50.2708 |          12.7828 |
[32m[20221214 00:03:00 @agent_ppo2.py:185][0m |          -0.0116 |          49.7916 |          12.7746 |
[32m[20221214 00:03:00 @agent_ppo2.py:185][0m |          -0.0147 |          49.5055 |          12.7669 |
[32m[20221214 00:03:01 @agent_ppo2.py:185][0m |          -0.0181 |          49.2510 |          12.7703 |
[32m[20221214 00:03:01 @agent_ppo2.py:185][0m |          -0.0122 |          49.2475 |          12.7425 |
[32m[20221214 00:03:01 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:03:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 368.54
[32m[20221214 00:03:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 472.73
[32m[20221214 00:03:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 590.43
[32m[20221214 00:03:01 @agent_ppo2.py:143][0m Total time:       5.51 min
[32m[20221214 00:03:01 @agent_ppo2.py:145][0m 499712 total steps have happened
[32m[20221214 00:03:01 @agent_ppo2.py:121][0m #------------------------ Iteration 4244 --------------------------#
[32m[20221214 00:03:01 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:03:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:03:01 @agent_ppo2.py:185][0m |           0.0010 |          61.6169 |          12.6363 |
[32m[20221214 00:03:01 @agent_ppo2.py:185][0m |          -0.0017 |          51.1320 |          12.6437 |
[32m[20221214 00:03:01 @agent_ppo2.py:185][0m |          -0.0049 |          48.9497 |          12.6142 |
[32m[20221214 00:03:01 @agent_ppo2.py:185][0m |           0.0007 |          52.1534 |          12.6300 |
[32m[20221214 00:03:01 @agent_ppo2.py:185][0m |          -0.0112 |          46.3711 |          12.6041 |
[32m[20221214 00:03:02 @agent_ppo2.py:185][0m |          -0.0141 |          45.5959 |          12.6362 |
[32m[20221214 00:03:02 @agent_ppo2.py:185][0m |          -0.0122 |          45.0519 |          12.6193 |
[32m[20221214 00:03:02 @agent_ppo2.py:185][0m |          -0.0131 |          44.2597 |          12.6317 |
[32m[20221214 00:03:02 @agent_ppo2.py:185][0m |          -0.0145 |          43.4842 |          12.6317 |
[32m[20221214 00:03:02 @agent_ppo2.py:185][0m |          -0.0183 |          43.3399 |          12.6407 |
[32m[20221214 00:03:02 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:03:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 402.93
[32m[20221214 00:03:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 462.40
[32m[20221214 00:03:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 462.24
[32m[20221214 00:03:02 @agent_ppo2.py:143][0m Total time:       5.53 min
[32m[20221214 00:03:02 @agent_ppo2.py:145][0m 501760 total steps have happened
[32m[20221214 00:03:02 @agent_ppo2.py:121][0m #------------------------ Iteration 4245 --------------------------#
[32m[20221214 00:03:02 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:03:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:03:02 @agent_ppo2.py:185][0m |           0.0053 |          77.8587 |          12.7065 |
[32m[20221214 00:03:02 @agent_ppo2.py:185][0m |           0.0061 |          74.0373 |          12.6911 |
[32m[20221214 00:03:03 @agent_ppo2.py:185][0m |          -0.0033 |          69.4975 |          12.6513 |
[32m[20221214 00:03:03 @agent_ppo2.py:185][0m |          -0.0030 |          66.5609 |          12.6270 |
[32m[20221214 00:03:03 @agent_ppo2.py:185][0m |          -0.0098 |          65.0948 |          12.6221 |
[32m[20221214 00:03:03 @agent_ppo2.py:185][0m |          -0.0119 |          64.5088 |          12.6380 |
[32m[20221214 00:03:03 @agent_ppo2.py:185][0m |          -0.0132 |          63.4513 |          12.6257 |
[32m[20221214 00:03:03 @agent_ppo2.py:185][0m |          -0.0116 |          63.0885 |          12.5832 |
[32m[20221214 00:03:03 @agent_ppo2.py:185][0m |          -0.0154 |          62.5739 |          12.5949 |
[32m[20221214 00:03:03 @agent_ppo2.py:185][0m |          -0.0164 |          61.8818 |          12.6040 |
[32m[20221214 00:03:03 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:03:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 511.89
[32m[20221214 00:03:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 528.12
[32m[20221214 00:03:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 288.91
[32m[20221214 00:03:03 @agent_ppo2.py:143][0m Total time:       5.55 min
[32m[20221214 00:03:03 @agent_ppo2.py:145][0m 503808 total steps have happened
[32m[20221214 00:03:03 @agent_ppo2.py:121][0m #------------------------ Iteration 4246 --------------------------#
[32m[20221214 00:03:03 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:03:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:03:04 @agent_ppo2.py:185][0m |           0.0004 |          45.8950 |          12.8141 |
[32m[20221214 00:03:04 @agent_ppo2.py:185][0m |          -0.0059 |          36.9223 |          12.8161 |
[32m[20221214 00:03:04 @agent_ppo2.py:185][0m |          -0.0102 |          35.2386 |          12.8036 |
[32m[20221214 00:03:04 @agent_ppo2.py:185][0m |          -0.0134 |          34.3378 |          12.8085 |
[32m[20221214 00:03:04 @agent_ppo2.py:185][0m |          -0.0121 |          33.7885 |          12.8154 |
[32m[20221214 00:03:04 @agent_ppo2.py:185][0m |          -0.0130 |          33.0760 |          12.8133 |
[32m[20221214 00:03:04 @agent_ppo2.py:185][0m |          -0.0157 |          32.7335 |          12.8306 |
[32m[20221214 00:03:04 @agent_ppo2.py:185][0m |          -0.0143 |          32.4168 |          12.8157 |
[32m[20221214 00:03:04 @agent_ppo2.py:185][0m |          -0.0140 |          32.1881 |          12.8303 |
[32m[20221214 00:03:04 @agent_ppo2.py:185][0m |          -0.0109 |          31.9895 |          12.8097 |
[32m[20221214 00:03:04 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:03:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 385.88
[32m[20221214 00:03:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 481.92
[32m[20221214 00:03:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 321.32
[32m[20221214 00:03:05 @agent_ppo2.py:143][0m Total time:       5.57 min
[32m[20221214 00:03:05 @agent_ppo2.py:145][0m 505856 total steps have happened
[32m[20221214 00:03:05 @agent_ppo2.py:121][0m #------------------------ Iteration 4247 --------------------------#
[32m[20221214 00:03:05 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:03:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:03:05 @agent_ppo2.py:185][0m |           0.0020 |          37.9572 |          12.6844 |
[32m[20221214 00:03:05 @agent_ppo2.py:185][0m |          -0.0101 |          32.2314 |          12.6370 |
[32m[20221214 00:03:05 @agent_ppo2.py:185][0m |          -0.0058 |          30.7207 |          12.5813 |
[32m[20221214 00:03:05 @agent_ppo2.py:185][0m |          -0.0136 |          30.0172 |          12.6059 |
[32m[20221214 00:03:05 @agent_ppo2.py:185][0m |          -0.0102 |          29.3431 |          12.5765 |
[32m[20221214 00:03:05 @agent_ppo2.py:185][0m |          -0.0135 |          28.9631 |          12.5714 |
[32m[20221214 00:03:05 @agent_ppo2.py:185][0m |          -0.0171 |          28.6448 |          12.5194 |
[32m[20221214 00:03:06 @agent_ppo2.py:185][0m |          -0.0188 |          28.3423 |          12.5118 |
[32m[20221214 00:03:06 @agent_ppo2.py:185][0m |          -0.0128 |          28.1489 |          12.4956 |
[32m[20221214 00:03:06 @agent_ppo2.py:185][0m |          -0.0180 |          27.8709 |          12.4879 |
[32m[20221214 00:03:06 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:03:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 425.92
[32m[20221214 00:03:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 493.94
[32m[20221214 00:03:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 203.94
[32m[20221214 00:03:06 @agent_ppo2.py:143][0m Total time:       5.59 min
[32m[20221214 00:03:06 @agent_ppo2.py:145][0m 507904 total steps have happened
[32m[20221214 00:03:06 @agent_ppo2.py:121][0m #------------------------ Iteration 4248 --------------------------#
[32m[20221214 00:03:06 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:03:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:03:06 @agent_ppo2.py:185][0m |           0.0027 |          46.0041 |          12.2295 |
[32m[20221214 00:03:06 @agent_ppo2.py:185][0m |          -0.0049 |          39.6825 |          12.2392 |
[32m[20221214 00:03:06 @agent_ppo2.py:185][0m |          -0.0072 |          37.4525 |          12.2489 |
[32m[20221214 00:03:06 @agent_ppo2.py:185][0m |          -0.0089 |          37.4384 |          12.2753 |
[32m[20221214 00:03:07 @agent_ppo2.py:185][0m |          -0.0104 |          35.4893 |          12.2587 |
[32m[20221214 00:03:07 @agent_ppo2.py:185][0m |          -0.0124 |          35.1511 |          12.2482 |
[32m[20221214 00:03:07 @agent_ppo2.py:185][0m |          -0.0105 |          34.7184 |          12.2691 |
[32m[20221214 00:03:07 @agent_ppo2.py:185][0m |          -0.0160 |          34.3275 |          12.2467 |
[32m[20221214 00:03:07 @agent_ppo2.py:185][0m |          -0.0134 |          33.9425 |          12.2680 |
[32m[20221214 00:03:07 @agent_ppo2.py:185][0m |          -0.0126 |          34.4029 |          12.2956 |
[32m[20221214 00:03:07 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:03:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 409.66
[32m[20221214 00:03:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 478.18
[32m[20221214 00:03:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 517.29
[32m[20221214 00:03:07 @agent_ppo2.py:143][0m Total time:       5.61 min
[32m[20221214 00:03:07 @agent_ppo2.py:145][0m 509952 total steps have happened
[32m[20221214 00:03:07 @agent_ppo2.py:121][0m #------------------------ Iteration 4249 --------------------------#
[32m[20221214 00:03:07 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:03:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:03:07 @agent_ppo2.py:185][0m |           0.0008 |          75.7820 |          12.4024 |
[32m[20221214 00:03:08 @agent_ppo2.py:185][0m |           0.0102 |          85.1250 |          12.3730 |
[32m[20221214 00:03:08 @agent_ppo2.py:185][0m |          -0.0060 |          68.7924 |          12.3898 |
[32m[20221214 00:03:08 @agent_ppo2.py:185][0m |          -0.0040 |          67.1356 |          12.3787 |
[32m[20221214 00:03:08 @agent_ppo2.py:185][0m |          -0.0073 |          65.9740 |          12.4027 |
[32m[20221214 00:03:08 @agent_ppo2.py:185][0m |          -0.0105 |          65.0947 |          12.3655 |
[32m[20221214 00:03:08 @agent_ppo2.py:185][0m |          -0.0069 |          64.9435 |          12.4463 |
[32m[20221214 00:03:08 @agent_ppo2.py:185][0m |          -0.0113 |          64.0365 |          12.3932 |
[32m[20221214 00:03:08 @agent_ppo2.py:185][0m |          -0.0036 |          68.8937 |          12.3943 |
[32m[20221214 00:03:08 @agent_ppo2.py:185][0m |          -0.0053 |          65.1416 |          12.4238 |
[32m[20221214 00:03:08 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:03:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 516.81
[32m[20221214 00:03:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 548.17
[32m[20221214 00:03:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 458.09
[32m[20221214 00:03:08 @agent_ppo2.py:143][0m Total time:       5.63 min
[32m[20221214 00:03:08 @agent_ppo2.py:145][0m 512000 total steps have happened
[32m[20221214 00:03:08 @agent_ppo2.py:121][0m #------------------------ Iteration 4250 --------------------------#
[32m[20221214 00:03:09 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:03:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:03:09 @agent_ppo2.py:185][0m |           0.0027 |          82.9922 |          12.6176 |
[32m[20221214 00:03:09 @agent_ppo2.py:185][0m |          -0.0091 |          76.8332 |          12.5980 |
[32m[20221214 00:03:09 @agent_ppo2.py:185][0m |          -0.0074 |          74.3140 |          12.6205 |
[32m[20221214 00:03:09 @agent_ppo2.py:185][0m |          -0.0128 |          72.5801 |          12.5893 |
[32m[20221214 00:03:09 @agent_ppo2.py:185][0m |          -0.0154 |          71.8001 |          12.6163 |
[32m[20221214 00:03:09 @agent_ppo2.py:185][0m |          -0.0123 |          70.4303 |          12.6294 |
[32m[20221214 00:03:09 @agent_ppo2.py:185][0m |          -0.0150 |          69.3924 |          12.6285 |
[32m[20221214 00:03:09 @agent_ppo2.py:185][0m |          -0.0188 |          69.3025 |          12.6247 |
[32m[20221214 00:03:09 @agent_ppo2.py:185][0m |          -0.0127 |          69.2926 |          12.6215 |
[32m[20221214 00:03:09 @agent_ppo2.py:185][0m |          -0.0175 |          68.2370 |          12.6732 |
[32m[20221214 00:03:09 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:03:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 482.28
[32m[20221214 00:03:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 532.68
[32m[20221214 00:03:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 449.83
[32m[20221214 00:03:10 @agent_ppo2.py:143][0m Total time:       5.65 min
[32m[20221214 00:03:10 @agent_ppo2.py:145][0m 514048 total steps have happened
[32m[20221214 00:03:10 @agent_ppo2.py:121][0m #------------------------ Iteration 4251 --------------------------#
[32m[20221214 00:03:10 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:03:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:03:10 @agent_ppo2.py:185][0m |          -0.0017 |          80.2099 |          12.4546 |
[32m[20221214 00:03:10 @agent_ppo2.py:185][0m |          -0.0037 |          77.3734 |          12.4971 |
[32m[20221214 00:03:10 @agent_ppo2.py:185][0m |          -0.0084 |          76.5640 |          12.4816 |
[32m[20221214 00:03:10 @agent_ppo2.py:185][0m |          -0.0113 |          76.1647 |          12.5019 |
[32m[20221214 00:03:10 @agent_ppo2.py:185][0m |           0.0006 |          79.6890 |          12.5487 |
[32m[20221214 00:03:10 @agent_ppo2.py:185][0m |          -0.0009 |          82.9546 |          12.5491 |
[32m[20221214 00:03:10 @agent_ppo2.py:185][0m |          -0.0128 |          75.4817 |          12.5542 |
[32m[20221214 00:03:11 @agent_ppo2.py:185][0m |          -0.0118 |          75.1913 |          12.5792 |
[32m[20221214 00:03:11 @agent_ppo2.py:185][0m |          -0.0105 |          75.2223 |          12.5490 |
[32m[20221214 00:03:11 @agent_ppo2.py:185][0m |          -0.0139 |          74.8579 |          12.5807 |
[32m[20221214 00:03:11 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:03:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 515.06
[32m[20221214 00:03:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 546.11
[32m[20221214 00:03:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 334.58
[32m[20221214 00:03:11 @agent_ppo2.py:143][0m Total time:       5.67 min
[32m[20221214 00:03:11 @agent_ppo2.py:145][0m 516096 total steps have happened
[32m[20221214 00:03:11 @agent_ppo2.py:121][0m #------------------------ Iteration 4252 --------------------------#
[32m[20221214 00:03:11 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:03:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:03:11 @agent_ppo2.py:185][0m |           0.0010 |          51.5803 |          12.5685 |
[32m[20221214 00:03:11 @agent_ppo2.py:185][0m |           0.0003 |          43.5898 |          12.5642 |
[32m[20221214 00:03:11 @agent_ppo2.py:185][0m |          -0.0060 |          40.5810 |          12.5483 |
[32m[20221214 00:03:11 @agent_ppo2.py:185][0m |          -0.0119 |          39.5057 |          12.5093 |
[32m[20221214 00:03:12 @agent_ppo2.py:185][0m |          -0.0164 |          38.9131 |          12.5252 |
[32m[20221214 00:03:12 @agent_ppo2.py:185][0m |          -0.0184 |          38.3844 |          12.5146 |
[32m[20221214 00:03:12 @agent_ppo2.py:185][0m |          -0.0193 |          37.9079 |          12.4766 |
[32m[20221214 00:03:12 @agent_ppo2.py:185][0m |          -0.0126 |          37.6564 |          12.4871 |
[32m[20221214 00:03:12 @agent_ppo2.py:185][0m |          -0.0171 |          37.3034 |          12.4840 |
[32m[20221214 00:03:12 @agent_ppo2.py:185][0m |          -0.0185 |          36.9439 |          12.5134 |
[32m[20221214 00:03:12 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:03:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 376.02
[32m[20221214 00:03:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 470.86
[32m[20221214 00:03:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 398.25
[32m[20221214 00:03:12 @agent_ppo2.py:143][0m Total time:       5.70 min
[32m[20221214 00:03:12 @agent_ppo2.py:145][0m 518144 total steps have happened
[32m[20221214 00:03:12 @agent_ppo2.py:121][0m #------------------------ Iteration 4253 --------------------------#
[32m[20221214 00:03:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:03:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:03:12 @agent_ppo2.py:185][0m |           0.0012 |          43.8987 |          12.4536 |
[32m[20221214 00:03:13 @agent_ppo2.py:185][0m |          -0.0064 |          40.1622 |          12.4136 |
[32m[20221214 00:03:13 @agent_ppo2.py:185][0m |           0.0047 |          44.9611 |          12.3775 |
[32m[20221214 00:03:13 @agent_ppo2.py:185][0m |          -0.0132 |          38.5508 |          12.3732 |
[32m[20221214 00:03:13 @agent_ppo2.py:185][0m |          -0.0118 |          38.0621 |          12.3478 |
[32m[20221214 00:03:13 @agent_ppo2.py:185][0m |          -0.0159 |          37.8288 |          12.3290 |
[32m[20221214 00:03:13 @agent_ppo2.py:185][0m |          -0.0164 |          37.5023 |          12.3529 |
[32m[20221214 00:03:13 @agent_ppo2.py:185][0m |          -0.0128 |          37.2886 |          12.3346 |
[32m[20221214 00:03:13 @agent_ppo2.py:185][0m |          -0.0098 |          37.1529 |          12.3155 |
[32m[20221214 00:03:13 @agent_ppo2.py:185][0m |          -0.0169 |          37.0349 |          12.3307 |
[32m[20221214 00:03:13 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:03:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 347.73
[32m[20221214 00:03:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 479.06
[32m[20221214 00:03:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 469.51
[32m[20221214 00:03:13 @agent_ppo2.py:143][0m Total time:       5.72 min
[32m[20221214 00:03:13 @agent_ppo2.py:145][0m 520192 total steps have happened
[32m[20221214 00:03:13 @agent_ppo2.py:121][0m #------------------------ Iteration 4254 --------------------------#
[32m[20221214 00:03:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:03:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:03:14 @agent_ppo2.py:185][0m |          -0.0025 |          67.8055 |          12.3718 |
[32m[20221214 00:03:14 @agent_ppo2.py:185][0m |          -0.0070 |          62.5710 |          12.3536 |
[32m[20221214 00:03:14 @agent_ppo2.py:185][0m |           0.0025 |          63.7360 |          12.4238 |
[32m[20221214 00:03:14 @agent_ppo2.py:185][0m |          -0.0092 |          59.8320 |          12.3248 |
[32m[20221214 00:03:14 @agent_ppo2.py:185][0m |          -0.0117 |          59.0013 |          12.3452 |
[32m[20221214 00:03:14 @agent_ppo2.py:185][0m |          -0.0087 |          59.6910 |          12.3465 |
[32m[20221214 00:03:14 @agent_ppo2.py:185][0m |          -0.0101 |          57.9409 |          12.3278 |
[32m[20221214 00:03:14 @agent_ppo2.py:185][0m |          -0.0174 |          57.5612 |          12.3140 |
[32m[20221214 00:03:14 @agent_ppo2.py:185][0m |          -0.0082 |          57.8786 |          12.3209 |
[32m[20221214 00:03:15 @agent_ppo2.py:185][0m |          -0.0155 |          56.9515 |          12.3118 |
[32m[20221214 00:03:15 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 00:03:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 492.69
[32m[20221214 00:03:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 541.43
[32m[20221214 00:03:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 542.88
[32m[20221214 00:03:15 @agent_ppo2.py:143][0m Total time:       5.74 min
[32m[20221214 00:03:15 @agent_ppo2.py:145][0m 522240 total steps have happened
[32m[20221214 00:03:15 @agent_ppo2.py:121][0m #------------------------ Iteration 4255 --------------------------#
[32m[20221214 00:03:15 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:03:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:03:15 @agent_ppo2.py:185][0m |          -0.0018 |          79.8644 |          12.3149 |
[32m[20221214 00:03:15 @agent_ppo2.py:185][0m |          -0.0040 |          77.2136 |          12.2280 |
[32m[20221214 00:03:15 @agent_ppo2.py:185][0m |          -0.0115 |          75.8765 |          12.3057 |
[32m[20221214 00:03:15 @agent_ppo2.py:185][0m |           0.0005 |          81.6364 |          12.3058 |
[32m[20221214 00:03:15 @agent_ppo2.py:185][0m |          -0.0125 |          75.1092 |          12.3497 |
[32m[20221214 00:03:15 @agent_ppo2.py:185][0m |          -0.0137 |          74.6726 |          12.3136 |
[32m[20221214 00:03:15 @agent_ppo2.py:185][0m |          -0.0170 |          74.1879 |          12.3471 |
[32m[20221214 00:03:16 @agent_ppo2.py:185][0m |          -0.0176 |          74.0396 |          12.3239 |
[32m[20221214 00:03:16 @agent_ppo2.py:185][0m |          -0.0177 |          73.8061 |          12.3514 |
[32m[20221214 00:03:16 @agent_ppo2.py:185][0m |          -0.0181 |          73.5085 |          12.3491 |
[32m[20221214 00:03:16 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:03:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 520.42
[32m[20221214 00:03:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 538.43
[32m[20221214 00:03:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 535.53
[32m[20221214 00:03:16 @agent_ppo2.py:143][0m Total time:       5.76 min
[32m[20221214 00:03:16 @agent_ppo2.py:145][0m 524288 total steps have happened
[32m[20221214 00:03:16 @agent_ppo2.py:121][0m #------------------------ Iteration 4256 --------------------------#
[32m[20221214 00:03:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:03:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:03:16 @agent_ppo2.py:185][0m |           0.0015 |          78.1471 |          12.6734 |
[32m[20221214 00:03:16 @agent_ppo2.py:185][0m |          -0.0059 |          73.0461 |          12.6631 |
[32m[20221214 00:03:16 @agent_ppo2.py:185][0m |          -0.0071 |          71.2840 |          12.6784 |
[32m[20221214 00:03:16 @agent_ppo2.py:185][0m |          -0.0061 |          70.2653 |          12.6624 |
[32m[20221214 00:03:17 @agent_ppo2.py:185][0m |          -0.0070 |          69.1682 |          12.6887 |
[32m[20221214 00:03:17 @agent_ppo2.py:185][0m |           0.0010 |          71.3611 |          12.6674 |
[32m[20221214 00:03:17 @agent_ppo2.py:185][0m |          -0.0092 |          68.4609 |          12.6902 |
[32m[20221214 00:03:17 @agent_ppo2.py:185][0m |           0.0108 |          81.0992 |          12.6622 |
[32m[20221214 00:03:17 @agent_ppo2.py:185][0m |          -0.0113 |          67.9322 |          12.6629 |
[32m[20221214 00:03:17 @agent_ppo2.py:185][0m |          -0.0039 |          71.8329 |          12.6729 |
[32m[20221214 00:03:17 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:03:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 487.06
[32m[20221214 00:03:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 535.74
[32m[20221214 00:03:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 523.95
[32m[20221214 00:03:17 @agent_ppo2.py:143][0m Total time:       5.78 min
[32m[20221214 00:03:17 @agent_ppo2.py:145][0m 526336 total steps have happened
[32m[20221214 00:03:17 @agent_ppo2.py:121][0m #------------------------ Iteration 4257 --------------------------#
[32m[20221214 00:03:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:03:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:03:18 @agent_ppo2.py:185][0m |          -0.0018 |          53.4452 |          12.3866 |
[32m[20221214 00:03:18 @agent_ppo2.py:185][0m |          -0.0011 |          41.7399 |          12.4094 |
[32m[20221214 00:03:18 @agent_ppo2.py:185][0m |          -0.0105 |          38.5018 |          12.4435 |
[32m[20221214 00:03:18 @agent_ppo2.py:185][0m |           0.0017 |          39.6203 |          12.4435 |
[32m[20221214 00:03:18 @agent_ppo2.py:185][0m |          -0.0095 |          35.8388 |          12.4249 |
[32m[20221214 00:03:18 @agent_ppo2.py:185][0m |          -0.0134 |          34.8694 |          12.4346 |
[32m[20221214 00:03:18 @agent_ppo2.py:185][0m |          -0.0147 |          33.8835 |          12.4159 |
[32m[20221214 00:03:18 @agent_ppo2.py:185][0m |          -0.0129 |          33.3144 |          12.4144 |
[32m[20221214 00:03:18 @agent_ppo2.py:185][0m |          -0.0141 |          32.6465 |          12.4067 |
[32m[20221214 00:03:18 @agent_ppo2.py:185][0m |          -0.0131 |          31.8947 |          12.3968 |
[32m[20221214 00:03:18 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:03:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 457.87
[32m[20221214 00:03:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 535.92
[32m[20221214 00:03:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 448.47
[32m[20221214 00:03:18 @agent_ppo2.py:143][0m Total time:       5.80 min
[32m[20221214 00:03:18 @agent_ppo2.py:145][0m 528384 total steps have happened
[32m[20221214 00:03:18 @agent_ppo2.py:121][0m #------------------------ Iteration 4258 --------------------------#
[32m[20221214 00:03:19 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:03:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:03:19 @agent_ppo2.py:185][0m |           0.0014 |          77.9023 |          12.7014 |
[32m[20221214 00:03:19 @agent_ppo2.py:185][0m |          -0.0087 |          70.5572 |          12.7299 |
[32m[20221214 00:03:19 @agent_ppo2.py:185][0m |          -0.0021 |          73.5614 |          12.7204 |
[32m[20221214 00:03:19 @agent_ppo2.py:185][0m |          -0.0140 |          68.2301 |          12.7202 |
[32m[20221214 00:03:19 @agent_ppo2.py:185][0m |          -0.0156 |          67.5962 |          12.6708 |
[32m[20221214 00:03:19 @agent_ppo2.py:185][0m |          -0.0193 |          67.2044 |          12.6979 |
[32m[20221214 00:03:19 @agent_ppo2.py:185][0m |          -0.0150 |          66.7402 |          12.6790 |
[32m[20221214 00:03:19 @agent_ppo2.py:185][0m |          -0.0162 |          66.6927 |          12.6400 |
[32m[20221214 00:03:19 @agent_ppo2.py:185][0m |          -0.0205 |          66.3721 |          12.6318 |
[32m[20221214 00:03:20 @agent_ppo2.py:185][0m |          -0.0139 |          70.7589 |          12.6283 |
[32m[20221214 00:03:20 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 00:03:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 480.49
[32m[20221214 00:03:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 540.85
[32m[20221214 00:03:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 522.60
[32m[20221214 00:03:20 @agent_ppo2.py:143][0m Total time:       5.82 min
[32m[20221214 00:03:20 @agent_ppo2.py:145][0m 530432 total steps have happened
[32m[20221214 00:03:20 @agent_ppo2.py:121][0m #------------------------ Iteration 4259 --------------------------#
[32m[20221214 00:03:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:03:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:03:20 @agent_ppo2.py:185][0m |           0.0039 |          58.1001 |          12.1662 |
[32m[20221214 00:03:20 @agent_ppo2.py:185][0m |           0.0017 |          51.1753 |          12.1619 |
[32m[20221214 00:03:20 @agent_ppo2.py:185][0m |          -0.0053 |          43.5576 |          12.1735 |
[32m[20221214 00:03:20 @agent_ppo2.py:185][0m |          -0.0097 |          42.2927 |          12.1626 |
[32m[20221214 00:03:20 @agent_ppo2.py:185][0m |          -0.0082 |          41.6985 |          12.1777 |
[32m[20221214 00:03:20 @agent_ppo2.py:185][0m |          -0.0111 |          41.1287 |          12.1776 |
[32m[20221214 00:03:20 @agent_ppo2.py:185][0m |          -0.0133 |          40.6767 |          12.1887 |
[32m[20221214 00:03:21 @agent_ppo2.py:185][0m |          -0.0069 |          43.7378 |          12.1914 |
[32m[20221214 00:03:21 @agent_ppo2.py:185][0m |          -0.0155 |          40.6649 |          12.2069 |
[32m[20221214 00:03:21 @agent_ppo2.py:185][0m |          -0.0164 |          40.0142 |          12.1969 |
[32m[20221214 00:03:21 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:03:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 491.81
[32m[20221214 00:03:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 507.90
[32m[20221214 00:03:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 410.49
[32m[20221214 00:03:21 @agent_ppo2.py:143][0m Total time:       5.84 min
[32m[20221214 00:03:21 @agent_ppo2.py:145][0m 532480 total steps have happened
[32m[20221214 00:03:21 @agent_ppo2.py:121][0m #------------------------ Iteration 4260 --------------------------#
[32m[20221214 00:03:21 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:03:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:03:21 @agent_ppo2.py:185][0m |           0.0024 |          33.7438 |          12.1017 |
[32m[20221214 00:03:21 @agent_ppo2.py:185][0m |          -0.0116 |          27.6654 |          12.1290 |
[32m[20221214 00:03:21 @agent_ppo2.py:185][0m |          -0.0053 |          25.5539 |          12.1735 |
[32m[20221214 00:03:22 @agent_ppo2.py:185][0m |          -0.0169 |          24.1850 |          12.1233 |
[32m[20221214 00:03:22 @agent_ppo2.py:185][0m |          -0.0095 |          23.4402 |          12.1516 |
[32m[20221214 00:03:22 @agent_ppo2.py:185][0m |          -0.0084 |          24.4449 |          12.1232 |
[32m[20221214 00:03:22 @agent_ppo2.py:185][0m |          -0.0196 |          22.0865 |          12.1177 |
[32m[20221214 00:03:22 @agent_ppo2.py:185][0m |          -0.0158 |          21.5737 |          12.1248 |
[32m[20221214 00:03:22 @agent_ppo2.py:185][0m |          -0.0218 |          21.3136 |          12.1167 |
[32m[20221214 00:03:22 @agent_ppo2.py:185][0m |          -0.0169 |          21.0960 |          12.0940 |
[32m[20221214 00:03:22 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:03:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 415.83
[32m[20221214 00:03:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 499.30
[32m[20221214 00:03:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 517.06
[32m[20221214 00:03:22 @agent_ppo2.py:143][0m Total time:       5.86 min
[32m[20221214 00:03:22 @agent_ppo2.py:145][0m 534528 total steps have happened
[32m[20221214 00:03:22 @agent_ppo2.py:121][0m #------------------------ Iteration 4261 --------------------------#
[32m[20221214 00:03:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:03:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:03:22 @agent_ppo2.py:185][0m |           0.0005 |          71.8904 |          12.3332 |
[32m[20221214 00:03:23 @agent_ppo2.py:185][0m |          -0.0056 |          67.6015 |          12.3658 |
[32m[20221214 00:03:23 @agent_ppo2.py:185][0m |          -0.0038 |          66.2237 |          12.3563 |
[32m[20221214 00:03:23 @agent_ppo2.py:185][0m |          -0.0105 |          65.7852 |          12.3698 |
[32m[20221214 00:03:23 @agent_ppo2.py:185][0m |          -0.0103 |          65.1550 |          12.3631 |
[32m[20221214 00:03:23 @agent_ppo2.py:185][0m |          -0.0106 |          64.9635 |          12.3838 |
[32m[20221214 00:03:23 @agent_ppo2.py:185][0m |          -0.0069 |          66.1397 |          12.3880 |
[32m[20221214 00:03:23 @agent_ppo2.py:185][0m |          -0.0160 |          64.6662 |          12.4140 |
[32m[20221214 00:03:23 @agent_ppo2.py:185][0m |          -0.0121 |          64.6755 |          12.4091 |
[32m[20221214 00:03:23 @agent_ppo2.py:185][0m |          -0.0081 |          65.0129 |          12.4294 |
[32m[20221214 00:03:23 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:03:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 516.63
[32m[20221214 00:03:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 537.69
[32m[20221214 00:03:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 373.31
[32m[20221214 00:03:23 @agent_ppo2.py:143][0m Total time:       5.88 min
[32m[20221214 00:03:23 @agent_ppo2.py:145][0m 536576 total steps have happened
[32m[20221214 00:03:23 @agent_ppo2.py:121][0m #------------------------ Iteration 4262 --------------------------#
[32m[20221214 00:03:24 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:03:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:03:24 @agent_ppo2.py:185][0m |           0.0012 |          57.2714 |          12.2138 |
[32m[20221214 00:03:24 @agent_ppo2.py:185][0m |          -0.0044 |          53.1969 |          12.2069 |
[32m[20221214 00:03:24 @agent_ppo2.py:185][0m |          -0.0153 |          51.8615 |          12.2023 |
[32m[20221214 00:03:24 @agent_ppo2.py:185][0m |          -0.0113 |          50.9836 |          12.2000 |
[32m[20221214 00:03:24 @agent_ppo2.py:185][0m |          -0.0163 |          50.3761 |          12.2019 |
[32m[20221214 00:03:24 @agent_ppo2.py:185][0m |          -0.0123 |          50.0398 |          12.2138 |
[32m[20221214 00:03:24 @agent_ppo2.py:185][0m |          -0.0147 |          49.6545 |          12.1694 |
[32m[20221214 00:03:24 @agent_ppo2.py:185][0m |          -0.0167 |          49.3034 |          12.1858 |
[32m[20221214 00:03:24 @agent_ppo2.py:185][0m |          -0.0102 |          48.9201 |          12.1807 |
[32m[20221214 00:03:25 @agent_ppo2.py:185][0m |          -0.0180 |          48.6879 |          12.1923 |
[32m[20221214 00:03:25 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:03:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 382.61
[32m[20221214 00:03:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 482.22
[32m[20221214 00:03:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 523.10
[32m[20221214 00:03:25 @agent_ppo2.py:143][0m Total time:       5.91 min
[32m[20221214 00:03:25 @agent_ppo2.py:145][0m 538624 total steps have happened
[32m[20221214 00:03:25 @agent_ppo2.py:121][0m #------------------------ Iteration 4263 --------------------------#
[32m[20221214 00:03:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:03:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:03:25 @agent_ppo2.py:185][0m |          -0.0003 |          75.8801 |          12.3520 |
[32m[20221214 00:03:25 @agent_ppo2.py:185][0m |          -0.0007 |          73.8051 |          12.3450 |
[32m[20221214 00:03:25 @agent_ppo2.py:185][0m |          -0.0001 |          73.0429 |          12.3291 |
[32m[20221214 00:03:25 @agent_ppo2.py:185][0m |          -0.0071 |          70.1128 |          12.3287 |
[32m[20221214 00:03:25 @agent_ppo2.py:185][0m |          -0.0106 |          69.3060 |          12.2998 |
[32m[20221214 00:03:26 @agent_ppo2.py:185][0m |          -0.0115 |          68.6618 |          12.2781 |
[32m[20221214 00:03:26 @agent_ppo2.py:185][0m |          -0.0130 |          68.2571 |          12.2721 |
[32m[20221214 00:03:26 @agent_ppo2.py:185][0m |          -0.0110 |          67.8215 |          12.2311 |
[32m[20221214 00:03:26 @agent_ppo2.py:185][0m |          -0.0149 |          67.4878 |          12.2279 |
[32m[20221214 00:03:26 @agent_ppo2.py:185][0m |          -0.0126 |          67.1123 |          12.2316 |
[32m[20221214 00:03:26 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:03:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 493.87
[32m[20221214 00:03:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 519.05
[32m[20221214 00:03:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 510.46
[32m[20221214 00:03:26 @agent_ppo2.py:143][0m Total time:       5.93 min
[32m[20221214 00:03:26 @agent_ppo2.py:145][0m 540672 total steps have happened
[32m[20221214 00:03:26 @agent_ppo2.py:121][0m #------------------------ Iteration 4264 --------------------------#
[32m[20221214 00:03:26 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:03:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:03:26 @agent_ppo2.py:185][0m |           0.0008 |          71.1870 |          12.3746 |
[32m[20221214 00:03:27 @agent_ppo2.py:185][0m |           0.0076 |          68.1851 |          12.3928 |
[32m[20221214 00:03:27 @agent_ppo2.py:185][0m |          -0.0019 |          65.8599 |          12.3364 |
[32m[20221214 00:03:27 @agent_ppo2.py:185][0m |          -0.0111 |          63.7815 |          12.3514 |
[32m[20221214 00:03:27 @agent_ppo2.py:185][0m |          -0.0088 |          64.0575 |          12.3417 |
[32m[20221214 00:03:27 @agent_ppo2.py:185][0m |          -0.0012 |          66.5679 |          12.3740 |
[32m[20221214 00:03:27 @agent_ppo2.py:185][0m |          -0.0131 |          62.6616 |          12.3260 |
[32m[20221214 00:03:27 @agent_ppo2.py:185][0m |          -0.0138 |          62.0859 |          12.3365 |
[32m[20221214 00:03:27 @agent_ppo2.py:185][0m |          -0.0160 |          61.7764 |          12.3737 |
[32m[20221214 00:03:27 @agent_ppo2.py:185][0m |          -0.0005 |          69.7012 |          12.3419 |
[32m[20221214 00:03:27 @agent_ppo2.py:130][0m Policy update time: 1.19 s
[32m[20221214 00:03:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 494.30
[32m[20221214 00:03:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 538.45
[32m[20221214 00:03:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 348.29
[32m[20221214 00:03:28 @agent_ppo2.py:143][0m Total time:       5.95 min
[32m[20221214 00:03:28 @agent_ppo2.py:145][0m 542720 total steps have happened
[32m[20221214 00:03:28 @agent_ppo2.py:121][0m #------------------------ Iteration 4265 --------------------------#
[32m[20221214 00:03:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:03:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:03:28 @agent_ppo2.py:185][0m |           0.0023 |          72.4628 |          12.0616 |
[32m[20221214 00:03:28 @agent_ppo2.py:185][0m |           0.0077 |          74.2494 |          12.1170 |
[32m[20221214 00:03:28 @agent_ppo2.py:185][0m |          -0.0065 |          68.4566 |          12.0715 |
[32m[20221214 00:03:28 @agent_ppo2.py:185][0m |          -0.0060 |          67.5540 |          12.0966 |
[32m[20221214 00:03:28 @agent_ppo2.py:185][0m |          -0.0122 |          66.6836 |          12.0959 |
[32m[20221214 00:03:28 @agent_ppo2.py:185][0m |          -0.0060 |          65.9334 |          12.1032 |
[32m[20221214 00:03:28 @agent_ppo2.py:185][0m |          -0.0118 |          65.4637 |          12.1302 |
[32m[20221214 00:03:29 @agent_ppo2.py:185][0m |          -0.0108 |          65.1185 |          12.1239 |
[32m[20221214 00:03:29 @agent_ppo2.py:185][0m |          -0.0113 |          65.0881 |          12.1113 |
[32m[20221214 00:03:29 @agent_ppo2.py:185][0m |          -0.0125 |          64.7824 |          12.1331 |
[32m[20221214 00:03:29 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:03:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 482.43
[32m[20221214 00:03:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 517.70
[32m[20221214 00:03:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 395.31
[32m[20221214 00:03:29 @agent_ppo2.py:143][0m Total time:       5.98 min
[32m[20221214 00:03:29 @agent_ppo2.py:145][0m 544768 total steps have happened
[32m[20221214 00:03:29 @agent_ppo2.py:121][0m #------------------------ Iteration 4266 --------------------------#
[32m[20221214 00:03:29 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:03:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:03:29 @agent_ppo2.py:185][0m |          -0.0011 |          44.6388 |          12.1345 |
[32m[20221214 00:03:29 @agent_ppo2.py:185][0m |          -0.0109 |          35.2757 |          12.1292 |
[32m[20221214 00:03:29 @agent_ppo2.py:185][0m |          -0.0068 |          33.6930 |          12.1436 |
[32m[20221214 00:03:30 @agent_ppo2.py:185][0m |          -0.0160 |          32.9118 |          12.1018 |
[32m[20221214 00:03:30 @agent_ppo2.py:185][0m |          -0.0078 |          32.3469 |          12.0963 |
[32m[20221214 00:03:30 @agent_ppo2.py:185][0m |          -0.0140 |          32.1220 |          12.1338 |
[32m[20221214 00:03:30 @agent_ppo2.py:185][0m |          -0.0217 |          31.6886 |          12.0483 |
[32m[20221214 00:03:30 @agent_ppo2.py:185][0m |          -0.0135 |          31.5777 |          12.1030 |
[32m[20221214 00:03:30 @agent_ppo2.py:185][0m |          -0.0199 |          31.2093 |          12.0984 |
[32m[20221214 00:03:30 @agent_ppo2.py:185][0m |          -0.0196 |          31.1683 |          12.0852 |
[32m[20221214 00:03:30 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:03:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 379.18
[32m[20221214 00:03:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 410.15
[32m[20221214 00:03:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 466.59
[32m[20221214 00:03:30 @agent_ppo2.py:143][0m Total time:       6.00 min
[32m[20221214 00:03:30 @agent_ppo2.py:145][0m 546816 total steps have happened
[32m[20221214 00:03:30 @agent_ppo2.py:121][0m #------------------------ Iteration 4267 --------------------------#
[32m[20221214 00:03:30 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:03:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:03:31 @agent_ppo2.py:185][0m |           0.0029 |          71.2274 |          12.2641 |
[32m[20221214 00:03:31 @agent_ppo2.py:185][0m |          -0.0095 |          68.5738 |          12.2395 |
[32m[20221214 00:03:31 @agent_ppo2.py:185][0m |          -0.0090 |          67.9672 |          12.2481 |
[32m[20221214 00:03:31 @agent_ppo2.py:185][0m |          -0.0141 |          67.1698 |          12.3054 |
[32m[20221214 00:03:31 @agent_ppo2.py:185][0m |          -0.0112 |          66.5692 |          12.2794 |
[32m[20221214 00:03:31 @agent_ppo2.py:185][0m |          -0.0101 |          66.2998 |          12.2707 |
[32m[20221214 00:03:31 @agent_ppo2.py:185][0m |          -0.0135 |          65.6194 |          12.2649 |
[32m[20221214 00:03:31 @agent_ppo2.py:185][0m |          -0.0145 |          65.4546 |          12.2650 |
[32m[20221214 00:03:31 @agent_ppo2.py:185][0m |          -0.0167 |          65.2055 |          12.2939 |
[32m[20221214 00:03:31 @agent_ppo2.py:185][0m |          -0.0152 |          64.7819 |          12.2823 |
[32m[20221214 00:03:31 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:03:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 503.16
[32m[20221214 00:03:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 511.50
[32m[20221214 00:03:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 517.47
[32m[20221214 00:03:31 @agent_ppo2.py:143][0m Total time:       6.02 min
[32m[20221214 00:03:31 @agent_ppo2.py:145][0m 548864 total steps have happened
[32m[20221214 00:03:31 @agent_ppo2.py:121][0m #------------------------ Iteration 4268 --------------------------#
[32m[20221214 00:03:32 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:03:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:03:32 @agent_ppo2.py:185][0m |          -0.0011 |          59.7096 |          12.4088 |
[32m[20221214 00:03:32 @agent_ppo2.py:185][0m |          -0.0017 |          54.7525 |          12.4569 |
[32m[20221214 00:03:32 @agent_ppo2.py:185][0m |           0.0097 |          61.3897 |          12.4447 |
[32m[20221214 00:03:32 @agent_ppo2.py:185][0m |          -0.0106 |          52.3354 |          12.4662 |
[32m[20221214 00:03:32 @agent_ppo2.py:185][0m |          -0.0072 |          51.6685 |          12.4552 |
[32m[20221214 00:03:32 @agent_ppo2.py:185][0m |          -0.0154 |          50.9211 |          12.4809 |
[32m[20221214 00:03:32 @agent_ppo2.py:185][0m |          -0.0150 |          50.5003 |          12.4482 |
[32m[20221214 00:03:33 @agent_ppo2.py:185][0m |          -0.0112 |          50.8249 |          12.4657 |
[32m[20221214 00:03:33 @agent_ppo2.py:185][0m |          -0.0157 |          49.8170 |          12.4951 |
[32m[20221214 00:03:33 @agent_ppo2.py:185][0m |          -0.0088 |          52.4583 |          12.5246 |
[32m[20221214 00:03:33 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:03:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 478.24
[32m[20221214 00:03:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 529.91
[32m[20221214 00:03:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 437.84
[32m[20221214 00:03:33 @agent_ppo2.py:143][0m Total time:       6.04 min
[32m[20221214 00:03:33 @agent_ppo2.py:145][0m 550912 total steps have happened
[32m[20221214 00:03:33 @agent_ppo2.py:121][0m #------------------------ Iteration 4269 --------------------------#
[32m[20221214 00:03:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:03:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:03:33 @agent_ppo2.py:185][0m |           0.0016 |          55.7861 |          12.1073 |
[32m[20221214 00:03:33 @agent_ppo2.py:185][0m |          -0.0105 |          50.1165 |          12.0576 |
[32m[20221214 00:03:33 @agent_ppo2.py:185][0m |          -0.0059 |          48.5802 |          12.1124 |
[32m[20221214 00:03:34 @agent_ppo2.py:185][0m |          -0.0077 |          47.7233 |          12.0765 |
[32m[20221214 00:03:34 @agent_ppo2.py:185][0m |          -0.0131 |          46.8377 |          12.0836 |
[32m[20221214 00:03:34 @agent_ppo2.py:185][0m |          -0.0094 |          46.5062 |          12.0524 |
[32m[20221214 00:03:34 @agent_ppo2.py:185][0m |          -0.0128 |          45.5809 |          12.0575 |
[32m[20221214 00:03:34 @agent_ppo2.py:185][0m |          -0.0075 |          46.2454 |          12.0897 |
[32m[20221214 00:03:34 @agent_ppo2.py:185][0m |          -0.0168 |          44.8293 |          12.0597 |
[32m[20221214 00:03:34 @agent_ppo2.py:185][0m |          -0.0106 |          44.9414 |          12.0421 |
[32m[20221214 00:03:34 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:03:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 417.64
[32m[20221214 00:03:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 438.84
[32m[20221214 00:03:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 405.26
[32m[20221214 00:03:34 @agent_ppo2.py:143][0m Total time:       6.06 min
[32m[20221214 00:03:34 @agent_ppo2.py:145][0m 552960 total steps have happened
[32m[20221214 00:03:34 @agent_ppo2.py:121][0m #------------------------ Iteration 4270 --------------------------#
[32m[20221214 00:03:34 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221214 00:03:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:03:35 @agent_ppo2.py:185][0m |           0.0089 |          50.9175 |          12.0151 |
[32m[20221214 00:03:35 @agent_ppo2.py:185][0m |          -0.0049 |          42.6260 |          12.0470 |
[32m[20221214 00:03:35 @agent_ppo2.py:185][0m |          -0.0022 |          41.2764 |          12.0001 |
[32m[20221214 00:03:35 @agent_ppo2.py:185][0m |          -0.0084 |          40.6166 |          12.0559 |
[32m[20221214 00:03:35 @agent_ppo2.py:185][0m |          -0.0136 |          40.0131 |          12.0294 |
[32m[20221214 00:03:35 @agent_ppo2.py:185][0m |          -0.0133 |          39.5237 |          12.0525 |
[32m[20221214 00:03:35 @agent_ppo2.py:185][0m |          -0.0132 |          39.3248 |          12.0754 |
[32m[20221214 00:03:35 @agent_ppo2.py:185][0m |          -0.0136 |          39.0284 |          12.0427 |
[32m[20221214 00:03:35 @agent_ppo2.py:185][0m |          -0.0106 |          38.8167 |          12.0637 |
[32m[20221214 00:03:36 @agent_ppo2.py:185][0m |          -0.0181 |          38.6649 |          12.0574 |
[32m[20221214 00:03:36 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:03:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 348.09
[32m[20221214 00:03:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 399.55
[32m[20221214 00:03:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 258.96
[32m[20221214 00:03:36 @agent_ppo2.py:143][0m Total time:       6.09 min
[32m[20221214 00:03:36 @agent_ppo2.py:145][0m 555008 total steps have happened
[32m[20221214 00:03:36 @agent_ppo2.py:121][0m #------------------------ Iteration 4271 --------------------------#
[32m[20221214 00:03:36 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:03:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:03:36 @agent_ppo2.py:185][0m |          -0.0027 |          52.8974 |          12.5304 |
[32m[20221214 00:03:36 @agent_ppo2.py:185][0m |          -0.0037 |          48.4548 |          12.4918 |
[32m[20221214 00:03:36 @agent_ppo2.py:185][0m |          -0.0053 |          47.4826 |          12.4992 |
[32m[20221214 00:03:36 @agent_ppo2.py:185][0m |          -0.0093 |          47.0516 |          12.5138 |
[32m[20221214 00:03:36 @agent_ppo2.py:185][0m |           0.0014 |          51.8614 |          12.5246 |
[32m[20221214 00:03:37 @agent_ppo2.py:185][0m |          -0.0091 |          46.6230 |          12.5380 |
[32m[20221214 00:03:37 @agent_ppo2.py:185][0m |           0.0037 |          52.6524 |          12.5291 |
[32m[20221214 00:03:37 @agent_ppo2.py:185][0m |          -0.0086 |          46.6436 |          12.5438 |
[32m[20221214 00:03:37 @agent_ppo2.py:185][0m |          -0.0153 |          45.9652 |          12.5548 |
[32m[20221214 00:03:37 @agent_ppo2.py:185][0m |          -0.0136 |          45.8609 |          12.5600 |
[32m[20221214 00:03:37 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:03:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 407.22
[32m[20221214 00:03:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 446.04
[32m[20221214 00:03:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 464.36
[32m[20221214 00:03:37 @agent_ppo2.py:143][0m Total time:       6.11 min
[32m[20221214 00:03:37 @agent_ppo2.py:145][0m 557056 total steps have happened
[32m[20221214 00:03:37 @agent_ppo2.py:121][0m #------------------------ Iteration 4272 --------------------------#
[32m[20221214 00:03:37 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:03:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:03:37 @agent_ppo2.py:185][0m |           0.0035 |          70.8671 |          11.9944 |
[32m[20221214 00:03:38 @agent_ppo2.py:185][0m |          -0.0069 |          68.1983 |          11.9700 |
[32m[20221214 00:03:38 @agent_ppo2.py:185][0m |          -0.0060 |          67.6253 |          12.0382 |
[32m[20221214 00:03:38 @agent_ppo2.py:185][0m |          -0.0136 |          67.1280 |          11.9939 |
[32m[20221214 00:03:38 @agent_ppo2.py:185][0m |          -0.0137 |          66.9788 |          12.0095 |
[32m[20221214 00:03:38 @agent_ppo2.py:185][0m |          -0.0121 |          66.6337 |          11.9954 |
[32m[20221214 00:03:38 @agent_ppo2.py:185][0m |           0.0016 |          70.1319 |          12.0100 |
[32m[20221214 00:03:38 @agent_ppo2.py:185][0m |          -0.0083 |          69.4547 |          12.0269 |
[32m[20221214 00:03:38 @agent_ppo2.py:185][0m |          -0.0053 |          68.4786 |          12.0289 |
[32m[20221214 00:03:38 @agent_ppo2.py:185][0m |          -0.0193 |          66.3238 |          12.0126 |
[32m[20221214 00:03:38 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:03:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 517.01
[32m[20221214 00:03:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 544.95
[32m[20221214 00:03:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 409.36
[32m[20221214 00:03:38 @agent_ppo2.py:143][0m Total time:       6.13 min
[32m[20221214 00:03:38 @agent_ppo2.py:145][0m 559104 total steps have happened
[32m[20221214 00:03:38 @agent_ppo2.py:121][0m #------------------------ Iteration 4273 --------------------------#
[32m[20221214 00:03:39 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:03:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:03:39 @agent_ppo2.py:185][0m |          -0.0002 |          56.3435 |          12.3465 |
[32m[20221214 00:03:39 @agent_ppo2.py:185][0m |          -0.0087 |          54.6960 |          12.3556 |
[32m[20221214 00:03:39 @agent_ppo2.py:185][0m |          -0.0078 |          54.0976 |          12.3171 |
[32m[20221214 00:03:39 @agent_ppo2.py:185][0m |          -0.0085 |          53.6451 |          12.3042 |
[32m[20221214 00:03:39 @agent_ppo2.py:185][0m |          -0.0097 |          53.4012 |          12.3046 |
[32m[20221214 00:03:39 @agent_ppo2.py:185][0m |          -0.0087 |          53.3531 |          12.3362 |
[32m[20221214 00:03:39 @agent_ppo2.py:185][0m |          -0.0086 |          53.1608 |          12.2944 |
[32m[20221214 00:03:39 @agent_ppo2.py:185][0m |          -0.0094 |          52.8158 |          12.2834 |
[32m[20221214 00:03:40 @agent_ppo2.py:185][0m |          -0.0117 |          52.8129 |          12.2985 |
[32m[20221214 00:03:40 @agent_ppo2.py:185][0m |          -0.0141 |          52.6220 |          12.2911 |
[32m[20221214 00:03:40 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:03:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 513.23
[32m[20221214 00:03:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 536.25
[32m[20221214 00:03:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 441.30
[32m[20221214 00:03:40 @agent_ppo2.py:143][0m Total time:       6.16 min
[32m[20221214 00:03:40 @agent_ppo2.py:145][0m 561152 total steps have happened
[32m[20221214 00:03:40 @agent_ppo2.py:121][0m #------------------------ Iteration 4274 --------------------------#
[32m[20221214 00:03:40 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:03:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:03:40 @agent_ppo2.py:185][0m |          -0.0028 |          54.8235 |          12.4063 |
[32m[20221214 00:03:40 @agent_ppo2.py:185][0m |          -0.0065 |          50.7409 |          12.3835 |
[32m[20221214 00:03:40 @agent_ppo2.py:185][0m |          -0.0094 |          49.3744 |          12.3612 |
[32m[20221214 00:03:40 @agent_ppo2.py:185][0m |          -0.0116 |          48.9369 |          12.3437 |
[32m[20221214 00:03:41 @agent_ppo2.py:185][0m |          -0.0126 |          48.5694 |          12.3730 |
[32m[20221214 00:03:41 @agent_ppo2.py:185][0m |          -0.0145 |          48.3754 |          12.3586 |
[32m[20221214 00:03:41 @agent_ppo2.py:185][0m |          -0.0192 |          47.9058 |          12.3200 |
[32m[20221214 00:03:41 @agent_ppo2.py:185][0m |          -0.0150 |          47.7146 |          12.3314 |
[32m[20221214 00:03:41 @agent_ppo2.py:185][0m |          -0.0118 |          47.3694 |          12.3457 |
[32m[20221214 00:03:41 @agent_ppo2.py:185][0m |          -0.0148 |          47.1859 |          12.3205 |
[32m[20221214 00:03:41 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:03:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 407.59
[32m[20221214 00:03:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 433.92
[32m[20221214 00:03:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 584.88
[32m[20221214 00:03:41 @agent_ppo2.py:143][0m Total time:       6.18 min
[32m[20221214 00:03:41 @agent_ppo2.py:145][0m 563200 total steps have happened
[32m[20221214 00:03:41 @agent_ppo2.py:121][0m #------------------------ Iteration 4275 --------------------------#
[32m[20221214 00:03:41 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:03:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:03:42 @agent_ppo2.py:185][0m |           0.0014 |          42.3955 |          12.0805 |
[32m[20221214 00:03:42 @agent_ppo2.py:185][0m |          -0.0041 |          36.7618 |          12.1130 |
[32m[20221214 00:03:42 @agent_ppo2.py:185][0m |          -0.0094 |          34.8486 |          12.0985 |
[32m[20221214 00:03:42 @agent_ppo2.py:185][0m |          -0.0114 |          33.3307 |          12.1400 |
[32m[20221214 00:03:42 @agent_ppo2.py:185][0m |          -0.0133 |          31.9494 |          12.1530 |
[32m[20221214 00:03:42 @agent_ppo2.py:185][0m |          -0.0082 |          30.9858 |          12.1365 |
[32m[20221214 00:03:42 @agent_ppo2.py:185][0m |          -0.0146 |          30.2362 |          12.1653 |
[32m[20221214 00:03:42 @agent_ppo2.py:185][0m |          -0.0183 |          29.5219 |          12.1750 |
[32m[20221214 00:03:42 @agent_ppo2.py:185][0m |          -0.0144 |          29.2663 |          12.2010 |
[32m[20221214 00:03:42 @agent_ppo2.py:185][0m |          -0.0121 |          28.7043 |          12.1752 |
[32m[20221214 00:03:42 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:03:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 348.74
[32m[20221214 00:03:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 452.07
[32m[20221214 00:03:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 385.11
[32m[20221214 00:03:43 @agent_ppo2.py:143][0m Total time:       6.20 min
[32m[20221214 00:03:43 @agent_ppo2.py:145][0m 565248 total steps have happened
[32m[20221214 00:03:43 @agent_ppo2.py:121][0m #------------------------ Iteration 4276 --------------------------#
[32m[20221214 00:03:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:03:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:03:43 @agent_ppo2.py:185][0m |           0.0054 |          53.0581 |          12.3987 |
[32m[20221214 00:03:43 @agent_ppo2.py:185][0m |          -0.0060 |          48.1184 |          12.3674 |
[32m[20221214 00:03:43 @agent_ppo2.py:185][0m |          -0.0028 |          49.2734 |          12.3315 |
[32m[20221214 00:03:43 @agent_ppo2.py:185][0m |          -0.0082 |          46.5421 |          12.3561 |
[32m[20221214 00:03:43 @agent_ppo2.py:185][0m |          -0.0106 |          46.1814 |          12.3599 |
[32m[20221214 00:03:43 @agent_ppo2.py:185][0m |          -0.0141 |          45.7880 |          12.2934 |
[32m[20221214 00:03:43 @agent_ppo2.py:185][0m |          -0.0115 |          46.3343 |          12.3204 |
[32m[20221214 00:03:43 @agent_ppo2.py:185][0m |          -0.0195 |          45.4509 |          12.3084 |
[32m[20221214 00:03:44 @agent_ppo2.py:185][0m |          -0.0164 |          45.2040 |          12.3161 |
[32m[20221214 00:03:44 @agent_ppo2.py:185][0m |          -0.0147 |          45.0959 |          12.2878 |
[32m[20221214 00:03:44 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:03:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 390.50
[32m[20221214 00:03:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 438.47
[32m[20221214 00:03:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 473.04
[32m[20221214 00:03:44 @agent_ppo2.py:143][0m Total time:       6.22 min
[32m[20221214 00:03:44 @agent_ppo2.py:145][0m 567296 total steps have happened
[32m[20221214 00:03:44 @agent_ppo2.py:121][0m #------------------------ Iteration 4277 --------------------------#
[32m[20221214 00:03:44 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:03:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:03:44 @agent_ppo2.py:185][0m |          -0.0027 |          67.6574 |          12.0910 |
[32m[20221214 00:03:44 @agent_ppo2.py:185][0m |          -0.0076 |          62.2044 |          12.0703 |
[32m[20221214 00:03:44 @agent_ppo2.py:185][0m |          -0.0089 |          60.0061 |          12.0998 |
[32m[20221214 00:03:44 @agent_ppo2.py:185][0m |          -0.0113 |          58.9010 |          12.0612 |
[32m[20221214 00:03:45 @agent_ppo2.py:185][0m |           0.0031 |          62.4955 |          12.0710 |
[32m[20221214 00:03:45 @agent_ppo2.py:185][0m |          -0.0080 |          57.8715 |          12.0203 |
[32m[20221214 00:03:45 @agent_ppo2.py:185][0m |          -0.0121 |          56.9332 |          12.0735 |
[32m[20221214 00:03:45 @agent_ppo2.py:185][0m |          -0.0154 |          56.6028 |          12.0611 |
[32m[20221214 00:03:45 @agent_ppo2.py:185][0m |          -0.0119 |          56.0723 |          12.0346 |
[32m[20221214 00:03:45 @agent_ppo2.py:185][0m |          -0.0134 |          55.7630 |          12.0504 |
[32m[20221214 00:03:45 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:03:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 475.38
[32m[20221214 00:03:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 530.17
[32m[20221214 00:03:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 471.24
[32m[20221214 00:03:45 @agent_ppo2.py:143][0m Total time:       6.25 min
[32m[20221214 00:03:45 @agent_ppo2.py:145][0m 569344 total steps have happened
[32m[20221214 00:03:45 @agent_ppo2.py:121][0m #------------------------ Iteration 4278 --------------------------#
[32m[20221214 00:03:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:03:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:03:45 @agent_ppo2.py:185][0m |           0.0031 |          53.3509 |          12.4652 |
[32m[20221214 00:03:46 @agent_ppo2.py:185][0m |          -0.0018 |          49.5556 |          12.4372 |
[32m[20221214 00:03:46 @agent_ppo2.py:185][0m |          -0.0097 |          48.5428 |          12.4062 |
[32m[20221214 00:03:46 @agent_ppo2.py:185][0m |          -0.0109 |          48.1492 |          12.3929 |
[32m[20221214 00:03:46 @agent_ppo2.py:185][0m |          -0.0105 |          47.5830 |          12.4009 |
[32m[20221214 00:03:46 @agent_ppo2.py:185][0m |          -0.0082 |          47.3729 |          12.4217 |
[32m[20221214 00:03:46 @agent_ppo2.py:185][0m |          -0.0103 |          47.1892 |          12.4062 |
[32m[20221214 00:03:46 @agent_ppo2.py:185][0m |          -0.0132 |          46.9228 |          12.4232 |
[32m[20221214 00:03:46 @agent_ppo2.py:185][0m |          -0.0106 |          46.7650 |          12.4204 |
[32m[20221214 00:03:46 @agent_ppo2.py:185][0m |          -0.0158 |          46.5484 |          12.4060 |
[32m[20221214 00:03:46 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:03:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 435.09
[32m[20221214 00:03:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 470.27
[32m[20221214 00:03:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 506.73
[32m[20221214 00:03:46 @agent_ppo2.py:143][0m Total time:       6.27 min
[32m[20221214 00:03:46 @agent_ppo2.py:145][0m 571392 total steps have happened
[32m[20221214 00:03:46 @agent_ppo2.py:121][0m #------------------------ Iteration 4279 --------------------------#
[32m[20221214 00:03:47 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:03:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:03:47 @agent_ppo2.py:185][0m |           0.0025 |          59.0717 |          12.3471 |
[32m[20221214 00:03:47 @agent_ppo2.py:185][0m |          -0.0030 |          55.6844 |          12.3608 |
[32m[20221214 00:03:47 @agent_ppo2.py:185][0m |          -0.0033 |          55.3216 |          12.3447 |
[32m[20221214 00:03:47 @agent_ppo2.py:185][0m |          -0.0062 |          54.8440 |          12.4066 |
[32m[20221214 00:03:47 @agent_ppo2.py:185][0m |          -0.0049 |          54.4762 |          12.4315 |
[32m[20221214 00:03:47 @agent_ppo2.py:185][0m |          -0.0054 |          54.3656 |          12.4661 |
[32m[20221214 00:03:47 @agent_ppo2.py:185][0m |          -0.0058 |          54.5904 |          12.4538 |
[32m[20221214 00:03:47 @agent_ppo2.py:185][0m |          -0.0091 |          54.2031 |          12.4686 |
[32m[20221214 00:03:47 @agent_ppo2.py:185][0m |          -0.0086 |          53.9068 |          12.4856 |
[32m[20221214 00:03:48 @agent_ppo2.py:185][0m |          -0.0114 |          53.7198 |          12.4764 |
[32m[20221214 00:03:48 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:03:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 505.66
[32m[20221214 00:03:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 530.72
[32m[20221214 00:03:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 506.20
[32m[20221214 00:03:48 @agent_ppo2.py:143][0m Total time:       6.29 min
[32m[20221214 00:03:48 @agent_ppo2.py:145][0m 573440 total steps have happened
[32m[20221214 00:03:48 @agent_ppo2.py:121][0m #------------------------ Iteration 4280 --------------------------#
[32m[20221214 00:03:48 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 00:03:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:03:48 @agent_ppo2.py:185][0m |           0.0007 |          22.4096 |          12.2126 |
[32m[20221214 00:03:48 @agent_ppo2.py:185][0m |           0.0021 |          19.5744 |          12.1951 |
[32m[20221214 00:03:48 @agent_ppo2.py:185][0m |          -0.0036 |          19.2798 |          12.2149 |
[32m[20221214 00:03:48 @agent_ppo2.py:185][0m |          -0.0001 |          19.1417 |          12.1782 |
[32m[20221214 00:03:48 @agent_ppo2.py:185][0m |          -0.0037 |          19.0262 |          12.1507 |
[32m[20221214 00:03:48 @agent_ppo2.py:185][0m |           0.0012 |          19.0047 |          12.1844 |
[32m[20221214 00:03:49 @agent_ppo2.py:185][0m |          -0.0036 |          18.9654 |          12.1003 |
[32m[20221214 00:03:49 @agent_ppo2.py:185][0m |          -0.0046 |          19.0276 |          12.1702 |
[32m[20221214 00:03:49 @agent_ppo2.py:185][0m |          -0.0022 |          18.9679 |          12.1377 |
[32m[20221214 00:03:49 @agent_ppo2.py:185][0m |          -0.0026 |          18.8697 |          12.1183 |
[32m[20221214 00:03:49 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:03:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221214 00:03:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221214 00:03:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 516.22
[32m[20221214 00:03:49 @agent_ppo2.py:143][0m Total time:       6.31 min
[32m[20221214 00:03:49 @agent_ppo2.py:145][0m 575488 total steps have happened
[32m[20221214 00:03:49 @agent_ppo2.py:121][0m #------------------------ Iteration 4281 --------------------------#
[32m[20221214 00:03:49 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:03:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:03:49 @agent_ppo2.py:185][0m |           0.0012 |          34.1457 |          12.5400 |
[32m[20221214 00:03:49 @agent_ppo2.py:185][0m |          -0.0059 |          28.1647 |          12.5599 |
[32m[20221214 00:03:49 @agent_ppo2.py:185][0m |          -0.0022 |          25.8109 |          12.5689 |
[32m[20221214 00:03:50 @agent_ppo2.py:185][0m |          -0.0069 |          24.4695 |          12.5968 |
[32m[20221214 00:03:50 @agent_ppo2.py:185][0m |          -0.0079 |          23.5548 |          12.6076 |
[32m[20221214 00:03:50 @agent_ppo2.py:185][0m |          -0.0165 |          22.8025 |          12.6377 |
[32m[20221214 00:03:50 @agent_ppo2.py:185][0m |          -0.0098 |          22.1382 |          12.6759 |
[32m[20221214 00:03:50 @agent_ppo2.py:185][0m |          -0.0108 |          21.7274 |          12.6822 |
[32m[20221214 00:03:50 @agent_ppo2.py:185][0m |          -0.0162 |          21.0588 |          12.7022 |
[32m[20221214 00:03:50 @agent_ppo2.py:185][0m |          -0.0214 |          20.8669 |          12.7123 |
[32m[20221214 00:03:50 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:03:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 385.00
[32m[20221214 00:03:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 430.41
[32m[20221214 00:03:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 484.66
[32m[20221214 00:03:50 @agent_ppo2.py:143][0m Total time:       6.33 min
[32m[20221214 00:03:50 @agent_ppo2.py:145][0m 577536 total steps have happened
[32m[20221214 00:03:50 @agent_ppo2.py:121][0m #------------------------ Iteration 4282 --------------------------#
[32m[20221214 00:03:50 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:03:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:03:51 @agent_ppo2.py:185][0m |           0.0023 |          60.8828 |          12.6449 |
[32m[20221214 00:03:51 @agent_ppo2.py:185][0m |          -0.0059 |          58.6555 |          12.6138 |
[32m[20221214 00:03:51 @agent_ppo2.py:185][0m |          -0.0095 |          58.0226 |          12.6620 |
[32m[20221214 00:03:51 @agent_ppo2.py:185][0m |          -0.0089 |          57.4703 |          12.6921 |
[32m[20221214 00:03:51 @agent_ppo2.py:185][0m |          -0.0080 |          57.0419 |          12.6901 |
[32m[20221214 00:03:51 @agent_ppo2.py:185][0m |          -0.0079 |          56.7648 |          12.7505 |
[32m[20221214 00:03:51 @agent_ppo2.py:185][0m |          -0.0005 |          61.2818 |          12.7484 |
[32m[20221214 00:03:51 @agent_ppo2.py:185][0m |          -0.0102 |          56.2662 |          12.7728 |
[32m[20221214 00:03:51 @agent_ppo2.py:185][0m |          -0.0145 |          55.9103 |          12.7801 |
[32m[20221214 00:03:51 @agent_ppo2.py:185][0m |          -0.0137 |          55.8008 |          12.8112 |
[32m[20221214 00:03:51 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:03:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 504.89
[32m[20221214 00:03:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 542.16
[32m[20221214 00:03:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 245.54
[32m[20221214 00:03:52 @agent_ppo2.py:143][0m Total time:       6.35 min
[32m[20221214 00:03:52 @agent_ppo2.py:145][0m 579584 total steps have happened
[32m[20221214 00:03:52 @agent_ppo2.py:121][0m #------------------------ Iteration 4283 --------------------------#
[32m[20221214 00:03:52 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:03:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:03:52 @agent_ppo2.py:185][0m |           0.0001 |          57.5919 |          12.7578 |
[32m[20221214 00:03:52 @agent_ppo2.py:185][0m |          -0.0053 |          52.8011 |          12.7233 |
[32m[20221214 00:03:52 @agent_ppo2.py:185][0m |          -0.0043 |          51.5242 |          12.7765 |
[32m[20221214 00:03:52 @agent_ppo2.py:185][0m |          -0.0093 |          49.1504 |          12.7611 |
[32m[20221214 00:03:52 @agent_ppo2.py:185][0m |          -0.0140 |          48.4504 |          12.7600 |
[32m[20221214 00:03:52 @agent_ppo2.py:185][0m |          -0.0101 |          48.1437 |          12.7795 |
[32m[20221214 00:03:52 @agent_ppo2.py:185][0m |          -0.0032 |          53.2532 |          12.7989 |
[32m[20221214 00:03:52 @agent_ppo2.py:185][0m |          -0.0126 |          47.2058 |          12.8397 |
[32m[20221214 00:03:53 @agent_ppo2.py:185][0m |          -0.0127 |          46.9200 |          12.8209 |
[32m[20221214 00:03:53 @agent_ppo2.py:185][0m |          -0.0160 |          46.5822 |          12.8481 |
[32m[20221214 00:03:53 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:03:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 396.19
[32m[20221214 00:03:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 456.73
[32m[20221214 00:03:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 541.37
[32m[20221214 00:03:53 @agent_ppo2.py:143][0m Total time:       6.37 min
[32m[20221214 00:03:53 @agent_ppo2.py:145][0m 581632 total steps have happened
[32m[20221214 00:03:53 @agent_ppo2.py:121][0m #------------------------ Iteration 4284 --------------------------#
[32m[20221214 00:03:53 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:03:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:03:53 @agent_ppo2.py:185][0m |          -0.0008 |          59.8251 |          12.4086 |
[32m[20221214 00:03:53 @agent_ppo2.py:185][0m |          -0.0040 |          56.5645 |          12.3564 |
[32m[20221214 00:03:53 @agent_ppo2.py:185][0m |          -0.0090 |          54.6869 |          12.3524 |
[32m[20221214 00:03:53 @agent_ppo2.py:185][0m |          -0.0081 |          53.5926 |          12.3375 |
[32m[20221214 00:03:54 @agent_ppo2.py:185][0m |          -0.0132 |          52.7578 |          12.2789 |
[32m[20221214 00:03:54 @agent_ppo2.py:185][0m |          -0.0080 |          52.0591 |          12.2329 |
[32m[20221214 00:03:54 @agent_ppo2.py:185][0m |          -0.0121 |          51.2126 |          12.2053 |
[32m[20221214 00:03:54 @agent_ppo2.py:185][0m |          -0.0135 |          50.6938 |          12.2160 |
[32m[20221214 00:03:54 @agent_ppo2.py:185][0m |          -0.0139 |          50.0407 |          12.2106 |
[32m[20221214 00:03:54 @agent_ppo2.py:185][0m |           0.0088 |          55.7590 |          12.1365 |
[32m[20221214 00:03:54 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:03:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 510.01
[32m[20221214 00:03:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 558.67
[32m[20221214 00:03:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 541.42
[32m[20221214 00:03:54 @agent_ppo2.py:143][0m Total time:       6.40 min
[32m[20221214 00:03:54 @agent_ppo2.py:145][0m 583680 total steps have happened
[32m[20221214 00:03:54 @agent_ppo2.py:121][0m #------------------------ Iteration 4285 --------------------------#
[32m[20221214 00:03:54 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:03:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:03:54 @agent_ppo2.py:185][0m |          -0.0026 |          44.4496 |          12.5733 |
[32m[20221214 00:03:55 @agent_ppo2.py:185][0m |           0.0002 |          40.3285 |          12.5336 |
[32m[20221214 00:03:55 @agent_ppo2.py:185][0m |          -0.0067 |          37.2730 |          12.5510 |
[32m[20221214 00:03:55 @agent_ppo2.py:185][0m |          -0.0058 |          36.4014 |          12.5367 |
[32m[20221214 00:03:55 @agent_ppo2.py:185][0m |          -0.0147 |          35.8367 |          12.5098 |
[32m[20221214 00:03:55 @agent_ppo2.py:185][0m |          -0.0141 |          35.4131 |          12.4846 |
[32m[20221214 00:03:55 @agent_ppo2.py:185][0m |          -0.0137 |          35.6007 |          12.4744 |
[32m[20221214 00:03:55 @agent_ppo2.py:185][0m |          -0.0165 |          34.8944 |          12.4667 |
[32m[20221214 00:03:55 @agent_ppo2.py:185][0m |          -0.0173 |          34.5706 |          12.4593 |
[32m[20221214 00:03:55 @agent_ppo2.py:185][0m |          -0.0163 |          34.3442 |          12.4297 |
[32m[20221214 00:03:55 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:03:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 378.69
[32m[20221214 00:03:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 467.03
[32m[20221214 00:03:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 576.97
[32m[20221214 00:03:55 @agent_ppo2.py:143][0m Total time:       6.42 min
[32m[20221214 00:03:55 @agent_ppo2.py:145][0m 585728 total steps have happened
[32m[20221214 00:03:55 @agent_ppo2.py:121][0m #------------------------ Iteration 4286 --------------------------#
[32m[20221214 00:03:56 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:03:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:03:56 @agent_ppo2.py:185][0m |          -0.0003 |          35.4288 |          12.1846 |
[32m[20221214 00:03:56 @agent_ppo2.py:185][0m |          -0.0060 |          31.7732 |          12.2308 |
[32m[20221214 00:03:56 @agent_ppo2.py:185][0m |          -0.0042 |          31.2636 |          12.2166 |
[32m[20221214 00:03:56 @agent_ppo2.py:185][0m |          -0.0018 |          30.2267 |          12.2011 |
[32m[20221214 00:03:56 @agent_ppo2.py:185][0m |          -0.0127 |          28.8394 |          12.1497 |
[32m[20221214 00:03:56 @agent_ppo2.py:185][0m |          -0.0139 |          28.6849 |          12.1935 |
[32m[20221214 00:03:56 @agent_ppo2.py:185][0m |          -0.0115 |          27.9908 |          12.1486 |
[32m[20221214 00:03:56 @agent_ppo2.py:185][0m |          -0.0161 |          27.6713 |          12.1263 |
[32m[20221214 00:03:56 @agent_ppo2.py:185][0m |          -0.0178 |          27.4794 |          12.1408 |
[32m[20221214 00:03:57 @agent_ppo2.py:185][0m |          -0.0146 |          27.2755 |          12.1118 |
[32m[20221214 00:03:57 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:03:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 402.51
[32m[20221214 00:03:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 426.21
[32m[20221214 00:03:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 496.89
[32m[20221214 00:03:57 @agent_ppo2.py:143][0m Total time:       6.44 min
[32m[20221214 00:03:57 @agent_ppo2.py:145][0m 587776 total steps have happened
[32m[20221214 00:03:57 @agent_ppo2.py:121][0m #------------------------ Iteration 4287 --------------------------#
[32m[20221214 00:03:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:03:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:03:57 @agent_ppo2.py:185][0m |           0.0075 |          38.0127 |          12.0080 |
[32m[20221214 00:03:57 @agent_ppo2.py:185][0m |          -0.0047 |          33.9432 |          11.9674 |
[32m[20221214 00:03:57 @agent_ppo2.py:185][0m |          -0.0125 |          32.2849 |          11.9512 |
[32m[20221214 00:03:57 @agent_ppo2.py:185][0m |          -0.0076 |          31.3640 |          11.9465 |
[32m[20221214 00:03:57 @agent_ppo2.py:185][0m |          -0.0124 |          30.3341 |          11.9561 |
[32m[20221214 00:03:57 @agent_ppo2.py:185][0m |          -0.0166 |          29.7657 |          11.8894 |
[32m[20221214 00:03:58 @agent_ppo2.py:185][0m |          -0.0127 |          29.5663 |          11.9080 |
[32m[20221214 00:03:58 @agent_ppo2.py:185][0m |          -0.0140 |          29.2832 |          11.8287 |
[32m[20221214 00:03:58 @agent_ppo2.py:185][0m |          -0.0155 |          28.8951 |          11.8209 |
[32m[20221214 00:03:58 @agent_ppo2.py:185][0m |          -0.0158 |          28.5993 |          11.7916 |
[32m[20221214 00:03:58 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:03:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 302.45
[32m[20221214 00:03:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 372.05
[32m[20221214 00:03:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 323.25
[32m[20221214 00:03:58 @agent_ppo2.py:143][0m Total time:       6.46 min
[32m[20221214 00:03:58 @agent_ppo2.py:145][0m 589824 total steps have happened
[32m[20221214 00:03:58 @agent_ppo2.py:121][0m #------------------------ Iteration 4288 --------------------------#
[32m[20221214 00:03:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:03:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:03:58 @agent_ppo2.py:185][0m |           0.0024 |          70.0917 |          12.0703 |
[32m[20221214 00:03:58 @agent_ppo2.py:185][0m |          -0.0041 |          64.3942 |          12.0853 |
[32m[20221214 00:03:58 @agent_ppo2.py:185][0m |           0.0258 |          73.1341 |          12.1248 |
[32m[20221214 00:03:59 @agent_ppo2.py:185][0m |          -0.0053 |          61.0501 |          12.0225 |
[32m[20221214 00:03:59 @agent_ppo2.py:185][0m |          -0.0060 |          58.7531 |          12.1141 |
[32m[20221214 00:03:59 @agent_ppo2.py:185][0m |          -0.0081 |          58.8395 |          12.1299 |
[32m[20221214 00:03:59 @agent_ppo2.py:185][0m |          -0.0087 |          57.9406 |          12.1093 |
[32m[20221214 00:03:59 @agent_ppo2.py:185][0m |          -0.0078 |          57.2772 |          12.1051 |
[32m[20221214 00:03:59 @agent_ppo2.py:185][0m |          -0.0020 |          57.5750 |          12.1255 |
[32m[20221214 00:03:59 @agent_ppo2.py:185][0m |          -0.0134 |          56.4639 |          12.0821 |
[32m[20221214 00:03:59 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:03:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 480.24
[32m[20221214 00:03:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 500.54
[32m[20221214 00:03:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 526.12
[32m[20221214 00:03:59 @agent_ppo2.py:143][0m Total time:       6.48 min
[32m[20221214 00:03:59 @agent_ppo2.py:145][0m 591872 total steps have happened
[32m[20221214 00:03:59 @agent_ppo2.py:121][0m #------------------------ Iteration 4289 --------------------------#
[32m[20221214 00:03:59 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:04:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:04:00 @agent_ppo2.py:185][0m |           0.0001 |          67.4621 |          12.2891 |
[32m[20221214 00:04:00 @agent_ppo2.py:185][0m |          -0.0023 |          64.3331 |          12.2576 |
[32m[20221214 00:04:00 @agent_ppo2.py:185][0m |           0.0004 |          63.6074 |          12.2467 |
[32m[20221214 00:04:00 @agent_ppo2.py:185][0m |          -0.0061 |          63.3214 |          12.2269 |
[32m[20221214 00:04:00 @agent_ppo2.py:185][0m |           0.0031 |          66.5321 |          12.2739 |
[32m[20221214 00:04:00 @agent_ppo2.py:185][0m |          -0.0039 |          62.7937 |          12.2756 |
[32m[20221214 00:04:00 @agent_ppo2.py:185][0m |          -0.0044 |          62.3750 |          12.2508 |
[32m[20221214 00:04:00 @agent_ppo2.py:185][0m |          -0.0049 |          61.5584 |          12.2234 |
[32m[20221214 00:04:00 @agent_ppo2.py:185][0m |          -0.0061 |          61.2182 |          12.2081 |
[32m[20221214 00:04:00 @agent_ppo2.py:185][0m |          -0.0068 |          60.9180 |          12.2096 |
[32m[20221214 00:04:00 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:04:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 501.72
[32m[20221214 00:04:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 537.80
[32m[20221214 00:04:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 507.42
[32m[20221214 00:04:01 @agent_ppo2.py:143][0m Total time:       6.50 min
[32m[20221214 00:04:01 @agent_ppo2.py:145][0m 593920 total steps have happened
[32m[20221214 00:04:01 @agent_ppo2.py:121][0m #------------------------ Iteration 4290 --------------------------#
[32m[20221214 00:04:01 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:04:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:04:01 @agent_ppo2.py:185][0m |           0.0023 |          35.8615 |          11.8966 |
[32m[20221214 00:04:01 @agent_ppo2.py:185][0m |          -0.0087 |          30.6207 |          11.8652 |
[32m[20221214 00:04:01 @agent_ppo2.py:185][0m |          -0.0083 |          27.8689 |          11.8095 |
[32m[20221214 00:04:01 @agent_ppo2.py:185][0m |          -0.0130 |          26.8489 |          11.8373 |
[32m[20221214 00:04:01 @agent_ppo2.py:185][0m |          -0.0051 |          26.1651 |          11.7326 |
[32m[20221214 00:04:01 @agent_ppo2.py:185][0m |          -0.0132 |          25.5683 |          11.7297 |
[32m[20221214 00:04:01 @agent_ppo2.py:185][0m |          -0.0156 |          24.9831 |          11.7242 |
[32m[20221214 00:04:02 @agent_ppo2.py:185][0m |          -0.0175 |          24.6958 |          11.6999 |
[32m[20221214 00:04:02 @agent_ppo2.py:185][0m |          -0.0183 |          24.2811 |          11.6570 |
[32m[20221214 00:04:02 @agent_ppo2.py:185][0m |          -0.0154 |          24.0148 |          11.6556 |
[32m[20221214 00:04:02 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:04:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 396.06
[32m[20221214 00:04:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 524.01
[32m[20221214 00:04:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 517.35
[32m[20221214 00:04:02 @agent_ppo2.py:143][0m Total time:       6.52 min
[32m[20221214 00:04:02 @agent_ppo2.py:145][0m 595968 total steps have happened
[32m[20221214 00:04:02 @agent_ppo2.py:121][0m #------------------------ Iteration 4291 --------------------------#
[32m[20221214 00:04:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:04:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:04:02 @agent_ppo2.py:185][0m |           0.0003 |          38.3564 |          11.9392 |
[32m[20221214 00:04:02 @agent_ppo2.py:185][0m |          -0.0109 |          35.8999 |          11.9392 |
[32m[20221214 00:04:02 @agent_ppo2.py:185][0m |          -0.0079 |          35.1970 |          11.9074 |
[32m[20221214 00:04:02 @agent_ppo2.py:185][0m |          -0.0057 |          34.7117 |          11.9466 |
[32m[20221214 00:04:03 @agent_ppo2.py:185][0m |          -0.0114 |          34.1679 |          11.9245 |
[32m[20221214 00:04:03 @agent_ppo2.py:185][0m |          -0.0124 |          33.4840 |          11.9367 |
[32m[20221214 00:04:03 @agent_ppo2.py:185][0m |          -0.0158 |          33.3467 |          11.9422 |
[32m[20221214 00:04:03 @agent_ppo2.py:185][0m |          -0.0127 |          33.0049 |          11.9562 |
[32m[20221214 00:04:03 @agent_ppo2.py:185][0m |          -0.0162 |          32.6639 |          11.9326 |
[32m[20221214 00:04:03 @agent_ppo2.py:185][0m |          -0.0154 |          32.4402 |          11.9516 |
[32m[20221214 00:04:03 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:04:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 403.00
[32m[20221214 00:04:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 465.66
[32m[20221214 00:04:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 319.45
[32m[20221214 00:04:03 @agent_ppo2.py:143][0m Total time:       6.55 min
[32m[20221214 00:04:03 @agent_ppo2.py:145][0m 598016 total steps have happened
[32m[20221214 00:04:03 @agent_ppo2.py:121][0m #------------------------ Iteration 4292 --------------------------#
[32m[20221214 00:04:03 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:04:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:04:03 @agent_ppo2.py:185][0m |          -0.0021 |          59.8757 |          12.1990 |
[32m[20221214 00:04:04 @agent_ppo2.py:185][0m |          -0.0027 |          51.6668 |          12.2689 |
[32m[20221214 00:04:04 @agent_ppo2.py:185][0m |          -0.0097 |          47.8215 |          12.2544 |
[32m[20221214 00:04:04 @agent_ppo2.py:185][0m |          -0.0084 |          48.7816 |          12.2685 |
[32m[20221214 00:04:04 @agent_ppo2.py:185][0m |          -0.0114 |          44.5943 |          12.2856 |
[32m[20221214 00:04:04 @agent_ppo2.py:185][0m |          -0.0166 |          43.7665 |          12.3164 |
[32m[20221214 00:04:04 @agent_ppo2.py:185][0m |          -0.0173 |          42.7407 |          12.2979 |
[32m[20221214 00:04:04 @agent_ppo2.py:185][0m |          -0.0155 |          41.9060 |          12.3129 |
[32m[20221214 00:04:04 @agent_ppo2.py:185][0m |          -0.0208 |          41.0829 |          12.3075 |
[32m[20221214 00:04:04 @agent_ppo2.py:185][0m |          -0.0161 |          40.5293 |          12.3104 |
[32m[20221214 00:04:04 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:04:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 335.49
[32m[20221214 00:04:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 442.45
[32m[20221214 00:04:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 399.53
[32m[20221214 00:04:05 @agent_ppo2.py:143][0m Total time:       6.57 min
[32m[20221214 00:04:05 @agent_ppo2.py:145][0m 600064 total steps have happened
[32m[20221214 00:04:05 @agent_ppo2.py:121][0m #------------------------ Iteration 4293 --------------------------#
[32m[20221214 00:04:05 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:04:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:04:05 @agent_ppo2.py:185][0m |           0.0089 |          43.9022 |          11.8455 |
[32m[20221214 00:04:05 @agent_ppo2.py:185][0m |          -0.0020 |          33.6093 |          11.8093 |
[32m[20221214 00:04:05 @agent_ppo2.py:185][0m |          -0.0069 |          31.9328 |          11.8357 |
[32m[20221214 00:04:05 @agent_ppo2.py:185][0m |          -0.0115 |          31.0145 |          11.8486 |
[32m[20221214 00:04:05 @agent_ppo2.py:185][0m |          -0.0089 |          30.3010 |          11.8256 |
[32m[20221214 00:04:05 @agent_ppo2.py:185][0m |          -0.0115 |          29.5545 |          11.8654 |
[32m[20221214 00:04:05 @agent_ppo2.py:185][0m |          -0.0071 |          29.1295 |          11.8696 |
[32m[20221214 00:04:05 @agent_ppo2.py:185][0m |          -0.0090 |          29.4587 |          11.8488 |
[32m[20221214 00:04:06 @agent_ppo2.py:185][0m |          -0.0111 |          28.4786 |          11.8804 |
[32m[20221214 00:04:06 @agent_ppo2.py:185][0m |          -0.0147 |          28.3698 |          11.8804 |
[32m[20221214 00:04:06 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:04:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 385.40
[32m[20221214 00:04:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 429.29
[32m[20221214 00:04:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 492.21
[32m[20221214 00:04:06 @agent_ppo2.py:143][0m Total time:       6.59 min
[32m[20221214 00:04:06 @agent_ppo2.py:145][0m 602112 total steps have happened
[32m[20221214 00:04:06 @agent_ppo2.py:121][0m #------------------------ Iteration 4294 --------------------------#
[32m[20221214 00:04:06 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:04:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:04:06 @agent_ppo2.py:185][0m |           0.0051 |          48.0448 |          11.6615 |
[32m[20221214 00:04:06 @agent_ppo2.py:185][0m |           0.0018 |          42.0159 |          11.6538 |
[32m[20221214 00:04:06 @agent_ppo2.py:185][0m |          -0.0070 |          38.9017 |          11.6433 |
[32m[20221214 00:04:06 @agent_ppo2.py:185][0m |          -0.0120 |          37.4076 |          11.6573 |
[32m[20221214 00:04:06 @agent_ppo2.py:185][0m |          -0.0137 |          36.3327 |          11.6419 |
[32m[20221214 00:04:07 @agent_ppo2.py:185][0m |          -0.0155 |          35.4424 |          11.6384 |
[32m[20221214 00:04:07 @agent_ppo2.py:185][0m |          -0.0151 |          34.7171 |          11.6075 |
[32m[20221214 00:04:07 @agent_ppo2.py:185][0m |          -0.0150 |          34.3482 |          11.6443 |
[32m[20221214 00:04:07 @agent_ppo2.py:185][0m |          -0.0123 |          33.6114 |          11.5869 |
[32m[20221214 00:04:07 @agent_ppo2.py:185][0m |          -0.0121 |          33.5007 |          11.6254 |
[32m[20221214 00:04:07 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:04:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 375.53
[32m[20221214 00:04:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 439.84
[32m[20221214 00:04:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 535.68
[32m[20221214 00:04:07 @agent_ppo2.py:143][0m Total time:       6.61 min
[32m[20221214 00:04:07 @agent_ppo2.py:145][0m 604160 total steps have happened
[32m[20221214 00:04:07 @agent_ppo2.py:121][0m #------------------------ Iteration 4295 --------------------------#
[32m[20221214 00:04:07 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:04:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:04:07 @agent_ppo2.py:185][0m |           0.0030 |          63.5461 |          11.7102 |
[32m[20221214 00:04:07 @agent_ppo2.py:185][0m |          -0.0040 |          58.9263 |          11.6941 |
[32m[20221214 00:04:08 @agent_ppo2.py:185][0m |          -0.0074 |          56.5787 |          11.6877 |
[32m[20221214 00:04:08 @agent_ppo2.py:185][0m |          -0.0073 |          54.5278 |          11.6609 |
[32m[20221214 00:04:08 @agent_ppo2.py:185][0m |          -0.0053 |          53.4594 |          11.6126 |
[32m[20221214 00:04:08 @agent_ppo2.py:185][0m |          -0.0097 |          51.8008 |          11.5948 |
[32m[20221214 00:04:08 @agent_ppo2.py:185][0m |          -0.0059 |          51.3528 |          11.5582 |
[32m[20221214 00:04:08 @agent_ppo2.py:185][0m |          -0.0078 |          50.4532 |          11.4866 |
[32m[20221214 00:04:08 @agent_ppo2.py:185][0m |          -0.0036 |          53.9510 |          11.5145 |
[32m[20221214 00:04:08 @agent_ppo2.py:185][0m |          -0.0105 |          49.9456 |          11.4760 |
[32m[20221214 00:04:08 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:04:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 484.63
[32m[20221214 00:04:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 514.14
[32m[20221214 00:04:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 498.26
[32m[20221214 00:04:08 @agent_ppo2.py:143][0m Total time:       6.63 min
[32m[20221214 00:04:08 @agent_ppo2.py:145][0m 606208 total steps have happened
[32m[20221214 00:04:08 @agent_ppo2.py:121][0m #------------------------ Iteration 4296 --------------------------#
[32m[20221214 00:04:08 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:04:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:04:09 @agent_ppo2.py:185][0m |          -0.0023 |          72.7220 |          11.6560 |
[32m[20221214 00:04:09 @agent_ppo2.py:185][0m |          -0.0056 |          64.7565 |          11.6941 |
[32m[20221214 00:04:09 @agent_ppo2.py:185][0m |          -0.0007 |          64.3254 |          11.6992 |
[32m[20221214 00:04:09 @agent_ppo2.py:185][0m |          -0.0095 |          59.8761 |          11.7486 |
[32m[20221214 00:04:09 @agent_ppo2.py:185][0m |          -0.0117 |          57.9400 |          11.7794 |
[32m[20221214 00:04:09 @agent_ppo2.py:185][0m |          -0.0075 |          56.4333 |          11.8028 |
[32m[20221214 00:04:09 @agent_ppo2.py:185][0m |          -0.0090 |          56.7826 |          11.8439 |
[32m[20221214 00:04:09 @agent_ppo2.py:185][0m |          -0.0077 |          60.9922 |          11.9186 |
[32m[20221214 00:04:09 @agent_ppo2.py:185][0m |          -0.0139 |          54.0544 |          11.9234 |
[32m[20221214 00:04:09 @agent_ppo2.py:185][0m |          -0.0182 |          53.4033 |          11.9411 |
[32m[20221214 00:04:09 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:04:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 380.68
[32m[20221214 00:04:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 430.60
[32m[20221214 00:04:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 439.75
[32m[20221214 00:04:10 @agent_ppo2.py:143][0m Total time:       6.65 min
[32m[20221214 00:04:10 @agent_ppo2.py:145][0m 608256 total steps have happened
[32m[20221214 00:04:10 @agent_ppo2.py:121][0m #------------------------ Iteration 4297 --------------------------#
[32m[20221214 00:04:10 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:04:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:04:10 @agent_ppo2.py:185][0m |           0.0023 |          36.7176 |          11.7645 |
[32m[20221214 00:04:10 @agent_ppo2.py:185][0m |          -0.0047 |          30.8628 |          11.7403 |
[32m[20221214 00:04:10 @agent_ppo2.py:185][0m |          -0.0082 |          28.8020 |          11.6739 |
[32m[20221214 00:04:10 @agent_ppo2.py:185][0m |          -0.0112 |          27.5240 |          11.6599 |
[32m[20221214 00:04:10 @agent_ppo2.py:185][0m |          -0.0077 |          26.6755 |          11.6330 |
[32m[20221214 00:04:10 @agent_ppo2.py:185][0m |          -0.0152 |          26.0079 |          11.6250 |
[32m[20221214 00:04:10 @agent_ppo2.py:185][0m |          -0.0150 |          26.1017 |          11.5535 |
[32m[20221214 00:04:11 @agent_ppo2.py:185][0m |          -0.0177 |          25.0204 |          11.5945 |
[32m[20221214 00:04:11 @agent_ppo2.py:185][0m |          -0.0148 |          24.6298 |          11.5119 |
[32m[20221214 00:04:11 @agent_ppo2.py:185][0m |          -0.0221 |          24.1592 |          11.4852 |
[32m[20221214 00:04:11 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:04:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 314.55
[32m[20221214 00:04:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 474.22
[32m[20221214 00:04:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 533.09
[32m[20221214 00:04:11 @agent_ppo2.py:143][0m Total time:       6.68 min
[32m[20221214 00:04:11 @agent_ppo2.py:145][0m 610304 total steps have happened
[32m[20221214 00:04:11 @agent_ppo2.py:121][0m #------------------------ Iteration 4298 --------------------------#
[32m[20221214 00:04:11 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:04:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:04:11 @agent_ppo2.py:185][0m |           0.0041 |          27.7620 |          11.7182 |
[32m[20221214 00:04:11 @agent_ppo2.py:185][0m |          -0.0069 |          21.5751 |          11.7218 |
[32m[20221214 00:04:11 @agent_ppo2.py:185][0m |          -0.0090 |          20.0698 |          11.7342 |
[32m[20221214 00:04:11 @agent_ppo2.py:185][0m |          -0.0137 |          19.2038 |          11.7335 |
[32m[20221214 00:04:12 @agent_ppo2.py:185][0m |          -0.0101 |          18.4868 |          11.7406 |
[32m[20221214 00:04:12 @agent_ppo2.py:185][0m |          -0.0185 |          18.0594 |          11.7080 |
[32m[20221214 00:04:12 @agent_ppo2.py:185][0m |          -0.0135 |          17.4756 |          11.7230 |
[32m[20221214 00:04:12 @agent_ppo2.py:185][0m |          -0.0130 |          17.3145 |          11.7173 |
[32m[20221214 00:04:12 @agent_ppo2.py:185][0m |          -0.0165 |          16.6844 |          11.7432 |
[32m[20221214 00:04:12 @agent_ppo2.py:185][0m |          -0.0151 |          16.2428 |          11.7345 |
[32m[20221214 00:04:12 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:04:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 331.68
[32m[20221214 00:04:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 436.89
[32m[20221214 00:04:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 519.38
[32m[20221214 00:04:12 @agent_ppo2.py:143][0m Total time:       6.70 min
[32m[20221214 00:04:12 @agent_ppo2.py:145][0m 612352 total steps have happened
[32m[20221214 00:04:12 @agent_ppo2.py:121][0m #------------------------ Iteration 4299 --------------------------#
[32m[20221214 00:04:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:04:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:04:12 @agent_ppo2.py:185][0m |          -0.0041 |          34.0218 |          11.3949 |
[32m[20221214 00:04:13 @agent_ppo2.py:185][0m |          -0.0054 |          28.9847 |          11.3977 |
[32m[20221214 00:04:13 @agent_ppo2.py:185][0m |          -0.0130 |          27.6755 |          11.4207 |
[32m[20221214 00:04:13 @agent_ppo2.py:185][0m |          -0.0124 |          26.2942 |          11.3919 |
[32m[20221214 00:04:13 @agent_ppo2.py:185][0m |          -0.0092 |          25.6976 |          11.3593 |
[32m[20221214 00:04:13 @agent_ppo2.py:185][0m |          -0.0147 |          25.3532 |          11.3598 |
[32m[20221214 00:04:13 @agent_ppo2.py:185][0m |          -0.0168 |          24.9683 |          11.3645 |
[32m[20221214 00:04:13 @agent_ppo2.py:185][0m |          -0.0147 |          24.4108 |          11.2830 |
[32m[20221214 00:04:13 @agent_ppo2.py:185][0m |          -0.0136 |          24.3025 |          11.3179 |
[32m[20221214 00:04:13 @agent_ppo2.py:185][0m |          -0.0203 |          23.8029 |          11.3038 |
[32m[20221214 00:04:13 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:04:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 413.15
[32m[20221214 00:04:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 513.44
[32m[20221214 00:04:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 510.59
[32m[20221214 00:04:13 @agent_ppo2.py:143][0m Total time:       6.72 min
[32m[20221214 00:04:13 @agent_ppo2.py:145][0m 614400 total steps have happened
[32m[20221214 00:04:13 @agent_ppo2.py:121][0m #------------------------ Iteration 4300 --------------------------#
[32m[20221214 00:04:14 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:04:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:04:14 @agent_ppo2.py:185][0m |          -0.0045 |          74.6880 |          11.6028 |
[32m[20221214 00:04:14 @agent_ppo2.py:185][0m |          -0.0024 |          69.7178 |          11.6122 |
[32m[20221214 00:04:14 @agent_ppo2.py:185][0m |          -0.0076 |          68.4947 |          11.6066 |
[32m[20221214 00:04:14 @agent_ppo2.py:185][0m |          -0.0058 |          67.8877 |          11.5999 |
[32m[20221214 00:04:14 @agent_ppo2.py:185][0m |          -0.0103 |          67.1024 |          11.6311 |
[32m[20221214 00:04:14 @agent_ppo2.py:185][0m |          -0.0140 |          66.6676 |          11.6511 |
[32m[20221214 00:04:14 @agent_ppo2.py:185][0m |          -0.0130 |          66.3484 |          11.6672 |
[32m[20221214 00:04:14 @agent_ppo2.py:185][0m |          -0.0133 |          66.0441 |          11.6297 |
[32m[20221214 00:04:14 @agent_ppo2.py:185][0m |          -0.0082 |          66.9270 |          11.6262 |
[32m[20221214 00:04:15 @agent_ppo2.py:185][0m |          -0.0117 |          65.6561 |          11.6509 |
[32m[20221214 00:04:15 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:04:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 522.72
[32m[20221214 00:04:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 584.84
[32m[20221214 00:04:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 531.31
[32m[20221214 00:04:15 @agent_ppo2.py:143][0m Total time:       6.74 min
[32m[20221214 00:04:15 @agent_ppo2.py:145][0m 616448 total steps have happened
[32m[20221214 00:04:15 @agent_ppo2.py:121][0m #------------------------ Iteration 4301 --------------------------#
[32m[20221214 00:04:15 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:04:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:04:15 @agent_ppo2.py:185][0m |          -0.0020 |          75.8048 |          11.7721 |
[32m[20221214 00:04:15 @agent_ppo2.py:185][0m |          -0.0126 |          71.0861 |          11.7791 |
[32m[20221214 00:04:15 @agent_ppo2.py:185][0m |          -0.0115 |          69.9090 |          11.7948 |
[32m[20221214 00:04:15 @agent_ppo2.py:185][0m |          -0.0115 |          69.3811 |          11.8387 |
[32m[20221214 00:04:15 @agent_ppo2.py:185][0m |          -0.0154 |          68.7876 |          11.8304 |
[32m[20221214 00:04:15 @agent_ppo2.py:185][0m |          -0.0179 |          68.7401 |          11.8805 |
[32m[20221214 00:04:16 @agent_ppo2.py:185][0m |          -0.0155 |          68.3059 |          11.8765 |
[32m[20221214 00:04:16 @agent_ppo2.py:185][0m |          -0.0098 |          69.5478 |          11.8867 |
[32m[20221214 00:04:16 @agent_ppo2.py:185][0m |          -0.0231 |          67.7156 |          11.9139 |
[32m[20221214 00:04:16 @agent_ppo2.py:185][0m |          -0.0200 |          67.5660 |          11.9508 |
[32m[20221214 00:04:16 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:04:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 503.75
[32m[20221214 00:04:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 540.86
[32m[20221214 00:04:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 387.46
[32m[20221214 00:04:16 @agent_ppo2.py:143][0m Total time:       6.76 min
[32m[20221214 00:04:16 @agent_ppo2.py:145][0m 618496 total steps have happened
[32m[20221214 00:04:16 @agent_ppo2.py:121][0m #------------------------ Iteration 4302 --------------------------#
[32m[20221214 00:04:16 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:04:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:04:16 @agent_ppo2.py:185][0m |           0.0026 |          73.0966 |          11.6669 |
[32m[20221214 00:04:16 @agent_ppo2.py:185][0m |          -0.0037 |          68.7061 |          11.5893 |
[32m[20221214 00:04:16 @agent_ppo2.py:185][0m |          -0.0106 |          67.3234 |          11.5620 |
[32m[20221214 00:04:17 @agent_ppo2.py:185][0m |          -0.0091 |          66.6433 |          11.5159 |
[32m[20221214 00:04:17 @agent_ppo2.py:185][0m |          -0.0048 |          72.6924 |          11.5796 |
[32m[20221214 00:04:17 @agent_ppo2.py:185][0m |          -0.0071 |          67.3216 |          11.5825 |
[32m[20221214 00:04:17 @agent_ppo2.py:185][0m |          -0.0051 |          74.1241 |          11.5706 |
[32m[20221214 00:04:17 @agent_ppo2.py:185][0m |          -0.0179 |          65.1112 |          11.5514 |
[32m[20221214 00:04:17 @agent_ppo2.py:185][0m |          -0.0159 |          64.6556 |          11.5516 |
[32m[20221214 00:04:17 @agent_ppo2.py:185][0m |          -0.0148 |          64.4553 |          11.5468 |
[32m[20221214 00:04:17 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:04:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 377.18
[32m[20221214 00:04:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 484.75
[32m[20221214 00:04:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 506.63
[32m[20221214 00:04:17 @agent_ppo2.py:143][0m Total time:       6.78 min
[32m[20221214 00:04:17 @agent_ppo2.py:145][0m 620544 total steps have happened
[32m[20221214 00:04:17 @agent_ppo2.py:121][0m #------------------------ Iteration 4303 --------------------------#
[32m[20221214 00:04:17 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:04:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:04:18 @agent_ppo2.py:185][0m |           0.0019 |          68.8860 |          11.4519 |
[32m[20221214 00:04:18 @agent_ppo2.py:185][0m |          -0.0032 |          66.6924 |          11.3980 |
[32m[20221214 00:04:18 @agent_ppo2.py:185][0m |          -0.0112 |          64.9278 |          11.4384 |
[32m[20221214 00:04:18 @agent_ppo2.py:185][0m |          -0.0112 |          64.1775 |          11.4593 |
[32m[20221214 00:04:18 @agent_ppo2.py:185][0m |          -0.0114 |          63.7612 |          11.4678 |
[32m[20221214 00:04:18 @agent_ppo2.py:185][0m |          -0.0145 |          63.3318 |          11.5088 |
[32m[20221214 00:04:18 @agent_ppo2.py:185][0m |          -0.0097 |          62.8640 |          11.5134 |
[32m[20221214 00:04:18 @agent_ppo2.py:185][0m |          -0.0168 |          62.8849 |          11.5578 |
[32m[20221214 00:04:18 @agent_ppo2.py:185][0m |          -0.0188 |          62.5315 |          11.5658 |
[32m[20221214 00:04:18 @agent_ppo2.py:185][0m |          -0.0166 |          62.2541 |          11.5648 |
[32m[20221214 00:04:18 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:04:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 368.56
[32m[20221214 00:04:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 490.42
[32m[20221214 00:04:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 399.61
[32m[20221214 00:04:18 @agent_ppo2.py:143][0m Total time:       6.80 min
[32m[20221214 00:04:18 @agent_ppo2.py:145][0m 622592 total steps have happened
[32m[20221214 00:04:19 @agent_ppo2.py:121][0m #------------------------ Iteration 4304 --------------------------#
[32m[20221214 00:04:19 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:04:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:04:19 @agent_ppo2.py:185][0m |           0.0000 |          68.1946 |          11.6365 |
[32m[20221214 00:04:19 @agent_ppo2.py:185][0m |          -0.0058 |          67.3055 |          11.6644 |
[32m[20221214 00:04:19 @agent_ppo2.py:185][0m |          -0.0055 |          66.9642 |          11.6447 |
[32m[20221214 00:04:19 @agent_ppo2.py:185][0m |           0.0019 |          71.8734 |          11.6498 |
[32m[20221214 00:04:19 @agent_ppo2.py:185][0m |          -0.0035 |          66.6450 |          11.6212 |
[32m[20221214 00:04:19 @agent_ppo2.py:185][0m |          -0.0102 |          66.2998 |          11.6622 |
[32m[20221214 00:04:19 @agent_ppo2.py:185][0m |          -0.0079 |          67.0001 |          11.6097 |
[32m[20221214 00:04:19 @agent_ppo2.py:185][0m |          -0.0120 |          66.1360 |          11.6521 |
[32m[20221214 00:04:20 @agent_ppo2.py:185][0m |          -0.0130 |          65.7742 |          11.6574 |
[32m[20221214 00:04:20 @agent_ppo2.py:185][0m |          -0.0035 |          68.9038 |          11.6626 |
[32m[20221214 00:04:20 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:04:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 477.46
[32m[20221214 00:04:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 508.40
[32m[20221214 00:04:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 257.46
[32m[20221214 00:04:20 @agent_ppo2.py:143][0m Total time:       6.82 min
[32m[20221214 00:04:20 @agent_ppo2.py:145][0m 624640 total steps have happened
[32m[20221214 00:04:20 @agent_ppo2.py:121][0m #------------------------ Iteration 4305 --------------------------#
[32m[20221214 00:04:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:04:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:04:20 @agent_ppo2.py:185][0m |           0.0060 |          67.3424 |          11.6778 |
[32m[20221214 00:04:20 @agent_ppo2.py:185][0m |          -0.0050 |          60.4506 |          11.7172 |
[32m[20221214 00:04:20 @agent_ppo2.py:185][0m |          -0.0081 |          59.0339 |          11.6683 |
[32m[20221214 00:04:20 @agent_ppo2.py:185][0m |          -0.0086 |          58.1345 |          11.6771 |
[32m[20221214 00:04:20 @agent_ppo2.py:185][0m |          -0.0105 |          57.7653 |          11.6901 |
[32m[20221214 00:04:21 @agent_ppo2.py:185][0m |          -0.0036 |          60.9678 |          11.6938 |
[32m[20221214 00:04:21 @agent_ppo2.py:185][0m |          -0.0112 |          56.6804 |          11.6700 |
[32m[20221214 00:04:21 @agent_ppo2.py:185][0m |          -0.0196 |          56.4122 |          11.7161 |
[32m[20221214 00:04:21 @agent_ppo2.py:185][0m |          -0.0122 |          56.7402 |          11.6821 |
[32m[20221214 00:04:21 @agent_ppo2.py:185][0m |          -0.0083 |          57.7377 |          11.6959 |
[32m[20221214 00:04:21 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:04:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 426.72
[32m[20221214 00:04:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 521.58
[32m[20221214 00:04:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 462.71
[32m[20221214 00:04:21 @agent_ppo2.py:143][0m Total time:       6.84 min
[32m[20221214 00:04:21 @agent_ppo2.py:145][0m 626688 total steps have happened
[32m[20221214 00:04:21 @agent_ppo2.py:121][0m #------------------------ Iteration 4306 --------------------------#
[32m[20221214 00:04:21 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:04:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:04:21 @agent_ppo2.py:185][0m |          -0.0000 |          77.7187 |          11.5988 |
[32m[20221214 00:04:21 @agent_ppo2.py:185][0m |          -0.0066 |          74.2914 |          11.5788 |
[32m[20221214 00:04:22 @agent_ppo2.py:185][0m |          -0.0075 |          73.2032 |          11.5524 |
[32m[20221214 00:04:22 @agent_ppo2.py:185][0m |          -0.0105 |          72.2148 |          11.5683 |
[32m[20221214 00:04:22 @agent_ppo2.py:185][0m |          -0.0103 |          71.7112 |          11.6016 |
[32m[20221214 00:04:22 @agent_ppo2.py:185][0m |          -0.0090 |          71.2095 |          11.5689 |
[32m[20221214 00:04:22 @agent_ppo2.py:185][0m |          -0.0136 |          70.7621 |          11.5554 |
[32m[20221214 00:04:22 @agent_ppo2.py:185][0m |          -0.0123 |          70.5131 |          11.5770 |
[32m[20221214 00:04:22 @agent_ppo2.py:185][0m |          -0.0136 |          70.8932 |          11.5893 |
[32m[20221214 00:04:22 @agent_ppo2.py:185][0m |          -0.0165 |          70.0258 |          11.6185 |
[32m[20221214 00:04:22 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:04:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 408.04
[32m[20221214 00:04:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 445.06
[32m[20221214 00:04:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 502.17
[32m[20221214 00:04:22 @agent_ppo2.py:143][0m Total time:       6.87 min
[32m[20221214 00:04:22 @agent_ppo2.py:145][0m 628736 total steps have happened
[32m[20221214 00:04:22 @agent_ppo2.py:121][0m #------------------------ Iteration 4307 --------------------------#
[32m[20221214 00:04:22 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:04:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:04:23 @agent_ppo2.py:185][0m |           0.0136 |          53.5690 |          12.1056 |
[32m[20221214 00:04:23 @agent_ppo2.py:185][0m |           0.0043 |          41.8050 |          12.1308 |
[32m[20221214 00:04:23 @agent_ppo2.py:185][0m |          -0.0076 |          39.9032 |          12.1532 |
[32m[20221214 00:04:23 @agent_ppo2.py:185][0m |          -0.0094 |          38.6394 |          12.1460 |
[32m[20221214 00:04:23 @agent_ppo2.py:185][0m |          -0.0068 |          38.1697 |          12.1719 |
[32m[20221214 00:04:23 @agent_ppo2.py:185][0m |          -0.0138 |          37.1853 |          12.1898 |
[32m[20221214 00:04:23 @agent_ppo2.py:185][0m |          -0.0119 |          36.8582 |          12.1855 |
[32m[20221214 00:04:23 @agent_ppo2.py:185][0m |          -0.0117 |          36.2063 |          12.1622 |
[32m[20221214 00:04:23 @agent_ppo2.py:185][0m |          -0.0142 |          35.9009 |          12.2106 |
[32m[20221214 00:04:23 @agent_ppo2.py:185][0m |           0.0006 |          40.4076 |          12.1772 |
[32m[20221214 00:04:23 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:04:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 505.95
[32m[20221214 00:04:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 591.51
[32m[20221214 00:04:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 142.27
[32m[20221214 00:04:24 @agent_ppo2.py:143][0m Total time:       6.89 min
[32m[20221214 00:04:24 @agent_ppo2.py:145][0m 630784 total steps have happened
[32m[20221214 00:04:24 @agent_ppo2.py:121][0m #------------------------ Iteration 4308 --------------------------#
[32m[20221214 00:04:24 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:04:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:04:24 @agent_ppo2.py:185][0m |          -0.0021 |          67.2357 |          11.8229 |
[32m[20221214 00:04:24 @agent_ppo2.py:185][0m |          -0.0061 |          60.2316 |          11.8176 |
[32m[20221214 00:04:24 @agent_ppo2.py:185][0m |          -0.0105 |          57.0627 |          11.8006 |
[32m[20221214 00:04:24 @agent_ppo2.py:185][0m |          -0.0052 |          56.7057 |          11.8649 |
[32m[20221214 00:04:24 @agent_ppo2.py:185][0m |          -0.0107 |          54.0622 |          11.8688 |
[32m[20221214 00:04:24 @agent_ppo2.py:185][0m |          -0.0121 |          53.1266 |          11.9020 |
[32m[20221214 00:04:24 @agent_ppo2.py:185][0m |          -0.0145 |          52.4747 |          11.9051 |
[32m[20221214 00:04:25 @agent_ppo2.py:185][0m |          -0.0152 |          51.6671 |          11.9436 |
[32m[20221214 00:04:25 @agent_ppo2.py:185][0m |          -0.0163 |          50.9388 |          11.9455 |
[32m[20221214 00:04:25 @agent_ppo2.py:185][0m |          -0.0171 |          50.4151 |          11.9865 |
[32m[20221214 00:04:25 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:04:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 423.60
[32m[20221214 00:04:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 461.43
[32m[20221214 00:04:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 551.99
[32m[20221214 00:04:25 @agent_ppo2.py:143][0m Total time:       6.91 min
[32m[20221214 00:04:25 @agent_ppo2.py:145][0m 632832 total steps have happened
[32m[20221214 00:04:25 @agent_ppo2.py:121][0m #------------------------ Iteration 4309 --------------------------#
[32m[20221214 00:04:25 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:04:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:04:25 @agent_ppo2.py:185][0m |           0.0013 |          69.0987 |          12.0059 |
[32m[20221214 00:04:25 @agent_ppo2.py:185][0m |          -0.0079 |          64.6820 |          12.0181 |
[32m[20221214 00:04:25 @agent_ppo2.py:185][0m |          -0.0056 |          62.8608 |          12.0123 |
[32m[20221214 00:04:25 @agent_ppo2.py:185][0m |          -0.0056 |          62.0102 |          12.0198 |
[32m[20221214 00:04:26 @agent_ppo2.py:185][0m |          -0.0067 |          61.3851 |          12.0239 |
[32m[20221214 00:04:26 @agent_ppo2.py:185][0m |          -0.0133 |          61.0004 |          12.0188 |
[32m[20221214 00:04:26 @agent_ppo2.py:185][0m |          -0.0063 |          60.7309 |          12.0404 |
[32m[20221214 00:04:26 @agent_ppo2.py:185][0m |          -0.0094 |          60.3638 |          12.0486 |
[32m[20221214 00:04:26 @agent_ppo2.py:185][0m |          -0.0123 |          60.0640 |          12.0957 |
[32m[20221214 00:04:26 @agent_ppo2.py:185][0m |          -0.0140 |          60.0699 |          12.1201 |
[32m[20221214 00:04:26 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:04:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 497.19
[32m[20221214 00:04:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 545.03
[32m[20221214 00:04:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 415.33
[32m[20221214 00:04:26 @agent_ppo2.py:143][0m Total time:       6.93 min
[32m[20221214 00:04:26 @agent_ppo2.py:145][0m 634880 total steps have happened
[32m[20221214 00:04:26 @agent_ppo2.py:121][0m #------------------------ Iteration 4310 --------------------------#
[32m[20221214 00:04:26 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:04:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:04:26 @agent_ppo2.py:185][0m |           0.0021 |          55.0822 |          11.8674 |
[32m[20221214 00:04:27 @agent_ppo2.py:185][0m |          -0.0023 |          48.3876 |          11.8755 |
[32m[20221214 00:04:27 @agent_ppo2.py:185][0m |          -0.0073 |          46.2715 |          11.8398 |
[32m[20221214 00:04:27 @agent_ppo2.py:185][0m |          -0.0114 |          45.1299 |          11.8712 |
[32m[20221214 00:04:27 @agent_ppo2.py:185][0m |          -0.0099 |          44.3535 |          11.8191 |
[32m[20221214 00:04:27 @agent_ppo2.py:185][0m |          -0.0162 |          43.9855 |          11.8809 |
[32m[20221214 00:04:27 @agent_ppo2.py:185][0m |          -0.0155 |          43.5280 |          11.8436 |
[32m[20221214 00:04:27 @agent_ppo2.py:185][0m |          -0.0118 |          43.0968 |          11.8411 |
[32m[20221214 00:04:27 @agent_ppo2.py:185][0m |          -0.0121 |          42.8082 |          11.8420 |
[32m[20221214 00:04:27 @agent_ppo2.py:185][0m |          -0.0187 |          42.6539 |          11.8358 |
[32m[20221214 00:04:27 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:04:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 483.72
[32m[20221214 00:04:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 525.46
[32m[20221214 00:04:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 527.16
[32m[20221214 00:04:27 @agent_ppo2.py:143][0m Total time:       6.95 min
[32m[20221214 00:04:27 @agent_ppo2.py:145][0m 636928 total steps have happened
[32m[20221214 00:04:27 @agent_ppo2.py:121][0m #------------------------ Iteration 4311 --------------------------#
[32m[20221214 00:04:28 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:04:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:04:28 @agent_ppo2.py:185][0m |           0.0034 |          60.5779 |          12.2961 |
[32m[20221214 00:04:28 @agent_ppo2.py:185][0m |          -0.0110 |          55.2302 |          12.2923 |
[32m[20221214 00:04:28 @agent_ppo2.py:185][0m |          -0.0078 |          53.1271 |          12.3335 |
[32m[20221214 00:04:28 @agent_ppo2.py:185][0m |          -0.0100 |          51.9940 |          12.3235 |
[32m[20221214 00:04:28 @agent_ppo2.py:185][0m |          -0.0093 |          51.3671 |          12.2909 |
[32m[20221214 00:04:28 @agent_ppo2.py:185][0m |          -0.0071 |          50.9208 |          12.3046 |
[32m[20221214 00:04:28 @agent_ppo2.py:185][0m |          -0.0110 |          50.3308 |          12.3112 |
[32m[20221214 00:04:28 @agent_ppo2.py:185][0m |          -0.0109 |          50.0026 |          12.3135 |
[32m[20221214 00:04:28 @agent_ppo2.py:185][0m |          -0.0047 |          51.1413 |          12.3103 |
[32m[20221214 00:04:29 @agent_ppo2.py:185][0m |          -0.0155 |          49.6372 |          12.3311 |
[32m[20221214 00:04:29 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:04:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 369.18
[32m[20221214 00:04:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 496.48
[32m[20221214 00:04:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 145.67
[32m[20221214 00:04:29 @agent_ppo2.py:143][0m Total time:       6.97 min
[32m[20221214 00:04:29 @agent_ppo2.py:145][0m 638976 total steps have happened
[32m[20221214 00:04:29 @agent_ppo2.py:121][0m #------------------------ Iteration 4312 --------------------------#
[32m[20221214 00:04:29 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:04:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:04:29 @agent_ppo2.py:185][0m |          -0.0029 |          66.3137 |          11.9656 |
[32m[20221214 00:04:29 @agent_ppo2.py:185][0m |          -0.0079 |          60.5238 |          11.9251 |
[32m[20221214 00:04:29 @agent_ppo2.py:185][0m |          -0.0137 |          58.9215 |          11.9490 |
[32m[20221214 00:04:29 @agent_ppo2.py:185][0m |          -0.0108 |          57.6027 |          11.9671 |
[32m[20221214 00:04:29 @agent_ppo2.py:185][0m |          -0.0107 |          56.8122 |          11.9837 |
[32m[20221214 00:04:29 @agent_ppo2.py:185][0m |          -0.0144 |          56.2499 |          11.9716 |
[32m[20221214 00:04:30 @agent_ppo2.py:185][0m |          -0.0143 |          56.0692 |          11.9722 |
[32m[20221214 00:04:30 @agent_ppo2.py:185][0m |          -0.0134 |          55.2991 |          11.9813 |
[32m[20221214 00:04:30 @agent_ppo2.py:185][0m |          -0.0135 |          55.0572 |          11.9554 |
[32m[20221214 00:04:30 @agent_ppo2.py:185][0m |          -0.0141 |          55.2325 |          11.9833 |
[32m[20221214 00:04:30 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:04:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 461.23
[32m[20221214 00:04:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 522.92
[32m[20221214 00:04:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 421.25
[32m[20221214 00:04:30 @agent_ppo2.py:143][0m Total time:       6.99 min
[32m[20221214 00:04:30 @agent_ppo2.py:145][0m 641024 total steps have happened
[32m[20221214 00:04:30 @agent_ppo2.py:121][0m #------------------------ Iteration 4313 --------------------------#
[32m[20221214 00:04:30 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:04:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:04:30 @agent_ppo2.py:185][0m |           0.0040 |          45.3728 |          12.1935 |
[32m[20221214 00:04:30 @agent_ppo2.py:185][0m |          -0.0070 |          39.3225 |          12.1506 |
[32m[20221214 00:04:31 @agent_ppo2.py:185][0m |          -0.0138 |          37.9954 |          12.2024 |
[32m[20221214 00:04:31 @agent_ppo2.py:185][0m |          -0.0167 |          37.2300 |          12.1753 |
[32m[20221214 00:04:31 @agent_ppo2.py:185][0m |          -0.0143 |          36.5587 |          12.1917 |
[32m[20221214 00:04:31 @agent_ppo2.py:185][0m |          -0.0153 |          36.4980 |          12.2065 |
[32m[20221214 00:04:31 @agent_ppo2.py:185][0m |           0.0006 |          43.9848 |          12.1965 |
[32m[20221214 00:04:31 @agent_ppo2.py:185][0m |          -0.0137 |          35.3762 |          12.2168 |
[32m[20221214 00:04:31 @agent_ppo2.py:185][0m |          -0.0221 |          35.0173 |          12.2349 |
[32m[20221214 00:04:31 @agent_ppo2.py:185][0m |          -0.0205 |          34.5518 |          12.2042 |
[32m[20221214 00:04:31 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221214 00:04:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 356.07
[32m[20221214 00:04:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 447.27
[32m[20221214 00:04:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 541.11
[32m[20221214 00:04:32 @agent_ppo2.py:143][0m Total time:       7.02 min
[32m[20221214 00:04:32 @agent_ppo2.py:145][0m 643072 total steps have happened
[32m[20221214 00:04:32 @agent_ppo2.py:121][0m #------------------------ Iteration 4314 --------------------------#
[32m[20221214 00:04:32 @agent_ppo2.py:127][0m Sampling time: 0.24 s by 5 slaves
[32m[20221214 00:04:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:04:32 @agent_ppo2.py:185][0m |           0.0017 |          35.9306 |          12.2291 |
[32m[20221214 00:04:32 @agent_ppo2.py:185][0m |          -0.0066 |          31.3744 |          12.2540 |
[32m[20221214 00:04:32 @agent_ppo2.py:185][0m |          -0.0106 |          30.0541 |          12.2023 |
[32m[20221214 00:04:32 @agent_ppo2.py:185][0m |          -0.0156 |          29.1514 |          12.2344 |
[32m[20221214 00:04:33 @agent_ppo2.py:185][0m |          -0.0128 |          28.4719 |          12.2228 |
[32m[20221214 00:04:33 @agent_ppo2.py:185][0m |          -0.0190 |          28.4161 |          12.2559 |
[32m[20221214 00:04:33 @agent_ppo2.py:185][0m |          -0.0147 |          27.6926 |          12.2320 |
[32m[20221214 00:04:33 @agent_ppo2.py:185][0m |          -0.0159 |          27.2643 |          12.2195 |
[32m[20221214 00:04:33 @agent_ppo2.py:185][0m |          -0.0181 |          27.0103 |          12.2328 |
[32m[20221214 00:04:33 @agent_ppo2.py:185][0m |          -0.0146 |          26.8628 |          12.2298 |
[32m[20221214 00:04:33 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:04:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 380.45
[32m[20221214 00:04:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 437.72
[32m[20221214 00:04:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 491.92
[32m[20221214 00:04:33 @agent_ppo2.py:143][0m Total time:       7.05 min
[32m[20221214 00:04:33 @agent_ppo2.py:145][0m 645120 total steps have happened
[32m[20221214 00:04:33 @agent_ppo2.py:121][0m #------------------------ Iteration 4315 --------------------------#
[32m[20221214 00:04:33 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:04:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:04:33 @agent_ppo2.py:185][0m |           0.0014 |          41.5929 |          11.8220 |
[32m[20221214 00:04:34 @agent_ppo2.py:185][0m |          -0.0020 |          36.8148 |          11.8063 |
[32m[20221214 00:04:34 @agent_ppo2.py:185][0m |          -0.0053 |          35.3061 |          11.8328 |
[32m[20221214 00:04:34 @agent_ppo2.py:185][0m |          -0.0113 |          34.1509 |          11.7967 |
[32m[20221214 00:04:34 @agent_ppo2.py:185][0m |          -0.0143 |          33.2440 |          11.8020 |
[32m[20221214 00:04:34 @agent_ppo2.py:185][0m |          -0.0168 |          32.6761 |          11.7880 |
[32m[20221214 00:04:34 @agent_ppo2.py:185][0m |          -0.0142 |          32.3217 |          11.8090 |
[32m[20221214 00:04:34 @agent_ppo2.py:185][0m |          -0.0141 |          31.5644 |          11.7925 |
[32m[20221214 00:04:34 @agent_ppo2.py:185][0m |          -0.0174 |          31.2112 |          11.7907 |
[32m[20221214 00:04:34 @agent_ppo2.py:185][0m |          -0.0232 |          31.2595 |          11.7895 |
[32m[20221214 00:04:34 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:04:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 386.26
[32m[20221214 00:04:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 441.55
[32m[20221214 00:04:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 453.31
[32m[20221214 00:04:34 @agent_ppo2.py:143][0m Total time:       7.07 min
[32m[20221214 00:04:34 @agent_ppo2.py:145][0m 647168 total steps have happened
[32m[20221214 00:04:34 @agent_ppo2.py:121][0m #------------------------ Iteration 4316 --------------------------#
[32m[20221214 00:04:35 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:04:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:04:35 @agent_ppo2.py:185][0m |          -0.0011 |          36.4432 |          12.0633 |
[32m[20221214 00:04:35 @agent_ppo2.py:185][0m |          -0.0056 |          33.0134 |          12.0321 |
[32m[20221214 00:04:35 @agent_ppo2.py:185][0m |          -0.0133 |          30.8031 |          12.0292 |
[32m[20221214 00:04:35 @agent_ppo2.py:185][0m |          -0.0068 |          32.1791 |          12.0222 |
[32m[20221214 00:04:35 @agent_ppo2.py:185][0m |          -0.0131 |          28.9070 |          11.9997 |
[32m[20221214 00:04:35 @agent_ppo2.py:185][0m |          -0.0147 |          28.3560 |          11.9743 |
[32m[20221214 00:04:35 @agent_ppo2.py:185][0m |          -0.0060 |          27.8896 |          11.9751 |
[32m[20221214 00:04:35 @agent_ppo2.py:185][0m |          -0.0178 |          27.4410 |          11.9531 |
[32m[20221214 00:04:36 @agent_ppo2.py:185][0m |          -0.0168 |          26.9358 |          11.9706 |
[32m[20221214 00:04:36 @agent_ppo2.py:185][0m |          -0.0177 |          26.4882 |          11.9412 |
[32m[20221214 00:04:36 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:04:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 346.63
[32m[20221214 00:04:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 422.91
[32m[20221214 00:04:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 511.77
[32m[20221214 00:04:36 @agent_ppo2.py:143][0m Total time:       7.09 min
[32m[20221214 00:04:36 @agent_ppo2.py:145][0m 649216 total steps have happened
[32m[20221214 00:04:36 @agent_ppo2.py:121][0m #------------------------ Iteration 4317 --------------------------#
[32m[20221214 00:04:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:04:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:04:36 @agent_ppo2.py:185][0m |          -0.0009 |          74.2297 |          11.7964 |
[32m[20221214 00:04:36 @agent_ppo2.py:185][0m |          -0.0045 |          72.3026 |          11.8021 |
[32m[20221214 00:04:36 @agent_ppo2.py:185][0m |          -0.0052 |          71.4545 |          11.8033 |
[32m[20221214 00:04:36 @agent_ppo2.py:185][0m |          -0.0090 |          71.0076 |          11.8277 |
[32m[20221214 00:04:37 @agent_ppo2.py:185][0m |           0.0012 |          76.3076 |          11.8146 |
[32m[20221214 00:04:37 @agent_ppo2.py:185][0m |          -0.0061 |          70.8865 |          11.7699 |
[32m[20221214 00:04:37 @agent_ppo2.py:185][0m |          -0.0093 |          70.9344 |          11.7865 |
[32m[20221214 00:04:37 @agent_ppo2.py:185][0m |          -0.0097 |          70.4245 |          11.8795 |
[32m[20221214 00:04:37 @agent_ppo2.py:185][0m |          -0.0109 |          70.0439 |          11.8479 |
[32m[20221214 00:04:37 @agent_ppo2.py:185][0m |          -0.0052 |          71.7979 |          11.8737 |
[32m[20221214 00:04:37 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:04:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 470.54
[32m[20221214 00:04:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 510.09
[32m[20221214 00:04:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 164.76
[32m[20221214 00:04:37 @agent_ppo2.py:143][0m Total time:       7.11 min
[32m[20221214 00:04:37 @agent_ppo2.py:145][0m 651264 total steps have happened
[32m[20221214 00:04:37 @agent_ppo2.py:121][0m #------------------------ Iteration 4318 --------------------------#
[32m[20221214 00:04:37 @agent_ppo2.py:127][0m Sampling time: 0.23 s by 5 slaves
[32m[20221214 00:04:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:04:38 @agent_ppo2.py:185][0m |          -0.0041 |          58.3819 |          11.7161 |
[32m[20221214 00:04:38 @agent_ppo2.py:185][0m |          -0.0025 |          55.2725 |          11.7225 |
[32m[20221214 00:04:38 @agent_ppo2.py:185][0m |           0.0011 |          55.8886 |          11.7142 |
[32m[20221214 00:04:38 @agent_ppo2.py:185][0m |          -0.0052 |          54.0424 |          11.6951 |
[32m[20221214 00:04:38 @agent_ppo2.py:185][0m |          -0.0006 |          56.7943 |          11.7225 |
[32m[20221214 00:04:38 @agent_ppo2.py:185][0m |          -0.0100 |          52.8708 |          11.6991 |
[32m[20221214 00:04:38 @agent_ppo2.py:185][0m |          -0.0094 |          52.6254 |          11.6776 |
[32m[20221214 00:04:38 @agent_ppo2.py:185][0m |          -0.0112 |          52.3749 |          11.6604 |
[32m[20221214 00:04:38 @agent_ppo2.py:185][0m |          -0.0142 |          52.1337 |          11.6757 |
[32m[20221214 00:04:38 @agent_ppo2.py:185][0m |          -0.0128 |          52.0564 |          11.6382 |
[32m[20221214 00:04:38 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:04:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 470.13
[32m[20221214 00:04:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 504.30
[32m[20221214 00:04:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 142.10
[32m[20221214 00:04:39 @agent_ppo2.py:143][0m Total time:       7.14 min
[32m[20221214 00:04:39 @agent_ppo2.py:145][0m 653312 total steps have happened
[32m[20221214 00:04:39 @agent_ppo2.py:121][0m #------------------------ Iteration 4319 --------------------------#
[32m[20221214 00:04:39 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:04:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:04:39 @agent_ppo2.py:185][0m |          -0.0020 |          46.1994 |          11.9770 |
[32m[20221214 00:04:39 @agent_ppo2.py:185][0m |          -0.0062 |          40.8350 |          11.9529 |
[32m[20221214 00:04:39 @agent_ppo2.py:185][0m |          -0.0097 |          38.3385 |          11.9524 |
[32m[20221214 00:04:39 @agent_ppo2.py:185][0m |          -0.0067 |          36.9781 |          11.9467 |
[32m[20221214 00:04:39 @agent_ppo2.py:185][0m |          -0.0197 |          35.6483 |          11.9295 |
[32m[20221214 00:04:39 @agent_ppo2.py:185][0m |          -0.0140 |          34.7261 |          11.9070 |
[32m[20221214 00:04:39 @agent_ppo2.py:185][0m |          -0.0159 |          33.7395 |          11.8997 |
[32m[20221214 00:04:40 @agent_ppo2.py:185][0m |          -0.0207 |          33.1144 |          11.8604 |
[32m[20221214 00:04:40 @agent_ppo2.py:185][0m |          -0.0173 |          32.2952 |          11.8410 |
[32m[20221214 00:04:40 @agent_ppo2.py:185][0m |          -0.0152 |          31.8434 |          11.8454 |
[32m[20221214 00:04:40 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:04:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 317.65
[32m[20221214 00:04:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 498.02
[32m[20221214 00:04:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 503.22
[32m[20221214 00:04:40 @agent_ppo2.py:143][0m Total time:       7.16 min
[32m[20221214 00:04:40 @agent_ppo2.py:145][0m 655360 total steps have happened
[32m[20221214 00:04:40 @agent_ppo2.py:121][0m #------------------------ Iteration 4320 --------------------------#
[32m[20221214 00:04:40 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:04:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:04:40 @agent_ppo2.py:185][0m |           0.0023 |          58.5188 |          11.6510 |
[32m[20221214 00:04:40 @agent_ppo2.py:185][0m |          -0.0049 |          55.0537 |          11.6774 |
[32m[20221214 00:04:40 @agent_ppo2.py:185][0m |          -0.0020 |          54.0314 |          11.6558 |
[32m[20221214 00:04:41 @agent_ppo2.py:185][0m |          -0.0052 |          53.0883 |          11.6410 |
[32m[20221214 00:04:41 @agent_ppo2.py:185][0m |          -0.0114 |          52.6358 |          11.6637 |
[32m[20221214 00:04:41 @agent_ppo2.py:185][0m |          -0.0164 |          52.2234 |          11.6982 |
[32m[20221214 00:04:41 @agent_ppo2.py:185][0m |          -0.0116 |          51.8908 |          11.6705 |
[32m[20221214 00:04:41 @agent_ppo2.py:185][0m |          -0.0150 |          51.7166 |          11.6892 |
[32m[20221214 00:04:41 @agent_ppo2.py:185][0m |          -0.0119 |          52.2539 |          11.6781 |
[32m[20221214 00:04:41 @agent_ppo2.py:185][0m |          -0.0113 |          51.5746 |          11.6659 |
[32m[20221214 00:04:41 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:04:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 484.83
[32m[20221214 00:04:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 531.53
[32m[20221214 00:04:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 361.52
[32m[20221214 00:04:41 @agent_ppo2.py:143][0m Total time:       7.18 min
[32m[20221214 00:04:41 @agent_ppo2.py:145][0m 657408 total steps have happened
[32m[20221214 00:04:41 @agent_ppo2.py:121][0m #------------------------ Iteration 4321 --------------------------#
[32m[20221214 00:04:41 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:04:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:04:42 @agent_ppo2.py:185][0m |           0.0037 |          75.9238 |          11.6793 |
[32m[20221214 00:04:42 @agent_ppo2.py:185][0m |          -0.0093 |          68.7714 |          11.6786 |
[32m[20221214 00:04:42 @agent_ppo2.py:185][0m |          -0.0005 |          69.0188 |          11.7191 |
[32m[20221214 00:04:42 @agent_ppo2.py:185][0m |          -0.0136 |          65.4714 |          11.7091 |
[32m[20221214 00:04:42 @agent_ppo2.py:185][0m |          -0.0158 |          65.1256 |          11.6973 |
[32m[20221214 00:04:42 @agent_ppo2.py:185][0m |          -0.0069 |          64.8785 |          11.7056 |
[32m[20221214 00:04:42 @agent_ppo2.py:185][0m |          -0.0139 |          63.8219 |          11.7323 |
[32m[20221214 00:04:42 @agent_ppo2.py:185][0m |          -0.0112 |          62.9741 |          11.6988 |
[32m[20221214 00:04:42 @agent_ppo2.py:185][0m |          -0.0151 |          62.5480 |          11.7007 |
[32m[20221214 00:04:42 @agent_ppo2.py:185][0m |          -0.0165 |          62.1719 |          11.7278 |
[32m[20221214 00:04:42 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:04:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 398.51
[32m[20221214 00:04:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 453.75
[32m[20221214 00:04:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 494.76
[32m[20221214 00:04:43 @agent_ppo2.py:143][0m Total time:       7.20 min
[32m[20221214 00:04:43 @agent_ppo2.py:145][0m 659456 total steps have happened
[32m[20221214 00:04:43 @agent_ppo2.py:121][0m #------------------------ Iteration 4322 --------------------------#
[32m[20221214 00:04:43 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:04:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:04:43 @agent_ppo2.py:185][0m |          -0.0015 |          45.8253 |          11.8874 |
[32m[20221214 00:04:43 @agent_ppo2.py:185][0m |           0.0013 |          44.3496 |          11.9597 |
[32m[20221214 00:04:43 @agent_ppo2.py:185][0m |          -0.0099 |          35.9680 |          11.9152 |
[32m[20221214 00:04:43 @agent_ppo2.py:185][0m |          -0.0144 |          34.4545 |          11.9341 |
[32m[20221214 00:04:43 @agent_ppo2.py:185][0m |          -0.0092 |          35.9236 |          11.9667 |
[32m[20221214 00:04:43 @agent_ppo2.py:185][0m |          -0.0211 |          32.3878 |          11.9815 |
[32m[20221214 00:04:44 @agent_ppo2.py:185][0m |          -0.0106 |          35.5945 |          11.9292 |
[32m[20221214 00:04:44 @agent_ppo2.py:185][0m |          -0.0207 |          31.3738 |          11.9636 |
[32m[20221214 00:04:44 @agent_ppo2.py:185][0m |          -0.0149 |          33.4780 |          11.9710 |
[32m[20221214 00:04:44 @agent_ppo2.py:185][0m |          -0.0203 |          30.4819 |          11.9520 |
[32m[20221214 00:04:44 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:04:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 394.14
[32m[20221214 00:04:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 469.80
[32m[20221214 00:04:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 523.02
[32m[20221214 00:04:44 @agent_ppo2.py:143][0m Total time:       7.23 min
[32m[20221214 00:04:44 @agent_ppo2.py:145][0m 661504 total steps have happened
[32m[20221214 00:04:44 @agent_ppo2.py:121][0m #------------------------ Iteration 4323 --------------------------#
[32m[20221214 00:04:44 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:04:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:04:44 @agent_ppo2.py:185][0m |           0.0012 |          46.7706 |          11.6260 |
[32m[20221214 00:04:44 @agent_ppo2.py:185][0m |          -0.0084 |          41.9561 |          11.6178 |
[32m[20221214 00:04:45 @agent_ppo2.py:185][0m |          -0.0066 |          40.7598 |          11.6244 |
[32m[20221214 00:04:45 @agent_ppo2.py:185][0m |          -0.0132 |          40.0549 |          11.5812 |
[32m[20221214 00:04:45 @agent_ppo2.py:185][0m |          -0.0088 |          39.3853 |          11.6102 |
[32m[20221214 00:04:45 @agent_ppo2.py:185][0m |          -0.0128 |          38.4368 |          11.5933 |
[32m[20221214 00:04:45 @agent_ppo2.py:185][0m |          -0.0090 |          38.2006 |          11.6323 |
[32m[20221214 00:04:45 @agent_ppo2.py:185][0m |          -0.0147 |          37.7314 |          11.6247 |
[32m[20221214 00:04:45 @agent_ppo2.py:185][0m |          -0.0197 |          37.2858 |          11.6221 |
[32m[20221214 00:04:45 @agent_ppo2.py:185][0m |          -0.0077 |          40.2589 |          11.6281 |
[32m[20221214 00:04:45 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:04:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 367.57
[32m[20221214 00:04:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 474.33
[32m[20221214 00:04:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 392.92
[32m[20221214 00:04:45 @agent_ppo2.py:143][0m Total time:       7.25 min
[32m[20221214 00:04:45 @agent_ppo2.py:145][0m 663552 total steps have happened
[32m[20221214 00:04:45 @agent_ppo2.py:121][0m #------------------------ Iteration 4324 --------------------------#
[32m[20221214 00:04:46 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:04:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:04:46 @agent_ppo2.py:185][0m |           0.0010 |          48.2082 |          11.5242 |
[32m[20221214 00:04:46 @agent_ppo2.py:185][0m |          -0.0094 |          43.5465 |          11.5527 |
[32m[20221214 00:04:46 @agent_ppo2.py:185][0m |          -0.0083 |          41.9670 |          11.5177 |
[32m[20221214 00:04:46 @agent_ppo2.py:185][0m |          -0.0119 |          40.9372 |          11.5007 |
[32m[20221214 00:04:46 @agent_ppo2.py:185][0m |          -0.0111 |          40.2236 |          11.5496 |
[32m[20221214 00:04:46 @agent_ppo2.py:185][0m |          -0.0168 |          39.6215 |          11.5314 |
[32m[20221214 00:04:46 @agent_ppo2.py:185][0m |          -0.0140 |          39.2683 |          11.5059 |
[32m[20221214 00:04:46 @agent_ppo2.py:185][0m |          -0.0193 |          39.0270 |          11.5163 |
[32m[20221214 00:04:46 @agent_ppo2.py:185][0m |          -0.0175 |          38.9379 |          11.5225 |
[32m[20221214 00:04:47 @agent_ppo2.py:185][0m |          -0.0191 |          38.1807 |          11.4893 |
[32m[20221214 00:04:47 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:04:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 402.23
[32m[20221214 00:04:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 514.30
[32m[20221214 00:04:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 414.18
[32m[20221214 00:04:47 @agent_ppo2.py:143][0m Total time:       7.27 min
[32m[20221214 00:04:47 @agent_ppo2.py:145][0m 665600 total steps have happened
[32m[20221214 00:04:47 @agent_ppo2.py:121][0m #------------------------ Iteration 4325 --------------------------#
[32m[20221214 00:04:47 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:04:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:04:47 @agent_ppo2.py:185][0m |          -0.0010 |          67.0322 |          11.5718 |
[32m[20221214 00:04:47 @agent_ppo2.py:185][0m |          -0.0073 |          61.9431 |          11.5749 |
[32m[20221214 00:04:47 @agent_ppo2.py:185][0m |          -0.0057 |          60.9401 |          11.6256 |
[32m[20221214 00:04:47 @agent_ppo2.py:185][0m |          -0.0124 |          58.6157 |          11.6412 |
[32m[20221214 00:04:47 @agent_ppo2.py:185][0m |          -0.0135 |          58.0896 |          11.6432 |
[32m[20221214 00:04:48 @agent_ppo2.py:185][0m |          -0.0114 |          57.6856 |          11.6433 |
[32m[20221214 00:04:48 @agent_ppo2.py:185][0m |          -0.0164 |          56.8142 |          11.6576 |
[32m[20221214 00:04:48 @agent_ppo2.py:185][0m |          -0.0167 |          56.5284 |          11.6456 |
[32m[20221214 00:04:48 @agent_ppo2.py:185][0m |          -0.0154 |          56.3710 |          11.6693 |
[32m[20221214 00:04:48 @agent_ppo2.py:185][0m |          -0.0176 |          55.9846 |          11.6880 |
[32m[20221214 00:04:48 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:04:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 461.19
[32m[20221214 00:04:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 510.93
[32m[20221214 00:04:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 561.72
[32m[20221214 00:04:48 @agent_ppo2.py:143][0m Total time:       7.29 min
[32m[20221214 00:04:48 @agent_ppo2.py:145][0m 667648 total steps have happened
[32m[20221214 00:04:48 @agent_ppo2.py:121][0m #------------------------ Iteration 4326 --------------------------#
[32m[20221214 00:04:48 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:04:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:04:48 @agent_ppo2.py:185][0m |          -0.0030 |          60.3577 |          12.0526 |
[32m[20221214 00:04:48 @agent_ppo2.py:185][0m |          -0.0078 |          52.6671 |          12.0766 |
[32m[20221214 00:04:49 @agent_ppo2.py:185][0m |          -0.0118 |          49.9770 |          12.0981 |
[32m[20221214 00:04:49 @agent_ppo2.py:185][0m |          -0.0159 |          48.4772 |          12.1306 |
[32m[20221214 00:04:49 @agent_ppo2.py:185][0m |          -0.0182 |          47.2621 |          12.1288 |
[32m[20221214 00:04:49 @agent_ppo2.py:185][0m |          -0.0184 |          46.6933 |          12.1557 |
[32m[20221214 00:04:49 @agent_ppo2.py:185][0m |          -0.0134 |          46.5664 |          12.1741 |
[32m[20221214 00:04:49 @agent_ppo2.py:185][0m |          -0.0191 |          46.0675 |          12.1414 |
[32m[20221214 00:04:49 @agent_ppo2.py:185][0m |          -0.0210 |          45.6078 |          12.1329 |
[32m[20221214 00:04:49 @agent_ppo2.py:185][0m |          -0.0194 |          45.4022 |          12.1626 |
[32m[20221214 00:04:49 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:04:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 472.05
[32m[20221214 00:04:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 563.15
[32m[20221214 00:04:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 531.69
[32m[20221214 00:04:49 @agent_ppo2.py:143][0m Total time:       7.32 min
[32m[20221214 00:04:49 @agent_ppo2.py:145][0m 669696 total steps have happened
[32m[20221214 00:04:49 @agent_ppo2.py:121][0m #------------------------ Iteration 4327 --------------------------#
[32m[20221214 00:04:49 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:04:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:04:50 @agent_ppo2.py:185][0m |           0.0001 |          32.2734 |          11.7271 |
[32m[20221214 00:04:50 @agent_ppo2.py:185][0m |          -0.0094 |          26.4675 |          11.7251 |
[32m[20221214 00:04:50 @agent_ppo2.py:185][0m |          -0.0058 |          25.3088 |          11.7392 |
[32m[20221214 00:04:50 @agent_ppo2.py:185][0m |          -0.0147 |          24.5157 |          11.7764 |
[32m[20221214 00:04:50 @agent_ppo2.py:185][0m |          -0.0157 |          23.5642 |          11.7644 |
[32m[20221214 00:04:50 @agent_ppo2.py:185][0m |          -0.0081 |          23.4892 |          11.7725 |
[32m[20221214 00:04:50 @agent_ppo2.py:185][0m |          -0.0118 |          22.9905 |          11.8135 |
[32m[20221214 00:04:50 @agent_ppo2.py:185][0m |          -0.0193 |          22.3241 |          11.8100 |
[32m[20221214 00:04:50 @agent_ppo2.py:185][0m |          -0.0234 |          22.0983 |          11.8191 |
[32m[20221214 00:04:50 @agent_ppo2.py:185][0m |          -0.0156 |          21.9173 |          11.8333 |
[32m[20221214 00:04:50 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:04:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 360.64
[32m[20221214 00:04:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 458.38
[32m[20221214 00:04:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 509.32
[32m[20221214 00:04:51 @agent_ppo2.py:143][0m Total time:       7.34 min
[32m[20221214 00:04:51 @agent_ppo2.py:145][0m 671744 total steps have happened
[32m[20221214 00:04:51 @agent_ppo2.py:121][0m #------------------------ Iteration 4328 --------------------------#
[32m[20221214 00:04:51 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:04:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:04:51 @agent_ppo2.py:185][0m |           0.0010 |          57.9362 |          12.2809 |
[32m[20221214 00:04:51 @agent_ppo2.py:185][0m |          -0.0036 |          54.0760 |          12.3019 |
[32m[20221214 00:04:51 @agent_ppo2.py:185][0m |          -0.0096 |          53.4203 |          12.3092 |
[32m[20221214 00:04:51 @agent_ppo2.py:185][0m |          -0.0113 |          52.2028 |          12.3085 |
[32m[20221214 00:04:51 @agent_ppo2.py:185][0m |          -0.0029 |          54.6230 |          12.3143 |
[32m[20221214 00:04:51 @agent_ppo2.py:185][0m |          -0.0109 |          51.5541 |          12.3277 |
[32m[20221214 00:04:51 @agent_ppo2.py:185][0m |          -0.0135 |          50.1638 |          12.3171 |
[32m[20221214 00:04:52 @agent_ppo2.py:185][0m |          -0.0163 |          50.0714 |          12.3219 |
[32m[20221214 00:04:52 @agent_ppo2.py:185][0m |          -0.0152 |          50.6401 |          12.3576 |
[32m[20221214 00:04:52 @agent_ppo2.py:185][0m |          -0.0151 |          49.4863 |          12.3645 |
[32m[20221214 00:04:52 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:04:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 522.87
[32m[20221214 00:04:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 562.43
[32m[20221214 00:04:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 491.98
[32m[20221214 00:04:52 @agent_ppo2.py:143][0m Total time:       7.36 min
[32m[20221214 00:04:52 @agent_ppo2.py:145][0m 673792 total steps have happened
[32m[20221214 00:04:52 @agent_ppo2.py:121][0m #------------------------ Iteration 4329 --------------------------#
[32m[20221214 00:04:52 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:04:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:04:52 @agent_ppo2.py:185][0m |           0.0048 |          57.7699 |          11.7413 |
[32m[20221214 00:04:52 @agent_ppo2.py:185][0m |          -0.0076 |          51.6540 |          11.7588 |
[32m[20221214 00:04:52 @agent_ppo2.py:185][0m |          -0.0071 |          49.4038 |          11.7995 |
[32m[20221214 00:04:52 @agent_ppo2.py:185][0m |          -0.0124 |          48.3286 |          11.7897 |
[32m[20221214 00:04:53 @agent_ppo2.py:185][0m |          -0.0105 |          47.4726 |          11.8362 |
[32m[20221214 00:04:53 @agent_ppo2.py:185][0m |          -0.0150 |          46.6755 |          11.8398 |
[32m[20221214 00:04:53 @agent_ppo2.py:185][0m |          -0.0167 |          46.1814 |          11.8357 |
[32m[20221214 00:04:53 @agent_ppo2.py:185][0m |          -0.0208 |          45.8788 |          11.8605 |
[32m[20221214 00:04:53 @agent_ppo2.py:185][0m |          -0.0144 |          45.5992 |          11.8864 |
[32m[20221214 00:04:53 @agent_ppo2.py:185][0m |          -0.0190 |          44.9617 |          11.8942 |
[32m[20221214 00:04:53 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:04:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 355.30
[32m[20221214 00:04:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 460.46
[32m[20221214 00:04:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 514.77
[32m[20221214 00:04:53 @agent_ppo2.py:143][0m Total time:       7.38 min
[32m[20221214 00:04:53 @agent_ppo2.py:145][0m 675840 total steps have happened
[32m[20221214 00:04:53 @agent_ppo2.py:121][0m #------------------------ Iteration 4330 --------------------------#
[32m[20221214 00:04:53 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:04:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:04:54 @agent_ppo2.py:185][0m |          -0.0006 |          37.6124 |          12.1975 |
[32m[20221214 00:04:54 @agent_ppo2.py:185][0m |          -0.0052 |          34.1151 |          12.2088 |
[32m[20221214 00:04:54 @agent_ppo2.py:185][0m |          -0.0084 |          32.0882 |          12.1520 |
[32m[20221214 00:04:54 @agent_ppo2.py:185][0m |          -0.0108 |          31.6946 |          12.1505 |
[32m[20221214 00:04:54 @agent_ppo2.py:185][0m |          -0.0107 |          31.0337 |          12.1328 |
[32m[20221214 00:04:54 @agent_ppo2.py:185][0m |          -0.0145 |          30.2201 |          12.1442 |
[32m[20221214 00:04:54 @agent_ppo2.py:185][0m |          -0.0105 |          30.3605 |          12.1228 |
[32m[20221214 00:04:54 @agent_ppo2.py:185][0m |          -0.0155 |          29.6754 |          12.0884 |
[32m[20221214 00:04:54 @agent_ppo2.py:185][0m |          -0.0190 |          29.6024 |          12.0953 |
[32m[20221214 00:04:54 @agent_ppo2.py:185][0m |          -0.0101 |          31.7155 |          12.0702 |
[32m[20221214 00:04:54 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:04:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 396.28
[32m[20221214 00:04:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 503.50
[32m[20221214 00:04:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 328.04
[32m[20221214 00:04:54 @agent_ppo2.py:143][0m Total time:       7.40 min
[32m[20221214 00:04:54 @agent_ppo2.py:145][0m 677888 total steps have happened
[32m[20221214 00:04:54 @agent_ppo2.py:121][0m #------------------------ Iteration 4331 --------------------------#
[32m[20221214 00:04:55 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:04:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:04:55 @agent_ppo2.py:185][0m |           0.0024 |          77.0310 |          12.2252 |
[32m[20221214 00:04:55 @agent_ppo2.py:185][0m |          -0.0086 |          68.5857 |          12.2601 |
[32m[20221214 00:04:55 @agent_ppo2.py:185][0m |          -0.0104 |          65.0761 |          12.2874 |
[32m[20221214 00:04:55 @agent_ppo2.py:185][0m |          -0.0112 |          63.4393 |          12.2552 |
[32m[20221214 00:04:55 @agent_ppo2.py:185][0m |          -0.0103 |          63.6444 |          12.2765 |
[32m[20221214 00:04:55 @agent_ppo2.py:185][0m |          -0.0150 |          61.6081 |          12.2687 |
[32m[20221214 00:04:55 @agent_ppo2.py:185][0m |          -0.0168 |          61.0931 |          12.2191 |
[32m[20221214 00:04:55 @agent_ppo2.py:185][0m |          -0.0172 |          60.5557 |          12.2266 |
[32m[20221214 00:04:55 @agent_ppo2.py:185][0m |          -0.0168 |          60.0062 |          12.2224 |
[32m[20221214 00:04:56 @agent_ppo2.py:185][0m |          -0.0173 |          59.4190 |          12.2133 |
[32m[20221214 00:04:56 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:04:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 437.13
[32m[20221214 00:04:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 503.75
[32m[20221214 00:04:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 513.71
[32m[20221214 00:04:56 @agent_ppo2.py:143][0m Total time:       7.42 min
[32m[20221214 00:04:56 @agent_ppo2.py:145][0m 679936 total steps have happened
[32m[20221214 00:04:56 @agent_ppo2.py:121][0m #------------------------ Iteration 4332 --------------------------#
[32m[20221214 00:04:56 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:04:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:04:56 @agent_ppo2.py:185][0m |           0.0014 |          70.2499 |          12.2390 |
[32m[20221214 00:04:56 @agent_ppo2.py:185][0m |          -0.0089 |          63.8056 |          12.2400 |
[32m[20221214 00:04:56 @agent_ppo2.py:185][0m |          -0.0106 |          61.3386 |          12.2587 |
[32m[20221214 00:04:56 @agent_ppo2.py:185][0m |          -0.0131 |          60.1265 |          12.2807 |
[32m[20221214 00:04:56 @agent_ppo2.py:185][0m |          -0.0115 |          60.2355 |          12.2605 |
[32m[20221214 00:04:57 @agent_ppo2.py:185][0m |          -0.0135 |          58.6715 |          12.2658 |
[32m[20221214 00:04:57 @agent_ppo2.py:185][0m |          -0.0039 |          61.8295 |          12.2606 |
[32m[20221214 00:04:57 @agent_ppo2.py:185][0m |          -0.0164 |          57.4879 |          12.2783 |
[32m[20221214 00:04:57 @agent_ppo2.py:185][0m |          -0.0171 |          57.0890 |          12.2897 |
[32m[20221214 00:04:57 @agent_ppo2.py:185][0m |          -0.0235 |          56.6746 |          12.2755 |
[32m[20221214 00:04:57 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:04:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 405.60
[32m[20221214 00:04:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 474.31
[32m[20221214 00:04:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 543.64
[32m[20221214 00:04:57 @agent_ppo2.py:143][0m Total time:       7.44 min
[32m[20221214 00:04:57 @agent_ppo2.py:145][0m 681984 total steps have happened
[32m[20221214 00:04:57 @agent_ppo2.py:121][0m #------------------------ Iteration 4333 --------------------------#
[32m[20221214 00:04:57 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:04:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:04:57 @agent_ppo2.py:185][0m |           0.0013 |          73.2159 |          12.2540 |
[32m[20221214 00:04:57 @agent_ppo2.py:185][0m |          -0.0070 |          68.6101 |          12.2470 |
[32m[20221214 00:04:58 @agent_ppo2.py:185][0m |          -0.0085 |          67.6683 |          12.2056 |
[32m[20221214 00:04:58 @agent_ppo2.py:185][0m |          -0.0095 |          67.3696 |          12.2111 |
[32m[20221214 00:04:58 @agent_ppo2.py:185][0m |          -0.0069 |          67.0388 |          12.1736 |
[32m[20221214 00:04:58 @agent_ppo2.py:185][0m |          -0.0169 |          66.5358 |          12.1631 |
[32m[20221214 00:04:58 @agent_ppo2.py:185][0m |          -0.0115 |          66.2583 |          12.1512 |
[32m[20221214 00:04:58 @agent_ppo2.py:185][0m |          -0.0145 |          66.1447 |          12.1517 |
[32m[20221214 00:04:58 @agent_ppo2.py:185][0m |          -0.0105 |          66.1434 |          12.1482 |
[32m[20221214 00:04:58 @agent_ppo2.py:185][0m |          -0.0125 |          66.3473 |          12.1295 |
[32m[20221214 00:04:58 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:04:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 492.45
[32m[20221214 00:04:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 526.71
[32m[20221214 00:04:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 490.28
[32m[20221214 00:04:58 @agent_ppo2.py:143][0m Total time:       7.47 min
[32m[20221214 00:04:58 @agent_ppo2.py:145][0m 684032 total steps have happened
[32m[20221214 00:04:58 @agent_ppo2.py:121][0m #------------------------ Iteration 4334 --------------------------#
[32m[20221214 00:04:58 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:04:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:04:59 @agent_ppo2.py:185][0m |           0.0044 |          66.7138 |          12.1071 |
[32m[20221214 00:04:59 @agent_ppo2.py:185][0m |          -0.0053 |          64.1840 |          12.1102 |
[32m[20221214 00:04:59 @agent_ppo2.py:185][0m |          -0.0068 |          63.5833 |          12.0743 |
[32m[20221214 00:04:59 @agent_ppo2.py:185][0m |          -0.0050 |          64.1806 |          12.0609 |
[32m[20221214 00:04:59 @agent_ppo2.py:185][0m |          -0.0012 |          66.9816 |          12.0504 |
[32m[20221214 00:04:59 @agent_ppo2.py:185][0m |          -0.0083 |          62.5343 |          12.0471 |
[32m[20221214 00:04:59 @agent_ppo2.py:185][0m |          -0.0059 |          62.4333 |          12.0432 |
[32m[20221214 00:04:59 @agent_ppo2.py:185][0m |          -0.0021 |          65.0576 |          12.0364 |
[32m[20221214 00:04:59 @agent_ppo2.py:185][0m |          -0.0086 |          62.2990 |          12.0148 |
[32m[20221214 00:04:59 @agent_ppo2.py:185][0m |          -0.0143 |          61.4820 |          11.9907 |
[32m[20221214 00:04:59 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:05:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 491.98
[32m[20221214 00:05:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 508.15
[32m[20221214 00:05:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 510.94
[32m[20221214 00:05:00 @agent_ppo2.py:143][0m Total time:       7.49 min
[32m[20221214 00:05:00 @agent_ppo2.py:145][0m 686080 total steps have happened
[32m[20221214 00:05:00 @agent_ppo2.py:121][0m #------------------------ Iteration 4335 --------------------------#
[32m[20221214 00:05:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:05:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:05:00 @agent_ppo2.py:185][0m |           0.0065 |          77.8557 |          11.4986 |
[32m[20221214 00:05:00 @agent_ppo2.py:185][0m |          -0.0051 |          66.9578 |          11.5173 |
[32m[20221214 00:05:00 @agent_ppo2.py:185][0m |          -0.0084 |          65.0006 |          11.5199 |
[32m[20221214 00:05:00 @agent_ppo2.py:185][0m |          -0.0150 |          63.6420 |          11.5219 |
[32m[20221214 00:05:00 @agent_ppo2.py:185][0m |          -0.0098 |          62.4877 |          11.5760 |
[32m[20221214 00:05:00 @agent_ppo2.py:185][0m |          -0.0116 |          61.6303 |          11.5503 |
[32m[20221214 00:05:00 @agent_ppo2.py:185][0m |          -0.0011 |          71.0565 |          11.5625 |
[32m[20221214 00:05:01 @agent_ppo2.py:185][0m |          -0.0179 |          60.6990 |          11.6089 |
[32m[20221214 00:05:01 @agent_ppo2.py:185][0m |          -0.0133 |          59.8979 |          11.5595 |
[32m[20221214 00:05:01 @agent_ppo2.py:185][0m |          -0.0179 |          59.6385 |          11.6190 |
[32m[20221214 00:05:01 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:05:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 501.00
[32m[20221214 00:05:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 521.57
[32m[20221214 00:05:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 453.12
[32m[20221214 00:05:01 @agent_ppo2.py:143][0m Total time:       7.51 min
[32m[20221214 00:05:01 @agent_ppo2.py:145][0m 688128 total steps have happened
[32m[20221214 00:05:01 @agent_ppo2.py:121][0m #------------------------ Iteration 4336 --------------------------#
[32m[20221214 00:05:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:05:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:05:01 @agent_ppo2.py:185][0m |           0.0004 |          65.6093 |          12.0587 |
[32m[20221214 00:05:01 @agent_ppo2.py:185][0m |          -0.0090 |          62.9858 |          12.0781 |
[32m[20221214 00:05:01 @agent_ppo2.py:185][0m |           0.0014 |          64.3629 |          12.0815 |
[32m[20221214 00:05:01 @agent_ppo2.py:185][0m |          -0.0114 |          61.5763 |          12.0850 |
[32m[20221214 00:05:02 @agent_ppo2.py:185][0m |          -0.0121 |          60.8832 |          12.0648 |
[32m[20221214 00:05:02 @agent_ppo2.py:185][0m |          -0.0070 |          62.9058 |          12.0617 |
[32m[20221214 00:05:02 @agent_ppo2.py:185][0m |          -0.0074 |          61.1919 |          12.0777 |
[32m[20221214 00:05:02 @agent_ppo2.py:185][0m |          -0.0141 |          60.0272 |          12.0352 |
[32m[20221214 00:05:02 @agent_ppo2.py:185][0m |          -0.0008 |          63.2812 |          12.0641 |
[32m[20221214 00:05:02 @agent_ppo2.py:185][0m |          -0.0141 |          59.3834 |          12.0685 |
[32m[20221214 00:05:02 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:05:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 486.40
[32m[20221214 00:05:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 511.51
[32m[20221214 00:05:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 445.63
[32m[20221214 00:05:02 @agent_ppo2.py:143][0m Total time:       7.53 min
[32m[20221214 00:05:02 @agent_ppo2.py:145][0m 690176 total steps have happened
[32m[20221214 00:05:02 @agent_ppo2.py:121][0m #------------------------ Iteration 4337 --------------------------#
[32m[20221214 00:05:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:05:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:05:02 @agent_ppo2.py:185][0m |          -0.0011 |          75.1089 |          12.0289 |
[32m[20221214 00:05:03 @agent_ppo2.py:185][0m |          -0.0069 |          72.2048 |          12.0271 |
[32m[20221214 00:05:03 @agent_ppo2.py:185][0m |          -0.0078 |          71.2973 |          12.0389 |
[32m[20221214 00:05:03 @agent_ppo2.py:185][0m |          -0.0113 |          70.4280 |          11.9997 |
[32m[20221214 00:05:03 @agent_ppo2.py:185][0m |          -0.0085 |          70.4416 |          12.0589 |
[32m[20221214 00:05:03 @agent_ppo2.py:185][0m |          -0.0093 |          70.2062 |          12.0436 |
[32m[20221214 00:05:03 @agent_ppo2.py:185][0m |          -0.0134 |          69.2190 |          12.0711 |
[32m[20221214 00:05:03 @agent_ppo2.py:185][0m |          -0.0135 |          68.7168 |          12.0471 |
[32m[20221214 00:05:03 @agent_ppo2.py:185][0m |          -0.0149 |          68.5075 |          12.0479 |
[32m[20221214 00:05:03 @agent_ppo2.py:185][0m |          -0.0158 |          68.4836 |          12.0626 |
[32m[20221214 00:05:03 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:05:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 517.87
[32m[20221214 00:05:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 559.55
[32m[20221214 00:05:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 517.20
[32m[20221214 00:05:03 @agent_ppo2.py:143][0m Total time:       7.55 min
[32m[20221214 00:05:03 @agent_ppo2.py:145][0m 692224 total steps have happened
[32m[20221214 00:05:03 @agent_ppo2.py:121][0m #------------------------ Iteration 4338 --------------------------#
[32m[20221214 00:05:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:05:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:05:04 @agent_ppo2.py:185][0m |          -0.0018 |          75.8402 |          12.0035 |
[32m[20221214 00:05:04 @agent_ppo2.py:185][0m |          -0.0024 |          75.1490 |          11.9906 |
[32m[20221214 00:05:04 @agent_ppo2.py:185][0m |          -0.0100 |          73.9625 |          11.9786 |
[32m[20221214 00:05:04 @agent_ppo2.py:185][0m |          -0.0106 |          73.5030 |          11.9995 |
[32m[20221214 00:05:04 @agent_ppo2.py:185][0m |          -0.0128 |          73.3859 |          12.0386 |
[32m[20221214 00:05:04 @agent_ppo2.py:185][0m |          -0.0125 |          73.0958 |          12.0218 |
[32m[20221214 00:05:04 @agent_ppo2.py:185][0m |          -0.0106 |          74.0692 |          12.0297 |
[32m[20221214 00:05:04 @agent_ppo2.py:185][0m |          -0.0144 |          72.7119 |          12.0416 |
[32m[20221214 00:05:04 @agent_ppo2.py:185][0m |          -0.0149 |          72.5900 |          12.0811 |
[32m[20221214 00:05:05 @agent_ppo2.py:185][0m |          -0.0141 |          72.3783 |          12.0666 |
[32m[20221214 00:05:05 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:05:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 484.54
[32m[20221214 00:05:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 507.44
[32m[20221214 00:05:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 541.19
[32m[20221214 00:05:05 @agent_ppo2.py:143][0m Total time:       7.57 min
[32m[20221214 00:05:05 @agent_ppo2.py:145][0m 694272 total steps have happened
[32m[20221214 00:05:05 @agent_ppo2.py:121][0m #------------------------ Iteration 4339 --------------------------#
[32m[20221214 00:05:05 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:05:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:05:05 @agent_ppo2.py:185][0m |          -0.0001 |          74.0800 |          11.9613 |
[32m[20221214 00:05:05 @agent_ppo2.py:185][0m |          -0.0045 |          72.0922 |          11.9191 |
[32m[20221214 00:05:05 @agent_ppo2.py:185][0m |          -0.0059 |          71.1727 |          11.9350 |
[32m[20221214 00:05:05 @agent_ppo2.py:185][0m |           0.0022 |          75.6980 |          11.9295 |
[32m[20221214 00:05:05 @agent_ppo2.py:185][0m |          -0.0097 |          70.8059 |          11.9758 |
[32m[20221214 00:05:05 @agent_ppo2.py:185][0m |          -0.0029 |          73.2339 |          11.9233 |
[32m[20221214 00:05:06 @agent_ppo2.py:185][0m |          -0.0116 |          69.5476 |          11.9768 |
[32m[20221214 00:05:06 @agent_ppo2.py:185][0m |          -0.0086 |          70.7474 |          11.9530 |
[32m[20221214 00:05:06 @agent_ppo2.py:185][0m |          -0.0124 |          69.0846 |          11.9444 |
[32m[20221214 00:05:06 @agent_ppo2.py:185][0m |          -0.0113 |          68.7833 |          11.9335 |
[32m[20221214 00:05:06 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:05:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 479.23
[32m[20221214 00:05:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 509.30
[32m[20221214 00:05:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 532.59
[32m[20221214 00:05:06 @agent_ppo2.py:143][0m Total time:       7.59 min
[32m[20221214 00:05:06 @agent_ppo2.py:145][0m 696320 total steps have happened
[32m[20221214 00:05:06 @agent_ppo2.py:121][0m #------------------------ Iteration 4340 --------------------------#
[32m[20221214 00:05:06 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:05:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:05:06 @agent_ppo2.py:185][0m |           0.0029 |          76.1713 |          11.9659 |
[32m[20221214 00:05:06 @agent_ppo2.py:185][0m |          -0.0003 |          72.8538 |          11.9621 |
[32m[20221214 00:05:06 @agent_ppo2.py:185][0m |          -0.0024 |          73.4625 |          12.0045 |
[32m[20221214 00:05:07 @agent_ppo2.py:185][0m |           0.0098 |          77.2375 |          11.9649 |
[32m[20221214 00:05:07 @agent_ppo2.py:185][0m |          -0.0125 |          70.1646 |          11.9813 |
[32m[20221214 00:05:07 @agent_ppo2.py:185][0m |          -0.0104 |          69.5361 |          11.9781 |
[32m[20221214 00:05:07 @agent_ppo2.py:185][0m |          -0.0153 |          69.1898 |          11.9925 |
[32m[20221214 00:05:07 @agent_ppo2.py:185][0m |          -0.0189 |          68.8591 |          11.9720 |
[32m[20221214 00:05:07 @agent_ppo2.py:185][0m |          -0.0095 |          70.6080 |          11.9929 |
[32m[20221214 00:05:07 @agent_ppo2.py:185][0m |          -0.0186 |          68.6048 |          12.0042 |
[32m[20221214 00:05:07 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:05:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 441.01
[32m[20221214 00:05:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 482.61
[32m[20221214 00:05:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 516.08
[32m[20221214 00:05:07 @agent_ppo2.py:143][0m Total time:       7.61 min
[32m[20221214 00:05:07 @agent_ppo2.py:145][0m 698368 total steps have happened
[32m[20221214 00:05:07 @agent_ppo2.py:121][0m #------------------------ Iteration 4341 --------------------------#
[32m[20221214 00:05:07 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:05:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:05:08 @agent_ppo2.py:185][0m |           0.0011 |          80.4975 |          11.7829 |
[32m[20221214 00:05:08 @agent_ppo2.py:185][0m |          -0.0059 |          77.4489 |          11.8161 |
[32m[20221214 00:05:08 @agent_ppo2.py:185][0m |          -0.0109 |          76.2332 |          11.8158 |
[32m[20221214 00:05:08 @agent_ppo2.py:185][0m |          -0.0118 |          75.4389 |          11.8123 |
[32m[20221214 00:05:08 @agent_ppo2.py:185][0m |          -0.0121 |          75.0144 |          11.7560 |
[32m[20221214 00:05:08 @agent_ppo2.py:185][0m |          -0.0105 |          74.8793 |          11.7787 |
[32m[20221214 00:05:08 @agent_ppo2.py:185][0m |          -0.0111 |          73.9132 |          11.7761 |
[32m[20221214 00:05:08 @agent_ppo2.py:185][0m |          -0.0034 |          79.0725 |          11.7491 |
[32m[20221214 00:05:08 @agent_ppo2.py:185][0m |          -0.0096 |          74.0797 |          11.7916 |
[32m[20221214 00:05:08 @agent_ppo2.py:185][0m |          -0.0143 |          73.0836 |          11.7497 |
[32m[20221214 00:05:08 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:05:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 499.64
[32m[20221214 00:05:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 545.92
[32m[20221214 00:05:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 469.99
[32m[20221214 00:05:08 @agent_ppo2.py:143][0m Total time:       7.63 min
[32m[20221214 00:05:08 @agent_ppo2.py:145][0m 700416 total steps have happened
[32m[20221214 00:05:08 @agent_ppo2.py:121][0m #------------------------ Iteration 4342 --------------------------#
[32m[20221214 00:05:09 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:05:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:05:09 @agent_ppo2.py:185][0m |           0.0019 |          61.6859 |          11.8183 |
[32m[20221214 00:05:09 @agent_ppo2.py:185][0m |          -0.0057 |          55.0489 |          11.8260 |
[32m[20221214 00:05:09 @agent_ppo2.py:185][0m |          -0.0075 |          52.7207 |          11.8310 |
[32m[20221214 00:05:09 @agent_ppo2.py:185][0m |          -0.0112 |          51.5804 |          11.8579 |
[32m[20221214 00:05:09 @agent_ppo2.py:185][0m |          -0.0087 |          50.8875 |          11.8496 |
[32m[20221214 00:05:09 @agent_ppo2.py:185][0m |          -0.0120 |          50.3267 |          11.8509 |
[32m[20221214 00:05:09 @agent_ppo2.py:185][0m |          -0.0073 |          50.8571 |          11.8818 |
[32m[20221214 00:05:09 @agent_ppo2.py:185][0m |          -0.0130 |          49.7466 |          11.9016 |
[32m[20221214 00:05:09 @agent_ppo2.py:185][0m |          -0.0141 |          49.7899 |          11.8949 |
[32m[20221214 00:05:10 @agent_ppo2.py:185][0m |          -0.0131 |          49.0925 |          11.9267 |
[32m[20221214 00:05:10 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:05:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 495.32
[32m[20221214 00:05:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 548.25
[32m[20221214 00:05:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 490.86
[32m[20221214 00:05:10 @agent_ppo2.py:143][0m Total time:       7.66 min
[32m[20221214 00:05:10 @agent_ppo2.py:145][0m 702464 total steps have happened
[32m[20221214 00:05:10 @agent_ppo2.py:121][0m #------------------------ Iteration 4343 --------------------------#
[32m[20221214 00:05:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:05:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:05:10 @agent_ppo2.py:185][0m |          -0.0008 |          73.4421 |          11.8727 |
[32m[20221214 00:05:10 @agent_ppo2.py:185][0m |          -0.0027 |          64.2751 |          11.8625 |
[32m[20221214 00:05:10 @agent_ppo2.py:185][0m |          -0.0042 |          62.5721 |          11.9013 |
[32m[20221214 00:05:10 @agent_ppo2.py:185][0m |          -0.0117 |          58.7266 |          11.9276 |
[32m[20221214 00:05:10 @agent_ppo2.py:185][0m |          -0.0100 |          56.7751 |          11.9136 |
[32m[20221214 00:05:10 @agent_ppo2.py:185][0m |          -0.0151 |          54.8768 |          11.9682 |
[32m[20221214 00:05:11 @agent_ppo2.py:185][0m |          -0.0164 |          53.6769 |          11.9687 |
[32m[20221214 00:05:11 @agent_ppo2.py:185][0m |           0.0041 |          59.9643 |          11.9827 |
[32m[20221214 00:05:11 @agent_ppo2.py:185][0m |          -0.0177 |          51.8102 |          12.0040 |
[32m[20221214 00:05:11 @agent_ppo2.py:185][0m |          -0.0223 |          50.7108 |          12.0060 |
[32m[20221214 00:05:11 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:05:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 429.94
[32m[20221214 00:05:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 476.51
[32m[20221214 00:05:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 403.47
[32m[20221214 00:05:11 @agent_ppo2.py:143][0m Total time:       7.68 min
[32m[20221214 00:05:11 @agent_ppo2.py:145][0m 704512 total steps have happened
[32m[20221214 00:05:11 @agent_ppo2.py:121][0m #------------------------ Iteration 4344 --------------------------#
[32m[20221214 00:05:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:05:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:05:11 @agent_ppo2.py:185][0m |           0.0018 |          45.8917 |          12.0023 |
[32m[20221214 00:05:11 @agent_ppo2.py:185][0m |          -0.0047 |          38.1255 |          11.9835 |
[32m[20221214 00:05:12 @agent_ppo2.py:185][0m |          -0.0051 |          35.4679 |          11.9606 |
[32m[20221214 00:05:12 @agent_ppo2.py:185][0m |          -0.0143 |          34.1921 |          11.9163 |
[32m[20221214 00:05:12 @agent_ppo2.py:185][0m |          -0.0090 |          33.2777 |          11.9029 |
[32m[20221214 00:05:12 @agent_ppo2.py:185][0m |          -0.0189 |          32.2172 |          11.8968 |
[32m[20221214 00:05:12 @agent_ppo2.py:185][0m |          -0.0098 |          31.8330 |          11.9118 |
[32m[20221214 00:05:12 @agent_ppo2.py:185][0m |          -0.0172 |          31.0900 |          11.8885 |
[32m[20221214 00:05:12 @agent_ppo2.py:185][0m |          -0.0182 |          30.5601 |          11.8916 |
[32m[20221214 00:05:12 @agent_ppo2.py:185][0m |          -0.0206 |          29.9719 |          11.8334 |
[32m[20221214 00:05:12 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:05:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 332.28
[32m[20221214 00:05:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 375.50
[32m[20221214 00:05:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 520.05
[32m[20221214 00:05:12 @agent_ppo2.py:143][0m Total time:       7.70 min
[32m[20221214 00:05:12 @agent_ppo2.py:145][0m 706560 total steps have happened
[32m[20221214 00:05:12 @agent_ppo2.py:121][0m #------------------------ Iteration 4345 --------------------------#
[32m[20221214 00:05:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:05:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:05:13 @agent_ppo2.py:185][0m |          -0.0035 |          73.4597 |          12.1234 |
[32m[20221214 00:05:13 @agent_ppo2.py:185][0m |          -0.0026 |          68.4102 |          12.1823 |
[32m[20221214 00:05:13 @agent_ppo2.py:185][0m |          -0.0091 |          64.5208 |          12.1866 |
[32m[20221214 00:05:13 @agent_ppo2.py:185][0m |          -0.0096 |          62.4895 |          12.1875 |
[32m[20221214 00:05:13 @agent_ppo2.py:185][0m |          -0.0062 |          61.1419 |          12.1843 |
[32m[20221214 00:05:13 @agent_ppo2.py:185][0m |          -0.0123 |          59.3994 |          12.1971 |
[32m[20221214 00:05:13 @agent_ppo2.py:185][0m |          -0.0062 |          60.7015 |          12.2201 |
[32m[20221214 00:05:13 @agent_ppo2.py:185][0m |          -0.0130 |          57.7395 |          12.2285 |
[32m[20221214 00:05:13 @agent_ppo2.py:185][0m |          -0.0138 |          57.0926 |          12.2510 |
[32m[20221214 00:05:13 @agent_ppo2.py:185][0m |          -0.0129 |          56.3278 |          12.2942 |
[32m[20221214 00:05:13 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:05:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 498.07
[32m[20221214 00:05:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 558.75
[32m[20221214 00:05:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 441.06
[32m[20221214 00:05:14 @agent_ppo2.py:143][0m Total time:       7.72 min
[32m[20221214 00:05:14 @agent_ppo2.py:145][0m 708608 total steps have happened
[32m[20221214 00:05:14 @agent_ppo2.py:121][0m #------------------------ Iteration 4346 --------------------------#
[32m[20221214 00:05:14 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:05:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:05:14 @agent_ppo2.py:185][0m |           0.0053 |          48.9653 |          11.5901 |
[32m[20221214 00:05:14 @agent_ppo2.py:185][0m |          -0.0082 |          38.8394 |          11.5478 |
[32m[20221214 00:05:14 @agent_ppo2.py:185][0m |          -0.0034 |          36.3380 |          11.5759 |
[32m[20221214 00:05:14 @agent_ppo2.py:185][0m |          -0.0085 |          35.0932 |          11.5680 |
[32m[20221214 00:05:14 @agent_ppo2.py:185][0m |          -0.0145 |          33.4417 |          11.5350 |
[32m[20221214 00:05:14 @agent_ppo2.py:185][0m |          -0.0173 |          32.6467 |          11.5286 |
[32m[20221214 00:05:14 @agent_ppo2.py:185][0m |          -0.0167 |          32.3097 |          11.5369 |
[32m[20221214 00:05:15 @agent_ppo2.py:185][0m |          -0.0215 |          31.3786 |          11.4851 |
[32m[20221214 00:05:15 @agent_ppo2.py:185][0m |          -0.0211 |          31.0398 |          11.5271 |
[32m[20221214 00:05:15 @agent_ppo2.py:185][0m |          -0.0191 |          30.5100 |          11.5226 |
[32m[20221214 00:05:15 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:05:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 399.07
[32m[20221214 00:05:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 486.45
[32m[20221214 00:05:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 505.95
[32m[20221214 00:05:15 @agent_ppo2.py:143][0m Total time:       7.74 min
[32m[20221214 00:05:15 @agent_ppo2.py:145][0m 710656 total steps have happened
[32m[20221214 00:05:15 @agent_ppo2.py:121][0m #------------------------ Iteration 4347 --------------------------#
[32m[20221214 00:05:15 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:05:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:05:15 @agent_ppo2.py:185][0m |           0.0049 |          77.3554 |          11.8642 |
[32m[20221214 00:05:15 @agent_ppo2.py:185][0m |          -0.0083 |          70.7394 |          11.9046 |
[32m[20221214 00:05:15 @agent_ppo2.py:185][0m |          -0.0105 |          69.6059 |          11.8863 |
[32m[20221214 00:05:15 @agent_ppo2.py:185][0m |          -0.0142 |          68.3182 |          11.8616 |
[32m[20221214 00:05:16 @agent_ppo2.py:185][0m |          -0.0054 |          71.9392 |          11.8675 |
[32m[20221214 00:05:16 @agent_ppo2.py:185][0m |          -0.0151 |          67.4585 |          11.8762 |
[32m[20221214 00:05:16 @agent_ppo2.py:185][0m |          -0.0168 |          66.8662 |          11.8768 |
[32m[20221214 00:05:16 @agent_ppo2.py:185][0m |          -0.0197 |          66.7775 |          11.8682 |
[32m[20221214 00:05:16 @agent_ppo2.py:185][0m |          -0.0155 |          66.0984 |          11.8652 |
[32m[20221214 00:05:16 @agent_ppo2.py:185][0m |          -0.0173 |          65.7608 |          11.8553 |
[32m[20221214 00:05:16 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:05:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 436.40
[32m[20221214 00:05:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 475.64
[32m[20221214 00:05:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 454.39
[32m[20221214 00:05:16 @agent_ppo2.py:143][0m Total time:       7.76 min
[32m[20221214 00:05:16 @agent_ppo2.py:145][0m 712704 total steps have happened
[32m[20221214 00:05:16 @agent_ppo2.py:121][0m #------------------------ Iteration 4348 --------------------------#
[32m[20221214 00:05:16 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:05:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:05:17 @agent_ppo2.py:185][0m |           0.0012 |          51.6197 |          11.8021 |
[32m[20221214 00:05:17 @agent_ppo2.py:185][0m |          -0.0069 |          45.9773 |          11.7605 |
[32m[20221214 00:05:17 @agent_ppo2.py:185][0m |          -0.0118 |          44.2576 |          11.7261 |
[32m[20221214 00:05:17 @agent_ppo2.py:185][0m |          -0.0111 |          43.5252 |          11.7047 |
[32m[20221214 00:05:17 @agent_ppo2.py:185][0m |          -0.0074 |          42.1726 |          11.6733 |
[32m[20221214 00:05:17 @agent_ppo2.py:185][0m |          -0.0122 |          41.3524 |          11.6564 |
[32m[20221214 00:05:17 @agent_ppo2.py:185][0m |          -0.0144 |          40.7736 |          11.6379 |
[32m[20221214 00:05:17 @agent_ppo2.py:185][0m |          -0.0164 |          40.1843 |          11.6635 |
[32m[20221214 00:05:17 @agent_ppo2.py:185][0m |          -0.0102 |          39.7205 |          11.6326 |
[32m[20221214 00:05:17 @agent_ppo2.py:185][0m |          -0.0195 |          39.2757 |          11.6266 |
[32m[20221214 00:05:17 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:05:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 428.06
[32m[20221214 00:05:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 501.55
[32m[20221214 00:05:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 566.27
[32m[20221214 00:05:18 @agent_ppo2.py:143][0m Total time:       7.79 min
[32m[20221214 00:05:18 @agent_ppo2.py:145][0m 714752 total steps have happened
[32m[20221214 00:05:18 @agent_ppo2.py:121][0m #------------------------ Iteration 4349 --------------------------#
[32m[20221214 00:05:18 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:05:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:05:18 @agent_ppo2.py:185][0m |           0.0175 |          70.6546 |          11.7052 |
[32m[20221214 00:05:18 @agent_ppo2.py:185][0m |          -0.0052 |          60.3765 |          11.6055 |
[32m[20221214 00:05:18 @agent_ppo2.py:185][0m |          -0.0071 |          59.3086 |          11.6188 |
[32m[20221214 00:05:18 @agent_ppo2.py:185][0m |          -0.0074 |          59.0568 |          11.6381 |
[32m[20221214 00:05:18 @agent_ppo2.py:185][0m |          -0.0096 |          59.0397 |          11.6072 |
[32m[20221214 00:05:18 @agent_ppo2.py:185][0m |          -0.0086 |          58.4375 |          11.6143 |
[32m[20221214 00:05:18 @agent_ppo2.py:185][0m |          -0.0132 |          58.2258 |          11.6086 |
[32m[20221214 00:05:18 @agent_ppo2.py:185][0m |          -0.0127 |          58.0176 |          11.6083 |
[32m[20221214 00:05:19 @agent_ppo2.py:185][0m |          -0.0153 |          57.9739 |          11.5722 |
[32m[20221214 00:05:19 @agent_ppo2.py:185][0m |          -0.0084 |          60.0239 |          11.6178 |
[32m[20221214 00:05:19 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:05:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 521.96
[32m[20221214 00:05:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 542.88
[32m[20221214 00:05:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 177.80
[32m[20221214 00:05:19 @agent_ppo2.py:143][0m Total time:       7.81 min
[32m[20221214 00:05:19 @agent_ppo2.py:145][0m 716800 total steps have happened
[32m[20221214 00:05:19 @agent_ppo2.py:121][0m #------------------------ Iteration 4350 --------------------------#
[32m[20221214 00:05:19 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:05:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:05:19 @agent_ppo2.py:185][0m |           0.0005 |          56.6865 |          11.4149 |
[32m[20221214 00:05:19 @agent_ppo2.py:185][0m |          -0.0038 |          50.5968 |          11.4151 |
[32m[20221214 00:05:19 @agent_ppo2.py:185][0m |          -0.0047 |          48.7138 |          11.4333 |
[32m[20221214 00:05:19 @agent_ppo2.py:185][0m |          -0.0111 |          47.3455 |          11.4465 |
[32m[20221214 00:05:20 @agent_ppo2.py:185][0m |          -0.0096 |          46.5234 |          11.4422 |
[32m[20221214 00:05:20 @agent_ppo2.py:185][0m |          -0.0124 |          45.8113 |          11.4518 |
[32m[20221214 00:05:20 @agent_ppo2.py:185][0m |          -0.0089 |          45.7359 |          11.4959 |
[32m[20221214 00:05:20 @agent_ppo2.py:185][0m |          -0.0126 |          45.1095 |          11.4819 |
[32m[20221214 00:05:20 @agent_ppo2.py:185][0m |          -0.0084 |          45.4834 |          11.5142 |
[32m[20221214 00:05:20 @agent_ppo2.py:185][0m |          -0.0141 |          44.5574 |          11.4810 |
[32m[20221214 00:05:20 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:05:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 496.91
[32m[20221214 00:05:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 563.52
[32m[20221214 00:05:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 579.26
[32m[20221214 00:05:20 @agent_ppo2.py:143][0m Total time:       7.83 min
[32m[20221214 00:05:20 @agent_ppo2.py:145][0m 718848 total steps have happened
[32m[20221214 00:05:20 @agent_ppo2.py:121][0m #------------------------ Iteration 4351 --------------------------#
[32m[20221214 00:05:20 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:05:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:05:20 @agent_ppo2.py:185][0m |          -0.0004 |          73.8261 |          11.7422 |
[32m[20221214 00:05:21 @agent_ppo2.py:185][0m |          -0.0006 |          66.5946 |          11.7259 |
[32m[20221214 00:05:21 @agent_ppo2.py:185][0m |          -0.0096 |          63.0171 |          11.7191 |
[32m[20221214 00:05:21 @agent_ppo2.py:185][0m |          -0.0115 |          61.4133 |          11.7408 |
[32m[20221214 00:05:21 @agent_ppo2.py:185][0m |          -0.0151 |          60.1295 |          11.7416 |
[32m[20221214 00:05:21 @agent_ppo2.py:185][0m |          -0.0128 |          59.1414 |          11.7915 |
[32m[20221214 00:05:21 @agent_ppo2.py:185][0m |          -0.0138 |          58.2822 |          11.7842 |
[32m[20221214 00:05:21 @agent_ppo2.py:185][0m |          -0.0146 |          57.6112 |          11.8107 |
[32m[20221214 00:05:21 @agent_ppo2.py:185][0m |          -0.0155 |          57.6129 |          11.8120 |
[32m[20221214 00:05:21 @agent_ppo2.py:185][0m |          -0.0197 |          56.5529 |          11.8202 |
[32m[20221214 00:05:21 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:05:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 406.92
[32m[20221214 00:05:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 464.44
[32m[20221214 00:05:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 510.39
[32m[20221214 00:05:21 @agent_ppo2.py:143][0m Total time:       7.85 min
[32m[20221214 00:05:21 @agent_ppo2.py:145][0m 720896 total steps have happened
[32m[20221214 00:05:21 @agent_ppo2.py:121][0m #------------------------ Iteration 4352 --------------------------#
[32m[20221214 00:05:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:05:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:05:22 @agent_ppo2.py:185][0m |           0.0026 |          65.6540 |          11.8583 |
[32m[20221214 00:05:22 @agent_ppo2.py:185][0m |          -0.0049 |          60.1374 |          11.8473 |
[32m[20221214 00:05:22 @agent_ppo2.py:185][0m |          -0.0100 |          58.6531 |          11.8287 |
[32m[20221214 00:05:22 @agent_ppo2.py:185][0m |          -0.0067 |          57.9666 |          11.8071 |
[32m[20221214 00:05:22 @agent_ppo2.py:185][0m |          -0.0151 |          56.9048 |          11.7861 |
[32m[20221214 00:05:22 @agent_ppo2.py:185][0m |          -0.0113 |          57.2750 |          11.7846 |
[32m[20221214 00:05:22 @agent_ppo2.py:185][0m |          -0.0178 |          56.0553 |          11.7714 |
[32m[20221214 00:05:22 @agent_ppo2.py:185][0m |          -0.0164 |          55.3620 |          11.7592 |
[32m[20221214 00:05:23 @agent_ppo2.py:185][0m |          -0.0107 |          57.4988 |          11.7494 |
[32m[20221214 00:05:23 @agent_ppo2.py:185][0m |          -0.0177 |          54.7670 |          11.7493 |
[32m[20221214 00:05:23 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:05:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 485.19
[32m[20221214 00:05:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 527.85
[32m[20221214 00:05:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 430.92
[32m[20221214 00:05:23 @agent_ppo2.py:143][0m Total time:       7.87 min
[32m[20221214 00:05:23 @agent_ppo2.py:145][0m 722944 total steps have happened
[32m[20221214 00:05:23 @agent_ppo2.py:121][0m #------------------------ Iteration 4353 --------------------------#
[32m[20221214 00:05:23 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:05:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:05:23 @agent_ppo2.py:185][0m |          -0.0079 |          47.4314 |          11.7031 |
[32m[20221214 00:05:23 @agent_ppo2.py:185][0m |          -0.0079 |          40.3229 |          11.6389 |
[32m[20221214 00:05:23 @agent_ppo2.py:185][0m |          -0.0105 |          37.4870 |          11.6604 |
[32m[20221214 00:05:23 @agent_ppo2.py:185][0m |          -0.0060 |          36.0554 |          11.6692 |
[32m[20221214 00:05:23 @agent_ppo2.py:185][0m |          -0.0123 |          35.5611 |          11.6605 |
[32m[20221214 00:05:24 @agent_ppo2.py:185][0m |          -0.0121 |          34.5494 |          11.6403 |
[32m[20221214 00:05:24 @agent_ppo2.py:185][0m |          -0.0118 |          33.9554 |          11.5972 |
[32m[20221214 00:05:24 @agent_ppo2.py:185][0m |          -0.0170 |          33.4877 |          11.5998 |
[32m[20221214 00:05:24 @agent_ppo2.py:185][0m |          -0.0152 |          33.2340 |          11.5788 |
[32m[20221214 00:05:24 @agent_ppo2.py:185][0m |          -0.0179 |          32.7606 |          11.5795 |
[32m[20221214 00:05:24 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:05:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 448.76
[32m[20221214 00:05:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 521.80
[32m[20221214 00:05:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 455.83
[32m[20221214 00:05:24 @agent_ppo2.py:143][0m Total time:       7.90 min
[32m[20221214 00:05:24 @agent_ppo2.py:145][0m 724992 total steps have happened
[32m[20221214 00:05:24 @agent_ppo2.py:121][0m #------------------------ Iteration 4354 --------------------------#
[32m[20221214 00:05:24 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:05:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:05:24 @agent_ppo2.py:185][0m |          -0.0009 |          71.6615 |          11.5160 |
[32m[20221214 00:05:25 @agent_ppo2.py:185][0m |          -0.0090 |          69.8568 |          11.4955 |
[32m[20221214 00:05:25 @agent_ppo2.py:185][0m |          -0.0111 |          67.8914 |          11.4943 |
[32m[20221214 00:05:25 @agent_ppo2.py:185][0m |          -0.0129 |          67.3888 |          11.4334 |
[32m[20221214 00:05:25 @agent_ppo2.py:185][0m |          -0.0148 |          67.1242 |          11.4667 |
[32m[20221214 00:05:25 @agent_ppo2.py:185][0m |          -0.0040 |          72.7303 |          11.4787 |
[32m[20221214 00:05:25 @agent_ppo2.py:185][0m |          -0.0169 |          67.0233 |          11.4831 |
[32m[20221214 00:05:25 @agent_ppo2.py:185][0m |          -0.0178 |          66.2476 |          11.4758 |
[32m[20221214 00:05:25 @agent_ppo2.py:185][0m |          -0.0175 |          66.0862 |          11.4520 |
[32m[20221214 00:05:25 @agent_ppo2.py:185][0m |          -0.0205 |          65.9276 |          11.4561 |
[32m[20221214 00:05:25 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:05:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 519.53
[32m[20221214 00:05:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 542.45
[32m[20221214 00:05:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 476.33
[32m[20221214 00:05:25 @agent_ppo2.py:143][0m Total time:       7.92 min
[32m[20221214 00:05:25 @agent_ppo2.py:145][0m 727040 total steps have happened
[32m[20221214 00:05:25 @agent_ppo2.py:121][0m #------------------------ Iteration 4355 --------------------------#
[32m[20221214 00:05:26 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:05:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:05:26 @agent_ppo2.py:185][0m |          -0.0004 |          67.2622 |          11.2909 |
[32m[20221214 00:05:26 @agent_ppo2.py:185][0m |          -0.0041 |          62.0079 |          11.2183 |
[32m[20221214 00:05:26 @agent_ppo2.py:185][0m |          -0.0076 |          60.3382 |          11.1914 |
[32m[20221214 00:05:26 @agent_ppo2.py:185][0m |          -0.0057 |          59.1753 |          11.1659 |
[32m[20221214 00:05:26 @agent_ppo2.py:185][0m |          -0.0088 |          58.3840 |          11.1260 |
[32m[20221214 00:05:26 @agent_ppo2.py:185][0m |          -0.0074 |          57.5631 |          11.0850 |
[32m[20221214 00:05:26 @agent_ppo2.py:185][0m |          -0.0102 |          57.3747 |          11.0554 |
[32m[20221214 00:05:26 @agent_ppo2.py:185][0m |          -0.0118 |          56.6547 |          11.0402 |
[32m[20221214 00:05:27 @agent_ppo2.py:185][0m |          -0.0139 |          56.2321 |          11.0579 |
[32m[20221214 00:05:27 @agent_ppo2.py:185][0m |          -0.0184 |          56.0241 |          11.0419 |
[32m[20221214 00:05:27 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:05:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 415.99
[32m[20221214 00:05:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 470.25
[32m[20221214 00:05:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 565.13
[32m[20221214 00:05:27 @agent_ppo2.py:143][0m Total time:       7.94 min
[32m[20221214 00:05:27 @agent_ppo2.py:145][0m 729088 total steps have happened
[32m[20221214 00:05:27 @agent_ppo2.py:121][0m #------------------------ Iteration 4356 --------------------------#
[32m[20221214 00:05:27 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:05:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:05:27 @agent_ppo2.py:185][0m |          -0.0022 |          67.3077 |          11.1984 |
[32m[20221214 00:05:27 @agent_ppo2.py:185][0m |          -0.0080 |          62.1557 |          11.2639 |
[32m[20221214 00:05:27 @agent_ppo2.py:185][0m |          -0.0087 |          59.9424 |          11.2507 |
[32m[20221214 00:05:27 @agent_ppo2.py:185][0m |          -0.0112 |          58.7439 |          11.2614 |
[32m[20221214 00:05:27 @agent_ppo2.py:185][0m |          -0.0119 |          57.6721 |          11.2975 |
[32m[20221214 00:05:28 @agent_ppo2.py:185][0m |          -0.0121 |          57.6190 |          11.3010 |
[32m[20221214 00:05:28 @agent_ppo2.py:185][0m |          -0.0120 |          56.6031 |          11.3282 |
[32m[20221214 00:05:28 @agent_ppo2.py:185][0m |          -0.0112 |          55.7382 |          11.3160 |
[32m[20221214 00:05:28 @agent_ppo2.py:185][0m |          -0.0148 |          55.3840 |          11.3000 |
[32m[20221214 00:05:28 @agent_ppo2.py:185][0m |          -0.0132 |          55.1135 |          11.3710 |
[32m[20221214 00:05:28 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:05:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 493.94
[32m[20221214 00:05:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 523.09
[32m[20221214 00:05:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 509.24
[32m[20221214 00:05:28 @agent_ppo2.py:143][0m Total time:       7.96 min
[32m[20221214 00:05:28 @agent_ppo2.py:145][0m 731136 total steps have happened
[32m[20221214 00:05:28 @agent_ppo2.py:121][0m #------------------------ Iteration 4357 --------------------------#
[32m[20221214 00:05:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:05:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:05:28 @agent_ppo2.py:185][0m |          -0.0020 |          70.3757 |          11.0060 |
[32m[20221214 00:05:28 @agent_ppo2.py:185][0m |          -0.0091 |          65.2573 |          11.0040 |
[32m[20221214 00:05:29 @agent_ppo2.py:185][0m |          -0.0048 |          64.6268 |          11.0012 |
[32m[20221214 00:05:29 @agent_ppo2.py:185][0m |          -0.0112 |          62.9837 |          11.0402 |
[32m[20221214 00:05:29 @agent_ppo2.py:185][0m |          -0.0117 |          62.3525 |          10.9973 |
[32m[20221214 00:05:29 @agent_ppo2.py:185][0m |          -0.0149 |          61.8744 |          11.0120 |
[32m[20221214 00:05:29 @agent_ppo2.py:185][0m |          -0.0110 |          61.4923 |          10.9557 |
[32m[20221214 00:05:29 @agent_ppo2.py:185][0m |          -0.0055 |          63.6679 |          11.0371 |
[32m[20221214 00:05:29 @agent_ppo2.py:185][0m |          -0.0122 |          60.9785 |          11.0112 |
[32m[20221214 00:05:29 @agent_ppo2.py:185][0m |          -0.0085 |          64.4262 |          11.0092 |
[32m[20221214 00:05:29 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:05:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 473.75
[32m[20221214 00:05:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 564.95
[32m[20221214 00:05:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 551.25
[32m[20221214 00:05:29 @agent_ppo2.py:143][0m Total time:       7.98 min
[32m[20221214 00:05:29 @agent_ppo2.py:145][0m 733184 total steps have happened
[32m[20221214 00:05:29 @agent_ppo2.py:121][0m #------------------------ Iteration 4358 --------------------------#
[32m[20221214 00:05:30 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:05:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:05:30 @agent_ppo2.py:185][0m |           0.0051 |          61.5745 |          11.6631 |
[32m[20221214 00:05:30 @agent_ppo2.py:185][0m |          -0.0066 |          48.7865 |          11.7302 |
[32m[20221214 00:05:30 @agent_ppo2.py:185][0m |          -0.0114 |          46.8664 |          11.6992 |
[32m[20221214 00:05:30 @agent_ppo2.py:185][0m |          -0.0114 |          45.0312 |          11.6874 |
[32m[20221214 00:05:30 @agent_ppo2.py:185][0m |          -0.0079 |          44.4018 |          11.7649 |
[32m[20221214 00:05:30 @agent_ppo2.py:185][0m |          -0.0177 |          44.0478 |          11.7377 |
[32m[20221214 00:05:30 @agent_ppo2.py:185][0m |          -0.0113 |          45.3682 |          11.7244 |
[32m[20221214 00:05:30 @agent_ppo2.py:185][0m |          -0.0110 |          42.8442 |          11.7591 |
[32m[20221214 00:05:30 @agent_ppo2.py:185][0m |          -0.0170 |          42.0008 |          11.7481 |
[32m[20221214 00:05:31 @agent_ppo2.py:185][0m |          -0.0204 |          41.7849 |          11.7383 |
[32m[20221214 00:05:31 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:05:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 394.42
[32m[20221214 00:05:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 451.46
[32m[20221214 00:05:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 469.49
[32m[20221214 00:05:31 @agent_ppo2.py:143][0m Total time:       8.00 min
[32m[20221214 00:05:31 @agent_ppo2.py:145][0m 735232 total steps have happened
[32m[20221214 00:05:31 @agent_ppo2.py:121][0m #------------------------ Iteration 4359 --------------------------#
[32m[20221214 00:05:31 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:05:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:05:31 @agent_ppo2.py:185][0m |           0.0005 |          77.1060 |          10.7019 |
[32m[20221214 00:05:31 @agent_ppo2.py:185][0m |          -0.0043 |          73.1340 |          10.6982 |
[32m[20221214 00:05:31 @agent_ppo2.py:185][0m |           0.0026 |          78.0116 |          10.6387 |
[32m[20221214 00:05:31 @agent_ppo2.py:185][0m |          -0.0113 |          71.2908 |          10.6618 |
[32m[20221214 00:05:31 @agent_ppo2.py:185][0m |          -0.0144 |          70.7250 |          10.6066 |
[32m[20221214 00:05:31 @agent_ppo2.py:185][0m |          -0.0051 |          76.5868 |          10.5971 |
[32m[20221214 00:05:32 @agent_ppo2.py:185][0m |          -0.0180 |          70.2593 |          10.5262 |
[32m[20221214 00:05:32 @agent_ppo2.py:185][0m |          -0.0166 |          69.9649 |          10.5091 |
[32m[20221214 00:05:32 @agent_ppo2.py:185][0m |          -0.0145 |          69.5580 |          10.5414 |
[32m[20221214 00:05:32 @agent_ppo2.py:185][0m |          -0.0137 |          69.8639 |          10.5104 |
[32m[20221214 00:05:32 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:05:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 553.67
[32m[20221214 00:05:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 578.27
[32m[20221214 00:05:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 526.33
[32m[20221214 00:05:32 @agent_ppo2.py:143][0m Total time:       8.03 min
[32m[20221214 00:05:32 @agent_ppo2.py:145][0m 737280 total steps have happened
[32m[20221214 00:05:32 @agent_ppo2.py:121][0m #------------------------ Iteration 4360 --------------------------#
[32m[20221214 00:05:32 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:05:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:05:32 @agent_ppo2.py:185][0m |          -0.0000 |          72.9203 |          11.1738 |
[32m[20221214 00:05:32 @agent_ppo2.py:185][0m |          -0.0005 |          68.3628 |          11.2088 |
[32m[20221214 00:05:33 @agent_ppo2.py:185][0m |          -0.0096 |          65.2223 |          11.2400 |
[32m[20221214 00:05:33 @agent_ppo2.py:185][0m |          -0.0060 |          63.7147 |          11.2612 |
[32m[20221214 00:05:33 @agent_ppo2.py:185][0m |          -0.0053 |          63.9761 |          11.2657 |
[32m[20221214 00:05:33 @agent_ppo2.py:185][0m |          -0.0184 |          62.1393 |          11.2888 |
[32m[20221214 00:05:33 @agent_ppo2.py:185][0m |          -0.0125 |          61.2912 |          11.3321 |
[32m[20221214 00:05:33 @agent_ppo2.py:185][0m |          -0.0186 |          60.9186 |          11.3052 |
[32m[20221214 00:05:33 @agent_ppo2.py:185][0m |          -0.0164 |          60.5100 |          11.3409 |
[32m[20221214 00:05:33 @agent_ppo2.py:185][0m |          -0.0155 |          59.6290 |          11.3396 |
[32m[20221214 00:05:33 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:05:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 424.36
[32m[20221214 00:05:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 502.48
[32m[20221214 00:05:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 478.63
[32m[20221214 00:05:33 @agent_ppo2.py:143][0m Total time:       8.05 min
[32m[20221214 00:05:33 @agent_ppo2.py:145][0m 739328 total steps have happened
[32m[20221214 00:05:33 @agent_ppo2.py:121][0m #------------------------ Iteration 4361 --------------------------#
[32m[20221214 00:05:34 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:05:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:05:34 @agent_ppo2.py:185][0m |           0.0089 |          54.9370 |          11.1917 |
[32m[20221214 00:05:34 @agent_ppo2.py:185][0m |          -0.0064 |          48.4029 |          11.1798 |
[32m[20221214 00:05:34 @agent_ppo2.py:185][0m |           0.0005 |          49.1103 |          11.1527 |
[32m[20221214 00:05:34 @agent_ppo2.py:185][0m |          -0.0096 |          46.2588 |          11.1473 |
[32m[20221214 00:05:34 @agent_ppo2.py:185][0m |          -0.0056 |          48.2795 |          11.1634 |
[32m[20221214 00:05:34 @agent_ppo2.py:185][0m |          -0.0116 |          44.7748 |          11.1476 |
[32m[20221214 00:05:34 @agent_ppo2.py:185][0m |          -0.0189 |          44.3872 |          11.1795 |
[32m[20221214 00:05:34 @agent_ppo2.py:185][0m |          -0.0198 |          43.9225 |          11.1688 |
[32m[20221214 00:05:34 @agent_ppo2.py:185][0m |          -0.0167 |          43.6871 |          11.1645 |
[32m[20221214 00:05:35 @agent_ppo2.py:185][0m |          -0.0170 |          43.4598 |          11.1867 |
[32m[20221214 00:05:35 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:05:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 350.77
[32m[20221214 00:05:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 533.28
[32m[20221214 00:05:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 467.27
[32m[20221214 00:05:35 @agent_ppo2.py:143][0m Total time:       8.07 min
[32m[20221214 00:05:35 @agent_ppo2.py:145][0m 741376 total steps have happened
[32m[20221214 00:05:35 @agent_ppo2.py:121][0m #------------------------ Iteration 4362 --------------------------#
[32m[20221214 00:05:35 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:05:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:05:35 @agent_ppo2.py:185][0m |          -0.0003 |          66.0983 |          11.2479 |
[32m[20221214 00:05:35 @agent_ppo2.py:185][0m |          -0.0017 |          61.8727 |          11.2614 |
[32m[20221214 00:05:35 @agent_ppo2.py:185][0m |          -0.0102 |          60.3902 |          11.2615 |
[32m[20221214 00:05:35 @agent_ppo2.py:185][0m |          -0.0099 |          59.6398 |          11.2930 |
[32m[20221214 00:05:35 @agent_ppo2.py:185][0m |          -0.0078 |          58.8347 |          11.3018 |
[32m[20221214 00:05:36 @agent_ppo2.py:185][0m |          -0.0127 |          58.3941 |          11.3077 |
[32m[20221214 00:05:36 @agent_ppo2.py:185][0m |          -0.0110 |          58.1065 |          11.3382 |
[32m[20221214 00:05:36 @agent_ppo2.py:185][0m |          -0.0143 |          57.6471 |          11.3194 |
[32m[20221214 00:05:36 @agent_ppo2.py:185][0m |          -0.0134 |          57.4893 |          11.3378 |
[32m[20221214 00:05:36 @agent_ppo2.py:185][0m |          -0.0136 |          57.0737 |          11.3650 |
[32m[20221214 00:05:36 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:05:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 525.25
[32m[20221214 00:05:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 548.92
[32m[20221214 00:05:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 393.44
[32m[20221214 00:05:36 @agent_ppo2.py:143][0m Total time:       8.09 min
[32m[20221214 00:05:36 @agent_ppo2.py:145][0m 743424 total steps have happened
[32m[20221214 00:05:36 @agent_ppo2.py:121][0m #------------------------ Iteration 4363 --------------------------#
[32m[20221214 00:05:36 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:05:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:05:36 @agent_ppo2.py:185][0m |          -0.0006 |          55.7196 |          11.8197 |
[32m[20221214 00:05:36 @agent_ppo2.py:185][0m |          -0.0067 |          52.8646 |          11.7848 |
[32m[20221214 00:05:37 @agent_ppo2.py:185][0m |          -0.0016 |          54.5654 |          11.8093 |
[32m[20221214 00:05:37 @agent_ppo2.py:185][0m |          -0.0087 |          52.1484 |          11.8216 |
[32m[20221214 00:05:37 @agent_ppo2.py:185][0m |          -0.0130 |          51.1673 |          11.8200 |
[32m[20221214 00:05:37 @agent_ppo2.py:185][0m |          -0.0138 |          50.8256 |          11.8611 |
[32m[20221214 00:05:37 @agent_ppo2.py:185][0m |          -0.0013 |          54.4958 |          11.8802 |
[32m[20221214 00:05:37 @agent_ppo2.py:185][0m |          -0.0161 |          50.5489 |          11.8754 |
[32m[20221214 00:05:37 @agent_ppo2.py:185][0m |          -0.0157 |          50.3219 |          11.8800 |
[32m[20221214 00:05:37 @agent_ppo2.py:185][0m |          -0.0051 |          51.5366 |          11.9115 |
[32m[20221214 00:05:37 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:05:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 500.50
[32m[20221214 00:05:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 539.01
[32m[20221214 00:05:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 515.73
[32m[20221214 00:05:37 @agent_ppo2.py:143][0m Total time:       8.12 min
[32m[20221214 00:05:37 @agent_ppo2.py:145][0m 745472 total steps have happened
[32m[20221214 00:05:37 @agent_ppo2.py:121][0m #------------------------ Iteration 4364 --------------------------#
[32m[20221214 00:05:38 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:05:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:05:38 @agent_ppo2.py:185][0m |           0.0039 |          56.1440 |          11.3765 |
[32m[20221214 00:05:38 @agent_ppo2.py:185][0m |          -0.0066 |          52.3848 |          11.3960 |
[32m[20221214 00:05:38 @agent_ppo2.py:185][0m |          -0.0113 |          50.6746 |          11.4007 |
[32m[20221214 00:05:38 @agent_ppo2.py:185][0m |          -0.0120 |          49.5126 |          11.3793 |
[32m[20221214 00:05:38 @agent_ppo2.py:185][0m |          -0.0145 |          48.8854 |          11.4449 |
[32m[20221214 00:05:38 @agent_ppo2.py:185][0m |          -0.0075 |          49.1598 |          11.4415 |
[32m[20221214 00:05:38 @agent_ppo2.py:185][0m |          -0.0186 |          47.6594 |          11.4704 |
[32m[20221214 00:05:38 @agent_ppo2.py:185][0m |          -0.0167 |          47.2460 |          11.4614 |
[32m[20221214 00:05:39 @agent_ppo2.py:185][0m |          -0.0154 |          47.1436 |          11.4791 |
[32m[20221214 00:05:39 @agent_ppo2.py:185][0m |          -0.0201 |          46.5244 |          11.4910 |
[32m[20221214 00:05:39 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:05:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 421.63
[32m[20221214 00:05:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 491.07
[32m[20221214 00:05:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 599.82
[32m[20221214 00:05:39 @agent_ppo2.py:143][0m Total time:       8.14 min
[32m[20221214 00:05:39 @agent_ppo2.py:145][0m 747520 total steps have happened
[32m[20221214 00:05:39 @agent_ppo2.py:121][0m #------------------------ Iteration 4365 --------------------------#
[32m[20221214 00:05:39 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:05:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:05:39 @agent_ppo2.py:185][0m |          -0.0029 |          49.9796 |          11.3418 |
[32m[20221214 00:05:39 @agent_ppo2.py:185][0m |          -0.0056 |          43.4292 |          11.3334 |
[32m[20221214 00:05:39 @agent_ppo2.py:185][0m |          -0.0081 |          40.5834 |          11.2947 |
[32m[20221214 00:05:39 @agent_ppo2.py:185][0m |          -0.0025 |          39.7023 |          11.3215 |
[32m[20221214 00:05:40 @agent_ppo2.py:185][0m |          -0.0105 |          38.0322 |          11.3116 |
[32m[20221214 00:05:40 @agent_ppo2.py:185][0m |          -0.0127 |          37.1134 |          11.3114 |
[32m[20221214 00:05:40 @agent_ppo2.py:185][0m |          -0.0169 |          36.4212 |          11.3147 |
[32m[20221214 00:05:40 @agent_ppo2.py:185][0m |          -0.0221 |          36.3975 |          11.2752 |
[32m[20221214 00:05:40 @agent_ppo2.py:185][0m |          -0.0104 |          36.3830 |          11.2815 |
[32m[20221214 00:05:40 @agent_ppo2.py:185][0m |          -0.0219 |          35.2331 |          11.3134 |
[32m[20221214 00:05:40 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:05:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 463.08
[32m[20221214 00:05:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 546.07
[32m[20221214 00:05:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 525.66
[32m[20221214 00:05:40 @agent_ppo2.py:143][0m Total time:       8.16 min
[32m[20221214 00:05:40 @agent_ppo2.py:145][0m 749568 total steps have happened
[32m[20221214 00:05:40 @agent_ppo2.py:121][0m #------------------------ Iteration 4366 --------------------------#
[32m[20221214 00:05:40 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:05:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:05:41 @agent_ppo2.py:185][0m |          -0.0030 |          69.1650 |          11.4699 |
[32m[20221214 00:05:41 @agent_ppo2.py:185][0m |          -0.0016 |          65.1233 |          11.4562 |
[32m[20221214 00:05:41 @agent_ppo2.py:185][0m |          -0.0075 |          63.0165 |          11.4545 |
[32m[20221214 00:05:41 @agent_ppo2.py:185][0m |          -0.0077 |          62.2741 |          11.4691 |
[32m[20221214 00:05:41 @agent_ppo2.py:185][0m |          -0.0121 |          61.2592 |          11.4598 |
[32m[20221214 00:05:41 @agent_ppo2.py:185][0m |          -0.0158 |          60.6545 |          11.4767 |
[32m[20221214 00:05:41 @agent_ppo2.py:185][0m |          -0.0125 |          63.1512 |          11.4574 |
[32m[20221214 00:05:41 @agent_ppo2.py:185][0m |          -0.0191 |          59.8266 |          11.4484 |
[32m[20221214 00:05:41 @agent_ppo2.py:185][0m |          -0.0203 |          59.3072 |          11.4334 |
[32m[20221214 00:05:41 @agent_ppo2.py:185][0m |          -0.0126 |          61.0194 |          11.4360 |
[32m[20221214 00:05:41 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:05:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 412.89
[32m[20221214 00:05:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 447.84
[32m[20221214 00:05:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 462.83
[32m[20221214 00:05:42 @agent_ppo2.py:143][0m Total time:       8.19 min
[32m[20221214 00:05:42 @agent_ppo2.py:145][0m 751616 total steps have happened
[32m[20221214 00:05:42 @agent_ppo2.py:121][0m #------------------------ Iteration 4367 --------------------------#
[32m[20221214 00:05:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:05:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:05:42 @agent_ppo2.py:185][0m |          -0.0018 |          59.9650 |          11.2105 |
[32m[20221214 00:05:42 @agent_ppo2.py:185][0m |           0.0028 |          58.4755 |          11.2371 |
[32m[20221214 00:05:42 @agent_ppo2.py:185][0m |          -0.0071 |          54.8458 |          11.2202 |
[32m[20221214 00:05:42 @agent_ppo2.py:185][0m |          -0.0089 |          53.7820 |          11.1903 |
[32m[20221214 00:05:42 @agent_ppo2.py:185][0m |          -0.0071 |          53.2049 |          11.1969 |
[32m[20221214 00:05:42 @agent_ppo2.py:185][0m |          -0.0106 |          52.6774 |          11.2640 |
[32m[20221214 00:05:43 @agent_ppo2.py:185][0m |          -0.0097 |          52.1721 |          11.2237 |
[32m[20221214 00:05:43 @agent_ppo2.py:185][0m |          -0.0127 |          51.5916 |          11.2060 |
[32m[20221214 00:05:43 @agent_ppo2.py:185][0m |          -0.0131 |          51.3546 |          11.1932 |
[32m[20221214 00:05:43 @agent_ppo2.py:185][0m |          -0.0115 |          50.9052 |          11.2328 |
[32m[20221214 00:05:43 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:05:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 517.59
[32m[20221214 00:05:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 556.26
[32m[20221214 00:05:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 510.57
[32m[20221214 00:05:43 @agent_ppo2.py:143][0m Total time:       8.21 min
[32m[20221214 00:05:43 @agent_ppo2.py:145][0m 753664 total steps have happened
[32m[20221214 00:05:43 @agent_ppo2.py:121][0m #------------------------ Iteration 4368 --------------------------#
[32m[20221214 00:05:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:05:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:05:43 @agent_ppo2.py:185][0m |           0.0020 |          62.0817 |          10.9629 |
[32m[20221214 00:05:43 @agent_ppo2.py:185][0m |          -0.0065 |          54.8981 |          10.9904 |
[32m[20221214 00:05:44 @agent_ppo2.py:185][0m |          -0.0099 |          53.3409 |          11.0361 |
[32m[20221214 00:05:44 @agent_ppo2.py:185][0m |          -0.0167 |          52.3527 |          10.9959 |
[32m[20221214 00:05:44 @agent_ppo2.py:185][0m |          -0.0129 |          51.5140 |          11.0403 |
[32m[20221214 00:05:44 @agent_ppo2.py:185][0m |          -0.0086 |          51.0246 |          11.0379 |
[32m[20221214 00:05:44 @agent_ppo2.py:185][0m |          -0.0045 |          55.3280 |          11.0452 |
[32m[20221214 00:05:44 @agent_ppo2.py:185][0m |          -0.0189 |          50.1678 |          11.0642 |
[32m[20221214 00:05:44 @agent_ppo2.py:185][0m |          -0.0215 |          49.7235 |          11.0548 |
[32m[20221214 00:05:44 @agent_ppo2.py:185][0m |          -0.0156 |          50.0064 |          11.0914 |
[32m[20221214 00:05:44 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:05:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 464.83
[32m[20221214 00:05:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 501.48
[32m[20221214 00:05:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 562.12
[32m[20221214 00:05:44 @agent_ppo2.py:143][0m Total time:       8.23 min
[32m[20221214 00:05:44 @agent_ppo2.py:145][0m 755712 total steps have happened
[32m[20221214 00:05:44 @agent_ppo2.py:121][0m #------------------------ Iteration 4369 --------------------------#
[32m[20221214 00:05:44 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:05:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:05:45 @agent_ppo2.py:185][0m |           0.0033 |          53.1697 |          11.3321 |
[32m[20221214 00:05:45 @agent_ppo2.py:185][0m |          -0.0052 |          48.0090 |          11.3463 |
[32m[20221214 00:05:45 @agent_ppo2.py:185][0m |          -0.0024 |          45.7668 |          11.3421 |
[32m[20221214 00:05:45 @agent_ppo2.py:185][0m |          -0.0133 |          44.6721 |          11.3284 |
[32m[20221214 00:05:45 @agent_ppo2.py:185][0m |          -0.0074 |          43.8465 |          11.3677 |
[32m[20221214 00:05:45 @agent_ppo2.py:185][0m |          -0.0134 |          42.8231 |          11.3384 |
[32m[20221214 00:05:45 @agent_ppo2.py:185][0m |          -0.0146 |          42.5122 |          11.3468 |
[32m[20221214 00:05:45 @agent_ppo2.py:185][0m |          -0.0179 |          41.9809 |          11.3325 |
[32m[20221214 00:05:45 @agent_ppo2.py:185][0m |          -0.0195 |          41.6201 |          11.3184 |
[32m[20221214 00:05:45 @agent_ppo2.py:185][0m |          -0.0189 |          41.2956 |          11.3673 |
[32m[20221214 00:05:45 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:05:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 386.24
[32m[20221214 00:05:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 466.98
[32m[20221214 00:05:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 490.29
[32m[20221214 00:05:46 @agent_ppo2.py:143][0m Total time:       8.25 min
[32m[20221214 00:05:46 @agent_ppo2.py:145][0m 757760 total steps have happened
[32m[20221214 00:05:46 @agent_ppo2.py:121][0m #------------------------ Iteration 4370 --------------------------#
[32m[20221214 00:05:46 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:05:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:05:46 @agent_ppo2.py:185][0m |          -0.0031 |          62.8628 |          10.9684 |
[32m[20221214 00:05:46 @agent_ppo2.py:185][0m |          -0.0106 |          52.6946 |          10.9708 |
[32m[20221214 00:05:46 @agent_ppo2.py:185][0m |          -0.0085 |          50.1711 |          10.9515 |
[32m[20221214 00:05:46 @agent_ppo2.py:185][0m |          -0.0119 |          48.3207 |          10.9359 |
[32m[20221214 00:05:46 @agent_ppo2.py:185][0m |          -0.0197 |          47.0296 |          10.9219 |
[32m[20221214 00:05:46 @agent_ppo2.py:185][0m |          -0.0135 |          45.9149 |          10.9233 |
[32m[20221214 00:05:47 @agent_ppo2.py:185][0m |          -0.0157 |          45.0209 |          10.9348 |
[32m[20221214 00:05:47 @agent_ppo2.py:185][0m |          -0.0176 |          44.4720 |          10.9145 |
[32m[20221214 00:05:47 @agent_ppo2.py:185][0m |          -0.0162 |          43.9336 |          10.9074 |
[32m[20221214 00:05:47 @agent_ppo2.py:185][0m |          -0.0201 |          43.2187 |          10.9310 |
[32m[20221214 00:05:47 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:05:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 434.11
[32m[20221214 00:05:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 518.41
[32m[20221214 00:05:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 429.84
[32m[20221214 00:05:47 @agent_ppo2.py:143][0m Total time:       8.28 min
[32m[20221214 00:05:47 @agent_ppo2.py:145][0m 759808 total steps have happened
[32m[20221214 00:05:47 @agent_ppo2.py:121][0m #------------------------ Iteration 4371 --------------------------#
[32m[20221214 00:05:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:05:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:05:47 @agent_ppo2.py:185][0m |          -0.0014 |          51.7879 |          11.0043 |
[32m[20221214 00:05:47 @agent_ppo2.py:185][0m |          -0.0044 |          46.2820 |          10.9955 |
[32m[20221214 00:05:47 @agent_ppo2.py:185][0m |          -0.0061 |          44.7015 |          11.0256 |
[32m[20221214 00:05:48 @agent_ppo2.py:185][0m |          -0.0127 |          43.6066 |          11.0638 |
[32m[20221214 00:05:48 @agent_ppo2.py:185][0m |          -0.0023 |          47.2570 |          11.0094 |
[32m[20221214 00:05:48 @agent_ppo2.py:185][0m |          -0.0119 |          43.1708 |          11.0562 |
[32m[20221214 00:05:48 @agent_ppo2.py:185][0m |          -0.0168 |          41.8994 |          11.0518 |
[32m[20221214 00:05:48 @agent_ppo2.py:185][0m |          -0.0131 |          41.4220 |          11.0522 |
[32m[20221214 00:05:48 @agent_ppo2.py:185][0m |          -0.0158 |          41.0190 |          11.0786 |
[32m[20221214 00:05:48 @agent_ppo2.py:185][0m |          -0.0222 |          41.0163 |          11.0617 |
[32m[20221214 00:05:48 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:05:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 405.93
[32m[20221214 00:05:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 482.51
[32m[20221214 00:05:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 567.55
[32m[20221214 00:05:48 @agent_ppo2.py:143][0m Total time:       8.30 min
[32m[20221214 00:05:48 @agent_ppo2.py:145][0m 761856 total steps have happened
[32m[20221214 00:05:48 @agent_ppo2.py:121][0m #------------------------ Iteration 4372 --------------------------#
[32m[20221214 00:05:48 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:05:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:05:49 @agent_ppo2.py:185][0m |           0.0021 |          57.8663 |          11.3613 |
[32m[20221214 00:05:49 @agent_ppo2.py:185][0m |           0.0025 |          56.3678 |          11.3339 |
[32m[20221214 00:05:49 @agent_ppo2.py:185][0m |           0.0032 |          57.4910 |          11.3644 |
[32m[20221214 00:05:49 @agent_ppo2.py:185][0m |          -0.0070 |          51.8707 |          11.3074 |
[32m[20221214 00:05:49 @agent_ppo2.py:185][0m |          -0.0013 |          54.2018 |          11.3119 |
[32m[20221214 00:05:49 @agent_ppo2.py:185][0m |          -0.0115 |          50.7936 |          11.3102 |
[32m[20221214 00:05:49 @agent_ppo2.py:185][0m |          -0.0104 |          50.3883 |          11.3380 |
[32m[20221214 00:05:49 @agent_ppo2.py:185][0m |          -0.0125 |          50.2046 |          11.3199 |
[32m[20221214 00:05:49 @agent_ppo2.py:185][0m |          -0.0169 |          49.8191 |          11.3020 |
[32m[20221214 00:05:49 @agent_ppo2.py:185][0m |          -0.0137 |          49.7592 |          11.3119 |
[32m[20221214 00:05:49 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:05:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 504.44
[32m[20221214 00:05:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 570.24
[32m[20221214 00:05:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 577.09
[32m[20221214 00:05:50 @agent_ppo2.py:143][0m Total time:       8.32 min
[32m[20221214 00:05:50 @agent_ppo2.py:145][0m 763904 total steps have happened
[32m[20221214 00:05:50 @agent_ppo2.py:121][0m #------------------------ Iteration 4373 --------------------------#
[32m[20221214 00:05:50 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:05:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:05:50 @agent_ppo2.py:185][0m |           0.0160 |          52.5002 |          11.2639 |
[32m[20221214 00:05:50 @agent_ppo2.py:185][0m |          -0.0027 |          41.2728 |          11.2838 |
[32m[20221214 00:05:50 @agent_ppo2.py:185][0m |          -0.0062 |          38.8314 |          11.2587 |
[32m[20221214 00:05:50 @agent_ppo2.py:185][0m |          -0.0114 |          37.7342 |          11.2962 |
[32m[20221214 00:05:50 @agent_ppo2.py:185][0m |          -0.0138 |          36.7571 |          11.3054 |
[32m[20221214 00:05:50 @agent_ppo2.py:185][0m |          -0.0002 |          38.8230 |          11.3406 |
[32m[20221214 00:05:50 @agent_ppo2.py:185][0m |           0.0030 |          37.8345 |          11.3608 |
[32m[20221214 00:05:50 @agent_ppo2.py:185][0m |          -0.0143 |          35.4916 |          11.3538 |
[32m[20221214 00:05:51 @agent_ppo2.py:185][0m |          -0.0154 |          35.0209 |          11.3134 |
[32m[20221214 00:05:51 @agent_ppo2.py:185][0m |          -0.0113 |          35.0108 |          11.3470 |
[32m[20221214 00:05:51 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:05:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 462.57
[32m[20221214 00:05:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 535.30
[32m[20221214 00:05:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 546.62
[32m[20221214 00:05:51 @agent_ppo2.py:143][0m Total time:       8.34 min
[32m[20221214 00:05:51 @agent_ppo2.py:145][0m 765952 total steps have happened
[32m[20221214 00:05:51 @agent_ppo2.py:121][0m #------------------------ Iteration 4374 --------------------------#
[32m[20221214 00:05:51 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:05:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:05:51 @agent_ppo2.py:185][0m |           0.0043 |          81.4951 |          10.8123 |
[32m[20221214 00:05:51 @agent_ppo2.py:185][0m |          -0.0049 |          76.3172 |          10.8250 |
[32m[20221214 00:05:51 @agent_ppo2.py:185][0m |          -0.0081 |          74.7698 |          10.7882 |
[32m[20221214 00:05:51 @agent_ppo2.py:185][0m |          -0.0091 |          73.2515 |          10.8352 |
[32m[20221214 00:05:52 @agent_ppo2.py:185][0m |          -0.0122 |          72.1952 |          10.8148 |
[32m[20221214 00:05:52 @agent_ppo2.py:185][0m |          -0.0110 |          71.2846 |          10.8153 |
[32m[20221214 00:05:52 @agent_ppo2.py:185][0m |          -0.0112 |          70.6630 |          10.8420 |
[32m[20221214 00:05:52 @agent_ppo2.py:185][0m |           0.0085 |          81.7159 |          10.8035 |
[32m[20221214 00:05:52 @agent_ppo2.py:185][0m |          -0.0098 |          69.6641 |          10.8256 |
[32m[20221214 00:05:52 @agent_ppo2.py:185][0m |          -0.0129 |          69.3098 |          10.8701 |
[32m[20221214 00:05:52 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:05:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 490.67
[32m[20221214 00:05:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 539.82
[32m[20221214 00:05:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 458.05
[32m[20221214 00:05:52 @agent_ppo2.py:143][0m Total time:       8.36 min
[32m[20221214 00:05:52 @agent_ppo2.py:145][0m 768000 total steps have happened
[32m[20221214 00:05:52 @agent_ppo2.py:121][0m #------------------------ Iteration 4375 --------------------------#
[32m[20221214 00:05:52 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:05:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:05:53 @agent_ppo2.py:185][0m |          -0.0009 |          53.0405 |          11.3499 |
[32m[20221214 00:05:53 @agent_ppo2.py:185][0m |          -0.0083 |          45.7587 |          11.3394 |
[32m[20221214 00:05:53 @agent_ppo2.py:185][0m |          -0.0116 |          43.0659 |          11.3716 |
[32m[20221214 00:05:53 @agent_ppo2.py:185][0m |          -0.0123 |          41.1191 |          11.3622 |
[32m[20221214 00:05:53 @agent_ppo2.py:185][0m |          -0.0142 |          39.6705 |          11.3670 |
[32m[20221214 00:05:53 @agent_ppo2.py:185][0m |          -0.0143 |          38.4772 |          11.3564 |
[32m[20221214 00:05:53 @agent_ppo2.py:185][0m |          -0.0183 |          37.8670 |          11.2951 |
[32m[20221214 00:05:53 @agent_ppo2.py:185][0m |          -0.0187 |          36.8354 |          11.3685 |
[32m[20221214 00:05:53 @agent_ppo2.py:185][0m |          -0.0205 |          36.0820 |          11.3209 |
[32m[20221214 00:05:53 @agent_ppo2.py:185][0m |          -0.0195 |          35.3661 |          11.3210 |
[32m[20221214 00:05:53 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:05:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 458.28
[32m[20221214 00:05:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 593.62
[32m[20221214 00:05:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 504.81
[32m[20221214 00:05:54 @agent_ppo2.py:143][0m Total time:       8.39 min
[32m[20221214 00:05:54 @agent_ppo2.py:145][0m 770048 total steps have happened
[32m[20221214 00:05:54 @agent_ppo2.py:121][0m #------------------------ Iteration 4376 --------------------------#
[32m[20221214 00:05:54 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:05:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:05:54 @agent_ppo2.py:185][0m |           0.0000 |          50.6557 |          10.6646 |
[32m[20221214 00:05:54 @agent_ppo2.py:185][0m |          -0.0101 |          43.6920 |          10.7094 |
[32m[20221214 00:05:54 @agent_ppo2.py:185][0m |          -0.0100 |          41.5974 |          10.6911 |
[32m[20221214 00:05:54 @agent_ppo2.py:185][0m |          -0.0160 |          40.4042 |          10.6970 |
[32m[20221214 00:05:54 @agent_ppo2.py:185][0m |          -0.0139 |          39.8864 |          10.7023 |
[32m[20221214 00:05:54 @agent_ppo2.py:185][0m |          -0.0194 |          39.0197 |          10.7015 |
[32m[20221214 00:05:54 @agent_ppo2.py:185][0m |          -0.0185 |          38.2491 |          10.7004 |
[32m[20221214 00:05:54 @agent_ppo2.py:185][0m |          -0.0187 |          38.0946 |          10.7280 |
[32m[20221214 00:05:55 @agent_ppo2.py:185][0m |          -0.0179 |          37.7840 |          10.7051 |
[32m[20221214 00:05:55 @agent_ppo2.py:185][0m |          -0.0194 |          37.1765 |          10.7422 |
[32m[20221214 00:05:55 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:05:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 375.66
[32m[20221214 00:05:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 410.90
[32m[20221214 00:05:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 601.67
[32m[20221214 00:05:55 @agent_ppo2.py:143][0m Total time:       8.41 min
[32m[20221214 00:05:55 @agent_ppo2.py:145][0m 772096 total steps have happened
[32m[20221214 00:05:55 @agent_ppo2.py:121][0m #------------------------ Iteration 4377 --------------------------#
[32m[20221214 00:05:55 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:05:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:05:55 @agent_ppo2.py:185][0m |           0.0013 |          71.3166 |          10.9140 |
[32m[20221214 00:05:55 @agent_ppo2.py:185][0m |          -0.0095 |          64.4212 |          10.8833 |
[32m[20221214 00:05:55 @agent_ppo2.py:185][0m |          -0.0032 |          65.5153 |          10.8888 |
[32m[20221214 00:05:55 @agent_ppo2.py:185][0m |          -0.0113 |          60.8743 |          10.9449 |
[32m[20221214 00:05:56 @agent_ppo2.py:185][0m |          -0.0173 |          60.1487 |          10.9266 |
[32m[20221214 00:05:56 @agent_ppo2.py:185][0m |          -0.0062 |          67.3373 |          10.9939 |
[32m[20221214 00:05:56 @agent_ppo2.py:185][0m |          -0.0137 |          60.1797 |          11.0319 |
[32m[20221214 00:05:56 @agent_ppo2.py:185][0m |          -0.0200 |          58.4517 |          11.0164 |
[32m[20221214 00:05:56 @agent_ppo2.py:185][0m |          -0.0146 |          59.0946 |          11.0242 |
[32m[20221214 00:05:56 @agent_ppo2.py:185][0m |          -0.0181 |          58.0412 |          11.0819 |
[32m[20221214 00:05:56 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:05:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 488.40
[32m[20221214 00:05:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 564.53
[32m[20221214 00:05:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 453.37
[32m[20221214 00:05:56 @agent_ppo2.py:143][0m Total time:       8.43 min
[32m[20221214 00:05:56 @agent_ppo2.py:145][0m 774144 total steps have happened
[32m[20221214 00:05:56 @agent_ppo2.py:121][0m #------------------------ Iteration 4378 --------------------------#
[32m[20221214 00:05:56 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:05:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:05:56 @agent_ppo2.py:185][0m |           0.0042 |          49.9756 |          10.8819 |
[32m[20221214 00:05:57 @agent_ppo2.py:185][0m |          -0.0063 |          44.6854 |          10.7978 |
[32m[20221214 00:05:57 @agent_ppo2.py:185][0m |          -0.0060 |          42.8740 |          10.8743 |
[32m[20221214 00:05:57 @agent_ppo2.py:185][0m |          -0.0090 |          41.6229 |          10.8842 |
[32m[20221214 00:05:57 @agent_ppo2.py:185][0m |          -0.0145 |          40.7845 |          10.8591 |
[32m[20221214 00:05:57 @agent_ppo2.py:185][0m |          -0.0150 |          40.3255 |          10.8329 |
[32m[20221214 00:05:57 @agent_ppo2.py:185][0m |          -0.0205 |          39.6020 |          10.8756 |
[32m[20221214 00:05:57 @agent_ppo2.py:185][0m |          -0.0147 |          39.0143 |          10.8697 |
[32m[20221214 00:05:57 @agent_ppo2.py:185][0m |          -0.0207 |          38.6073 |          10.9002 |
[32m[20221214 00:05:57 @agent_ppo2.py:185][0m |          -0.0165 |          38.2046 |          10.8940 |
[32m[20221214 00:05:57 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:05:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 464.03
[32m[20221214 00:05:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 533.12
[32m[20221214 00:05:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 384.01
[32m[20221214 00:05:57 @agent_ppo2.py:143][0m Total time:       8.45 min
[32m[20221214 00:05:57 @agent_ppo2.py:145][0m 776192 total steps have happened
[32m[20221214 00:05:57 @agent_ppo2.py:121][0m #------------------------ Iteration 4379 --------------------------#
[32m[20221214 00:05:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:05:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:05:58 @agent_ppo2.py:185][0m |           0.0009 |          66.0099 |          11.4095 |
[32m[20221214 00:05:58 @agent_ppo2.py:185][0m |           0.0005 |          57.5989 |          11.4583 |
[32m[20221214 00:05:58 @agent_ppo2.py:185][0m |          -0.0076 |          55.5646 |          11.4446 |
[32m[20221214 00:05:58 @agent_ppo2.py:185][0m |          -0.0114 |          54.1890 |          11.4525 |
[32m[20221214 00:05:58 @agent_ppo2.py:185][0m |          -0.0008 |          54.0469 |          11.4694 |
[32m[20221214 00:05:58 @agent_ppo2.py:185][0m |           0.0004 |          56.3119 |          11.5224 |
[32m[20221214 00:05:58 @agent_ppo2.py:185][0m |          -0.0092 |          52.2045 |          11.5565 |
[32m[20221214 00:05:58 @agent_ppo2.py:185][0m |          -0.0072 |          52.7638 |          11.5139 |
[32m[20221214 00:05:58 @agent_ppo2.py:185][0m |          -0.0024 |          54.6475 |          11.5406 |
[32m[20221214 00:05:59 @agent_ppo2.py:185][0m |          -0.0123 |          51.5627 |          11.5386 |
[32m[20221214 00:05:59 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:05:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 481.10
[32m[20221214 00:05:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 540.30
[32m[20221214 00:05:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 531.79
[32m[20221214 00:05:59 @agent_ppo2.py:143][0m Total time:       8.47 min
[32m[20221214 00:05:59 @agent_ppo2.py:145][0m 778240 total steps have happened
[32m[20221214 00:05:59 @agent_ppo2.py:121][0m #------------------------ Iteration 4380 --------------------------#
[32m[20221214 00:05:59 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:05:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:05:59 @agent_ppo2.py:185][0m |          -0.0013 |          45.6034 |          11.3038 |
[32m[20221214 00:05:59 @agent_ppo2.py:185][0m |          -0.0032 |          37.7660 |          11.2712 |
[32m[20221214 00:05:59 @agent_ppo2.py:185][0m |          -0.0086 |          35.7399 |          11.2939 |
[32m[20221214 00:05:59 @agent_ppo2.py:185][0m |          -0.0102 |          34.5227 |          11.3232 |
[32m[20221214 00:05:59 @agent_ppo2.py:185][0m |          -0.0159 |          33.6672 |          11.3187 |
[32m[20221214 00:05:59 @agent_ppo2.py:185][0m |          -0.0213 |          33.1025 |          11.3062 |
[32m[20221214 00:06:00 @agent_ppo2.py:185][0m |          -0.0089 |          32.7499 |          11.3268 |
[32m[20221214 00:06:00 @agent_ppo2.py:185][0m |          -0.0063 |          37.5194 |          11.3198 |
[32m[20221214 00:06:00 @agent_ppo2.py:185][0m |          -0.0123 |          31.7982 |          11.3353 |
[32m[20221214 00:06:00 @agent_ppo2.py:185][0m |          -0.0168 |          31.2944 |          11.3180 |
[32m[20221214 00:06:00 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:06:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 454.44
[32m[20221214 00:06:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 487.79
[32m[20221214 00:06:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 562.42
[32m[20221214 00:06:00 @agent_ppo2.py:143][0m Total time:       8.49 min
[32m[20221214 00:06:00 @agent_ppo2.py:145][0m 780288 total steps have happened
[32m[20221214 00:06:00 @agent_ppo2.py:121][0m #------------------------ Iteration 4381 --------------------------#
[32m[20221214 00:06:00 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:06:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:06:00 @agent_ppo2.py:185][0m |          -0.0032 |          63.8842 |          11.6585 |
[32m[20221214 00:06:00 @agent_ppo2.py:185][0m |          -0.0071 |          51.7470 |          11.6711 |
[32m[20221214 00:06:00 @agent_ppo2.py:185][0m |          -0.0089 |          45.6363 |          11.7265 |
[32m[20221214 00:06:01 @agent_ppo2.py:185][0m |          -0.0116 |          42.2982 |          11.7038 |
[32m[20221214 00:06:01 @agent_ppo2.py:185][0m |          -0.0072 |          42.4338 |          11.6814 |
[32m[20221214 00:06:01 @agent_ppo2.py:185][0m |          -0.0130 |          40.7165 |          11.6741 |
[32m[20221214 00:06:01 @agent_ppo2.py:185][0m |          -0.0124 |          40.2202 |          11.6889 |
[32m[20221214 00:06:01 @agent_ppo2.py:185][0m |          -0.0168 |          39.8606 |          11.6943 |
[32m[20221214 00:06:01 @agent_ppo2.py:185][0m |          -0.0156 |          39.5969 |          11.6870 |
[32m[20221214 00:06:01 @agent_ppo2.py:185][0m |          -0.0194 |          39.3441 |          11.6972 |
[32m[20221214 00:06:01 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:06:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 451.27
[32m[20221214 00:06:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 544.66
[32m[20221214 00:06:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 471.78
[32m[20221214 00:06:01 @agent_ppo2.py:143][0m Total time:       8.51 min
[32m[20221214 00:06:01 @agent_ppo2.py:145][0m 782336 total steps have happened
[32m[20221214 00:06:01 @agent_ppo2.py:121][0m #------------------------ Iteration 4382 --------------------------#
[32m[20221214 00:06:01 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:06:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:06:02 @agent_ppo2.py:185][0m |           0.0060 |          44.1418 |          11.7308 |
[32m[20221214 00:06:02 @agent_ppo2.py:185][0m |          -0.0106 |          27.5365 |          11.7352 |
[32m[20221214 00:06:02 @agent_ppo2.py:185][0m |          -0.0087 |          25.2217 |          11.7488 |
[32m[20221214 00:06:02 @agent_ppo2.py:185][0m |          -0.0052 |          24.1590 |          11.7369 |
[32m[20221214 00:06:02 @agent_ppo2.py:185][0m |          -0.0053 |          23.0801 |          11.7754 |
[32m[20221214 00:06:02 @agent_ppo2.py:185][0m |           0.0028 |          24.0444 |          11.7341 |
[32m[20221214 00:06:02 @agent_ppo2.py:185][0m |          -0.0108 |          21.8516 |          11.7722 |
[32m[20221214 00:06:02 @agent_ppo2.py:185][0m |          -0.0106 |          21.4057 |          11.7569 |
[32m[20221214 00:06:02 @agent_ppo2.py:185][0m |          -0.0128 |          21.0748 |          11.7704 |
[32m[20221214 00:06:02 @agent_ppo2.py:185][0m |          -0.0133 |          20.6254 |          11.7523 |
[32m[20221214 00:06:02 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:06:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 306.78
[32m[20221214 00:06:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 428.90
[32m[20221214 00:06:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 417.34
[32m[20221214 00:06:03 @agent_ppo2.py:143][0m Total time:       8.54 min
[32m[20221214 00:06:03 @agent_ppo2.py:145][0m 784384 total steps have happened
[32m[20221214 00:06:03 @agent_ppo2.py:121][0m #------------------------ Iteration 4383 --------------------------#
[32m[20221214 00:06:03 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:06:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:06:03 @agent_ppo2.py:185][0m |           0.0003 |          48.7200 |          11.4570 |
[32m[20221214 00:06:03 @agent_ppo2.py:185][0m |          -0.0045 |          36.7642 |          11.4508 |
[32m[20221214 00:06:03 @agent_ppo2.py:185][0m |          -0.0139 |          35.1596 |          11.4975 |
[32m[20221214 00:06:03 @agent_ppo2.py:185][0m |          -0.0121 |          34.4473 |          11.4969 |
[32m[20221214 00:06:03 @agent_ppo2.py:185][0m |          -0.0082 |          34.0551 |          11.4840 |
[32m[20221214 00:06:03 @agent_ppo2.py:185][0m |          -0.0118 |          33.0347 |          11.5072 |
[32m[20221214 00:06:03 @agent_ppo2.py:185][0m |          -0.0153 |          32.3248 |          11.5388 |
[32m[20221214 00:06:04 @agent_ppo2.py:185][0m |          -0.0111 |          32.7994 |          11.5327 |
[32m[20221214 00:06:04 @agent_ppo2.py:185][0m |          -0.0186 |          31.4885 |          11.5336 |
[32m[20221214 00:06:04 @agent_ppo2.py:185][0m |          -0.0193 |          31.5478 |          11.5087 |
[32m[20221214 00:06:04 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:06:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 406.29
[32m[20221214 00:06:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 502.30
[32m[20221214 00:06:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 480.16
[32m[20221214 00:06:04 @agent_ppo2.py:143][0m Total time:       8.56 min
[32m[20221214 00:06:04 @agent_ppo2.py:145][0m 786432 total steps have happened
[32m[20221214 00:06:04 @agent_ppo2.py:121][0m #------------------------ Iteration 4384 --------------------------#
[32m[20221214 00:06:04 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:06:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:06:04 @agent_ppo2.py:185][0m |           0.0017 |          32.8946 |          11.3340 |
[32m[20221214 00:06:04 @agent_ppo2.py:185][0m |          -0.0076 |          24.9758 |          11.3000 |
[32m[20221214 00:06:04 @agent_ppo2.py:185][0m |          -0.0100 |          23.3291 |          11.3121 |
[32m[20221214 00:06:05 @agent_ppo2.py:185][0m |          -0.0092 |          23.0601 |          11.3013 |
[32m[20221214 00:06:05 @agent_ppo2.py:185][0m |          -0.0175 |          22.1672 |          11.2864 |
[32m[20221214 00:06:05 @agent_ppo2.py:185][0m |          -0.0170 |          21.5869 |          11.2814 |
[32m[20221214 00:06:05 @agent_ppo2.py:185][0m |          -0.0197 |          21.2238 |          11.3277 |
[32m[20221214 00:06:05 @agent_ppo2.py:185][0m |          -0.0224 |          21.0052 |          11.2741 |
[32m[20221214 00:06:05 @agent_ppo2.py:185][0m |          -0.0184 |          20.6313 |          11.2763 |
[32m[20221214 00:06:05 @agent_ppo2.py:185][0m |          -0.0226 |          20.3912 |          11.2886 |
[32m[20221214 00:06:05 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:06:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 399.54
[32m[20221214 00:06:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 467.15
[32m[20221214 00:06:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 589.64
[32m[20221214 00:06:05 @agent_ppo2.py:143][0m Total time:       8.58 min
[32m[20221214 00:06:05 @agent_ppo2.py:145][0m 788480 total steps have happened
[32m[20221214 00:06:05 @agent_ppo2.py:121][0m #------------------------ Iteration 4385 --------------------------#
[32m[20221214 00:06:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:06:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:06:06 @agent_ppo2.py:185][0m |           0.0047 |          36.4057 |          11.7929 |
[32m[20221214 00:06:06 @agent_ppo2.py:185][0m |          -0.0004 |          30.8902 |          11.7524 |
[32m[20221214 00:06:06 @agent_ppo2.py:185][0m |          -0.0041 |          28.8173 |          11.7493 |
[32m[20221214 00:06:06 @agent_ppo2.py:185][0m |          -0.0036 |          27.8612 |          11.7028 |
[32m[20221214 00:06:06 @agent_ppo2.py:185][0m |          -0.0055 |          27.0667 |          11.6785 |
[32m[20221214 00:06:06 @agent_ppo2.py:185][0m |          -0.0113 |          26.6181 |          11.6506 |
[32m[20221214 00:06:06 @agent_ppo2.py:185][0m |          -0.0059 |          26.3362 |          11.6339 |
[32m[20221214 00:06:06 @agent_ppo2.py:185][0m |          -0.0162 |          25.7683 |          11.6162 |
[32m[20221214 00:06:06 @agent_ppo2.py:185][0m |          -0.0151 |          25.5158 |          11.6146 |
[32m[20221214 00:06:06 @agent_ppo2.py:185][0m |          -0.0166 |          25.3669 |          11.6531 |
[32m[20221214 00:06:06 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:06:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 385.83
[32m[20221214 00:06:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 445.08
[32m[20221214 00:06:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 509.34
[32m[20221214 00:06:06 @agent_ppo2.py:143][0m Total time:       8.60 min
[32m[20221214 00:06:06 @agent_ppo2.py:145][0m 790528 total steps have happened
[32m[20221214 00:06:06 @agent_ppo2.py:121][0m #------------------------ Iteration 4386 --------------------------#
[32m[20221214 00:06:07 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:06:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:06:07 @agent_ppo2.py:185][0m |          -0.0005 |          43.7976 |          11.7972 |
[32m[20221214 00:06:07 @agent_ppo2.py:185][0m |          -0.0015 |          38.3193 |          11.7764 |
[32m[20221214 00:06:07 @agent_ppo2.py:185][0m |          -0.0110 |          37.1962 |          11.7456 |
[32m[20221214 00:06:07 @agent_ppo2.py:185][0m |          -0.0104 |          36.4834 |          11.6981 |
[32m[20221214 00:06:07 @agent_ppo2.py:185][0m |          -0.0073 |          36.4249 |          11.7373 |
[32m[20221214 00:06:07 @agent_ppo2.py:185][0m |          -0.0145 |          35.4556 |          11.7264 |
[32m[20221214 00:06:07 @agent_ppo2.py:185][0m |          -0.0126 |          35.5210 |          11.7141 |
[32m[20221214 00:06:07 @agent_ppo2.py:185][0m |          -0.0156 |          35.3630 |          11.6857 |
[32m[20221214 00:06:07 @agent_ppo2.py:185][0m |          -0.0134 |          35.2290 |          11.7012 |
[32m[20221214 00:06:08 @agent_ppo2.py:185][0m |          -0.0160 |          34.9432 |          11.6745 |
[32m[20221214 00:06:08 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:06:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 359.61
[32m[20221214 00:06:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 436.29
[32m[20221214 00:06:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 546.65
[32m[20221214 00:06:08 @agent_ppo2.py:143][0m Total time:       8.62 min
[32m[20221214 00:06:08 @agent_ppo2.py:145][0m 792576 total steps have happened
[32m[20221214 00:06:08 @agent_ppo2.py:121][0m #------------------------ Iteration 4387 --------------------------#
[32m[20221214 00:06:08 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:06:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:06:08 @agent_ppo2.py:185][0m |           0.0022 |          51.7896 |          11.7057 |
[32m[20221214 00:06:08 @agent_ppo2.py:185][0m |          -0.0079 |          48.1778 |          11.7171 |
[32m[20221214 00:06:08 @agent_ppo2.py:185][0m |          -0.0111 |          47.1743 |          11.7098 |
[32m[20221214 00:06:08 @agent_ppo2.py:185][0m |          -0.0135 |          46.5769 |          11.7262 |
[32m[20221214 00:06:08 @agent_ppo2.py:185][0m |          -0.0144 |          45.8442 |          11.7442 |
[32m[20221214 00:06:09 @agent_ppo2.py:185][0m |          -0.0167 |          45.3866 |          11.7590 |
[32m[20221214 00:06:09 @agent_ppo2.py:185][0m |          -0.0177 |          45.1199 |          11.7587 |
[32m[20221214 00:06:09 @agent_ppo2.py:185][0m |          -0.0170 |          44.8160 |          11.7663 |
[32m[20221214 00:06:09 @agent_ppo2.py:185][0m |          -0.0201 |          44.5006 |          11.8201 |
[32m[20221214 00:06:09 @agent_ppo2.py:185][0m |          -0.0179 |          44.1500 |          11.7938 |
[32m[20221214 00:06:09 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:06:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 344.27
[32m[20221214 00:06:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 452.47
[32m[20221214 00:06:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 522.39
[32m[20221214 00:06:09 @agent_ppo2.py:143][0m Total time:       8.64 min
[32m[20221214 00:06:09 @agent_ppo2.py:145][0m 794624 total steps have happened
[32m[20221214 00:06:09 @agent_ppo2.py:121][0m #------------------------ Iteration 4388 --------------------------#
[32m[20221214 00:06:09 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:06:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:06:09 @agent_ppo2.py:185][0m |           0.0012 |          67.1240 |          11.3176 |
[32m[20221214 00:06:09 @agent_ppo2.py:185][0m |          -0.0068 |          63.1262 |          11.3103 |
[32m[20221214 00:06:10 @agent_ppo2.py:185][0m |          -0.0078 |          61.7786 |          11.3177 |
[32m[20221214 00:06:10 @agent_ppo2.py:185][0m |          -0.0103 |          61.5035 |          11.2745 |
[32m[20221214 00:06:10 @agent_ppo2.py:185][0m |          -0.0133 |          60.6008 |          11.2755 |
[32m[20221214 00:06:10 @agent_ppo2.py:185][0m |          -0.0091 |          61.8102 |          11.2593 |
[32m[20221214 00:06:10 @agent_ppo2.py:185][0m |          -0.0141 |          60.0361 |          11.2401 |
[32m[20221214 00:06:10 @agent_ppo2.py:185][0m |          -0.0145 |          59.8306 |          11.3088 |
[32m[20221214 00:06:10 @agent_ppo2.py:185][0m |          -0.0169 |          59.6224 |          11.2540 |
[32m[20221214 00:06:10 @agent_ppo2.py:185][0m |          -0.0190 |          59.5918 |          11.2899 |
[32m[20221214 00:06:10 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:06:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 499.51
[32m[20221214 00:06:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 506.08
[32m[20221214 00:06:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 488.59
[32m[20221214 00:06:10 @agent_ppo2.py:143][0m Total time:       8.67 min
[32m[20221214 00:06:10 @agent_ppo2.py:145][0m 796672 total steps have happened
[32m[20221214 00:06:10 @agent_ppo2.py:121][0m #------------------------ Iteration 4389 --------------------------#
[32m[20221214 00:06:10 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:06:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:06:11 @agent_ppo2.py:185][0m |          -0.0002 |          50.8660 |          11.4884 |
[32m[20221214 00:06:11 @agent_ppo2.py:185][0m |          -0.0080 |          46.4321 |          11.4654 |
[32m[20221214 00:06:11 @agent_ppo2.py:185][0m |          -0.0125 |          44.4698 |          11.4714 |
[32m[20221214 00:06:11 @agent_ppo2.py:185][0m |          -0.0078 |          43.0991 |          11.4062 |
[32m[20221214 00:06:11 @agent_ppo2.py:185][0m |          -0.0089 |          42.1130 |          11.4140 |
[32m[20221214 00:06:11 @agent_ppo2.py:185][0m |          -0.0160 |          41.2486 |          11.4087 |
[32m[20221214 00:06:11 @agent_ppo2.py:185][0m |          -0.0134 |          40.6396 |          11.3784 |
[32m[20221214 00:06:11 @agent_ppo2.py:185][0m |          -0.0203 |          40.6338 |          11.3824 |
[32m[20221214 00:06:11 @agent_ppo2.py:185][0m |          -0.0181 |          39.6954 |          11.3707 |
[32m[20221214 00:06:11 @agent_ppo2.py:185][0m |          -0.0206 |          39.4203 |          11.3083 |
[32m[20221214 00:06:11 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:06:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 374.13
[32m[20221214 00:06:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 434.57
[32m[20221214 00:06:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 508.63
[32m[20221214 00:06:12 @agent_ppo2.py:143][0m Total time:       8.69 min
[32m[20221214 00:06:12 @agent_ppo2.py:145][0m 798720 total steps have happened
[32m[20221214 00:06:12 @agent_ppo2.py:121][0m #------------------------ Iteration 4390 --------------------------#
[32m[20221214 00:06:12 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:06:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:06:12 @agent_ppo2.py:185][0m |           0.0019 |          45.1828 |          11.1477 |
[32m[20221214 00:06:12 @agent_ppo2.py:185][0m |          -0.0060 |          41.2728 |          11.1727 |
[32m[20221214 00:06:12 @agent_ppo2.py:185][0m |          -0.0039 |          40.0747 |          11.1785 |
[32m[20221214 00:06:12 @agent_ppo2.py:185][0m |          -0.0093 |          39.0501 |          11.2150 |
[32m[20221214 00:06:12 @agent_ppo2.py:185][0m |          -0.0112 |          38.5874 |          11.1888 |
[32m[20221214 00:06:12 @agent_ppo2.py:185][0m |          -0.0123 |          37.9990 |          11.1880 |
[32m[20221214 00:06:12 @agent_ppo2.py:185][0m |          -0.0111 |          37.7445 |          11.1398 |
[32m[20221214 00:06:13 @agent_ppo2.py:185][0m |          -0.0134 |          37.4121 |          11.1260 |
[32m[20221214 00:06:13 @agent_ppo2.py:185][0m |          -0.0140 |          37.0935 |          11.1812 |
[32m[20221214 00:06:13 @agent_ppo2.py:185][0m |          -0.0090 |          37.3814 |          11.1203 |
[32m[20221214 00:06:13 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:06:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 463.84
[32m[20221214 00:06:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 504.28
[32m[20221214 00:06:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 516.45
[32m[20221214 00:06:13 @agent_ppo2.py:143][0m Total time:       8.71 min
[32m[20221214 00:06:13 @agent_ppo2.py:145][0m 800768 total steps have happened
[32m[20221214 00:06:13 @agent_ppo2.py:121][0m #------------------------ Iteration 4391 --------------------------#
[32m[20221214 00:06:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:06:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:06:13 @agent_ppo2.py:185][0m |          -0.0018 |          43.0751 |          11.1234 |
[32m[20221214 00:06:13 @agent_ppo2.py:185][0m |          -0.0055 |          38.1779 |          11.1467 |
[32m[20221214 00:06:13 @agent_ppo2.py:185][0m |          -0.0132 |          36.2914 |          11.1209 |
[32m[20221214 00:06:13 @agent_ppo2.py:185][0m |          -0.0083 |          35.5100 |          11.0593 |
[32m[20221214 00:06:14 @agent_ppo2.py:185][0m |          -0.0166 |          34.2831 |          11.0590 |
[32m[20221214 00:06:14 @agent_ppo2.py:185][0m |          -0.0135 |          33.7504 |          11.0488 |
[32m[20221214 00:06:14 @agent_ppo2.py:185][0m |          -0.0076 |          35.5417 |          11.0782 |
[32m[20221214 00:06:14 @agent_ppo2.py:185][0m |          -0.0168 |          32.8721 |          11.0308 |
[32m[20221214 00:06:14 @agent_ppo2.py:185][0m |          -0.0188 |          32.4629 |          11.0594 |
[32m[20221214 00:06:14 @agent_ppo2.py:185][0m |          -0.0170 |          32.1025 |          11.0486 |
[32m[20221214 00:06:14 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:06:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 445.60
[32m[20221214 00:06:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 519.65
[32m[20221214 00:06:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 466.61
[32m[20221214 00:06:14 @agent_ppo2.py:143][0m Total time:       8.73 min
[32m[20221214 00:06:14 @agent_ppo2.py:145][0m 802816 total steps have happened
[32m[20221214 00:06:14 @agent_ppo2.py:121][0m #------------------------ Iteration 4392 --------------------------#
[32m[20221214 00:06:14 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:06:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:06:15 @agent_ppo2.py:185][0m |          -0.0005 |          56.4291 |          11.0413 |
[32m[20221214 00:06:15 @agent_ppo2.py:185][0m |          -0.0073 |          53.2580 |          10.9990 |
[32m[20221214 00:06:15 @agent_ppo2.py:185][0m |          -0.0119 |          52.3359 |          11.0378 |
[32m[20221214 00:06:15 @agent_ppo2.py:185][0m |          -0.0105 |          51.3557 |          11.0354 |
[32m[20221214 00:06:15 @agent_ppo2.py:185][0m |          -0.0153 |          50.8609 |          10.9819 |
[32m[20221214 00:06:15 @agent_ppo2.py:185][0m |          -0.0147 |          50.1892 |          10.9746 |
[32m[20221214 00:06:15 @agent_ppo2.py:185][0m |          -0.0189 |          50.0072 |          10.9660 |
[32m[20221214 00:06:15 @agent_ppo2.py:185][0m |          -0.0162 |          49.5450 |          10.9853 |
[32m[20221214 00:06:15 @agent_ppo2.py:185][0m |          -0.0151 |          49.6118 |          10.9377 |
[32m[20221214 00:06:15 @agent_ppo2.py:185][0m |          -0.0203 |          49.4371 |          10.9268 |
[32m[20221214 00:06:15 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:06:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 422.25
[32m[20221214 00:06:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 501.02
[32m[20221214 00:06:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 534.57
[32m[20221214 00:06:16 @agent_ppo2.py:143][0m Total time:       8.75 min
[32m[20221214 00:06:16 @agent_ppo2.py:145][0m 804864 total steps have happened
[32m[20221214 00:06:16 @agent_ppo2.py:121][0m #------------------------ Iteration 4393 --------------------------#
[32m[20221214 00:06:16 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:06:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:06:16 @agent_ppo2.py:185][0m |          -0.0011 |          49.0050 |          11.1673 |
[32m[20221214 00:06:16 @agent_ppo2.py:185][0m |          -0.0113 |          43.6654 |          11.1395 |
[32m[20221214 00:06:16 @agent_ppo2.py:185][0m |          -0.0132 |          42.4262 |          11.1303 |
[32m[20221214 00:06:16 @agent_ppo2.py:185][0m |          -0.0131 |          41.1981 |          11.0881 |
[32m[20221214 00:06:16 @agent_ppo2.py:185][0m |          -0.0158 |          40.4843 |          11.0804 |
[32m[20221214 00:06:16 @agent_ppo2.py:185][0m |          -0.0178 |          40.0620 |          11.0237 |
[32m[20221214 00:06:17 @agent_ppo2.py:185][0m |          -0.0181 |          39.6449 |          11.0430 |
[32m[20221214 00:06:17 @agent_ppo2.py:185][0m |          -0.0164 |          39.2233 |          10.9742 |
[32m[20221214 00:06:17 @agent_ppo2.py:185][0m |          -0.0190 |          38.9153 |          10.9812 |
[32m[20221214 00:06:17 @agent_ppo2.py:185][0m |          -0.0245 |          38.5678 |          10.9315 |
[32m[20221214 00:06:17 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:06:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 397.85
[32m[20221214 00:06:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 485.50
[32m[20221214 00:06:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 492.40
[32m[20221214 00:06:17 @agent_ppo2.py:143][0m Total time:       8.78 min
[32m[20221214 00:06:17 @agent_ppo2.py:145][0m 806912 total steps have happened
[32m[20221214 00:06:17 @agent_ppo2.py:121][0m #------------------------ Iteration 4394 --------------------------#
[32m[20221214 00:06:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:06:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:06:17 @agent_ppo2.py:185][0m |           0.0012 |          62.4482 |          10.4312 |
[32m[20221214 00:06:17 @agent_ppo2.py:185][0m |          -0.0095 |          55.9201 |          10.4029 |
[32m[20221214 00:06:18 @agent_ppo2.py:185][0m |          -0.0097 |          53.8175 |          10.4401 |
[32m[20221214 00:06:18 @agent_ppo2.py:185][0m |          -0.0012 |          57.1687 |          10.4279 |
[32m[20221214 00:06:18 @agent_ppo2.py:185][0m |          -0.0110 |          51.7539 |          10.4399 |
[32m[20221214 00:06:18 @agent_ppo2.py:185][0m |          -0.0095 |          51.1492 |          10.4558 |
[32m[20221214 00:06:18 @agent_ppo2.py:185][0m |          -0.0129 |          50.5023 |          10.4660 |
[32m[20221214 00:06:18 @agent_ppo2.py:185][0m |          -0.0171 |          50.0289 |          10.4758 |
[32m[20221214 00:06:18 @agent_ppo2.py:185][0m |          -0.0043 |          50.8103 |          10.4566 |
[32m[20221214 00:06:18 @agent_ppo2.py:185][0m |          -0.0100 |          50.3609 |          10.4722 |
[32m[20221214 00:06:18 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:06:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 514.80
[32m[20221214 00:06:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 550.48
[32m[20221214 00:06:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221214 00:06:18 @agent_ppo2.py:143][0m Total time:       8.80 min
[32m[20221214 00:06:18 @agent_ppo2.py:145][0m 808960 total steps have happened
[32m[20221214 00:06:18 @agent_ppo2.py:121][0m #------------------------ Iteration 4395 --------------------------#
[32m[20221214 00:06:18 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:06:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:06:19 @agent_ppo2.py:185][0m |           0.0020 |          48.3400 |          10.6774 |
[32m[20221214 00:06:19 @agent_ppo2.py:185][0m |          -0.0065 |          40.9352 |          10.7431 |
[32m[20221214 00:06:19 @agent_ppo2.py:185][0m |          -0.0075 |          39.0494 |          10.7462 |
[32m[20221214 00:06:19 @agent_ppo2.py:185][0m |          -0.0122 |          38.2244 |          10.7678 |
[32m[20221214 00:06:19 @agent_ppo2.py:185][0m |          -0.0183 |          37.6349 |          10.8077 |
[32m[20221214 00:06:19 @agent_ppo2.py:185][0m |          -0.0116 |          37.3814 |          10.8134 |
[32m[20221214 00:06:19 @agent_ppo2.py:185][0m |          -0.0102 |          37.5980 |          10.8067 |
[32m[20221214 00:06:19 @agent_ppo2.py:185][0m |          -0.0113 |          38.2957 |          10.8395 |
[32m[20221214 00:06:19 @agent_ppo2.py:185][0m |          -0.0173 |          36.8324 |          10.8685 |
[32m[20221214 00:06:19 @agent_ppo2.py:185][0m |          -0.0220 |          36.4586 |          10.8723 |
[32m[20221214 00:06:19 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:06:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 425.53
[32m[20221214 00:06:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 470.10
[32m[20221214 00:06:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 452.90
[32m[20221214 00:06:20 @agent_ppo2.py:143][0m Total time:       8.82 min
[32m[20221214 00:06:20 @agent_ppo2.py:145][0m 811008 total steps have happened
[32m[20221214 00:06:20 @agent_ppo2.py:121][0m #------------------------ Iteration 4396 --------------------------#
[32m[20221214 00:06:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:06:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:06:20 @agent_ppo2.py:185][0m |           0.0073 |          65.3066 |          10.9574 |
[32m[20221214 00:06:20 @agent_ppo2.py:185][0m |           0.0080 |          62.6900 |          10.9348 |
[32m[20221214 00:06:20 @agent_ppo2.py:185][0m |          -0.0055 |          57.9550 |          10.9316 |
[32m[20221214 00:06:20 @agent_ppo2.py:185][0m |          -0.0093 |          57.0525 |          10.9522 |
[32m[20221214 00:06:20 @agent_ppo2.py:185][0m |          -0.0082 |          56.6072 |          10.9604 |
[32m[20221214 00:06:20 @agent_ppo2.py:185][0m |          -0.0076 |          55.8930 |          10.9588 |
[32m[20221214 00:06:21 @agent_ppo2.py:185][0m |           0.0011 |          56.2825 |          10.9184 |
[32m[20221214 00:06:21 @agent_ppo2.py:185][0m |          -0.0099 |          55.3991 |          10.9208 |
[32m[20221214 00:06:21 @agent_ppo2.py:185][0m |           0.0021 |          58.2234 |          10.9027 |
[32m[20221214 00:06:21 @agent_ppo2.py:185][0m |          -0.0087 |          54.8649 |          10.8588 |
[32m[20221214 00:06:21 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:06:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 499.59
[32m[20221214 00:06:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 516.80
[32m[20221214 00:06:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 404.22
[32m[20221214 00:06:21 @agent_ppo2.py:143][0m Total time:       8.84 min
[32m[20221214 00:06:21 @agent_ppo2.py:145][0m 813056 total steps have happened
[32m[20221214 00:06:21 @agent_ppo2.py:121][0m #------------------------ Iteration 4397 --------------------------#
[32m[20221214 00:06:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:06:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:06:21 @agent_ppo2.py:185][0m |          -0.0011 |          56.9314 |          11.0188 |
[32m[20221214 00:06:21 @agent_ppo2.py:185][0m |          -0.0050 |          52.3375 |          11.0049 |
[32m[20221214 00:06:22 @agent_ppo2.py:185][0m |          -0.0075 |          51.0455 |          10.9907 |
[32m[20221214 00:06:22 @agent_ppo2.py:185][0m |          -0.0054 |          50.4451 |          10.9979 |
[32m[20221214 00:06:22 @agent_ppo2.py:185][0m |          -0.0067 |          49.8711 |          10.9989 |
[32m[20221214 00:06:22 @agent_ppo2.py:185][0m |          -0.0120 |          49.3219 |          10.9886 |
[32m[20221214 00:06:22 @agent_ppo2.py:185][0m |          -0.0152 |          48.8707 |          10.9830 |
[32m[20221214 00:06:22 @agent_ppo2.py:185][0m |          -0.0149 |          48.4235 |          10.9547 |
[32m[20221214 00:06:22 @agent_ppo2.py:185][0m |          -0.0130 |          48.1343 |          10.9543 |
[32m[20221214 00:06:22 @agent_ppo2.py:185][0m |          -0.0178 |          47.9285 |          10.9249 |
[32m[20221214 00:06:22 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:06:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 431.31
[32m[20221214 00:06:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 495.33
[32m[20221214 00:06:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 518.45
[32m[20221214 00:06:22 @agent_ppo2.py:143][0m Total time:       8.87 min
[32m[20221214 00:06:22 @agent_ppo2.py:145][0m 815104 total steps have happened
[32m[20221214 00:06:22 @agent_ppo2.py:121][0m #------------------------ Iteration 4398 --------------------------#
[32m[20221214 00:06:22 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:06:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:06:23 @agent_ppo2.py:185][0m |          -0.0003 |          64.2491 |          10.6720 |
[32m[20221214 00:06:23 @agent_ppo2.py:185][0m |          -0.0055 |          62.3826 |          10.6869 |
[32m[20221214 00:06:23 @agent_ppo2.py:185][0m |          -0.0100 |          61.2161 |          10.6222 |
[32m[20221214 00:06:23 @agent_ppo2.py:185][0m |          -0.0135 |          60.6702 |          10.6466 |
[32m[20221214 00:06:23 @agent_ppo2.py:185][0m |          -0.0124 |          60.3392 |          10.6023 |
[32m[20221214 00:06:23 @agent_ppo2.py:185][0m |          -0.0144 |          59.9356 |          10.5976 |
[32m[20221214 00:06:23 @agent_ppo2.py:185][0m |          -0.0107 |          61.7107 |          10.5740 |
[32m[20221214 00:06:23 @agent_ppo2.py:185][0m |          -0.0150 |          59.3724 |          10.5810 |
[32m[20221214 00:06:23 @agent_ppo2.py:185][0m |          -0.0158 |          59.1147 |          10.5697 |
[32m[20221214 00:06:23 @agent_ppo2.py:185][0m |          -0.0200 |          58.9662 |          10.5787 |
[32m[20221214 00:06:23 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:06:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 370.04
[32m[20221214 00:06:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 469.02
[32m[20221214 00:06:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 464.24
[32m[20221214 00:06:24 @agent_ppo2.py:143][0m Total time:       8.89 min
[32m[20221214 00:06:24 @agent_ppo2.py:145][0m 817152 total steps have happened
[32m[20221214 00:06:24 @agent_ppo2.py:121][0m #------------------------ Iteration 4399 --------------------------#
[32m[20221214 00:06:24 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:06:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:06:24 @agent_ppo2.py:185][0m |          -0.0004 |          66.5531 |          10.6124 |
[32m[20221214 00:06:24 @agent_ppo2.py:185][0m |          -0.0046 |          63.6215 |          10.6122 |
[32m[20221214 00:06:24 @agent_ppo2.py:185][0m |          -0.0051 |          62.3636 |          10.5684 |
[32m[20221214 00:06:24 @agent_ppo2.py:185][0m |          -0.0051 |          61.1468 |          10.6292 |
[32m[20221214 00:06:24 @agent_ppo2.py:185][0m |          -0.0074 |          60.6410 |          10.5444 |
[32m[20221214 00:06:24 @agent_ppo2.py:185][0m |          -0.0097 |          59.9171 |          10.5757 |
[32m[20221214 00:06:24 @agent_ppo2.py:185][0m |          -0.0112 |          59.4592 |          10.5970 |
[32m[20221214 00:06:25 @agent_ppo2.py:185][0m |          -0.0106 |          59.1954 |          10.5447 |
[32m[20221214 00:06:25 @agent_ppo2.py:185][0m |           0.0018 |          63.1215 |          10.5190 |
[32m[20221214 00:06:25 @agent_ppo2.py:185][0m |          -0.0160 |          59.0441 |          10.6039 |
[32m[20221214 00:06:25 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:06:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 446.95
[32m[20221214 00:06:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 466.96
[32m[20221214 00:06:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 493.52
[32m[20221214 00:06:25 @agent_ppo2.py:143][0m Total time:       8.91 min
[32m[20221214 00:06:25 @agent_ppo2.py:145][0m 819200 total steps have happened
[32m[20221214 00:06:25 @agent_ppo2.py:121][0m #------------------------ Iteration 4400 --------------------------#
[32m[20221214 00:06:25 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:06:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:06:25 @agent_ppo2.py:185][0m |           0.0002 |          59.8061 |          10.5950 |
[32m[20221214 00:06:25 @agent_ppo2.py:185][0m |          -0.0005 |          56.7324 |          10.6335 |
[32m[20221214 00:06:25 @agent_ppo2.py:185][0m |           0.0047 |          64.0751 |          10.6098 |
[32m[20221214 00:06:26 @agent_ppo2.py:185][0m |          -0.0077 |          55.1150 |          10.6492 |
[32m[20221214 00:06:26 @agent_ppo2.py:185][0m |          -0.0125 |          54.1563 |          10.5712 |
[32m[20221214 00:06:26 @agent_ppo2.py:185][0m |          -0.0110 |          53.6660 |          10.6165 |
[32m[20221214 00:06:26 @agent_ppo2.py:185][0m |          -0.0092 |          53.4637 |          10.5898 |
[32m[20221214 00:06:26 @agent_ppo2.py:185][0m |          -0.0124 |          53.2268 |          10.5889 |
[32m[20221214 00:06:26 @agent_ppo2.py:185][0m |          -0.0109 |          52.7188 |          10.5649 |
[32m[20221214 00:06:26 @agent_ppo2.py:185][0m |          -0.0153 |          52.5578 |          10.5756 |
[32m[20221214 00:06:26 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:06:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 440.56
[32m[20221214 00:06:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 466.08
[32m[20221214 00:06:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 384.50
[32m[20221214 00:06:26 @agent_ppo2.py:143][0m Total time:       8.93 min
[32m[20221214 00:06:26 @agent_ppo2.py:145][0m 821248 total steps have happened
[32m[20221214 00:06:26 @agent_ppo2.py:121][0m #------------------------ Iteration 4401 --------------------------#
[32m[20221214 00:06:26 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:06:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:06:27 @agent_ppo2.py:185][0m |           0.0074 |          63.5019 |          10.2906 |
[32m[20221214 00:06:27 @agent_ppo2.py:185][0m |          -0.0071 |          56.2470 |          10.2655 |
[32m[20221214 00:06:27 @agent_ppo2.py:185][0m |          -0.0088 |          55.0753 |          10.2579 |
[32m[20221214 00:06:27 @agent_ppo2.py:185][0m |          -0.0104 |          54.3000 |          10.2658 |
[32m[20221214 00:06:27 @agent_ppo2.py:185][0m |          -0.0109 |          53.7099 |          10.2699 |
[32m[20221214 00:06:27 @agent_ppo2.py:185][0m |          -0.0123 |          53.2019 |          10.2992 |
[32m[20221214 00:06:27 @agent_ppo2.py:185][0m |          -0.0123 |          52.9883 |          10.3215 |
[32m[20221214 00:06:27 @agent_ppo2.py:185][0m |          -0.0123 |          52.4147 |          10.3121 |
[32m[20221214 00:06:27 @agent_ppo2.py:185][0m |          -0.0161 |          52.0554 |          10.3169 |
[32m[20221214 00:06:27 @agent_ppo2.py:185][0m |          -0.0168 |          51.8810 |          10.3498 |
[32m[20221214 00:06:27 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:06:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 427.78
[32m[20221214 00:06:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 471.03
[32m[20221214 00:06:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 509.08
[32m[20221214 00:06:27 @agent_ppo2.py:143][0m Total time:       8.95 min
[32m[20221214 00:06:27 @agent_ppo2.py:145][0m 823296 total steps have happened
[32m[20221214 00:06:27 @agent_ppo2.py:121][0m #------------------------ Iteration 4402 --------------------------#
[32m[20221214 00:06:28 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:06:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:06:28 @agent_ppo2.py:185][0m |          -0.0003 |          65.1668 |          10.5943 |
[32m[20221214 00:06:28 @agent_ppo2.py:185][0m |          -0.0070 |          60.5850 |          10.6115 |
[32m[20221214 00:06:28 @agent_ppo2.py:185][0m |          -0.0094 |          59.0201 |          10.5725 |
[32m[20221214 00:06:28 @agent_ppo2.py:185][0m |          -0.0123 |          58.0056 |          10.6227 |
[32m[20221214 00:06:28 @agent_ppo2.py:185][0m |          -0.0136 |          57.2458 |          10.6263 |
[32m[20221214 00:06:28 @agent_ppo2.py:185][0m |          -0.0146 |          57.1403 |          10.5921 |
[32m[20221214 00:06:28 @agent_ppo2.py:185][0m |          -0.0156 |          56.1150 |          10.5964 |
[32m[20221214 00:06:28 @agent_ppo2.py:185][0m |          -0.0162 |          55.6341 |          10.5790 |
[32m[20221214 00:06:29 @agent_ppo2.py:185][0m |          -0.0138 |          55.9977 |          10.5420 |
[32m[20221214 00:06:29 @agent_ppo2.py:185][0m |          -0.0168 |          54.9384 |          10.5703 |
[32m[20221214 00:06:29 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:06:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 470.60
[32m[20221214 00:06:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 516.36
[32m[20221214 00:06:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 518.63
[32m[20221214 00:06:29 @agent_ppo2.py:143][0m Total time:       8.97 min
[32m[20221214 00:06:29 @agent_ppo2.py:145][0m 825344 total steps have happened
[32m[20221214 00:06:29 @agent_ppo2.py:121][0m #------------------------ Iteration 4403 --------------------------#
[32m[20221214 00:06:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:06:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:06:29 @agent_ppo2.py:185][0m |           0.0008 |          60.6432 |          10.3219 |
[32m[20221214 00:06:29 @agent_ppo2.py:185][0m |           0.0018 |          55.2516 |          10.3146 |
[32m[20221214 00:06:29 @agent_ppo2.py:185][0m |          -0.0068 |          52.9069 |          10.2671 |
[32m[20221214 00:06:29 @agent_ppo2.py:185][0m |          -0.0048 |          51.9460 |          10.2932 |
[32m[20221214 00:06:29 @agent_ppo2.py:185][0m |          -0.0090 |          51.1076 |          10.2732 |
[32m[20221214 00:06:30 @agent_ppo2.py:185][0m |          -0.0095 |          50.5913 |          10.2840 |
[32m[20221214 00:06:30 @agent_ppo2.py:185][0m |          -0.0105 |          50.3820 |          10.2554 |
[32m[20221214 00:06:30 @agent_ppo2.py:185][0m |          -0.0179 |          49.9966 |          10.2455 |
[32m[20221214 00:06:30 @agent_ppo2.py:185][0m |          -0.0097 |          49.8914 |          10.2113 |
[32m[20221214 00:06:30 @agent_ppo2.py:185][0m |          -0.0071 |          50.3444 |          10.2640 |
[32m[20221214 00:06:30 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:06:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 427.18
[32m[20221214 00:06:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 468.68
[32m[20221214 00:06:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 458.19
[32m[20221214 00:06:30 @agent_ppo2.py:143][0m Total time:       9.00 min
[32m[20221214 00:06:30 @agent_ppo2.py:145][0m 827392 total steps have happened
[32m[20221214 00:06:30 @agent_ppo2.py:121][0m #------------------------ Iteration 4404 --------------------------#
[32m[20221214 00:06:30 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:06:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:06:30 @agent_ppo2.py:185][0m |           0.0013 |          60.3155 |          10.8051 |
[32m[20221214 00:06:31 @agent_ppo2.py:185][0m |          -0.0073 |          56.3238 |          10.9067 |
[32m[20221214 00:06:31 @agent_ppo2.py:185][0m |          -0.0107 |          55.1248 |          10.9526 |
[32m[20221214 00:06:31 @agent_ppo2.py:185][0m |          -0.0139 |          54.1400 |          10.9489 |
[32m[20221214 00:06:31 @agent_ppo2.py:185][0m |          -0.0108 |          53.8444 |          10.9541 |
[32m[20221214 00:06:31 @agent_ppo2.py:185][0m |          -0.0067 |          54.7224 |          10.9290 |
[32m[20221214 00:06:31 @agent_ppo2.py:185][0m |          -0.0167 |          52.6110 |          11.0120 |
[32m[20221214 00:06:31 @agent_ppo2.py:185][0m |          -0.0116 |          53.6508 |          10.9820 |
[32m[20221214 00:06:31 @agent_ppo2.py:185][0m |          -0.0183 |          51.6748 |          11.0178 |
[32m[20221214 00:06:31 @agent_ppo2.py:185][0m |          -0.0074 |          54.2419 |          11.0318 |
[32m[20221214 00:06:31 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:06:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 410.20
[32m[20221214 00:06:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 470.27
[32m[20221214 00:06:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 527.53
[32m[20221214 00:06:31 @agent_ppo2.py:143][0m Total time:       9.02 min
[32m[20221214 00:06:31 @agent_ppo2.py:145][0m 829440 total steps have happened
[32m[20221214 00:06:31 @agent_ppo2.py:121][0m #------------------------ Iteration 4405 --------------------------#
[32m[20221214 00:06:32 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:06:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:06:32 @agent_ppo2.py:185][0m |           0.0016 |          56.4872 |          10.4707 |
[32m[20221214 00:06:32 @agent_ppo2.py:185][0m |          -0.0008 |          53.4314 |          10.4427 |
[32m[20221214 00:06:32 @agent_ppo2.py:185][0m |          -0.0040 |          52.4899 |          10.4197 |
[32m[20221214 00:06:32 @agent_ppo2.py:185][0m |          -0.0036 |          52.0500 |          10.4654 |
[32m[20221214 00:06:32 @agent_ppo2.py:185][0m |           0.0037 |          56.8478 |          10.4552 |
[32m[20221214 00:06:32 @agent_ppo2.py:185][0m |          -0.0015 |          52.1620 |          10.4428 |
[32m[20221214 00:06:32 @agent_ppo2.py:185][0m |          -0.0044 |          51.6221 |          10.4131 |
[32m[20221214 00:06:32 @agent_ppo2.py:185][0m |          -0.0049 |          51.3515 |          10.3764 |
[32m[20221214 00:06:32 @agent_ppo2.py:185][0m |          -0.0093 |          50.7610 |          10.3761 |
[32m[20221214 00:06:32 @agent_ppo2.py:185][0m |          -0.0071 |          51.1508 |          10.3921 |
[32m[20221214 00:06:32 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:06:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 501.05
[32m[20221214 00:06:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 546.98
[32m[20221214 00:06:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 498.16
[32m[20221214 00:06:33 @agent_ppo2.py:143][0m Total time:       9.04 min
[32m[20221214 00:06:33 @agent_ppo2.py:145][0m 831488 total steps have happened
[32m[20221214 00:06:33 @agent_ppo2.py:121][0m #------------------------ Iteration 4406 --------------------------#
[32m[20221214 00:06:33 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:06:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:06:33 @agent_ppo2.py:185][0m |           0.0033 |          67.2111 |          10.4689 |
[32m[20221214 00:06:33 @agent_ppo2.py:185][0m |          -0.0027 |          62.2849 |          10.4776 |
[32m[20221214 00:06:33 @agent_ppo2.py:185][0m |          -0.0034 |          61.4123 |          10.4929 |
[32m[20221214 00:06:33 @agent_ppo2.py:185][0m |          -0.0012 |          64.7318 |          10.4992 |
[32m[20221214 00:06:33 @agent_ppo2.py:185][0m |          -0.0105 |          60.0643 |          10.5530 |
[32m[20221214 00:06:33 @agent_ppo2.py:185][0m |          -0.0001 |          68.2081 |          10.5935 |
[32m[20221214 00:06:34 @agent_ppo2.py:185][0m |          -0.0166 |          59.4776 |          10.6116 |
[32m[20221214 00:06:34 @agent_ppo2.py:185][0m |          -0.0135 |          59.2206 |          10.6334 |
[32m[20221214 00:06:34 @agent_ppo2.py:185][0m |          -0.0152 |          59.1493 |          10.6680 |
[32m[20221214 00:06:34 @agent_ppo2.py:185][0m |          -0.0153 |          58.8646 |          10.6678 |
[32m[20221214 00:06:34 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:06:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 456.33
[32m[20221214 00:06:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 498.09
[32m[20221214 00:06:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 535.84
[32m[20221214 00:06:34 @agent_ppo2.py:143][0m Total time:       9.06 min
[32m[20221214 00:06:34 @agent_ppo2.py:145][0m 833536 total steps have happened
[32m[20221214 00:06:34 @agent_ppo2.py:121][0m #------------------------ Iteration 4407 --------------------------#
[32m[20221214 00:06:34 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:06:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:06:34 @agent_ppo2.py:185][0m |           0.0013 |          61.9244 |          10.6960 |
[32m[20221214 00:06:34 @agent_ppo2.py:185][0m |          -0.0065 |          56.7564 |          10.7288 |
[32m[20221214 00:06:35 @agent_ppo2.py:185][0m |          -0.0050 |          55.5772 |          10.7765 |
[32m[20221214 00:06:35 @agent_ppo2.py:185][0m |          -0.0062 |          55.2973 |          10.7552 |
[32m[20221214 00:06:35 @agent_ppo2.py:185][0m |          -0.0017 |          56.2118 |          10.7567 |
[32m[20221214 00:06:35 @agent_ppo2.py:185][0m |          -0.0088 |          53.9410 |          10.7284 |
[32m[20221214 00:06:35 @agent_ppo2.py:185][0m |          -0.0092 |          53.8334 |          10.7448 |
[32m[20221214 00:06:35 @agent_ppo2.py:185][0m |          -0.0092 |          53.3969 |          10.7493 |
[32m[20221214 00:06:35 @agent_ppo2.py:185][0m |          -0.0086 |          53.5196 |          10.7226 |
[32m[20221214 00:06:35 @agent_ppo2.py:185][0m |          -0.0086 |          53.5227 |          10.7135 |
[32m[20221214 00:06:35 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:06:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 505.64
[32m[20221214 00:06:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 556.02
[32m[20221214 00:06:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 573.51
[32m[20221214 00:06:35 @agent_ppo2.py:143][0m Total time:       9.08 min
[32m[20221214 00:06:35 @agent_ppo2.py:145][0m 835584 total steps have happened
[32m[20221214 00:06:35 @agent_ppo2.py:121][0m #------------------------ Iteration 4408 --------------------------#
[32m[20221214 00:06:36 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:06:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:06:36 @agent_ppo2.py:185][0m |          -0.0022 |          69.7371 |          10.7164 |
[32m[20221214 00:06:36 @agent_ppo2.py:185][0m |          -0.0042 |          68.0527 |          10.7491 |
[32m[20221214 00:06:36 @agent_ppo2.py:185][0m |          -0.0093 |          66.4969 |          10.7220 |
[32m[20221214 00:06:36 @agent_ppo2.py:185][0m |          -0.0119 |          65.7903 |          10.7034 |
[32m[20221214 00:06:36 @agent_ppo2.py:185][0m |          -0.0116 |          65.1856 |          10.7073 |
[32m[20221214 00:06:36 @agent_ppo2.py:185][0m |          -0.0048 |          69.8694 |          10.7032 |
[32m[20221214 00:06:36 @agent_ppo2.py:185][0m |          -0.0163 |          64.5552 |          10.7128 |
[32m[20221214 00:06:36 @agent_ppo2.py:185][0m |          -0.0134 |          64.6111 |          10.6815 |
[32m[20221214 00:06:36 @agent_ppo2.py:185][0m |          -0.0087 |          64.7900 |          10.7322 |
[32m[20221214 00:06:36 @agent_ppo2.py:185][0m |          -0.0166 |          63.9519 |          10.6597 |
[32m[20221214 00:06:36 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:06:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 473.16
[32m[20221214 00:06:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 486.03
[32m[20221214 00:06:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 508.62
[32m[20221214 00:06:37 @agent_ppo2.py:143][0m Total time:       9.10 min
[32m[20221214 00:06:37 @agent_ppo2.py:145][0m 837632 total steps have happened
[32m[20221214 00:06:37 @agent_ppo2.py:121][0m #------------------------ Iteration 4409 --------------------------#
[32m[20221214 00:06:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:06:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:06:37 @agent_ppo2.py:185][0m |           0.0006 |          56.1751 |          11.1430 |
[32m[20221214 00:06:37 @agent_ppo2.py:185][0m |          -0.0055 |          52.2889 |          11.1816 |
[32m[20221214 00:06:37 @agent_ppo2.py:185][0m |          -0.0033 |          51.7758 |          11.2150 |
[32m[20221214 00:06:37 @agent_ppo2.py:185][0m |          -0.0094 |          50.1398 |          11.2280 |
[32m[20221214 00:06:37 @agent_ppo2.py:185][0m |          -0.0151 |          49.5432 |          11.2271 |
[32m[20221214 00:06:37 @agent_ppo2.py:185][0m |          -0.0055 |          49.4560 |          11.2197 |
[32m[20221214 00:06:37 @agent_ppo2.py:185][0m |          -0.0154 |          48.4648 |          11.2255 |
[32m[20221214 00:06:38 @agent_ppo2.py:185][0m |          -0.0205 |          48.2631 |          11.2203 |
[32m[20221214 00:06:38 @agent_ppo2.py:185][0m |          -0.0141 |          47.7412 |          11.2793 |
[32m[20221214 00:06:38 @agent_ppo2.py:185][0m |          -0.0137 |          47.5288 |          11.2703 |
[32m[20221214 00:06:38 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:06:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 501.00
[32m[20221214 00:06:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 552.42
[32m[20221214 00:06:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 430.88
[32m[20221214 00:06:38 @agent_ppo2.py:143][0m Total time:       9.12 min
[32m[20221214 00:06:38 @agent_ppo2.py:145][0m 839680 total steps have happened
[32m[20221214 00:06:38 @agent_ppo2.py:121][0m #------------------------ Iteration 4410 --------------------------#
[32m[20221214 00:06:38 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:06:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:06:38 @agent_ppo2.py:185][0m |          -0.0036 |          60.3403 |          10.5621 |
[32m[20221214 00:06:38 @agent_ppo2.py:185][0m |          -0.0100 |          57.8957 |          10.5615 |
[32m[20221214 00:06:38 @agent_ppo2.py:185][0m |          -0.0095 |          57.1639 |          10.5980 |
[32m[20221214 00:06:38 @agent_ppo2.py:185][0m |          -0.0106 |          56.6158 |          10.6106 |
[32m[20221214 00:06:39 @agent_ppo2.py:185][0m |          -0.0103 |          56.3011 |          10.6444 |
[32m[20221214 00:06:39 @agent_ppo2.py:185][0m |          -0.0153 |          55.9137 |          10.6318 |
[32m[20221214 00:06:39 @agent_ppo2.py:185][0m |          -0.0090 |          57.3866 |          10.6380 |
[32m[20221214 00:06:39 @agent_ppo2.py:185][0m |          -0.0099 |          56.4484 |          10.6982 |
[32m[20221214 00:06:39 @agent_ppo2.py:185][0m |          -0.0158 |          55.0533 |          10.6921 |
[32m[20221214 00:06:39 @agent_ppo2.py:185][0m |          -0.0171 |          55.0083 |          10.6991 |
[32m[20221214 00:06:39 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:06:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 439.08
[32m[20221214 00:06:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 478.40
[32m[20221214 00:06:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 458.65
[32m[20221214 00:06:39 @agent_ppo2.py:143][0m Total time:       9.15 min
[32m[20221214 00:06:39 @agent_ppo2.py:145][0m 841728 total steps have happened
[32m[20221214 00:06:39 @agent_ppo2.py:121][0m #------------------------ Iteration 4411 --------------------------#
[32m[20221214 00:06:39 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:06:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:06:39 @agent_ppo2.py:185][0m |           0.0049 |          67.4396 |          10.7329 |
[32m[20221214 00:06:40 @agent_ppo2.py:185][0m |          -0.0046 |          64.3645 |          10.7103 |
[32m[20221214 00:06:40 @agent_ppo2.py:185][0m |          -0.0077 |          63.5667 |          10.7202 |
[32m[20221214 00:06:40 @agent_ppo2.py:185][0m |          -0.0079 |          62.7566 |          10.7067 |
[32m[20221214 00:06:40 @agent_ppo2.py:185][0m |          -0.0090 |          62.2643 |          10.7074 |
[32m[20221214 00:06:40 @agent_ppo2.py:185][0m |          -0.0119 |          61.9251 |          10.7299 |
[32m[20221214 00:06:40 @agent_ppo2.py:185][0m |          -0.0141 |          61.4486 |          10.7618 |
[32m[20221214 00:06:40 @agent_ppo2.py:185][0m |          -0.0128 |          61.2823 |          10.7424 |
[32m[20221214 00:06:40 @agent_ppo2.py:185][0m |          -0.0174 |          60.8744 |          10.7253 |
[32m[20221214 00:06:40 @agent_ppo2.py:185][0m |          -0.0142 |          60.7378 |          10.7289 |
[32m[20221214 00:06:40 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:06:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 484.99
[32m[20221214 00:06:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 516.22
[32m[20221214 00:06:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 542.94
[32m[20221214 00:06:40 @agent_ppo2.py:143][0m Total time:       9.17 min
[32m[20221214 00:06:40 @agent_ppo2.py:145][0m 843776 total steps have happened
[32m[20221214 00:06:40 @agent_ppo2.py:121][0m #------------------------ Iteration 4412 --------------------------#
[32m[20221214 00:06:41 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:06:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:06:41 @agent_ppo2.py:185][0m |          -0.0019 |          46.8820 |          10.4551 |
[32m[20221214 00:06:41 @agent_ppo2.py:185][0m |          -0.0056 |          40.9555 |          10.4525 |
[32m[20221214 00:06:41 @agent_ppo2.py:185][0m |          -0.0112 |          39.3541 |          10.4764 |
[32m[20221214 00:06:41 @agent_ppo2.py:185][0m |          -0.0132 |          38.3001 |          10.4939 |
[32m[20221214 00:06:41 @agent_ppo2.py:185][0m |          -0.0074 |          37.6479 |          10.4671 |
[32m[20221214 00:06:41 @agent_ppo2.py:185][0m |          -0.0120 |          37.2591 |          10.5270 |
[32m[20221214 00:06:41 @agent_ppo2.py:185][0m |          -0.0145 |          36.6609 |          10.5024 |
[32m[20221214 00:06:41 @agent_ppo2.py:185][0m |          -0.0132 |          36.0862 |          10.5299 |
[32m[20221214 00:06:41 @agent_ppo2.py:185][0m |          -0.0171 |          35.8610 |          10.5008 |
[32m[20221214 00:06:42 @agent_ppo2.py:185][0m |          -0.0204 |          35.4583 |          10.4915 |
[32m[20221214 00:06:42 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:06:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 409.49
[32m[20221214 00:06:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 486.96
[32m[20221214 00:06:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 416.78
[32m[20221214 00:06:42 @agent_ppo2.py:143][0m Total time:       9.19 min
[32m[20221214 00:06:42 @agent_ppo2.py:145][0m 845824 total steps have happened
[32m[20221214 00:06:42 @agent_ppo2.py:121][0m #------------------------ Iteration 4413 --------------------------#
[32m[20221214 00:06:42 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:06:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:06:42 @agent_ppo2.py:185][0m |           0.0170 |          64.8326 |          10.8227 |
[32m[20221214 00:06:42 @agent_ppo2.py:185][0m |          -0.0045 |          49.6604 |          10.9023 |
[32m[20221214 00:06:42 @agent_ppo2.py:185][0m |          -0.0081 |          48.0383 |          10.8798 |
[32m[20221214 00:06:42 @agent_ppo2.py:185][0m |          -0.0139 |          47.4947 |          10.8090 |
[32m[20221214 00:06:42 @agent_ppo2.py:185][0m |           0.0003 |          59.1807 |          10.8476 |
[32m[20221214 00:06:42 @agent_ppo2.py:185][0m |          -0.0115 |          47.0227 |          10.8544 |
[32m[20221214 00:06:43 @agent_ppo2.py:185][0m |          -0.0181 |          46.4592 |          10.8736 |
[32m[20221214 00:06:43 @agent_ppo2.py:185][0m |          -0.0208 |          46.2083 |          10.8138 |
[32m[20221214 00:06:43 @agent_ppo2.py:185][0m |          -0.0210 |          45.9249 |          10.8406 |
[32m[20221214 00:06:43 @agent_ppo2.py:185][0m |          -0.0120 |          45.7029 |          10.8409 |
[32m[20221214 00:06:43 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:06:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 414.31
[32m[20221214 00:06:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 450.52
[32m[20221214 00:06:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 530.54
[32m[20221214 00:06:43 @agent_ppo2.py:143][0m Total time:       9.21 min
[32m[20221214 00:06:43 @agent_ppo2.py:145][0m 847872 total steps have happened
[32m[20221214 00:06:43 @agent_ppo2.py:121][0m #------------------------ Iteration 4414 --------------------------#
[32m[20221214 00:06:43 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:06:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:06:43 @agent_ppo2.py:185][0m |           0.0035 |          60.4790 |          10.3474 |
[32m[20221214 00:06:43 @agent_ppo2.py:185][0m |          -0.0069 |          53.9443 |          10.3352 |
[32m[20221214 00:06:44 @agent_ppo2.py:185][0m |          -0.0052 |          52.2509 |          10.3750 |
[32m[20221214 00:06:44 @agent_ppo2.py:185][0m |          -0.0096 |          50.9042 |          10.3762 |
[32m[20221214 00:06:44 @agent_ppo2.py:185][0m |          -0.0133 |          49.5927 |          10.3994 |
[32m[20221214 00:06:44 @agent_ppo2.py:185][0m |          -0.0116 |          48.7087 |          10.3773 |
[32m[20221214 00:06:44 @agent_ppo2.py:185][0m |          -0.0163 |          47.8564 |          10.3631 |
[32m[20221214 00:06:44 @agent_ppo2.py:185][0m |          -0.0179 |          47.2690 |          10.4021 |
[32m[20221214 00:06:44 @agent_ppo2.py:185][0m |          -0.0180 |          46.8152 |          10.3819 |
[32m[20221214 00:06:44 @agent_ppo2.py:185][0m |          -0.0161 |          46.6023 |          10.4002 |
[32m[20221214 00:06:44 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:06:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 461.83
[32m[20221214 00:06:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 551.06
[32m[20221214 00:06:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 400.97
[32m[20221214 00:06:44 @agent_ppo2.py:143][0m Total time:       9.23 min
[32m[20221214 00:06:44 @agent_ppo2.py:145][0m 849920 total steps have happened
[32m[20221214 00:06:44 @agent_ppo2.py:121][0m #------------------------ Iteration 4415 --------------------------#
[32m[20221214 00:06:44 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:06:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:06:45 @agent_ppo2.py:185][0m |          -0.0046 |          33.0406 |          10.4298 |
[32m[20221214 00:06:45 @agent_ppo2.py:185][0m |          -0.0063 |          26.1902 |          10.4608 |
[32m[20221214 00:06:45 @agent_ppo2.py:185][0m |          -0.0070 |          24.3027 |          10.4578 |
[32m[20221214 00:06:45 @agent_ppo2.py:185][0m |          -0.0149 |          23.3495 |          10.4382 |
[32m[20221214 00:06:45 @agent_ppo2.py:185][0m |          -0.0063 |          22.8089 |          10.3905 |
[32m[20221214 00:06:45 @agent_ppo2.py:185][0m |          -0.0093 |          22.0894 |          10.4458 |
[32m[20221214 00:06:45 @agent_ppo2.py:185][0m |          -0.0091 |          21.6873 |          10.4371 |
[32m[20221214 00:06:45 @agent_ppo2.py:185][0m |          -0.0118 |          21.3209 |          10.4500 |
[32m[20221214 00:06:45 @agent_ppo2.py:185][0m |          -0.0211 |          21.1630 |          10.4237 |
[32m[20221214 00:06:45 @agent_ppo2.py:185][0m |          -0.0130 |          20.7228 |          10.4348 |
[32m[20221214 00:06:45 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:06:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 349.57
[32m[20221214 00:06:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 463.95
[32m[20221214 00:06:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 560.23
[32m[20221214 00:06:46 @agent_ppo2.py:143][0m Total time:       9.25 min
[32m[20221214 00:06:46 @agent_ppo2.py:145][0m 851968 total steps have happened
[32m[20221214 00:06:46 @agent_ppo2.py:121][0m #------------------------ Iteration 4416 --------------------------#
[32m[20221214 00:06:46 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:06:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:06:46 @agent_ppo2.py:185][0m |          -0.0020 |          44.0287 |          10.3821 |
[32m[20221214 00:06:46 @agent_ppo2.py:185][0m |          -0.0098 |          39.8952 |          10.4605 |
[32m[20221214 00:06:46 @agent_ppo2.py:185][0m |          -0.0067 |          38.2270 |          10.4265 |
[32m[20221214 00:06:46 @agent_ppo2.py:185][0m |           0.0005 |          42.0018 |          10.4606 |
[32m[20221214 00:06:46 @agent_ppo2.py:185][0m |          -0.0140 |          36.2342 |          10.4092 |
[32m[20221214 00:06:46 @agent_ppo2.py:185][0m |          -0.0150 |          35.5349 |          10.4546 |
[32m[20221214 00:06:46 @agent_ppo2.py:185][0m |          -0.0200 |          34.8245 |          10.4028 |
[32m[20221214 00:06:47 @agent_ppo2.py:185][0m |          -0.0149 |          34.6543 |          10.3865 |
[32m[20221214 00:06:47 @agent_ppo2.py:185][0m |          -0.0144 |          34.6714 |          10.3442 |
[32m[20221214 00:06:47 @agent_ppo2.py:185][0m |          -0.0201 |          33.7578 |          10.3701 |
[32m[20221214 00:06:47 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:06:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 353.15
[32m[20221214 00:06:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 444.95
[32m[20221214 00:06:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 446.85
[32m[20221214 00:06:47 @agent_ppo2.py:143][0m Total time:       9.28 min
[32m[20221214 00:06:47 @agent_ppo2.py:145][0m 854016 total steps have happened
[32m[20221214 00:06:47 @agent_ppo2.py:121][0m #------------------------ Iteration 4417 --------------------------#
[32m[20221214 00:06:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:06:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:06:47 @agent_ppo2.py:185][0m |           0.0021 |          57.6084 |          10.5210 |
[32m[20221214 00:06:47 @agent_ppo2.py:185][0m |          -0.0001 |          51.7886 |          10.4538 |
[32m[20221214 00:06:47 @agent_ppo2.py:185][0m |          -0.0109 |          48.9989 |          10.4388 |
[32m[20221214 00:06:47 @agent_ppo2.py:185][0m |          -0.0117 |          47.5862 |          10.4399 |
[32m[20221214 00:06:48 @agent_ppo2.py:185][0m |          -0.0192 |          45.9969 |          10.4272 |
[32m[20221214 00:06:48 @agent_ppo2.py:185][0m |          -0.0094 |          46.4099 |          10.4260 |
[32m[20221214 00:06:48 @agent_ppo2.py:185][0m |          -0.0216 |          44.6148 |          10.4534 |
[32m[20221214 00:06:48 @agent_ppo2.py:185][0m |          -0.0150 |          44.1030 |          10.4268 |
[32m[20221214 00:06:48 @agent_ppo2.py:185][0m |          -0.0199 |          43.7062 |          10.4556 |
[32m[20221214 00:06:48 @agent_ppo2.py:185][0m |          -0.0200 |          43.3984 |          10.4173 |
[32m[20221214 00:06:48 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:06:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 323.53
[32m[20221214 00:06:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 423.19
[32m[20221214 00:06:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 461.63
[32m[20221214 00:06:48 @agent_ppo2.py:143][0m Total time:       9.30 min
[32m[20221214 00:06:48 @agent_ppo2.py:145][0m 856064 total steps have happened
[32m[20221214 00:06:48 @agent_ppo2.py:121][0m #------------------------ Iteration 4418 --------------------------#
[32m[20221214 00:06:48 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:06:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:06:49 @agent_ppo2.py:185][0m |          -0.0004 |          67.8713 |          10.1210 |
[32m[20221214 00:06:49 @agent_ppo2.py:185][0m |          -0.0066 |          61.2217 |          10.1344 |
[32m[20221214 00:06:49 @agent_ppo2.py:185][0m |          -0.0043 |          57.8816 |          10.1672 |
[32m[20221214 00:06:49 @agent_ppo2.py:185][0m |          -0.0117 |          55.8939 |          10.1654 |
[32m[20221214 00:06:49 @agent_ppo2.py:185][0m |          -0.0113 |          54.7965 |          10.1793 |
[32m[20221214 00:06:49 @agent_ppo2.py:185][0m |          -0.0116 |          54.0172 |          10.1491 |
[32m[20221214 00:06:49 @agent_ppo2.py:185][0m |          -0.0110 |          52.5307 |          10.1542 |
[32m[20221214 00:06:49 @agent_ppo2.py:185][0m |          -0.0130 |          52.2461 |          10.1591 |
[32m[20221214 00:06:49 @agent_ppo2.py:185][0m |          -0.0112 |          52.1296 |          10.1601 |
[32m[20221214 00:06:49 @agent_ppo2.py:185][0m |          -0.0064 |          54.6620 |          10.1982 |
[32m[20221214 00:06:49 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:06:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 518.03
[32m[20221214 00:06:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 544.59
[32m[20221214 00:06:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 502.84
[32m[20221214 00:06:49 @agent_ppo2.py:143][0m Total time:       9.32 min
[32m[20221214 00:06:49 @agent_ppo2.py:145][0m 858112 total steps have happened
[32m[20221214 00:06:49 @agent_ppo2.py:121][0m #------------------------ Iteration 4419 --------------------------#
[32m[20221214 00:06:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:06:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:06:50 @agent_ppo2.py:185][0m |           0.0034 |          56.5526 |          10.3221 |
[32m[20221214 00:06:50 @agent_ppo2.py:185][0m |          -0.0065 |          50.3017 |          10.3017 |
[32m[20221214 00:06:50 @agent_ppo2.py:185][0m |           0.0012 |          53.6158 |          10.3062 |
[32m[20221214 00:06:50 @agent_ppo2.py:185][0m |          -0.0040 |          47.5917 |          10.2522 |
[32m[20221214 00:06:50 @agent_ppo2.py:185][0m |          -0.0085 |          47.5350 |          10.2775 |
[32m[20221214 00:06:50 @agent_ppo2.py:185][0m |          -0.0044 |          47.4065 |          10.2020 |
[32m[20221214 00:06:50 @agent_ppo2.py:185][0m |          -0.0167 |          46.0687 |          10.1926 |
[32m[20221214 00:06:50 @agent_ppo2.py:185][0m |          -0.0106 |          45.6958 |          10.1455 |
[32m[20221214 00:06:50 @agent_ppo2.py:185][0m |          -0.0147 |          45.4642 |          10.1079 |
[32m[20221214 00:06:51 @agent_ppo2.py:185][0m |          -0.0179 |          45.1806 |          10.1280 |
[32m[20221214 00:06:51 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:06:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 456.00
[32m[20221214 00:06:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 542.22
[32m[20221214 00:06:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 501.86
[32m[20221214 00:06:51 @agent_ppo2.py:143][0m Total time:       9.34 min
[32m[20221214 00:06:51 @agent_ppo2.py:145][0m 860160 total steps have happened
[32m[20221214 00:06:51 @agent_ppo2.py:121][0m #------------------------ Iteration 4420 --------------------------#
[32m[20221214 00:06:51 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:06:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:06:51 @agent_ppo2.py:185][0m |           0.0019 |          70.3312 |           9.7961 |
[32m[20221214 00:06:51 @agent_ppo2.py:185][0m |          -0.0093 |          66.0657 |           9.8798 |
[32m[20221214 00:06:51 @agent_ppo2.py:185][0m |          -0.0064 |          64.8210 |           9.8492 |
[32m[20221214 00:06:51 @agent_ppo2.py:185][0m |          -0.0127 |          64.3533 |           9.8667 |
[32m[20221214 00:06:51 @agent_ppo2.py:185][0m |          -0.0099 |          63.7597 |           9.8844 |
[32m[20221214 00:06:51 @agent_ppo2.py:185][0m |          -0.0143 |          63.3218 |           9.8774 |
[32m[20221214 00:06:52 @agent_ppo2.py:185][0m |          -0.0127 |          63.0639 |           9.9028 |
[32m[20221214 00:06:52 @agent_ppo2.py:185][0m |          -0.0155 |          62.6712 |           9.8248 |
[32m[20221214 00:06:52 @agent_ppo2.py:185][0m |          -0.0064 |          63.7329 |           9.8503 |
[32m[20221214 00:06:52 @agent_ppo2.py:185][0m |          -0.0158 |          62.3008 |           9.8526 |
[32m[20221214 00:06:52 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:06:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 437.41
[32m[20221214 00:06:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 489.50
[32m[20221214 00:06:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 509.05
[32m[20221214 00:06:52 @agent_ppo2.py:143][0m Total time:       9.36 min
[32m[20221214 00:06:52 @agent_ppo2.py:145][0m 862208 total steps have happened
[32m[20221214 00:06:52 @agent_ppo2.py:121][0m #------------------------ Iteration 4421 --------------------------#
[32m[20221214 00:06:52 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:06:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:06:52 @agent_ppo2.py:185][0m |           0.0002 |          64.0852 |          10.3217 |
[32m[20221214 00:06:52 @agent_ppo2.py:185][0m |          -0.0046 |          57.2386 |          10.3145 |
[32m[20221214 00:06:53 @agent_ppo2.py:185][0m |          -0.0104 |          55.7124 |          10.2480 |
[32m[20221214 00:06:53 @agent_ppo2.py:185][0m |          -0.0112 |          54.2309 |          10.2493 |
[32m[20221214 00:06:53 @agent_ppo2.py:185][0m |          -0.0141 |          53.4719 |          10.2202 |
[32m[20221214 00:06:53 @agent_ppo2.py:185][0m |          -0.0125 |          52.7535 |          10.1904 |
[32m[20221214 00:06:53 @agent_ppo2.py:185][0m |          -0.0017 |          58.8889 |          10.1686 |
[32m[20221214 00:06:53 @agent_ppo2.py:185][0m |          -0.0129 |          51.7617 |          10.1390 |
[32m[20221214 00:06:53 @agent_ppo2.py:185][0m |          -0.0101 |          51.3640 |          10.1135 |
[32m[20221214 00:06:53 @agent_ppo2.py:185][0m |          -0.0130 |          51.5181 |          10.0746 |
[32m[20221214 00:06:53 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:06:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 467.70
[32m[20221214 00:06:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 512.56
[32m[20221214 00:06:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 527.12
[32m[20221214 00:06:53 @agent_ppo2.py:143][0m Total time:       9.38 min
[32m[20221214 00:06:53 @agent_ppo2.py:145][0m 864256 total steps have happened
[32m[20221214 00:06:53 @agent_ppo2.py:121][0m #------------------------ Iteration 4422 --------------------------#
[32m[20221214 00:06:53 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:06:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:06:54 @agent_ppo2.py:185][0m |           0.0027 |          61.1247 |           9.4227 |
[32m[20221214 00:06:54 @agent_ppo2.py:185][0m |          -0.0040 |          54.5180 |           9.4401 |
[32m[20221214 00:06:54 @agent_ppo2.py:185][0m |          -0.0116 |          51.7568 |           9.4811 |
[32m[20221214 00:06:54 @agent_ppo2.py:185][0m |          -0.0050 |          52.4835 |           9.5177 |
[32m[20221214 00:06:54 @agent_ppo2.py:185][0m |          -0.0134 |          49.2192 |           9.4953 |
[32m[20221214 00:06:54 @agent_ppo2.py:185][0m |          -0.0134 |          48.4618 |           9.5437 |
[32m[20221214 00:06:54 @agent_ppo2.py:185][0m |           0.0039 |          53.3568 |           9.5670 |
[32m[20221214 00:06:54 @agent_ppo2.py:185][0m |          -0.0079 |          50.4811 |           9.6019 |
[32m[20221214 00:06:54 @agent_ppo2.py:185][0m |          -0.0188 |          48.0052 |           9.6081 |
[32m[20221214 00:06:54 @agent_ppo2.py:185][0m |          -0.0166 |          47.4281 |           9.6458 |
[32m[20221214 00:06:54 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:06:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 473.51
[32m[20221214 00:06:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 544.09
[32m[20221214 00:06:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 565.00
[32m[20221214 00:06:55 @agent_ppo2.py:143][0m Total time:       9.40 min
[32m[20221214 00:06:55 @agent_ppo2.py:145][0m 866304 total steps have happened
[32m[20221214 00:06:55 @agent_ppo2.py:121][0m #------------------------ Iteration 4423 --------------------------#
[32m[20221214 00:06:55 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:06:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:06:55 @agent_ppo2.py:185][0m |          -0.0025 |          65.8856 |           9.9704 |
[32m[20221214 00:06:55 @agent_ppo2.py:185][0m |          -0.0076 |          61.6317 |          10.0211 |
[32m[20221214 00:06:55 @agent_ppo2.py:185][0m |          -0.0072 |          58.8664 |           9.9586 |
[32m[20221214 00:06:55 @agent_ppo2.py:185][0m |          -0.0089 |          57.2935 |           9.9443 |
[32m[20221214 00:06:55 @agent_ppo2.py:185][0m |          -0.0157 |          56.0495 |           9.9702 |
[32m[20221214 00:06:55 @agent_ppo2.py:185][0m |          -0.0109 |          54.7773 |           9.9647 |
[32m[20221214 00:06:55 @agent_ppo2.py:185][0m |          -0.0138 |          53.4712 |           9.9887 |
[32m[20221214 00:06:55 @agent_ppo2.py:185][0m |          -0.0174 |          52.6651 |           9.9869 |
[32m[20221214 00:06:56 @agent_ppo2.py:185][0m |          -0.0164 |          52.1791 |           9.9984 |
[32m[20221214 00:06:56 @agent_ppo2.py:185][0m |          -0.0129 |          51.6234 |          10.0103 |
[32m[20221214 00:06:56 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:06:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 493.02
[32m[20221214 00:06:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 531.11
[32m[20221214 00:06:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 380.40
[32m[20221214 00:06:56 @agent_ppo2.py:143][0m Total time:       9.42 min
[32m[20221214 00:06:56 @agent_ppo2.py:145][0m 868352 total steps have happened
[32m[20221214 00:06:56 @agent_ppo2.py:121][0m #------------------------ Iteration 4424 --------------------------#
[32m[20221214 00:06:56 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:06:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:06:56 @agent_ppo2.py:185][0m |          -0.0027 |          83.4884 |           9.6714 |
[32m[20221214 00:06:56 @agent_ppo2.py:185][0m |          -0.0059 |          75.9535 |           9.7323 |
[32m[20221214 00:06:56 @agent_ppo2.py:185][0m |          -0.0034 |          72.8157 |           9.6849 |
[32m[20221214 00:06:56 @agent_ppo2.py:185][0m |          -0.0113 |          70.6674 |           9.6727 |
[32m[20221214 00:06:57 @agent_ppo2.py:185][0m |          -0.0110 |          68.9809 |           9.7020 |
[32m[20221214 00:06:57 @agent_ppo2.py:185][0m |          -0.0023 |          71.0102 |           9.6498 |
[32m[20221214 00:06:57 @agent_ppo2.py:185][0m |          -0.0146 |          67.3677 |           9.6518 |
[32m[20221214 00:06:57 @agent_ppo2.py:185][0m |          -0.0108 |          67.6041 |           9.6839 |
[32m[20221214 00:06:57 @agent_ppo2.py:185][0m |          -0.0149 |          66.8501 |           9.6192 |
[32m[20221214 00:06:57 @agent_ppo2.py:185][0m |          -0.0045 |          76.5044 |           9.6123 |
[32m[20221214 00:06:57 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:06:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 486.24
[32m[20221214 00:06:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 569.96
[32m[20221214 00:06:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 518.51
[32m[20221214 00:06:57 @agent_ppo2.py:143][0m Total time:       9.45 min
[32m[20221214 00:06:57 @agent_ppo2.py:145][0m 870400 total steps have happened
[32m[20221214 00:06:57 @agent_ppo2.py:121][0m #------------------------ Iteration 4425 --------------------------#
[32m[20221214 00:06:57 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:06:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:06:57 @agent_ppo2.py:185][0m |          -0.0011 |          54.3884 |           9.4622 |
[32m[20221214 00:06:58 @agent_ppo2.py:185][0m |          -0.0072 |          41.2100 |           9.4556 |
[32m[20221214 00:06:58 @agent_ppo2.py:185][0m |          -0.0003 |          38.1788 |           9.4880 |
[32m[20221214 00:06:58 @agent_ppo2.py:185][0m |          -0.0133 |          36.6928 |           9.4810 |
[32m[20221214 00:06:58 @agent_ppo2.py:185][0m |          -0.0167 |          36.0088 |           9.5109 |
[32m[20221214 00:06:58 @agent_ppo2.py:185][0m |          -0.0063 |          35.4692 |           9.5216 |
[32m[20221214 00:06:58 @agent_ppo2.py:185][0m |          -0.0102 |          35.1890 |           9.5517 |
[32m[20221214 00:06:58 @agent_ppo2.py:185][0m |          -0.0176 |          34.5486 |           9.5413 |
[32m[20221214 00:06:58 @agent_ppo2.py:185][0m |          -0.0150 |          34.1781 |           9.6127 |
[32m[20221214 00:06:58 @agent_ppo2.py:185][0m |          -0.0196 |          33.7227 |           9.6130 |
[32m[20221214 00:06:58 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:06:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 502.56
[32m[20221214 00:06:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 512.94
[32m[20221214 00:06:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 517.49
[32m[20221214 00:06:58 @agent_ppo2.py:143][0m Total time:       9.47 min
[32m[20221214 00:06:58 @agent_ppo2.py:145][0m 872448 total steps have happened
[32m[20221214 00:06:58 @agent_ppo2.py:121][0m #------------------------ Iteration 4426 --------------------------#
[32m[20221214 00:06:59 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:06:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:06:59 @agent_ppo2.py:185][0m |           0.0021 |          56.4221 |           9.9660 |
[32m[20221214 00:06:59 @agent_ppo2.py:185][0m |          -0.0041 |          47.9184 |          10.0060 |
[32m[20221214 00:06:59 @agent_ppo2.py:185][0m |           0.0046 |          48.7865 |           9.9758 |
[32m[20221214 00:06:59 @agent_ppo2.py:185][0m |          -0.0088 |          43.2335 |           9.9759 |
[32m[20221214 00:06:59 @agent_ppo2.py:185][0m |          -0.0128 |          42.0942 |           9.9881 |
[32m[20221214 00:06:59 @agent_ppo2.py:185][0m |          -0.0117 |          41.3230 |          10.0440 |
[32m[20221214 00:06:59 @agent_ppo2.py:185][0m |          -0.0004 |          47.7144 |           9.9719 |
[32m[20221214 00:06:59 @agent_ppo2.py:185][0m |          -0.0095 |          39.9034 |           9.9959 |
[32m[20221214 00:06:59 @agent_ppo2.py:185][0m |          -0.0118 |          39.3970 |           9.9925 |
[32m[20221214 00:07:00 @agent_ppo2.py:185][0m |          -0.0117 |          38.9896 |           9.9934 |
[32m[20221214 00:07:00 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:07:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 474.91
[32m[20221214 00:07:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 552.86
[32m[20221214 00:07:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 443.23
[32m[20221214 00:07:00 @agent_ppo2.py:143][0m Total time:       9.49 min
[32m[20221214 00:07:00 @agent_ppo2.py:145][0m 874496 total steps have happened
[32m[20221214 00:07:00 @agent_ppo2.py:121][0m #------------------------ Iteration 4427 --------------------------#
[32m[20221214 00:07:00 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:07:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:07:00 @agent_ppo2.py:185][0m |          -0.0012 |          36.1604 |          10.1031 |
[32m[20221214 00:07:00 @agent_ppo2.py:185][0m |           0.0025 |          28.1056 |          10.1087 |
[32m[20221214 00:07:00 @agent_ppo2.py:185][0m |          -0.0072 |          26.1262 |          10.1167 |
[32m[20221214 00:07:00 @agent_ppo2.py:185][0m |          -0.0110 |          24.2924 |          10.1308 |
[32m[20221214 00:07:00 @agent_ppo2.py:185][0m |          -0.0112 |          23.5024 |          10.0781 |
[32m[20221214 00:07:01 @agent_ppo2.py:185][0m |          -0.0143 |          23.0824 |          10.1212 |
[32m[20221214 00:07:01 @agent_ppo2.py:185][0m |          -0.0203 |          22.6252 |          10.0989 |
[32m[20221214 00:07:01 @agent_ppo2.py:185][0m |          -0.0193 |          22.1988 |           9.9999 |
[32m[20221214 00:07:01 @agent_ppo2.py:185][0m |          -0.0205 |          21.8723 |           9.9953 |
[32m[20221214 00:07:01 @agent_ppo2.py:185][0m |          -0.0180 |          21.5835 |           9.9949 |
[32m[20221214 00:07:01 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:07:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 393.29
[32m[20221214 00:07:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 465.30
[32m[20221214 00:07:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 527.29
[32m[20221214 00:07:01 @agent_ppo2.py:143][0m Total time:       9.51 min
[32m[20221214 00:07:01 @agent_ppo2.py:145][0m 876544 total steps have happened
[32m[20221214 00:07:01 @agent_ppo2.py:121][0m #------------------------ Iteration 4428 --------------------------#
[32m[20221214 00:07:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:07:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:07:01 @agent_ppo2.py:185][0m |           0.0014 |          51.7692 |          10.0750 |
[32m[20221214 00:07:02 @agent_ppo2.py:185][0m |          -0.0069 |          48.3769 |          10.1424 |
[32m[20221214 00:07:02 @agent_ppo2.py:185][0m |          -0.0104 |          47.2066 |          10.1314 |
[32m[20221214 00:07:02 @agent_ppo2.py:185][0m |          -0.0143 |          45.7972 |          10.1549 |
[32m[20221214 00:07:02 @agent_ppo2.py:185][0m |          -0.0105 |          45.4643 |          10.0896 |
[32m[20221214 00:07:02 @agent_ppo2.py:185][0m |          -0.0136 |          44.9524 |          10.1216 |
[32m[20221214 00:07:02 @agent_ppo2.py:185][0m |          -0.0104 |          44.4384 |          10.0928 |
[32m[20221214 00:07:02 @agent_ppo2.py:185][0m |          -0.0123 |          44.6835 |          10.1410 |
[32m[20221214 00:07:02 @agent_ppo2.py:185][0m |          -0.0160 |          44.0119 |          10.1198 |
[32m[20221214 00:07:02 @agent_ppo2.py:185][0m |          -0.0169 |          43.8965 |          10.1212 |
[32m[20221214 00:07:02 @agent_ppo2.py:130][0m Policy update time: 1.17 s
[32m[20221214 00:07:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 443.34
[32m[20221214 00:07:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 526.31
[32m[20221214 00:07:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 485.03
[32m[20221214 00:07:03 @agent_ppo2.py:143][0m Total time:       9.54 min
[32m[20221214 00:07:03 @agent_ppo2.py:145][0m 878592 total steps have happened
[32m[20221214 00:07:03 @agent_ppo2.py:121][0m #------------------------ Iteration 4429 --------------------------#
[32m[20221214 00:07:03 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:07:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:07:03 @agent_ppo2.py:185][0m |          -0.0030 |          71.6569 |           9.3781 |
[32m[20221214 00:07:03 @agent_ppo2.py:185][0m |          -0.0105 |          66.2806 |           9.3963 |
[32m[20221214 00:07:03 @agent_ppo2.py:185][0m |          -0.0126 |          63.4268 |           9.4103 |
[32m[20221214 00:07:03 @agent_ppo2.py:185][0m |          -0.0119 |          61.7420 |           9.3914 |
[32m[20221214 00:07:03 @agent_ppo2.py:185][0m |          -0.0140 |          60.4472 |           9.3579 |
[32m[20221214 00:07:03 @agent_ppo2.py:185][0m |          -0.0160 |          59.2621 |           9.3923 |
[32m[20221214 00:07:03 @agent_ppo2.py:185][0m |          -0.0142 |          59.1797 |           9.3386 |
[32m[20221214 00:07:03 @agent_ppo2.py:185][0m |          -0.0186 |          57.8192 |           9.3196 |
[32m[20221214 00:07:04 @agent_ppo2.py:185][0m |          -0.0163 |          58.9060 |           9.3253 |
[32m[20221214 00:07:04 @agent_ppo2.py:185][0m |          -0.0180 |          56.5981 |           9.2927 |
[32m[20221214 00:07:04 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:07:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 516.10
[32m[20221214 00:07:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 568.52
[32m[20221214 00:07:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 451.31
[32m[20221214 00:07:04 @agent_ppo2.py:143][0m Total time:       9.56 min
[32m[20221214 00:07:04 @agent_ppo2.py:145][0m 880640 total steps have happened
[32m[20221214 00:07:04 @agent_ppo2.py:121][0m #------------------------ Iteration 4430 --------------------------#
[32m[20221214 00:07:04 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:07:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:07:04 @agent_ppo2.py:185][0m |           0.0013 |          39.9602 |           9.8093 |
[32m[20221214 00:07:04 @agent_ppo2.py:185][0m |           0.0051 |          27.9487 |           9.8263 |
[32m[20221214 00:07:04 @agent_ppo2.py:185][0m |          -0.0149 |          26.0109 |           9.7961 |
[32m[20221214 00:07:04 @agent_ppo2.py:185][0m |          -0.0008 |          24.9961 |           9.8621 |
[32m[20221214 00:07:05 @agent_ppo2.py:185][0m |          -0.0051 |          24.4173 |           9.8649 |
[32m[20221214 00:07:05 @agent_ppo2.py:185][0m |          -0.0153 |          23.6808 |           9.9286 |
[32m[20221214 00:07:05 @agent_ppo2.py:185][0m |          -0.0067 |          23.2104 |           9.9118 |
[32m[20221214 00:07:05 @agent_ppo2.py:185][0m |          -0.0111 |          22.7835 |           9.9213 |
[32m[20221214 00:07:05 @agent_ppo2.py:185][0m |          -0.0161 |          22.4808 |           9.9713 |
[32m[20221214 00:07:05 @agent_ppo2.py:185][0m |          -0.0135 |          22.3070 |          10.0096 |
[32m[20221214 00:07:05 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:07:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 374.51
[32m[20221214 00:07:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 488.52
[32m[20221214 00:07:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 405.24
[32m[20221214 00:07:05 @agent_ppo2.py:143][0m Total time:       9.58 min
[32m[20221214 00:07:05 @agent_ppo2.py:145][0m 882688 total steps have happened
[32m[20221214 00:07:05 @agent_ppo2.py:121][0m #------------------------ Iteration 4431 --------------------------#
[32m[20221214 00:07:05 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:07:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:07:06 @agent_ppo2.py:185][0m |          -0.0017 |          40.7807 |           9.5736 |
[32m[20221214 00:07:06 @agent_ppo2.py:185][0m |          -0.0033 |          34.9137 |           9.6104 |
[32m[20221214 00:07:06 @agent_ppo2.py:185][0m |          -0.0085 |          33.8832 |           9.6780 |
[32m[20221214 00:07:06 @agent_ppo2.py:185][0m |          -0.0121 |          32.8857 |           9.6230 |
[32m[20221214 00:07:06 @agent_ppo2.py:185][0m |          -0.0097 |          33.5099 |           9.6100 |
[32m[20221214 00:07:06 @agent_ppo2.py:185][0m |          -0.0110 |          32.1946 |           9.6375 |
[32m[20221214 00:07:06 @agent_ppo2.py:185][0m |          -0.0216 |          31.6779 |           9.6552 |
[32m[20221214 00:07:06 @agent_ppo2.py:185][0m |          -0.0130 |          31.4045 |           9.6538 |
[32m[20221214 00:07:06 @agent_ppo2.py:185][0m |          -0.0194 |          31.3824 |           9.6553 |
[32m[20221214 00:07:06 @agent_ppo2.py:185][0m |          -0.0168 |          30.8920 |           9.6606 |
[32m[20221214 00:07:06 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:07:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 400.12
[32m[20221214 00:07:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 488.17
[32m[20221214 00:07:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 506.86
[32m[20221214 00:07:06 @agent_ppo2.py:143][0m Total time:       9.60 min
[32m[20221214 00:07:06 @agent_ppo2.py:145][0m 884736 total steps have happened
[32m[20221214 00:07:06 @agent_ppo2.py:121][0m #------------------------ Iteration 4432 --------------------------#
[32m[20221214 00:07:07 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:07:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:07:07 @agent_ppo2.py:185][0m |          -0.0030 |          52.4753 |           9.6443 |
[32m[20221214 00:07:07 @agent_ppo2.py:185][0m |          -0.0101 |          48.1860 |           9.6360 |
[32m[20221214 00:07:07 @agent_ppo2.py:185][0m |          -0.0110 |          46.7050 |           9.7336 |
[32m[20221214 00:07:07 @agent_ppo2.py:185][0m |          -0.0045 |          45.8466 |           9.6740 |
[32m[20221214 00:07:07 @agent_ppo2.py:185][0m |          -0.0169 |          43.9961 |           9.6781 |
[32m[20221214 00:07:07 @agent_ppo2.py:185][0m |          -0.0129 |          42.9071 |           9.7075 |
[32m[20221214 00:07:07 @agent_ppo2.py:185][0m |          -0.0149 |          42.3399 |           9.7099 |
[32m[20221214 00:07:07 @agent_ppo2.py:185][0m |          -0.0133 |          41.2317 |           9.7125 |
[32m[20221214 00:07:07 @agent_ppo2.py:185][0m |          -0.0082 |          42.6962 |           9.7508 |
[32m[20221214 00:07:08 @agent_ppo2.py:185][0m |          -0.0223 |          40.1888 |           9.7203 |
[32m[20221214 00:07:08 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:07:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 420.81
[32m[20221214 00:07:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 467.45
[32m[20221214 00:07:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 449.48
[32m[20221214 00:07:08 @agent_ppo2.py:143][0m Total time:       9.62 min
[32m[20221214 00:07:08 @agent_ppo2.py:145][0m 886784 total steps have happened
[32m[20221214 00:07:08 @agent_ppo2.py:121][0m #------------------------ Iteration 4433 --------------------------#
[32m[20221214 00:07:08 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:07:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:07:08 @agent_ppo2.py:185][0m |           0.0128 |          51.5129 |          10.1140 |
[32m[20221214 00:07:08 @agent_ppo2.py:185][0m |          -0.0030 |          42.1802 |          10.1862 |
[32m[20221214 00:07:08 @agent_ppo2.py:185][0m |          -0.0056 |          40.7154 |          10.1924 |
[32m[20221214 00:07:08 @agent_ppo2.py:185][0m |          -0.0152 |          39.8340 |          10.1902 |
[32m[20221214 00:07:08 @agent_ppo2.py:185][0m |          -0.0105 |          39.2004 |          10.1933 |
[32m[20221214 00:07:09 @agent_ppo2.py:185][0m |          -0.0137 |          39.0437 |          10.2301 |
[32m[20221214 00:07:09 @agent_ppo2.py:185][0m |          -0.0133 |          38.1986 |          10.1511 |
[32m[20221214 00:07:09 @agent_ppo2.py:185][0m |          -0.0136 |          38.2838 |          10.2248 |
[32m[20221214 00:07:09 @agent_ppo2.py:185][0m |          -0.0164 |          37.5948 |          10.2456 |
[32m[20221214 00:07:09 @agent_ppo2.py:185][0m |          -0.0155 |          37.3037 |          10.2105 |
[32m[20221214 00:07:09 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:07:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 487.82
[32m[20221214 00:07:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 552.48
[32m[20221214 00:07:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 182.15
[32m[20221214 00:07:09 @agent_ppo2.py:143][0m Total time:       9.64 min
[32m[20221214 00:07:09 @agent_ppo2.py:145][0m 888832 total steps have happened
[32m[20221214 00:07:09 @agent_ppo2.py:121][0m #------------------------ Iteration 4434 --------------------------#
[32m[20221214 00:07:09 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:07:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:07:09 @agent_ppo2.py:185][0m |           0.0018 |          39.0854 |           9.3932 |
[32m[20221214 00:07:09 @agent_ppo2.py:185][0m |          -0.0087 |          32.6583 |           9.4054 |
[32m[20221214 00:07:10 @agent_ppo2.py:185][0m |          -0.0116 |          30.7079 |           9.4086 |
[32m[20221214 00:07:10 @agent_ppo2.py:185][0m |          -0.0103 |          29.4490 |           9.4324 |
[32m[20221214 00:07:10 @agent_ppo2.py:185][0m |          -0.0123 |          28.6172 |           9.4211 |
[32m[20221214 00:07:10 @agent_ppo2.py:185][0m |          -0.0178 |          27.8566 |           9.3692 |
[32m[20221214 00:07:10 @agent_ppo2.py:185][0m |          -0.0137 |          27.1859 |           9.3948 |
[32m[20221214 00:07:10 @agent_ppo2.py:185][0m |          -0.0201 |          26.7759 |           9.4044 |
[32m[20221214 00:07:10 @agent_ppo2.py:185][0m |          -0.0030 |          31.0694 |           9.4066 |
[32m[20221214 00:07:10 @agent_ppo2.py:185][0m |          -0.0166 |          26.1738 |           9.3987 |
[32m[20221214 00:07:10 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:07:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 384.45
[32m[20221214 00:07:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 466.96
[32m[20221214 00:07:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 554.86
[32m[20221214 00:07:10 @agent_ppo2.py:143][0m Total time:       9.67 min
[32m[20221214 00:07:10 @agent_ppo2.py:145][0m 890880 total steps have happened
[32m[20221214 00:07:10 @agent_ppo2.py:121][0m #------------------------ Iteration 4435 --------------------------#
[32m[20221214 00:07:11 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:07:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:07:11 @agent_ppo2.py:185][0m |           0.0033 |          53.2309 |           9.6894 |
[32m[20221214 00:07:11 @agent_ppo2.py:185][0m |          -0.0027 |          47.1617 |           9.7044 |
[32m[20221214 00:07:11 @agent_ppo2.py:185][0m |          -0.0122 |          45.0032 |           9.7478 |
[32m[20221214 00:07:11 @agent_ppo2.py:185][0m |          -0.0124 |          43.8781 |           9.7673 |
[32m[20221214 00:07:11 @agent_ppo2.py:185][0m |          -0.0132 |          43.0994 |           9.7935 |
[32m[20221214 00:07:11 @agent_ppo2.py:185][0m |          -0.0114 |          42.9036 |           9.8013 |
[32m[20221214 00:07:11 @agent_ppo2.py:185][0m |          -0.0138 |          42.1189 |           9.8590 |
[32m[20221214 00:07:11 @agent_ppo2.py:185][0m |          -0.0170 |          41.3949 |           9.8246 |
[32m[20221214 00:07:11 @agent_ppo2.py:185][0m |          -0.0155 |          41.0759 |           9.8185 |
[32m[20221214 00:07:12 @agent_ppo2.py:185][0m |          -0.0150 |          40.9365 |           9.8379 |
[32m[20221214 00:07:12 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:07:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 516.08
[32m[20221214 00:07:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 529.53
[32m[20221214 00:07:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 554.81
[32m[20221214 00:07:12 @agent_ppo2.py:143][0m Total time:       9.69 min
[32m[20221214 00:07:12 @agent_ppo2.py:145][0m 892928 total steps have happened
[32m[20221214 00:07:12 @agent_ppo2.py:121][0m #------------------------ Iteration 4436 --------------------------#
[32m[20221214 00:07:12 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:07:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:07:12 @agent_ppo2.py:185][0m |          -0.0009 |          56.6598 |           9.5106 |
[32m[20221214 00:07:12 @agent_ppo2.py:185][0m |          -0.0095 |          50.6352 |           9.4370 |
[32m[20221214 00:07:12 @agent_ppo2.py:185][0m |          -0.0122 |          49.1188 |           9.3736 |
[32m[20221214 00:07:12 @agent_ppo2.py:185][0m |          -0.0093 |          51.8209 |           9.3799 |
[32m[20221214 00:07:12 @agent_ppo2.py:185][0m |          -0.0232 |          47.7787 |           9.3699 |
[32m[20221214 00:07:12 @agent_ppo2.py:185][0m |          -0.0115 |          48.2150 |           9.3793 |
[32m[20221214 00:07:13 @agent_ppo2.py:185][0m |          -0.0162 |          46.8190 |           9.3694 |
[32m[20221214 00:07:13 @agent_ppo2.py:185][0m |          -0.0200 |          46.5968 |           9.3350 |
[32m[20221214 00:07:13 @agent_ppo2.py:185][0m |          -0.0226 |          46.2609 |           9.3221 |
[32m[20221214 00:07:13 @agent_ppo2.py:185][0m |          -0.0197 |          46.5892 |           9.2779 |
[32m[20221214 00:07:13 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:07:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 444.82
[32m[20221214 00:07:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 514.93
[32m[20221214 00:07:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 555.29
[32m[20221214 00:07:13 @agent_ppo2.py:143][0m Total time:       9.71 min
[32m[20221214 00:07:13 @agent_ppo2.py:145][0m 894976 total steps have happened
[32m[20221214 00:07:13 @agent_ppo2.py:121][0m #------------------------ Iteration 4437 --------------------------#
[32m[20221214 00:07:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:07:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:07:13 @agent_ppo2.py:185][0m |          -0.0011 |          42.4622 |           9.6751 |
[32m[20221214 00:07:13 @agent_ppo2.py:185][0m |           0.0011 |          36.0451 |           9.7123 |
[32m[20221214 00:07:14 @agent_ppo2.py:185][0m |          -0.0061 |          33.9346 |           9.7218 |
[32m[20221214 00:07:14 @agent_ppo2.py:185][0m |          -0.0100 |          32.7293 |           9.7254 |
[32m[20221214 00:07:14 @agent_ppo2.py:185][0m |          -0.0139 |          32.1003 |           9.7486 |
[32m[20221214 00:07:14 @agent_ppo2.py:185][0m |          -0.0121 |          31.7041 |           9.7321 |
[32m[20221214 00:07:14 @agent_ppo2.py:185][0m |          -0.0093 |          31.0432 |           9.7363 |
[32m[20221214 00:07:14 @agent_ppo2.py:185][0m |          -0.0193 |          30.9204 |           9.7995 |
[32m[20221214 00:07:14 @agent_ppo2.py:185][0m |          -0.0115 |          30.5674 |           9.7295 |
[32m[20221214 00:07:14 @agent_ppo2.py:185][0m |          -0.0170 |          30.0716 |           9.7977 |
[32m[20221214 00:07:14 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:07:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 404.00
[32m[20221214 00:07:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 519.24
[32m[20221214 00:07:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 530.37
[32m[20221214 00:07:14 @agent_ppo2.py:143][0m Total time:       9.73 min
[32m[20221214 00:07:14 @agent_ppo2.py:145][0m 897024 total steps have happened
[32m[20221214 00:07:14 @agent_ppo2.py:121][0m #------------------------ Iteration 4438 --------------------------#
[32m[20221214 00:07:14 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:07:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:07:15 @agent_ppo2.py:185][0m |           0.0012 |          58.5449 |           9.9370 |
[32m[20221214 00:07:15 @agent_ppo2.py:185][0m |          -0.0061 |          52.0068 |           9.8535 |
[32m[20221214 00:07:15 @agent_ppo2.py:185][0m |          -0.0122 |          50.0139 |           9.8062 |
[32m[20221214 00:07:15 @agent_ppo2.py:185][0m |          -0.0073 |          48.8352 |           9.8564 |
[32m[20221214 00:07:15 @agent_ppo2.py:185][0m |          -0.0109 |          47.7827 |           9.8198 |
[32m[20221214 00:07:15 @agent_ppo2.py:185][0m |          -0.0134 |          46.9501 |           9.7863 |
[32m[20221214 00:07:15 @agent_ppo2.py:185][0m |          -0.0157 |          46.4358 |           9.7801 |
[32m[20221214 00:07:15 @agent_ppo2.py:185][0m |          -0.0137 |          46.4323 |           9.8073 |
[32m[20221214 00:07:15 @agent_ppo2.py:185][0m |          -0.0106 |          46.3413 |           9.8161 |
[32m[20221214 00:07:15 @agent_ppo2.py:185][0m |          -0.0152 |          45.6203 |           9.8154 |
[32m[20221214 00:07:15 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:07:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 391.31
[32m[20221214 00:07:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 465.20
[32m[20221214 00:07:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 461.05
[32m[20221214 00:07:16 @agent_ppo2.py:143][0m Total time:       9.75 min
[32m[20221214 00:07:16 @agent_ppo2.py:145][0m 899072 total steps have happened
[32m[20221214 00:07:16 @agent_ppo2.py:121][0m #------------------------ Iteration 4439 --------------------------#
[32m[20221214 00:07:16 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:07:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:07:16 @agent_ppo2.py:185][0m |           0.0089 |          45.1004 |           9.0684 |
[32m[20221214 00:07:16 @agent_ppo2.py:185][0m |          -0.0088 |          35.7928 |           9.1092 |
[32m[20221214 00:07:16 @agent_ppo2.py:185][0m |          -0.0086 |          30.9174 |           9.1126 |
[32m[20221214 00:07:16 @agent_ppo2.py:185][0m |          -0.0105 |          28.4408 |           9.0864 |
[32m[20221214 00:07:16 @agent_ppo2.py:185][0m |          -0.0123 |          26.5944 |           9.0655 |
[32m[20221214 00:07:16 @agent_ppo2.py:185][0m |          -0.0164 |          25.4352 |           9.0713 |
[32m[20221214 00:07:17 @agent_ppo2.py:185][0m |          -0.0160 |          24.3283 |           9.1352 |
[32m[20221214 00:07:17 @agent_ppo2.py:185][0m |          -0.0190 |          23.3531 |           9.1347 |
[32m[20221214 00:07:17 @agent_ppo2.py:185][0m |          -0.0163 |          22.5157 |           9.1460 |
[32m[20221214 00:07:17 @agent_ppo2.py:185][0m |          -0.0183 |          22.0270 |           9.1435 |
[32m[20221214 00:07:17 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:07:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 471.42
[32m[20221214 00:07:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 548.07
[32m[20221214 00:07:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 251.81
[32m[20221214 00:07:17 @agent_ppo2.py:143][0m Total time:       9.78 min
[32m[20221214 00:07:17 @agent_ppo2.py:145][0m 901120 total steps have happened
[32m[20221214 00:07:17 @agent_ppo2.py:121][0m #------------------------ Iteration 4440 --------------------------#
[32m[20221214 00:07:17 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:07:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:07:17 @agent_ppo2.py:185][0m |          -0.0000 |          43.0683 |           9.9226 |
[32m[20221214 00:07:17 @agent_ppo2.py:185][0m |          -0.0058 |          38.8093 |           9.9155 |
[32m[20221214 00:07:18 @agent_ppo2.py:185][0m |          -0.0075 |          37.0231 |           9.8467 |
[32m[20221214 00:07:18 @agent_ppo2.py:185][0m |          -0.0118 |          35.8919 |           9.8461 |
[32m[20221214 00:07:18 @agent_ppo2.py:185][0m |          -0.0111 |          34.9889 |           9.8336 |
[32m[20221214 00:07:18 @agent_ppo2.py:185][0m |          -0.0103 |          34.4793 |           9.8575 |
[32m[20221214 00:07:18 @agent_ppo2.py:185][0m |          -0.0130 |          34.2510 |           9.8558 |
[32m[20221214 00:07:18 @agent_ppo2.py:185][0m |          -0.0175 |          33.3704 |           9.8276 |
[32m[20221214 00:07:18 @agent_ppo2.py:185][0m |          -0.0079 |          41.8160 |           9.7972 |
[32m[20221214 00:07:18 @agent_ppo2.py:185][0m |          -0.0224 |          32.9000 |           9.8363 |
[32m[20221214 00:07:18 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:07:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 411.42
[32m[20221214 00:07:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 489.04
[32m[20221214 00:07:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 399.05
[32m[20221214 00:07:18 @agent_ppo2.py:143][0m Total time:       9.80 min
[32m[20221214 00:07:18 @agent_ppo2.py:145][0m 903168 total steps have happened
[32m[20221214 00:07:18 @agent_ppo2.py:121][0m #------------------------ Iteration 4441 --------------------------#
[32m[20221214 00:07:18 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:07:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:07:19 @agent_ppo2.py:185][0m |           0.0122 |          69.5567 |           9.1702 |
[32m[20221214 00:07:19 @agent_ppo2.py:185][0m |          -0.0079 |          53.2200 |           9.1713 |
[32m[20221214 00:07:19 @agent_ppo2.py:185][0m |           0.0001 |          51.4474 |           9.1600 |
[32m[20221214 00:07:19 @agent_ppo2.py:185][0m |           0.0006 |          51.8493 |           9.1412 |
[32m[20221214 00:07:19 @agent_ppo2.py:185][0m |          -0.0108 |          46.6340 |           9.0873 |
[32m[20221214 00:07:19 @agent_ppo2.py:185][0m |          -0.0171 |          45.3565 |           9.1293 |
[32m[20221214 00:07:19 @agent_ppo2.py:185][0m |          -0.0166 |          44.8426 |           9.0622 |
[32m[20221214 00:07:19 @agent_ppo2.py:185][0m |          -0.0136 |          43.8995 |           9.0537 |
[32m[20221214 00:07:19 @agent_ppo2.py:185][0m |          -0.0174 |          43.4629 |           9.0230 |
[32m[20221214 00:07:19 @agent_ppo2.py:185][0m |          -0.0138 |          43.2273 |           9.0645 |
[32m[20221214 00:07:19 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:07:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 480.21
[32m[20221214 00:07:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 582.08
[32m[20221214 00:07:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 441.01
[32m[20221214 00:07:20 @agent_ppo2.py:143][0m Total time:       9.82 min
[32m[20221214 00:07:20 @agent_ppo2.py:145][0m 905216 total steps have happened
[32m[20221214 00:07:20 @agent_ppo2.py:121][0m #------------------------ Iteration 4442 --------------------------#
[32m[20221214 00:07:20 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:07:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:07:20 @agent_ppo2.py:185][0m |          -0.0032 |          46.7828 |           9.6096 |
[32m[20221214 00:07:20 @agent_ppo2.py:185][0m |          -0.0044 |          39.0124 |           9.5929 |
[32m[20221214 00:07:20 @agent_ppo2.py:185][0m |          -0.0090 |          36.2437 |           9.5943 |
[32m[20221214 00:07:20 @agent_ppo2.py:185][0m |          -0.0020 |          34.7388 |           9.6113 |
[32m[20221214 00:07:20 @agent_ppo2.py:185][0m |          -0.0100 |          33.5596 |           9.5773 |
[32m[20221214 00:07:20 @agent_ppo2.py:185][0m |          -0.0158 |          32.7220 |           9.6036 |
[32m[20221214 00:07:21 @agent_ppo2.py:185][0m |          -0.0157 |          31.9560 |           9.6281 |
[32m[20221214 00:07:21 @agent_ppo2.py:185][0m |          -0.0131 |          31.3692 |           9.6147 |
[32m[20221214 00:07:21 @agent_ppo2.py:185][0m |          -0.0179 |          31.0320 |           9.6613 |
[32m[20221214 00:07:21 @agent_ppo2.py:185][0m |          -0.0235 |          30.9058 |           9.6583 |
[32m[20221214 00:07:21 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:07:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 420.36
[32m[20221214 00:07:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 453.80
[32m[20221214 00:07:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 446.45
[32m[20221214 00:07:21 @agent_ppo2.py:143][0m Total time:       9.84 min
[32m[20221214 00:07:21 @agent_ppo2.py:145][0m 907264 total steps have happened
[32m[20221214 00:07:21 @agent_ppo2.py:121][0m #------------------------ Iteration 4443 --------------------------#
[32m[20221214 00:07:21 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:07:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:07:21 @agent_ppo2.py:185][0m |           0.0016 |          38.4678 |           9.1996 |
[32m[20221214 00:07:21 @agent_ppo2.py:185][0m |          -0.0037 |          30.3888 |           9.1171 |
[32m[20221214 00:07:21 @agent_ppo2.py:185][0m |          -0.0107 |          27.9678 |           9.0772 |
[32m[20221214 00:07:22 @agent_ppo2.py:185][0m |          -0.0123 |          27.0955 |           9.0471 |
[32m[20221214 00:07:22 @agent_ppo2.py:185][0m |          -0.0072 |          26.7920 |           9.0420 |
[32m[20221214 00:07:22 @agent_ppo2.py:185][0m |          -0.0218 |          26.2445 |           9.0115 |
[32m[20221214 00:07:22 @agent_ppo2.py:185][0m |          -0.0161 |          25.7657 |           8.9281 |
[32m[20221214 00:07:22 @agent_ppo2.py:185][0m |          -0.0130 |          25.4592 |           8.9217 |
[32m[20221214 00:07:22 @agent_ppo2.py:185][0m |          -0.0115 |          25.4942 |           8.9377 |
[32m[20221214 00:07:22 @agent_ppo2.py:185][0m |          -0.0171 |          25.3488 |           8.9113 |
[32m[20221214 00:07:22 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:07:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 426.96
[32m[20221214 00:07:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 553.03
[32m[20221214 00:07:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 388.39
[32m[20221214 00:07:22 @agent_ppo2.py:143][0m Total time:       9.86 min
[32m[20221214 00:07:22 @agent_ppo2.py:145][0m 909312 total steps have happened
[32m[20221214 00:07:22 @agent_ppo2.py:121][0m #------------------------ Iteration 4444 --------------------------#
[32m[20221214 00:07:22 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:07:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:07:23 @agent_ppo2.py:185][0m |          -0.0012 |          46.0343 |           8.4573 |
[32m[20221214 00:07:23 @agent_ppo2.py:185][0m |          -0.0051 |          40.7106 |           8.4277 |
[32m[20221214 00:07:23 @agent_ppo2.py:185][0m |          -0.0079 |          38.8261 |           8.3831 |
[32m[20221214 00:07:23 @agent_ppo2.py:185][0m |          -0.0101 |          37.8328 |           8.3788 |
[32m[20221214 00:07:23 @agent_ppo2.py:185][0m |          -0.0133 |          37.1054 |           8.3139 |
[32m[20221214 00:07:23 @agent_ppo2.py:185][0m |          -0.0178 |          36.8730 |           8.3383 |
[32m[20221214 00:07:23 @agent_ppo2.py:185][0m |          -0.0147 |          36.1993 |           8.2928 |
[32m[20221214 00:07:23 @agent_ppo2.py:185][0m |          -0.0105 |          36.7181 |           8.2739 |
[32m[20221214 00:07:23 @agent_ppo2.py:185][0m |          -0.0210 |          35.3586 |           8.2419 |
[32m[20221214 00:07:23 @agent_ppo2.py:185][0m |          -0.0183 |          34.9868 |           8.2751 |
[32m[20221214 00:07:23 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:07:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 395.71
[32m[20221214 00:07:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 464.43
[32m[20221214 00:07:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 554.22
[32m[20221214 00:07:24 @agent_ppo2.py:143][0m Total time:       9.89 min
[32m[20221214 00:07:24 @agent_ppo2.py:145][0m 911360 total steps have happened
[32m[20221214 00:07:24 @agent_ppo2.py:121][0m #------------------------ Iteration 4445 --------------------------#
[32m[20221214 00:07:24 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:07:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:07:24 @agent_ppo2.py:185][0m |           0.0006 |          83.4126 |           8.8595 |
[32m[20221214 00:07:24 @agent_ppo2.py:185][0m |           0.0011 |          77.1450 |           8.9481 |
[32m[20221214 00:07:24 @agent_ppo2.py:185][0m |          -0.0046 |          75.4554 |           8.8431 |
[32m[20221214 00:07:24 @agent_ppo2.py:185][0m |          -0.0042 |          73.9932 |           8.8567 |
[32m[20221214 00:07:24 @agent_ppo2.py:185][0m |          -0.0098 |          73.1257 |           8.8854 |
[32m[20221214 00:07:24 @agent_ppo2.py:185][0m |          -0.0099 |          72.7113 |           8.7907 |
[32m[20221214 00:07:24 @agent_ppo2.py:185][0m |           0.0035 |          76.7551 |           8.7893 |
[32m[20221214 00:07:25 @agent_ppo2.py:185][0m |          -0.0061 |          71.6065 |           8.8025 |
[32m[20221214 00:07:25 @agent_ppo2.py:185][0m |          -0.0082 |          70.9548 |           8.7605 |
[32m[20221214 00:07:25 @agent_ppo2.py:185][0m |          -0.0078 |          69.9122 |           8.7298 |
[32m[20221214 00:07:25 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:07:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 505.62
[32m[20221214 00:07:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 555.85
[32m[20221214 00:07:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 582.09
[32m[20221214 00:07:25 @agent_ppo2.py:143][0m Total time:       9.91 min
[32m[20221214 00:07:25 @agent_ppo2.py:145][0m 913408 total steps have happened
[32m[20221214 00:07:25 @agent_ppo2.py:121][0m #------------------------ Iteration 4446 --------------------------#
[32m[20221214 00:07:25 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:07:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:07:25 @agent_ppo2.py:185][0m |           0.0061 |          48.1859 |           8.8670 |
[32m[20221214 00:07:25 @agent_ppo2.py:185][0m |          -0.0059 |          42.0312 |           8.8492 |
[32m[20221214 00:07:25 @agent_ppo2.py:185][0m |          -0.0123 |          40.2335 |           8.8262 |
[32m[20221214 00:07:25 @agent_ppo2.py:185][0m |          -0.0076 |          39.0159 |           8.8060 |
[32m[20221214 00:07:26 @agent_ppo2.py:185][0m |           0.0009 |          43.1284 |           8.8178 |
[32m[20221214 00:07:26 @agent_ppo2.py:185][0m |          -0.0129 |          38.1418 |           8.8179 |
[32m[20221214 00:07:26 @agent_ppo2.py:185][0m |          -0.0047 |          37.1560 |           8.7749 |
[32m[20221214 00:07:26 @agent_ppo2.py:185][0m |          -0.0160 |          36.6428 |           8.7195 |
[32m[20221214 00:07:26 @agent_ppo2.py:185][0m |          -0.0105 |          36.2546 |           8.7269 |
[32m[20221214 00:07:26 @agent_ppo2.py:185][0m |          -0.0142 |          35.7690 |           8.7474 |
[32m[20221214 00:07:26 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:07:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 383.56
[32m[20221214 00:07:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 480.00
[32m[20221214 00:07:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 597.84
[32m[20221214 00:07:26 @agent_ppo2.py:143][0m Total time:       9.93 min
[32m[20221214 00:07:26 @agent_ppo2.py:145][0m 915456 total steps have happened
[32m[20221214 00:07:26 @agent_ppo2.py:121][0m #------------------------ Iteration 4447 --------------------------#
[32m[20221214 00:07:26 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:07:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:07:27 @agent_ppo2.py:185][0m |           0.0054 |          71.2840 |           8.5054 |
[32m[20221214 00:07:27 @agent_ppo2.py:185][0m |          -0.0076 |          63.8878 |           8.5395 |
[32m[20221214 00:07:27 @agent_ppo2.py:185][0m |          -0.0091 |          61.3999 |           8.5000 |
[32m[20221214 00:07:27 @agent_ppo2.py:185][0m |          -0.0144 |          59.4121 |           8.5783 |
[32m[20221214 00:07:27 @agent_ppo2.py:185][0m |          -0.0151 |          57.8546 |           8.5344 |
[32m[20221214 00:07:27 @agent_ppo2.py:185][0m |          -0.0143 |          56.7959 |           8.5784 |
[32m[20221214 00:07:27 @agent_ppo2.py:185][0m |          -0.0162 |          55.8938 |           8.5901 |
[32m[20221214 00:07:27 @agent_ppo2.py:185][0m |          -0.0173 |          55.2818 |           8.5996 |
[32m[20221214 00:07:27 @agent_ppo2.py:185][0m |          -0.0151 |          54.9285 |           8.5979 |
[32m[20221214 00:07:27 @agent_ppo2.py:185][0m |          -0.0192 |          54.3439 |           8.6333 |
[32m[20221214 00:07:27 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:07:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 470.79
[32m[20221214 00:07:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 515.07
[32m[20221214 00:07:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 586.40
[32m[20221214 00:07:28 @agent_ppo2.py:143][0m Total time:       9.95 min
[32m[20221214 00:07:28 @agent_ppo2.py:145][0m 917504 total steps have happened
[32m[20221214 00:07:28 @agent_ppo2.py:121][0m #------------------------ Iteration 4448 --------------------------#
[32m[20221214 00:07:28 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:07:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:07:28 @agent_ppo2.py:185][0m |           0.0039 |          61.5310 |           8.1501 |
[32m[20221214 00:07:28 @agent_ppo2.py:185][0m |          -0.0050 |          53.0746 |           8.1094 |
[32m[20221214 00:07:28 @agent_ppo2.py:185][0m |          -0.0115 |          50.2795 |           8.1553 |
[32m[20221214 00:07:28 @agent_ppo2.py:185][0m |          -0.0134 |          48.5014 |           8.1354 |
[32m[20221214 00:07:28 @agent_ppo2.py:185][0m |          -0.0040 |          48.7782 |           8.1229 |
[32m[20221214 00:07:28 @agent_ppo2.py:185][0m |          -0.0147 |          46.3741 |           8.1722 |
[32m[20221214 00:07:28 @agent_ppo2.py:185][0m |          -0.0069 |          50.0728 |           8.1557 |
[32m[20221214 00:07:29 @agent_ppo2.py:185][0m |          -0.0144 |          44.7635 |           8.1072 |
[32m[20221214 00:07:29 @agent_ppo2.py:185][0m |          -0.0100 |          46.1065 |           8.1337 |
[32m[20221214 00:07:29 @agent_ppo2.py:185][0m |          -0.0180 |          43.5572 |           8.0773 |
[32m[20221214 00:07:29 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:07:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 467.02
[32m[20221214 00:07:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 558.32
[32m[20221214 00:07:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 571.45
[32m[20221214 00:07:29 @agent_ppo2.py:143][0m Total time:       9.97 min
[32m[20221214 00:07:29 @agent_ppo2.py:145][0m 919552 total steps have happened
[32m[20221214 00:07:29 @agent_ppo2.py:121][0m #------------------------ Iteration 4449 --------------------------#
[32m[20221214 00:07:29 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:07:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:07:29 @agent_ppo2.py:185][0m |           0.0008 |          59.9569 |           8.5764 |
[32m[20221214 00:07:29 @agent_ppo2.py:185][0m |          -0.0048 |          52.9921 |           8.6256 |
[32m[20221214 00:07:29 @agent_ppo2.py:185][0m |          -0.0119 |          50.2784 |           8.6283 |
[32m[20221214 00:07:29 @agent_ppo2.py:185][0m |          -0.0138 |          48.6190 |           8.6577 |
[32m[20221214 00:07:30 @agent_ppo2.py:185][0m |          -0.0075 |          54.2135 |           8.6715 |
[32m[20221214 00:07:30 @agent_ppo2.py:185][0m |          -0.0195 |          46.3830 |           8.6854 |
[32m[20221214 00:07:30 @agent_ppo2.py:185][0m |          -0.0185 |          45.6762 |           8.6842 |
[32m[20221214 00:07:30 @agent_ppo2.py:185][0m |          -0.0156 |          45.4998 |           8.6629 |
[32m[20221214 00:07:30 @agent_ppo2.py:185][0m |          -0.0184 |          44.5925 |           8.6689 |
[32m[20221214 00:07:30 @agent_ppo2.py:185][0m |          -0.0230 |          44.3022 |           8.6974 |
[32m[20221214 00:07:30 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:07:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 510.50
[32m[20221214 00:07:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 552.49
[32m[20221214 00:07:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 545.76
[32m[20221214 00:07:30 @agent_ppo2.py:143][0m Total time:      10.00 min
[32m[20221214 00:07:30 @agent_ppo2.py:145][0m 921600 total steps have happened
[32m[20221214 00:07:30 @agent_ppo2.py:121][0m #------------------------ Iteration 4450 --------------------------#
[32m[20221214 00:07:30 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:07:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:07:30 @agent_ppo2.py:185][0m |           0.0037 |          52.8704 |           8.3594 |
[32m[20221214 00:07:31 @agent_ppo2.py:185][0m |          -0.0038 |          45.7123 |           8.3742 |
[32m[20221214 00:07:31 @agent_ppo2.py:185][0m |          -0.0062 |          43.5995 |           8.3493 |
[32m[20221214 00:07:31 @agent_ppo2.py:185][0m |          -0.0084 |          42.6891 |           8.3688 |
[32m[20221214 00:07:31 @agent_ppo2.py:185][0m |          -0.0109 |          42.0769 |           8.3831 |
[32m[20221214 00:07:31 @agent_ppo2.py:185][0m |          -0.0024 |          41.6909 |           8.3337 |
[32m[20221214 00:07:31 @agent_ppo2.py:185][0m |          -0.0124 |          40.8406 |           8.3289 |
[32m[20221214 00:07:31 @agent_ppo2.py:185][0m |          -0.0110 |          40.5877 |           8.3842 |
[32m[20221214 00:07:31 @agent_ppo2.py:185][0m |          -0.0144 |          40.1237 |           8.3521 |
[32m[20221214 00:07:31 @agent_ppo2.py:185][0m |          -0.0124 |          40.0138 |           8.3870 |
[32m[20221214 00:07:31 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:07:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 429.99
[32m[20221214 00:07:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 522.65
[32m[20221214 00:07:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 474.29
[32m[20221214 00:07:31 @agent_ppo2.py:143][0m Total time:      10.02 min
[32m[20221214 00:07:31 @agent_ppo2.py:145][0m 923648 total steps have happened
[32m[20221214 00:07:31 @agent_ppo2.py:121][0m #------------------------ Iteration 4451 --------------------------#
[32m[20221214 00:07:32 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:07:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:07:32 @agent_ppo2.py:185][0m |           0.0071 |          70.8130 |           8.2020 |
[32m[20221214 00:07:32 @agent_ppo2.py:185][0m |          -0.0080 |          61.6580 |           8.2647 |
[32m[20221214 00:07:32 @agent_ppo2.py:185][0m |          -0.0112 |          58.7880 |           8.2292 |
[32m[20221214 00:07:32 @agent_ppo2.py:185][0m |          -0.0119 |          56.9339 |           8.2270 |
[32m[20221214 00:07:32 @agent_ppo2.py:185][0m |          -0.0147 |          55.7429 |           8.1835 |
[32m[20221214 00:07:32 @agent_ppo2.py:185][0m |          -0.0097 |          55.7415 |           8.2261 |
[32m[20221214 00:07:32 @agent_ppo2.py:185][0m |          -0.0170 |          54.2587 |           8.2030 |
[32m[20221214 00:07:32 @agent_ppo2.py:185][0m |          -0.0164 |          53.3750 |           8.2579 |
[32m[20221214 00:07:32 @agent_ppo2.py:185][0m |          -0.0201 |          52.8546 |           8.2252 |
[32m[20221214 00:07:33 @agent_ppo2.py:185][0m |          -0.0172 |          52.3190 |           8.2372 |
[32m[20221214 00:07:33 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:07:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 473.99
[32m[20221214 00:07:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 563.83
[32m[20221214 00:07:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 545.58
[32m[20221214 00:07:33 @agent_ppo2.py:143][0m Total time:      10.04 min
[32m[20221214 00:07:33 @agent_ppo2.py:145][0m 925696 total steps have happened
[32m[20221214 00:07:33 @agent_ppo2.py:121][0m #------------------------ Iteration 4452 --------------------------#
[32m[20221214 00:07:33 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:07:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:07:33 @agent_ppo2.py:185][0m |          -0.0032 |          62.6630 |           8.4722 |
[32m[20221214 00:07:33 @agent_ppo2.py:185][0m |          -0.0122 |          53.8449 |           8.4892 |
[32m[20221214 00:07:33 @agent_ppo2.py:185][0m |          -0.0150 |          51.1872 |           8.4492 |
[32m[20221214 00:07:33 @agent_ppo2.py:185][0m |          -0.0149 |          49.4723 |           8.4380 |
[32m[20221214 00:07:33 @agent_ppo2.py:185][0m |          -0.0151 |          48.2667 |           8.4383 |
[32m[20221214 00:07:33 @agent_ppo2.py:185][0m |          -0.0151 |          47.1257 |           8.4474 |
[32m[20221214 00:07:34 @agent_ppo2.py:185][0m |          -0.0196 |          46.2395 |           8.4675 |
[32m[20221214 00:07:34 @agent_ppo2.py:185][0m |          -0.0149 |          45.8065 |           8.4241 |
[32m[20221214 00:07:34 @agent_ppo2.py:185][0m |          -0.0185 |          45.3489 |           8.4060 |
[32m[20221214 00:07:34 @agent_ppo2.py:185][0m |          -0.0174 |          44.4366 |           8.3789 |
[32m[20221214 00:07:34 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:07:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 456.62
[32m[20221214 00:07:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 582.88
[32m[20221214 00:07:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 544.80
[32m[20221214 00:07:34 @agent_ppo2.py:143][0m Total time:      10.06 min
[32m[20221214 00:07:34 @agent_ppo2.py:145][0m 927744 total steps have happened
[32m[20221214 00:07:34 @agent_ppo2.py:121][0m #------------------------ Iteration 4453 --------------------------#
[32m[20221214 00:07:34 @agent_ppo2.py:127][0m Sampling time: 0.28 s by 5 slaves
[32m[20221214 00:07:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:07:34 @agent_ppo2.py:185][0m |           0.0029 |          49.0367 |           8.3567 |
[32m[20221214 00:07:35 @agent_ppo2.py:185][0m |          -0.0080 |          41.0400 |           8.3786 |
[32m[20221214 00:07:35 @agent_ppo2.py:185][0m |          -0.0116 |          38.7168 |           8.3849 |
[32m[20221214 00:07:35 @agent_ppo2.py:185][0m |          -0.0107 |          37.5398 |           8.3790 |
[32m[20221214 00:07:35 @agent_ppo2.py:185][0m |          -0.0135 |          36.5854 |           8.3543 |
[32m[20221214 00:07:35 @agent_ppo2.py:185][0m |          -0.0122 |          36.0803 |           8.3655 |
[32m[20221214 00:07:35 @agent_ppo2.py:185][0m |          -0.0134 |          35.8511 |           8.3827 |
[32m[20221214 00:07:35 @agent_ppo2.py:185][0m |          -0.0165 |          35.4277 |           8.3878 |
[32m[20221214 00:07:35 @agent_ppo2.py:185][0m |          -0.0144 |          35.1287 |           8.3535 |
[32m[20221214 00:07:35 @agent_ppo2.py:185][0m |          -0.0234 |          34.7370 |           8.4141 |
[32m[20221214 00:07:35 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:07:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 377.94
[32m[20221214 00:07:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 470.02
[32m[20221214 00:07:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 582.50
[32m[20221214 00:07:35 @agent_ppo2.py:143][0m Total time:      10.09 min
[32m[20221214 00:07:35 @agent_ppo2.py:145][0m 929792 total steps have happened
[32m[20221214 00:07:35 @agent_ppo2.py:121][0m #------------------------ Iteration 4454 --------------------------#
[32m[20221214 00:07:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:07:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:07:36 @agent_ppo2.py:185][0m |           0.0094 |          73.2407 |           8.0746 |
[32m[20221214 00:07:36 @agent_ppo2.py:185][0m |          -0.0098 |          63.1532 |           8.1408 |
[32m[20221214 00:07:36 @agent_ppo2.py:185][0m |          -0.0113 |          61.0467 |           8.1582 |
[32m[20221214 00:07:36 @agent_ppo2.py:185][0m |          -0.0157 |          59.8641 |           8.1895 |
[32m[20221214 00:07:36 @agent_ppo2.py:185][0m |          -0.0153 |          59.3767 |           8.1857 |
[32m[20221214 00:07:36 @agent_ppo2.py:185][0m |          -0.0158 |          58.5212 |           8.1658 |
[32m[20221214 00:07:36 @agent_ppo2.py:185][0m |          -0.0173 |          58.1737 |           8.2036 |
[32m[20221214 00:07:36 @agent_ppo2.py:185][0m |          -0.0182 |          57.6591 |           8.2092 |
[32m[20221214 00:07:37 @agent_ppo2.py:185][0m |          -0.0194 |          57.2965 |           8.2380 |
[32m[20221214 00:07:37 @agent_ppo2.py:185][0m |          -0.0201 |          56.9303 |           8.1969 |
[32m[20221214 00:07:37 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:07:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 512.31
[32m[20221214 00:07:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 573.58
[32m[20221214 00:07:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 551.16
[32m[20221214 00:07:37 @agent_ppo2.py:143][0m Total time:      10.11 min
[32m[20221214 00:07:37 @agent_ppo2.py:145][0m 931840 total steps have happened
[32m[20221214 00:07:37 @agent_ppo2.py:121][0m #------------------------ Iteration 4455 --------------------------#
[32m[20221214 00:07:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:07:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:07:37 @agent_ppo2.py:185][0m |           0.0040 |          57.7679 |           8.2283 |
[32m[20221214 00:07:37 @agent_ppo2.py:185][0m |          -0.0043 |          46.2717 |           8.2576 |
[32m[20221214 00:07:37 @agent_ppo2.py:185][0m |          -0.0081 |          44.5211 |           8.2146 |
[32m[20221214 00:07:37 @agent_ppo2.py:185][0m |          -0.0038 |          43.8763 |           8.2334 |
[32m[20221214 00:07:37 @agent_ppo2.py:185][0m |          -0.0066 |          43.1402 |           8.2617 |
[32m[20221214 00:07:38 @agent_ppo2.py:185][0m |          -0.0092 |          42.6508 |           8.2994 |
[32m[20221214 00:07:38 @agent_ppo2.py:185][0m |          -0.0101 |          42.3804 |           8.2901 |
[32m[20221214 00:07:38 @agent_ppo2.py:185][0m |          -0.0178 |          42.0531 |           8.2493 |
[32m[20221214 00:07:38 @agent_ppo2.py:185][0m |          -0.0092 |          41.9163 |           8.3000 |
[32m[20221214 00:07:38 @agent_ppo2.py:185][0m |          -0.0188 |          41.9418 |           8.3108 |
[32m[20221214 00:07:38 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:07:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 444.05
[32m[20221214 00:07:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 516.19
[32m[20221214 00:07:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 530.11
[32m[20221214 00:07:38 @agent_ppo2.py:143][0m Total time:      10.13 min
[32m[20221214 00:07:38 @agent_ppo2.py:145][0m 933888 total steps have happened
[32m[20221214 00:07:38 @agent_ppo2.py:121][0m #------------------------ Iteration 4456 --------------------------#
[32m[20221214 00:07:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:07:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:07:38 @agent_ppo2.py:185][0m |           0.0076 |          49.3592 |           8.6849 |
[32m[20221214 00:07:38 @agent_ppo2.py:185][0m |           0.0004 |          43.8080 |           8.7337 |
[32m[20221214 00:07:39 @agent_ppo2.py:185][0m |          -0.0094 |          42.3421 |           8.7043 |
[32m[20221214 00:07:39 @agent_ppo2.py:185][0m |          -0.0139 |          41.5326 |           8.6525 |
[32m[20221214 00:07:39 @agent_ppo2.py:185][0m |          -0.0138 |          40.9715 |           8.6221 |
[32m[20221214 00:07:39 @agent_ppo2.py:185][0m |          -0.0146 |          40.6897 |           8.6785 |
[32m[20221214 00:07:39 @agent_ppo2.py:185][0m |          -0.0144 |          40.2865 |           8.6635 |
[32m[20221214 00:07:39 @agent_ppo2.py:185][0m |          -0.0173 |          40.2959 |           8.6600 |
[32m[20221214 00:07:39 @agent_ppo2.py:185][0m |          -0.0145 |          39.6761 |           8.6668 |
[32m[20221214 00:07:39 @agent_ppo2.py:185][0m |          -0.0196 |          40.1068 |           8.7467 |
[32m[20221214 00:07:39 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:07:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 515.71
[32m[20221214 00:07:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 596.33
[32m[20221214 00:07:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 476.05
[32m[20221214 00:07:39 @agent_ppo2.py:143][0m Total time:      10.15 min
[32m[20221214 00:07:39 @agent_ppo2.py:145][0m 935936 total steps have happened
[32m[20221214 00:07:39 @agent_ppo2.py:121][0m #------------------------ Iteration 4457 --------------------------#
[32m[20221214 00:07:40 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:07:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:07:40 @agent_ppo2.py:185][0m |           0.0033 |          52.0614 |           8.2388 |
[32m[20221214 00:07:40 @agent_ppo2.py:185][0m |          -0.0092 |          45.8453 |           8.2734 |
[32m[20221214 00:07:40 @agent_ppo2.py:185][0m |          -0.0013 |          46.3412 |           8.1851 |
[32m[20221214 00:07:40 @agent_ppo2.py:185][0m |          -0.0096 |          43.4470 |           8.2381 |
[32m[20221214 00:07:40 @agent_ppo2.py:185][0m |          -0.0110 |          42.7258 |           8.2372 |
[32m[20221214 00:07:40 @agent_ppo2.py:185][0m |          -0.0129 |          42.2960 |           8.1981 |
[32m[20221214 00:07:40 @agent_ppo2.py:185][0m |          -0.0151 |          41.9112 |           8.2099 |
[32m[20221214 00:07:40 @agent_ppo2.py:185][0m |          -0.0127 |          41.4750 |           8.2388 |
[32m[20221214 00:07:40 @agent_ppo2.py:185][0m |          -0.0139 |          41.2630 |           8.2311 |
[32m[20221214 00:07:41 @agent_ppo2.py:185][0m |          -0.0166 |          41.0003 |           8.2614 |
[32m[20221214 00:07:41 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:07:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 523.22
[32m[20221214 00:07:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 561.28
[32m[20221214 00:07:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 483.27
[32m[20221214 00:07:41 @agent_ppo2.py:143][0m Total time:      10.17 min
[32m[20221214 00:07:41 @agent_ppo2.py:145][0m 937984 total steps have happened
[32m[20221214 00:07:41 @agent_ppo2.py:121][0m #------------------------ Iteration 4458 --------------------------#
[32m[20221214 00:07:41 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:07:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:07:41 @agent_ppo2.py:185][0m |          -0.0017 |          89.2557 |           8.6295 |
[32m[20221214 00:07:41 @agent_ppo2.py:185][0m |           0.0014 |          85.6695 |           8.6561 |
[32m[20221214 00:07:41 @agent_ppo2.py:185][0m |          -0.0055 |          82.3912 |           8.6119 |
[32m[20221214 00:07:41 @agent_ppo2.py:185][0m |          -0.0110 |          81.5408 |           8.6931 |
[32m[20221214 00:07:41 @agent_ppo2.py:185][0m |          -0.0077 |          81.3629 |           8.6426 |
[32m[20221214 00:07:42 @agent_ppo2.py:185][0m |          -0.0102 |          80.5607 |           8.7082 |
[32m[20221214 00:07:42 @agent_ppo2.py:185][0m |          -0.0121 |          80.2706 |           8.7124 |
[32m[20221214 00:07:42 @agent_ppo2.py:185][0m |          -0.0017 |          82.7867 |           8.7558 |
[32m[20221214 00:07:42 @agent_ppo2.py:185][0m |          -0.0147 |          79.2492 |           8.7815 |
[32m[20221214 00:07:42 @agent_ppo2.py:185][0m |          -0.0171 |          78.4080 |           8.8096 |
[32m[20221214 00:07:42 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:07:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 485.01
[32m[20221214 00:07:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 526.54
[32m[20221214 00:07:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 539.22
[32m[20221214 00:07:42 @agent_ppo2.py:143][0m Total time:      10.19 min
[32m[20221214 00:07:42 @agent_ppo2.py:145][0m 940032 total steps have happened
[32m[20221214 00:07:42 @agent_ppo2.py:121][0m #------------------------ Iteration 4459 --------------------------#
[32m[20221214 00:07:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:07:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:07:42 @agent_ppo2.py:185][0m |           0.0038 |          62.1789 |           8.8562 |
[32m[20221214 00:07:42 @agent_ppo2.py:185][0m |          -0.0073 |          57.7070 |           8.7773 |
[32m[20221214 00:07:43 @agent_ppo2.py:185][0m |           0.0007 |          58.9009 |           8.8171 |
[32m[20221214 00:07:43 @agent_ppo2.py:185][0m |          -0.0114 |          54.8663 |           8.7975 |
[32m[20221214 00:07:43 @agent_ppo2.py:185][0m |          -0.0136 |          54.0398 |           8.8075 |
[32m[20221214 00:07:43 @agent_ppo2.py:185][0m |          -0.0144 |          53.5969 |           8.8176 |
[32m[20221214 00:07:43 @agent_ppo2.py:185][0m |          -0.0146 |          53.0540 |           8.7513 |
[32m[20221214 00:07:43 @agent_ppo2.py:185][0m |          -0.0157 |          52.1515 |           8.7522 |
[32m[20221214 00:07:43 @agent_ppo2.py:185][0m |          -0.0174 |          51.6475 |           8.7806 |
[32m[20221214 00:07:43 @agent_ppo2.py:185][0m |          -0.0202 |          51.0724 |           8.7820 |
[32m[20221214 00:07:43 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:07:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 496.89
[32m[20221214 00:07:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 528.18
[32m[20221214 00:07:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 539.14
[32m[20221214 00:07:43 @agent_ppo2.py:143][0m Total time:      10.22 min
[32m[20221214 00:07:43 @agent_ppo2.py:145][0m 942080 total steps have happened
[32m[20221214 00:07:43 @agent_ppo2.py:121][0m #------------------------ Iteration 4460 --------------------------#
[32m[20221214 00:07:44 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:07:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:07:44 @agent_ppo2.py:185][0m |           0.0020 |          50.6797 |           8.7662 |
[32m[20221214 00:07:44 @agent_ppo2.py:185][0m |          -0.0065 |          42.6334 |           8.6835 |
[32m[20221214 00:07:44 @agent_ppo2.py:185][0m |          -0.0000 |          48.0528 |           8.7141 |
[32m[20221214 00:07:44 @agent_ppo2.py:185][0m |          -0.0035 |          40.3920 |           8.6927 |
[32m[20221214 00:07:44 @agent_ppo2.py:185][0m |          -0.0160 |          39.2136 |           8.6572 |
[32m[20221214 00:07:44 @agent_ppo2.py:185][0m |          -0.0182 |          38.6801 |           8.6529 |
[32m[20221214 00:07:44 @agent_ppo2.py:185][0m |          -0.0171 |          38.1150 |           8.6424 |
[32m[20221214 00:07:44 @agent_ppo2.py:185][0m |          -0.0175 |          37.9708 |           8.6274 |
[32m[20221214 00:07:44 @agent_ppo2.py:185][0m |          -0.0200 |          37.4945 |           8.5979 |
[32m[20221214 00:07:45 @agent_ppo2.py:185][0m |          -0.0018 |          45.3990 |           8.6322 |
[32m[20221214 00:07:45 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:07:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 441.03
[32m[20221214 00:07:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 485.49
[32m[20221214 00:07:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 451.08
[32m[20221214 00:07:45 @agent_ppo2.py:143][0m Total time:      10.24 min
[32m[20221214 00:07:45 @agent_ppo2.py:145][0m 944128 total steps have happened
[32m[20221214 00:07:45 @agent_ppo2.py:121][0m #------------------------ Iteration 4461 --------------------------#
[32m[20221214 00:07:45 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:07:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:07:45 @agent_ppo2.py:185][0m |          -0.0019 |          41.7357 |           8.4703 |
[32m[20221214 00:07:45 @agent_ppo2.py:185][0m |          -0.0134 |          34.8370 |           8.4593 |
[32m[20221214 00:07:45 @agent_ppo2.py:185][0m |          -0.0054 |          32.6471 |           8.4648 |
[32m[20221214 00:07:45 @agent_ppo2.py:185][0m |          -0.0122 |          30.8734 |           8.4633 |
[32m[20221214 00:07:45 @agent_ppo2.py:185][0m |          -0.0122 |          29.7543 |           8.4541 |
[32m[20221214 00:07:46 @agent_ppo2.py:185][0m |          -0.0165 |          28.9233 |           8.4903 |
[32m[20221214 00:07:46 @agent_ppo2.py:185][0m |          -0.0162 |          28.2429 |           8.4814 |
[32m[20221214 00:07:46 @agent_ppo2.py:185][0m |          -0.0162 |          27.7004 |           8.4499 |
[32m[20221214 00:07:46 @agent_ppo2.py:185][0m |          -0.0197 |          27.4155 |           8.4421 |
[32m[20221214 00:07:46 @agent_ppo2.py:185][0m |          -0.0165 |          26.8148 |           8.4440 |
[32m[20221214 00:07:46 @agent_ppo2.py:130][0m Policy update time: 1.17 s
[32m[20221214 00:07:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 436.39
[32m[20221214 00:07:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 482.86
[32m[20221214 00:07:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 515.31
[32m[20221214 00:07:46 @agent_ppo2.py:143][0m Total time:      10.26 min
[32m[20221214 00:07:46 @agent_ppo2.py:145][0m 946176 total steps have happened
[32m[20221214 00:07:46 @agent_ppo2.py:121][0m #------------------------ Iteration 4462 --------------------------#
[32m[20221214 00:07:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:07:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:07:47 @agent_ppo2.py:185][0m |           0.0015 |          63.8381 |           8.5511 |
[32m[20221214 00:07:47 @agent_ppo2.py:185][0m |          -0.0055 |          56.9459 |           8.5612 |
[32m[20221214 00:07:47 @agent_ppo2.py:185][0m |          -0.0094 |          53.8844 |           8.5568 |
[32m[20221214 00:07:47 @agent_ppo2.py:185][0m |          -0.0099 |          52.1035 |           8.5260 |
[32m[20221214 00:07:47 @agent_ppo2.py:185][0m |          -0.0127 |          50.9993 |           8.5806 |
[32m[20221214 00:07:47 @agent_ppo2.py:185][0m |          -0.0123 |          50.3200 |           8.5476 |
[32m[20221214 00:07:47 @agent_ppo2.py:185][0m |          -0.0103 |          49.9577 |           8.5693 |
[32m[20221214 00:07:47 @agent_ppo2.py:185][0m |          -0.0167 |          49.2334 |           8.5849 |
[32m[20221214 00:07:47 @agent_ppo2.py:185][0m |          -0.0140 |          48.8097 |           8.6265 |
[32m[20221214 00:07:47 @agent_ppo2.py:185][0m |          -0.0117 |          48.7549 |           8.6254 |
[32m[20221214 00:07:47 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:07:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 494.74
[32m[20221214 00:07:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 580.13
[32m[20221214 00:07:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 546.08
[32m[20221214 00:07:48 @agent_ppo2.py:143][0m Total time:      10.29 min
[32m[20221214 00:07:48 @agent_ppo2.py:145][0m 948224 total steps have happened
[32m[20221214 00:07:48 @agent_ppo2.py:121][0m #------------------------ Iteration 4463 --------------------------#
[32m[20221214 00:07:48 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:07:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:07:48 @agent_ppo2.py:185][0m |           0.0033 |          86.3445 |           8.1874 |
[32m[20221214 00:07:48 @agent_ppo2.py:185][0m |          -0.0060 |          81.8387 |           8.1668 |
[32m[20221214 00:07:48 @agent_ppo2.py:185][0m |          -0.0087 |          80.2732 |           8.2089 |
[32m[20221214 00:07:48 @agent_ppo2.py:185][0m |          -0.0099 |          79.4292 |           8.1857 |
[32m[20221214 00:07:48 @agent_ppo2.py:185][0m |          -0.0134 |          78.7704 |           8.2273 |
[32m[20221214 00:07:48 @agent_ppo2.py:185][0m |          -0.0068 |          79.6417 |           8.2091 |
[32m[20221214 00:07:48 @agent_ppo2.py:185][0m |          -0.0083 |          77.7556 |           8.2282 |
[32m[20221214 00:07:49 @agent_ppo2.py:185][0m |          -0.0118 |          77.6057 |           8.2441 |
[32m[20221214 00:07:49 @agent_ppo2.py:185][0m |          -0.0140 |          77.0965 |           8.2117 |
[32m[20221214 00:07:49 @agent_ppo2.py:185][0m |          -0.0137 |          76.6670 |           8.2014 |
[32m[20221214 00:07:49 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:07:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 502.34
[32m[20221214 00:07:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 520.26
[32m[20221214 00:07:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 505.02
[32m[20221214 00:07:49 @agent_ppo2.py:143][0m Total time:      10.31 min
[32m[20221214 00:07:49 @agent_ppo2.py:145][0m 950272 total steps have happened
[32m[20221214 00:07:49 @agent_ppo2.py:121][0m #------------------------ Iteration 4464 --------------------------#
[32m[20221214 00:07:49 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:07:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:07:49 @agent_ppo2.py:185][0m |           0.0033 |          57.7554 |           8.5382 |
[32m[20221214 00:07:49 @agent_ppo2.py:185][0m |           0.0064 |          54.7416 |           8.4898 |
[32m[20221214 00:07:49 @agent_ppo2.py:185][0m |          -0.0079 |          49.8033 |           8.4440 |
[32m[20221214 00:07:49 @agent_ppo2.py:185][0m |          -0.0056 |          49.6155 |           8.4121 |
[32m[20221214 00:07:50 @agent_ppo2.py:185][0m |          -0.0130 |          48.9645 |           8.3868 |
[32m[20221214 00:07:50 @agent_ppo2.py:185][0m |          -0.0109 |          49.6793 |           8.3442 |
[32m[20221214 00:07:50 @agent_ppo2.py:185][0m |          -0.0139 |          48.0929 |           8.3644 |
[32m[20221214 00:07:50 @agent_ppo2.py:185][0m |          -0.0057 |          51.3820 |           8.3155 |
[32m[20221214 00:07:50 @agent_ppo2.py:185][0m |          -0.0162 |          48.0575 |           8.3163 |
[32m[20221214 00:07:50 @agent_ppo2.py:185][0m |          -0.0148 |          47.3469 |           8.3511 |
[32m[20221214 00:07:50 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:07:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 522.16
[32m[20221214 00:07:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 556.63
[32m[20221214 00:07:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 489.22
[32m[20221214 00:07:50 @agent_ppo2.py:143][0m Total time:      10.33 min
[32m[20221214 00:07:50 @agent_ppo2.py:145][0m 952320 total steps have happened
[32m[20221214 00:07:50 @agent_ppo2.py:121][0m #------------------------ Iteration 4465 --------------------------#
[32m[20221214 00:07:50 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:07:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:07:50 @agent_ppo2.py:185][0m |          -0.0010 |          45.6908 |           8.5329 |
[32m[20221214 00:07:51 @agent_ppo2.py:185][0m |          -0.0050 |          40.5516 |           8.5087 |
[32m[20221214 00:07:51 @agent_ppo2.py:185][0m |          -0.0127 |          38.9412 |           8.4916 |
[32m[20221214 00:07:51 @agent_ppo2.py:185][0m |          -0.0093 |          37.7352 |           8.4194 |
[32m[20221214 00:07:51 @agent_ppo2.py:185][0m |          -0.0185 |          37.0845 |           8.4715 |
[32m[20221214 00:07:51 @agent_ppo2.py:185][0m |          -0.0146 |          36.5993 |           8.5474 |
[32m[20221214 00:07:51 @agent_ppo2.py:185][0m |          -0.0148 |          36.1201 |           8.4543 |
[32m[20221214 00:07:51 @agent_ppo2.py:185][0m |          -0.0111 |          36.6685 |           8.4692 |
[32m[20221214 00:07:51 @agent_ppo2.py:185][0m |          -0.0210 |          35.7166 |           8.4448 |
[32m[20221214 00:07:51 @agent_ppo2.py:185][0m |          -0.0193 |          35.0653 |           8.5042 |
[32m[20221214 00:07:51 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:07:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 421.27
[32m[20221214 00:07:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 496.53
[32m[20221214 00:07:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 428.67
[32m[20221214 00:07:51 @agent_ppo2.py:143][0m Total time:      10.35 min
[32m[20221214 00:07:51 @agent_ppo2.py:145][0m 954368 total steps have happened
[32m[20221214 00:07:51 @agent_ppo2.py:121][0m #------------------------ Iteration 4466 --------------------------#
[32m[20221214 00:07:52 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:07:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:07:52 @agent_ppo2.py:185][0m |          -0.0060 |          46.7631 |           8.5011 |
[32m[20221214 00:07:52 @agent_ppo2.py:185][0m |          -0.0057 |          42.7796 |           8.4935 |
[32m[20221214 00:07:52 @agent_ppo2.py:185][0m |          -0.0086 |          39.5913 |           8.4720 |
[32m[20221214 00:07:52 @agent_ppo2.py:185][0m |          -0.0130 |          38.1633 |           8.4246 |
[32m[20221214 00:07:52 @agent_ppo2.py:185][0m |          -0.0158 |          37.3577 |           8.4476 |
[32m[20221214 00:07:52 @agent_ppo2.py:185][0m |          -0.0118 |          37.0331 |           8.4426 |
[32m[20221214 00:07:52 @agent_ppo2.py:185][0m |          -0.0198 |          36.2309 |           8.4660 |
[32m[20221214 00:07:52 @agent_ppo2.py:185][0m |          -0.0152 |          35.6233 |           8.4123 |
[32m[20221214 00:07:52 @agent_ppo2.py:185][0m |          -0.0202 |          35.1955 |           8.4191 |
[32m[20221214 00:07:53 @agent_ppo2.py:185][0m |          -0.0221 |          34.7389 |           8.3772 |
[32m[20221214 00:07:53 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:07:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 418.07
[32m[20221214 00:07:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 467.33
[32m[20221214 00:07:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 427.31
[32m[20221214 00:07:53 @agent_ppo2.py:143][0m Total time:      10.37 min
[32m[20221214 00:07:53 @agent_ppo2.py:145][0m 956416 total steps have happened
[32m[20221214 00:07:53 @agent_ppo2.py:121][0m #------------------------ Iteration 4467 --------------------------#
[32m[20221214 00:07:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:07:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:07:53 @agent_ppo2.py:185][0m |           0.0083 |          71.0046 |           8.2332 |
[32m[20221214 00:07:53 @agent_ppo2.py:185][0m |           0.0021 |          63.4592 |           8.1942 |
[32m[20221214 00:07:53 @agent_ppo2.py:185][0m |          -0.0055 |          59.7624 |           8.1894 |
[32m[20221214 00:07:53 @agent_ppo2.py:185][0m |          -0.0122 |          58.1089 |           8.1800 |
[32m[20221214 00:07:53 @agent_ppo2.py:185][0m |          -0.0117 |          56.9085 |           8.2010 |
[32m[20221214 00:07:53 @agent_ppo2.py:185][0m |          -0.0036 |          64.0762 |           8.1715 |
[32m[20221214 00:07:54 @agent_ppo2.py:185][0m |          -0.0142 |          56.0521 |           8.1722 |
[32m[20221214 00:07:54 @agent_ppo2.py:185][0m |          -0.0175 |          55.5381 |           8.1745 |
[32m[20221214 00:07:54 @agent_ppo2.py:185][0m |          -0.0153 |          55.1753 |           8.1855 |
[32m[20221214 00:07:54 @agent_ppo2.py:185][0m |          -0.0138 |          55.5038 |           8.1768 |
[32m[20221214 00:07:54 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:07:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 503.01
[32m[20221214 00:07:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 524.99
[32m[20221214 00:07:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 476.05
[32m[20221214 00:07:54 @agent_ppo2.py:143][0m Total time:      10.39 min
[32m[20221214 00:07:54 @agent_ppo2.py:145][0m 958464 total steps have happened
[32m[20221214 00:07:54 @agent_ppo2.py:121][0m #------------------------ Iteration 4468 --------------------------#
[32m[20221214 00:07:54 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:07:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:07:54 @agent_ppo2.py:185][0m |           0.0008 |          78.9849 |           8.3524 |
[32m[20221214 00:07:54 @agent_ppo2.py:185][0m |          -0.0107 |          74.4849 |           8.3753 |
[32m[20221214 00:07:55 @agent_ppo2.py:185][0m |          -0.0122 |          73.4828 |           8.4015 |
[32m[20221214 00:07:55 @agent_ppo2.py:185][0m |          -0.0109 |          73.1398 |           8.3561 |
[32m[20221214 00:07:55 @agent_ppo2.py:185][0m |          -0.0122 |          72.9355 |           8.3956 |
[32m[20221214 00:07:55 @agent_ppo2.py:185][0m |          -0.0125 |          72.5840 |           8.3831 |
[32m[20221214 00:07:55 @agent_ppo2.py:185][0m |          -0.0167 |          72.3146 |           8.4203 |
[32m[20221214 00:07:55 @agent_ppo2.py:185][0m |          -0.0172 |          72.0106 |           8.3781 |
[32m[20221214 00:07:55 @agent_ppo2.py:185][0m |          -0.0180 |          72.0065 |           8.4594 |
[32m[20221214 00:07:55 @agent_ppo2.py:185][0m |          -0.0051 |          78.0547 |           8.4406 |
[32m[20221214 00:07:55 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:07:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 531.56
[32m[20221214 00:07:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 561.81
[32m[20221214 00:07:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 540.83
[32m[20221214 00:07:55 @agent_ppo2.py:143][0m Total time:      10.42 min
[32m[20221214 00:07:55 @agent_ppo2.py:145][0m 960512 total steps have happened
[32m[20221214 00:07:55 @agent_ppo2.py:121][0m #------------------------ Iteration 4469 --------------------------#
[32m[20221214 00:07:56 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:07:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:07:56 @agent_ppo2.py:185][0m |           0.0019 |          53.6007 |           8.0501 |
[32m[20221214 00:07:56 @agent_ppo2.py:185][0m |          -0.0072 |          48.7301 |           8.0598 |
[32m[20221214 00:07:56 @agent_ppo2.py:185][0m |          -0.0043 |          46.8511 |           8.0546 |
[32m[20221214 00:07:56 @agent_ppo2.py:185][0m |          -0.0102 |          45.6561 |           8.0694 |
[32m[20221214 00:07:56 @agent_ppo2.py:185][0m |          -0.0111 |          44.8969 |           8.0104 |
[32m[20221214 00:07:56 @agent_ppo2.py:185][0m |          -0.0131 |          44.5086 |           8.0411 |
[32m[20221214 00:07:56 @agent_ppo2.py:185][0m |          -0.0106 |          47.6392 |           8.0491 |
[32m[20221214 00:07:56 @agent_ppo2.py:185][0m |          -0.0118 |          43.6752 |           8.0786 |
[32m[20221214 00:07:56 @agent_ppo2.py:185][0m |          -0.0154 |          43.4358 |           8.0487 |
[32m[20221214 00:07:56 @agent_ppo2.py:185][0m |          -0.0108 |          43.0761 |           8.0249 |
[32m[20221214 00:07:56 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:07:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 507.24
[32m[20221214 00:07:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 525.13
[32m[20221214 00:07:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 503.88
[32m[20221214 00:07:57 @agent_ppo2.py:143][0m Total time:      10.44 min
[32m[20221214 00:07:57 @agent_ppo2.py:145][0m 962560 total steps have happened
[32m[20221214 00:07:57 @agent_ppo2.py:121][0m #------------------------ Iteration 4470 --------------------------#
[32m[20221214 00:07:57 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:07:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:07:57 @agent_ppo2.py:185][0m |           0.0008 |          72.9310 |           8.2623 |
[32m[20221214 00:07:57 @agent_ppo2.py:185][0m |          -0.0056 |          71.5673 |           8.2983 |
[32m[20221214 00:07:57 @agent_ppo2.py:185][0m |          -0.0088 |          70.9543 |           8.2978 |
[32m[20221214 00:07:57 @agent_ppo2.py:185][0m |          -0.0091 |          70.6555 |           8.2597 |
[32m[20221214 00:07:57 @agent_ppo2.py:185][0m |          -0.0126 |          70.5236 |           8.2072 |
[32m[20221214 00:07:57 @agent_ppo2.py:185][0m |          -0.0106 |          70.2499 |           8.1790 |
[32m[20221214 00:07:58 @agent_ppo2.py:185][0m |          -0.0122 |          70.0957 |           8.1949 |
[32m[20221214 00:07:58 @agent_ppo2.py:185][0m |          -0.0040 |          73.7627 |           8.1757 |
[32m[20221214 00:07:58 @agent_ppo2.py:185][0m |          -0.0125 |          69.8610 |           8.1348 |
[32m[20221214 00:07:58 @agent_ppo2.py:185][0m |          -0.0089 |          71.1802 |           8.1304 |
[32m[20221214 00:07:58 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:07:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 512.94
[32m[20221214 00:07:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 527.15
[32m[20221214 00:07:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 497.30
[32m[20221214 00:07:58 @agent_ppo2.py:143][0m Total time:      10.46 min
[32m[20221214 00:07:58 @agent_ppo2.py:145][0m 964608 total steps have happened
[32m[20221214 00:07:58 @agent_ppo2.py:121][0m #------------------------ Iteration 4471 --------------------------#
[32m[20221214 00:07:58 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:07:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:07:58 @agent_ppo2.py:185][0m |           0.0001 |          81.8553 |           7.7150 |
[32m[20221214 00:07:58 @agent_ppo2.py:185][0m |          -0.0078 |          78.1847 |           7.7149 |
[32m[20221214 00:07:58 @agent_ppo2.py:185][0m |          -0.0043 |          81.1600 |           7.6972 |
[32m[20221214 00:07:59 @agent_ppo2.py:185][0m |          -0.0118 |          75.4809 |           7.6689 |
[32m[20221214 00:07:59 @agent_ppo2.py:185][0m |          -0.0130 |          74.7383 |           7.6473 |
[32m[20221214 00:07:59 @agent_ppo2.py:185][0m |          -0.0128 |          74.0045 |           7.6840 |
[32m[20221214 00:07:59 @agent_ppo2.py:185][0m |          -0.0118 |          73.6601 |           7.6853 |
[32m[20221214 00:07:59 @agent_ppo2.py:185][0m |          -0.0148 |          73.0550 |           7.7214 |
[32m[20221214 00:07:59 @agent_ppo2.py:185][0m |          -0.0149 |          72.5712 |           7.7375 |
[32m[20221214 00:07:59 @agent_ppo2.py:185][0m |          -0.0140 |          72.2170 |           7.7102 |
[32m[20221214 00:07:59 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:07:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 525.74
[32m[20221214 00:07:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 535.92
[32m[20221214 00:07:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 546.01
[32m[20221214 00:07:59 @agent_ppo2.py:143][0m Total time:      10.48 min
[32m[20221214 00:07:59 @agent_ppo2.py:145][0m 966656 total steps have happened
[32m[20221214 00:07:59 @agent_ppo2.py:121][0m #------------------------ Iteration 4472 --------------------------#
[32m[20221214 00:07:59 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:07:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:08:00 @agent_ppo2.py:185][0m |          -0.0024 |          69.2126 |           7.8469 |
[32m[20221214 00:08:00 @agent_ppo2.py:185][0m |          -0.0032 |          60.0658 |           7.8613 |
[32m[20221214 00:08:00 @agent_ppo2.py:185][0m |           0.0037 |          65.2752 |           7.8861 |
[32m[20221214 00:08:00 @agent_ppo2.py:185][0m |          -0.0080 |          55.1283 |           7.9662 |
[32m[20221214 00:08:00 @agent_ppo2.py:185][0m |          -0.0102 |          53.6468 |           8.0090 |
[32m[20221214 00:08:00 @agent_ppo2.py:185][0m |          -0.0127 |          52.7040 |           7.9841 |
[32m[20221214 00:08:00 @agent_ppo2.py:185][0m |          -0.0166 |          51.9006 |           8.0025 |
[32m[20221214 00:08:00 @agent_ppo2.py:185][0m |          -0.0141 |          51.2612 |           7.9810 |
[32m[20221214 00:08:00 @agent_ppo2.py:185][0m |          -0.0180 |          50.5610 |           8.0528 |
[32m[20221214 00:08:00 @agent_ppo2.py:185][0m |          -0.0104 |          54.5487 |           8.0981 |
[32m[20221214 00:08:00 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:08:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 379.08
[32m[20221214 00:08:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 439.73
[32m[20221214 00:08:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 445.29
[32m[20221214 00:08:00 @agent_ppo2.py:143][0m Total time:      10.50 min
[32m[20221214 00:08:00 @agent_ppo2.py:145][0m 968704 total steps have happened
[32m[20221214 00:08:00 @agent_ppo2.py:121][0m #------------------------ Iteration 4473 --------------------------#
[32m[20221214 00:08:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:08:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:08:01 @agent_ppo2.py:185][0m |          -0.0047 |          60.8238 |           8.8670 |
[32m[20221214 00:08:01 @agent_ppo2.py:185][0m |          -0.0069 |          54.4909 |           8.8462 |
[32m[20221214 00:08:01 @agent_ppo2.py:185][0m |          -0.0087 |          52.0350 |           8.7788 |
[32m[20221214 00:08:01 @agent_ppo2.py:185][0m |          -0.0097 |          50.6395 |           8.8825 |
[32m[20221214 00:08:01 @agent_ppo2.py:185][0m |          -0.0118 |          49.4176 |           8.8442 |
[32m[20221214 00:08:01 @agent_ppo2.py:185][0m |          -0.0165 |          48.7625 |           8.8602 |
[32m[20221214 00:08:01 @agent_ppo2.py:185][0m |          -0.0125 |          47.8813 |           8.8799 |
[32m[20221214 00:08:01 @agent_ppo2.py:185][0m |          -0.0131 |          47.3725 |           8.8823 |
[32m[20221214 00:08:01 @agent_ppo2.py:185][0m |          -0.0074 |          47.8605 |           8.8851 |
[32m[20221214 00:08:02 @agent_ppo2.py:185][0m |          -0.0169 |          46.4190 |           8.9374 |
[32m[20221214 00:08:02 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:08:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 412.53
[32m[20221214 00:08:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 451.59
[32m[20221214 00:08:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 468.72
[32m[20221214 00:08:02 @agent_ppo2.py:143][0m Total time:      10.52 min
[32m[20221214 00:08:02 @agent_ppo2.py:145][0m 970752 total steps have happened
[32m[20221214 00:08:02 @agent_ppo2.py:121][0m #------------------------ Iteration 4474 --------------------------#
[32m[20221214 00:08:02 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:08:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:08:02 @agent_ppo2.py:185][0m |          -0.0034 |          63.4303 |           8.2274 |
[32m[20221214 00:08:02 @agent_ppo2.py:185][0m |           0.0078 |          62.3409 |           8.2409 |
[32m[20221214 00:08:02 @agent_ppo2.py:185][0m |          -0.0063 |          59.9598 |           8.2687 |
[32m[20221214 00:08:02 @agent_ppo2.py:185][0m |          -0.0085 |          59.5660 |           8.2806 |
[32m[20221214 00:08:02 @agent_ppo2.py:185][0m |          -0.0119 |          59.1130 |           8.3394 |
[32m[20221214 00:08:03 @agent_ppo2.py:185][0m |          -0.0080 |          58.9980 |           8.3752 |
[32m[20221214 00:08:03 @agent_ppo2.py:185][0m |          -0.0134 |          58.6450 |           8.3669 |
[32m[20221214 00:08:03 @agent_ppo2.py:185][0m |          -0.0109 |          58.5308 |           8.4209 |
[32m[20221214 00:08:03 @agent_ppo2.py:185][0m |          -0.0118 |          58.2125 |           8.4432 |
[32m[20221214 00:08:03 @agent_ppo2.py:185][0m |          -0.0139 |          57.9704 |           8.4525 |
[32m[20221214 00:08:03 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:08:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 497.43
[32m[20221214 00:08:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 514.30
[32m[20221214 00:08:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 521.90
[32m[20221214 00:08:03 @agent_ppo2.py:143][0m Total time:      10.54 min
[32m[20221214 00:08:03 @agent_ppo2.py:145][0m 972800 total steps have happened
[32m[20221214 00:08:03 @agent_ppo2.py:121][0m #------------------------ Iteration 4475 --------------------------#
[32m[20221214 00:08:03 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:08:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:08:03 @agent_ppo2.py:185][0m |           0.0016 |          79.2676 |           8.6223 |
[32m[20221214 00:08:03 @agent_ppo2.py:185][0m |          -0.0032 |          76.4676 |           8.5648 |
[32m[20221214 00:08:04 @agent_ppo2.py:185][0m |          -0.0089 |          75.5764 |           8.5227 |
[32m[20221214 00:08:04 @agent_ppo2.py:185][0m |          -0.0075 |          75.2389 |           8.5171 |
[32m[20221214 00:08:04 @agent_ppo2.py:185][0m |          -0.0128 |          74.8710 |           8.4973 |
[32m[20221214 00:08:04 @agent_ppo2.py:185][0m |           0.0039 |          83.7443 |           8.5469 |
[32m[20221214 00:08:04 @agent_ppo2.py:185][0m |          -0.0030 |          77.6389 |           8.5346 |
[32m[20221214 00:08:04 @agent_ppo2.py:185][0m |          -0.0123 |          74.0934 |           8.5237 |
[32m[20221214 00:08:04 @agent_ppo2.py:185][0m |          -0.0152 |          73.8468 |           8.5407 |
[32m[20221214 00:08:04 @agent_ppo2.py:185][0m |          -0.0128 |          73.4102 |           8.5397 |
[32m[20221214 00:08:04 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:08:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 505.62
[32m[20221214 00:08:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 549.83
[32m[20221214 00:08:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 518.26
[32m[20221214 00:08:04 @agent_ppo2.py:143][0m Total time:      10.57 min
[32m[20221214 00:08:04 @agent_ppo2.py:145][0m 974848 total steps have happened
[32m[20221214 00:08:04 @agent_ppo2.py:121][0m #------------------------ Iteration 4476 --------------------------#
[32m[20221214 00:08:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:08:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:08:05 @agent_ppo2.py:185][0m |           0.0018 |          54.3437 |           8.4833 |
[32m[20221214 00:08:05 @agent_ppo2.py:185][0m |           0.0025 |          49.5291 |           8.5216 |
[32m[20221214 00:08:05 @agent_ppo2.py:185][0m |          -0.0006 |          48.7314 |           8.5017 |
[32m[20221214 00:08:05 @agent_ppo2.py:185][0m |          -0.0108 |          47.1137 |           8.5004 |
[32m[20221214 00:08:05 @agent_ppo2.py:185][0m |          -0.0133 |          46.7984 |           8.5048 |
[32m[20221214 00:08:05 @agent_ppo2.py:185][0m |          -0.0092 |          46.2260 |           8.5057 |
[32m[20221214 00:08:05 @agent_ppo2.py:185][0m |          -0.0033 |          48.4225 |           8.4869 |
[32m[20221214 00:08:05 @agent_ppo2.py:185][0m |          -0.0146 |          45.3061 |           8.4684 |
[32m[20221214 00:08:05 @agent_ppo2.py:185][0m |          -0.0175 |          45.0529 |           8.4890 |
[32m[20221214 00:08:05 @agent_ppo2.py:185][0m |          -0.0185 |          44.6421 |           8.4372 |
[32m[20221214 00:08:05 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:08:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 510.15
[32m[20221214 00:08:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 574.37
[32m[20221214 00:08:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 576.80
[32m[20221214 00:08:06 @agent_ppo2.py:143][0m Total time:      10.59 min
[32m[20221214 00:08:06 @agent_ppo2.py:145][0m 976896 total steps have happened
[32m[20221214 00:08:06 @agent_ppo2.py:121][0m #------------------------ Iteration 4477 --------------------------#
[32m[20221214 00:08:06 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:08:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:08:06 @agent_ppo2.py:185][0m |           0.0148 |          50.4563 |           8.1543 |
[32m[20221214 00:08:06 @agent_ppo2.py:185][0m |          -0.0058 |          42.0403 |           8.1901 |
[32m[20221214 00:08:06 @agent_ppo2.py:185][0m |          -0.0093 |          40.6703 |           8.1550 |
[32m[20221214 00:08:06 @agent_ppo2.py:185][0m |          -0.0150 |          40.6486 |           8.1084 |
[32m[20221214 00:08:06 @agent_ppo2.py:185][0m |          -0.0087 |          39.7966 |           8.1182 |
[32m[20221214 00:08:06 @agent_ppo2.py:185][0m |          -0.0178 |          39.2683 |           8.1142 |
[32m[20221214 00:08:06 @agent_ppo2.py:185][0m |          -0.0151 |          39.0088 |           8.1465 |
[32m[20221214 00:08:07 @agent_ppo2.py:185][0m |          -0.0159 |          38.8468 |           8.1261 |
[32m[20221214 00:08:07 @agent_ppo2.py:185][0m |          -0.0199 |          38.4905 |           8.1073 |
[32m[20221214 00:08:07 @agent_ppo2.py:185][0m |          -0.0250 |          38.3165 |           8.1484 |
[32m[20221214 00:08:07 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:08:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 446.85
[32m[20221214 00:08:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 516.78
[32m[20221214 00:08:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 463.77
[32m[20221214 00:08:07 @agent_ppo2.py:143][0m Total time:      10.61 min
[32m[20221214 00:08:07 @agent_ppo2.py:145][0m 978944 total steps have happened
[32m[20221214 00:08:07 @agent_ppo2.py:121][0m #------------------------ Iteration 4478 --------------------------#
[32m[20221214 00:08:07 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:08:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:08:07 @agent_ppo2.py:185][0m |           0.0006 |          76.1673 |           8.1561 |
[32m[20221214 00:08:07 @agent_ppo2.py:185][0m |          -0.0017 |          72.9859 |           8.2039 |
[32m[20221214 00:08:07 @agent_ppo2.py:185][0m |          -0.0049 |          71.0805 |           8.1577 |
[32m[20221214 00:08:07 @agent_ppo2.py:185][0m |          -0.0105 |          70.1787 |           8.1704 |
[32m[20221214 00:08:08 @agent_ppo2.py:185][0m |          -0.0115 |          69.3581 |           8.2009 |
[32m[20221214 00:08:08 @agent_ppo2.py:185][0m |          -0.0047 |          72.0634 |           8.1402 |
[32m[20221214 00:08:08 @agent_ppo2.py:185][0m |          -0.0136 |          68.7258 |           8.2111 |
[32m[20221214 00:08:08 @agent_ppo2.py:185][0m |          -0.0155 |          68.3654 |           8.2532 |
[32m[20221214 00:08:08 @agent_ppo2.py:185][0m |          -0.0167 |          68.1163 |           8.2359 |
[32m[20221214 00:08:08 @agent_ppo2.py:185][0m |          -0.0153 |          67.8811 |           8.2445 |
[32m[20221214 00:08:08 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:08:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 486.27
[32m[20221214 00:08:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 551.72
[32m[20221214 00:08:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 535.23
[32m[20221214 00:08:08 @agent_ppo2.py:143][0m Total time:      10.63 min
[32m[20221214 00:08:08 @agent_ppo2.py:145][0m 980992 total steps have happened
[32m[20221214 00:08:08 @agent_ppo2.py:121][0m #------------------------ Iteration 4479 --------------------------#
[32m[20221214 00:08:08 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:08:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:08:09 @agent_ppo2.py:185][0m |          -0.0012 |          50.6293 |           7.8625 |
[32m[20221214 00:08:09 @agent_ppo2.py:185][0m |          -0.0096 |          44.3553 |           7.9118 |
[32m[20221214 00:08:09 @agent_ppo2.py:185][0m |          -0.0107 |          41.6413 |           7.9657 |
[32m[20221214 00:08:09 @agent_ppo2.py:185][0m |          -0.0112 |          40.1858 |           7.9767 |
[32m[20221214 00:08:09 @agent_ppo2.py:185][0m |          -0.0079 |          38.8464 |           8.0255 |
[32m[20221214 00:08:09 @agent_ppo2.py:185][0m |          -0.0177 |          37.6689 |           8.0194 |
[32m[20221214 00:08:09 @agent_ppo2.py:185][0m |          -0.0158 |          36.8629 |           8.0553 |
[32m[20221214 00:08:09 @agent_ppo2.py:185][0m |          -0.0160 |          36.3491 |           8.0660 |
[32m[20221214 00:08:09 @agent_ppo2.py:185][0m |          -0.0121 |          36.5487 |           8.1302 |
[32m[20221214 00:08:09 @agent_ppo2.py:185][0m |          -0.0161 |          36.1088 |           8.1572 |
[32m[20221214 00:08:09 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:08:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 508.44
[32m[20221214 00:08:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 555.29
[32m[20221214 00:08:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 487.61
[32m[20221214 00:08:10 @agent_ppo2.py:143][0m Total time:      10.65 min
[32m[20221214 00:08:10 @agent_ppo2.py:145][0m 983040 total steps have happened
[32m[20221214 00:08:10 @agent_ppo2.py:121][0m #------------------------ Iteration 4480 --------------------------#
[32m[20221214 00:08:10 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:08:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:08:10 @agent_ppo2.py:185][0m |          -0.0016 |          73.5522 |           8.8252 |
[32m[20221214 00:08:10 @agent_ppo2.py:185][0m |           0.0091 |          76.6763 |           8.8342 |
[32m[20221214 00:08:10 @agent_ppo2.py:185][0m |          -0.0054 |          62.6585 |           8.8568 |
[32m[20221214 00:08:10 @agent_ppo2.py:185][0m |          -0.0099 |          59.9437 |           8.8855 |
[32m[20221214 00:08:10 @agent_ppo2.py:185][0m |          -0.0013 |          61.4011 |           8.8809 |
[32m[20221214 00:08:10 @agent_ppo2.py:185][0m |          -0.0058 |          58.1864 |           8.9075 |
[32m[20221214 00:08:10 @agent_ppo2.py:185][0m |          -0.0135 |          57.4620 |           8.8756 |
[32m[20221214 00:08:10 @agent_ppo2.py:185][0m |          -0.0107 |          56.9379 |           8.8704 |
[32m[20221214 00:08:11 @agent_ppo2.py:185][0m |          -0.0095 |          57.7493 |           8.9244 |
[32m[20221214 00:08:11 @agent_ppo2.py:185][0m |          -0.0148 |          55.7621 |           8.9257 |
[32m[20221214 00:08:11 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:08:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 464.65
[32m[20221214 00:08:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 486.30
[32m[20221214 00:08:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 534.83
[32m[20221214 00:08:11 @agent_ppo2.py:143][0m Total time:      10.67 min
[32m[20221214 00:08:11 @agent_ppo2.py:145][0m 985088 total steps have happened
[32m[20221214 00:08:11 @agent_ppo2.py:121][0m #------------------------ Iteration 4481 --------------------------#
[32m[20221214 00:08:11 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:08:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:08:11 @agent_ppo2.py:185][0m |          -0.0049 |          66.3571 |           8.4691 |
[32m[20221214 00:08:11 @agent_ppo2.py:185][0m |          -0.0081 |          61.7153 |           8.4540 |
[32m[20221214 00:08:11 @agent_ppo2.py:185][0m |          -0.0127 |          60.3218 |           8.4385 |
[32m[20221214 00:08:11 @agent_ppo2.py:185][0m |          -0.0134 |          59.6526 |           8.4786 |
[32m[20221214 00:08:11 @agent_ppo2.py:185][0m |          -0.0195 |          58.9711 |           8.4486 |
[32m[20221214 00:08:12 @agent_ppo2.py:185][0m |          -0.0173 |          58.4453 |           8.4503 |
[32m[20221214 00:08:12 @agent_ppo2.py:185][0m |          -0.0090 |          58.2432 |           8.5108 |
[32m[20221214 00:08:12 @agent_ppo2.py:185][0m |          -0.0171 |          57.6413 |           8.5202 |
[32m[20221214 00:08:12 @agent_ppo2.py:185][0m |          -0.0183 |          57.1813 |           8.4671 |
[32m[20221214 00:08:12 @agent_ppo2.py:185][0m |          -0.0216 |          57.0295 |           8.4822 |
[32m[20221214 00:08:12 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:08:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 451.13
[32m[20221214 00:08:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 490.22
[32m[20221214 00:08:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 537.87
[32m[20221214 00:08:12 @agent_ppo2.py:143][0m Total time:      10.70 min
[32m[20221214 00:08:12 @agent_ppo2.py:145][0m 987136 total steps have happened
[32m[20221214 00:08:12 @agent_ppo2.py:121][0m #------------------------ Iteration 4482 --------------------------#
[32m[20221214 00:08:12 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:08:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:08:12 @agent_ppo2.py:185][0m |          -0.0021 |          62.7917 |           8.6673 |
[32m[20221214 00:08:13 @agent_ppo2.py:185][0m |           0.0011 |          62.1339 |           8.7244 |
[32m[20221214 00:08:13 @agent_ppo2.py:185][0m |          -0.0098 |          59.0210 |           8.7102 |
[32m[20221214 00:08:13 @agent_ppo2.py:185][0m |          -0.0091 |          58.8599 |           8.7617 |
[32m[20221214 00:08:13 @agent_ppo2.py:185][0m |          -0.0011 |          65.9814 |           8.6947 |
[32m[20221214 00:08:13 @agent_ppo2.py:185][0m |          -0.0140 |          58.2547 |           8.7723 |
[32m[20221214 00:08:13 @agent_ppo2.py:185][0m |          -0.0182 |          57.4984 |           8.7438 |
[32m[20221214 00:08:13 @agent_ppo2.py:185][0m |          -0.0194 |          57.3179 |           8.7850 |
[32m[20221214 00:08:13 @agent_ppo2.py:185][0m |          -0.0163 |          57.1717 |           8.7617 |
[32m[20221214 00:08:13 @agent_ppo2.py:185][0m |          -0.0137 |          57.5032 |           8.7691 |
[32m[20221214 00:08:13 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:08:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 458.82
[32m[20221214 00:08:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 504.58
[32m[20221214 00:08:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 474.25
[32m[20221214 00:08:13 @agent_ppo2.py:143][0m Total time:      10.72 min
[32m[20221214 00:08:13 @agent_ppo2.py:145][0m 989184 total steps have happened
[32m[20221214 00:08:13 @agent_ppo2.py:121][0m #------------------------ Iteration 4483 --------------------------#
[32m[20221214 00:08:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:08:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:08:14 @agent_ppo2.py:185][0m |           0.0006 |          72.9753 |           8.0806 |
[32m[20221214 00:08:14 @agent_ppo2.py:185][0m |          -0.0066 |          70.5062 |           8.0707 |
[32m[20221214 00:08:14 @agent_ppo2.py:185][0m |          -0.0116 |          69.7767 |           7.9942 |
[32m[20221214 00:08:14 @agent_ppo2.py:185][0m |          -0.0095 |          69.4087 |           8.0212 |
[32m[20221214 00:08:14 @agent_ppo2.py:185][0m |          -0.0061 |          70.2876 |           8.0620 |
[32m[20221214 00:08:14 @agent_ppo2.py:185][0m |          -0.0119 |          68.6269 |           8.0362 |
[32m[20221214 00:08:14 @agent_ppo2.py:185][0m |          -0.0127 |          68.4234 |           7.9685 |
[32m[20221214 00:08:14 @agent_ppo2.py:185][0m |          -0.0190 |          68.2280 |           7.9852 |
[32m[20221214 00:08:14 @agent_ppo2.py:185][0m |          -0.0177 |          68.1334 |           7.9876 |
[32m[20221214 00:08:15 @agent_ppo2.py:185][0m |          -0.0181 |          67.9223 |           8.0012 |
[32m[20221214 00:08:15 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:08:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 453.96
[32m[20221214 00:08:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 495.00
[32m[20221214 00:08:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 566.41
[32m[20221214 00:08:15 @agent_ppo2.py:143][0m Total time:      10.74 min
[32m[20221214 00:08:15 @agent_ppo2.py:145][0m 991232 total steps have happened
[32m[20221214 00:08:15 @agent_ppo2.py:121][0m #------------------------ Iteration 4484 --------------------------#
[32m[20221214 00:08:15 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:08:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:08:15 @agent_ppo2.py:185][0m |           0.0183 |          69.2307 |           8.3430 |
[32m[20221214 00:08:15 @agent_ppo2.py:185][0m |          -0.0074 |          58.5859 |           8.3672 |
[32m[20221214 00:08:15 @agent_ppo2.py:185][0m |          -0.0083 |          56.9528 |           8.4229 |
[32m[20221214 00:08:15 @agent_ppo2.py:185][0m |          -0.0129 |          55.7344 |           8.4525 |
[32m[20221214 00:08:15 @agent_ppo2.py:185][0m |          -0.0071 |          55.0548 |           8.4679 |
[32m[20221214 00:08:16 @agent_ppo2.py:185][0m |          -0.0112 |          54.3400 |           8.4716 |
[32m[20221214 00:08:16 @agent_ppo2.py:185][0m |          -0.0013 |          60.9711 |           8.5135 |
[32m[20221214 00:08:16 @agent_ppo2.py:185][0m |          -0.0154 |          53.1345 |           8.5053 |
[32m[20221214 00:08:16 @agent_ppo2.py:185][0m |          -0.0159 |          52.7232 |           8.5266 |
[32m[20221214 00:08:16 @agent_ppo2.py:185][0m |          -0.0194 |          52.4521 |           8.5477 |
[32m[20221214 00:08:16 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:08:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 403.11
[32m[20221214 00:08:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 456.37
[32m[20221214 00:08:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 500.03
[32m[20221214 00:08:16 @agent_ppo2.py:143][0m Total time:      10.76 min
[32m[20221214 00:08:16 @agent_ppo2.py:145][0m 993280 total steps have happened
[32m[20221214 00:08:16 @agent_ppo2.py:121][0m #------------------------ Iteration 4485 --------------------------#
[32m[20221214 00:08:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:08:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:08:16 @agent_ppo2.py:185][0m |          -0.0013 |          70.5497 |           9.1934 |
[32m[20221214 00:08:16 @agent_ppo2.py:185][0m |          -0.0012 |          67.6252 |           9.2567 |
[32m[20221214 00:08:17 @agent_ppo2.py:185][0m |          -0.0098 |          65.1075 |           9.2621 |
[32m[20221214 00:08:17 @agent_ppo2.py:185][0m |          -0.0099 |          64.4931 |           9.2592 |
[32m[20221214 00:08:17 @agent_ppo2.py:185][0m |          -0.0102 |          63.6414 |           9.2718 |
[32m[20221214 00:08:17 @agent_ppo2.py:185][0m |          -0.0076 |          65.3221 |           9.2886 |
[32m[20221214 00:08:17 @agent_ppo2.py:185][0m |          -0.0126 |          62.6105 |           9.3008 |
[32m[20221214 00:08:17 @agent_ppo2.py:185][0m |          -0.0134 |          62.1513 |           9.3228 |
[32m[20221214 00:08:17 @agent_ppo2.py:185][0m |          -0.0168 |          61.7203 |           9.3518 |
[32m[20221214 00:08:17 @agent_ppo2.py:185][0m |          -0.0169 |          61.2447 |           9.3261 |
[32m[20221214 00:08:17 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:08:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 413.48
[32m[20221214 00:08:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 476.30
[32m[20221214 00:08:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 530.38
[32m[20221214 00:08:17 @agent_ppo2.py:143][0m Total time:      10.78 min
[32m[20221214 00:08:17 @agent_ppo2.py:145][0m 995328 total steps have happened
[32m[20221214 00:08:17 @agent_ppo2.py:121][0m #------------------------ Iteration 4486 --------------------------#
[32m[20221214 00:08:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:08:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:08:18 @agent_ppo2.py:185][0m |          -0.0002 |          62.0719 |           8.6367 |
[32m[20221214 00:08:18 @agent_ppo2.py:185][0m |          -0.0095 |          54.3685 |           8.6322 |
[32m[20221214 00:08:18 @agent_ppo2.py:185][0m |          -0.0088 |          52.3557 |           8.6888 |
[32m[20221214 00:08:18 @agent_ppo2.py:185][0m |          -0.0132 |          50.8813 |           8.6983 |
[32m[20221214 00:08:18 @agent_ppo2.py:185][0m |          -0.0159 |          49.8936 |           8.6953 |
[32m[20221214 00:08:18 @agent_ppo2.py:185][0m |          -0.0170 |          49.3790 |           8.6726 |
[32m[20221214 00:08:18 @agent_ppo2.py:185][0m |          -0.0130 |          48.9320 |           8.6736 |
[32m[20221214 00:08:18 @agent_ppo2.py:185][0m |          -0.0173 |          48.4193 |           8.6517 |
[32m[20221214 00:08:18 @agent_ppo2.py:185][0m |          -0.0218 |          47.8497 |           8.6914 |
[32m[20221214 00:08:18 @agent_ppo2.py:185][0m |          -0.0101 |          51.0421 |           8.6893 |
[32m[20221214 00:08:18 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:08:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 482.98
[32m[20221214 00:08:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 534.69
[32m[20221214 00:08:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 460.18
[32m[20221214 00:08:19 @agent_ppo2.py:143][0m Total time:      10.80 min
[32m[20221214 00:08:19 @agent_ppo2.py:145][0m 997376 total steps have happened
[32m[20221214 00:08:19 @agent_ppo2.py:121][0m #------------------------ Iteration 4487 --------------------------#
[32m[20221214 00:08:19 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:08:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:08:19 @agent_ppo2.py:185][0m |          -0.0006 |          64.5350 |           8.8733 |
[32m[20221214 00:08:19 @agent_ppo2.py:185][0m |          -0.0059 |          54.8242 |           8.9005 |
[32m[20221214 00:08:19 @agent_ppo2.py:185][0m |          -0.0014 |          52.4916 |           8.8638 |
[32m[20221214 00:08:19 @agent_ppo2.py:185][0m |          -0.0141 |          47.6003 |           8.9391 |
[32m[20221214 00:08:19 @agent_ppo2.py:185][0m |          -0.0113 |          45.9659 |           8.9242 |
[32m[20221214 00:08:19 @agent_ppo2.py:185][0m |          -0.0133 |          44.8055 |           8.9730 |
[32m[20221214 00:08:19 @agent_ppo2.py:185][0m |          -0.0080 |          45.7914 |           9.0036 |
[32m[20221214 00:08:20 @agent_ppo2.py:185][0m |          -0.0178 |          42.7342 |           9.0056 |
[32m[20221214 00:08:20 @agent_ppo2.py:185][0m |          -0.0064 |          44.9854 |           9.0283 |
[32m[20221214 00:08:20 @agent_ppo2.py:185][0m |          -0.0134 |          41.5881 |           9.0510 |
[32m[20221214 00:08:20 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:08:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 434.14
[32m[20221214 00:08:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 464.02
[32m[20221214 00:08:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 502.68
[32m[20221214 00:08:20 @agent_ppo2.py:143][0m Total time:      10.82 min
[32m[20221214 00:08:20 @agent_ppo2.py:145][0m 999424 total steps have happened
[32m[20221214 00:08:20 @agent_ppo2.py:121][0m #------------------------ Iteration 4488 --------------------------#
[32m[20221214 00:08:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:08:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:08:20 @agent_ppo2.py:185][0m |           0.0010 |          82.3028 |           9.1196 |
[32m[20221214 00:08:20 @agent_ppo2.py:185][0m |          -0.0000 |          75.6712 |           9.1353 |
[32m[20221214 00:08:20 @agent_ppo2.py:185][0m |          -0.0078 |          73.2226 |           9.1322 |
[32m[20221214 00:08:20 @agent_ppo2.py:185][0m |          -0.0096 |          72.0270 |           9.0973 |
[32m[20221214 00:08:21 @agent_ppo2.py:185][0m |          -0.0140 |          71.5619 |           9.1815 |
[32m[20221214 00:08:21 @agent_ppo2.py:185][0m |          -0.0123 |          70.9291 |           9.1444 |
[32m[20221214 00:08:21 @agent_ppo2.py:185][0m |          -0.0075 |          72.9892 |           9.1311 |
[32m[20221214 00:08:21 @agent_ppo2.py:185][0m |          -0.0129 |          69.9580 |           9.0978 |
[32m[20221214 00:08:21 @agent_ppo2.py:185][0m |          -0.0119 |          70.0276 |           9.0767 |
[32m[20221214 00:08:21 @agent_ppo2.py:185][0m |          -0.0129 |          70.4989 |           9.1480 |
[32m[20221214 00:08:21 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:08:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 454.58
[32m[20221214 00:08:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 470.18
[32m[20221214 00:08:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 527.00
[32m[20221214 00:08:21 @agent_ppo2.py:143][0m Total time:      10.85 min
[32m[20221214 00:08:21 @agent_ppo2.py:145][0m 1001472 total steps have happened
[32m[20221214 00:08:21 @agent_ppo2.py:121][0m #------------------------ Iteration 4489 --------------------------#
[32m[20221214 00:08:21 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:08:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:08:21 @agent_ppo2.py:185][0m |           0.0012 |          80.4301 |           9.0992 |
[32m[20221214 00:08:22 @agent_ppo2.py:185][0m |           0.0076 |          83.5311 |           9.1420 |
[32m[20221214 00:08:22 @agent_ppo2.py:185][0m |          -0.0045 |          75.0279 |           9.1445 |
[32m[20221214 00:08:22 @agent_ppo2.py:185][0m |          -0.0072 |          74.1968 |           9.1877 |
[32m[20221214 00:08:22 @agent_ppo2.py:185][0m |          -0.0141 |          73.7868 |           9.1671 |
[32m[20221214 00:08:22 @agent_ppo2.py:185][0m |          -0.0103 |          73.6272 |           9.2031 |
[32m[20221214 00:08:22 @agent_ppo2.py:185][0m |          -0.0112 |          73.3107 |           9.1977 |
[32m[20221214 00:08:22 @agent_ppo2.py:185][0m |          -0.0121 |          73.2177 |           9.2412 |
[32m[20221214 00:08:22 @agent_ppo2.py:185][0m |          -0.0094 |          72.8050 |           9.1984 |
[32m[20221214 00:08:22 @agent_ppo2.py:185][0m |          -0.0127 |          73.2644 |           9.2901 |
[32m[20221214 00:08:22 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:08:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 491.19
[32m[20221214 00:08:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 505.50
[32m[20221214 00:08:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 513.81
[32m[20221214 00:08:22 @agent_ppo2.py:143][0m Total time:      10.87 min
[32m[20221214 00:08:22 @agent_ppo2.py:145][0m 1003520 total steps have happened
[32m[20221214 00:08:22 @agent_ppo2.py:121][0m #------------------------ Iteration 4490 --------------------------#
[32m[20221214 00:08:23 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:08:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:08:23 @agent_ppo2.py:185][0m |          -0.0005 |          77.8283 |           9.1576 |
[32m[20221214 00:08:23 @agent_ppo2.py:185][0m |          -0.0044 |          70.0408 |           9.2265 |
[32m[20221214 00:08:23 @agent_ppo2.py:185][0m |          -0.0064 |          66.7158 |           9.2022 |
[32m[20221214 00:08:23 @agent_ppo2.py:185][0m |          -0.0075 |          65.9522 |           9.2973 |
[32m[20221214 00:08:23 @agent_ppo2.py:185][0m |          -0.0110 |          64.2172 |           9.2768 |
[32m[20221214 00:08:23 @agent_ppo2.py:185][0m |          -0.0127 |          63.1350 |           9.3258 |
[32m[20221214 00:08:23 @agent_ppo2.py:185][0m |          -0.0146 |          62.1398 |           9.3496 |
[32m[20221214 00:08:23 @agent_ppo2.py:185][0m |          -0.0169 |          61.4289 |           9.3473 |
[32m[20221214 00:08:24 @agent_ppo2.py:185][0m |          -0.0124 |          60.7434 |           9.4000 |
[32m[20221214 00:08:24 @agent_ppo2.py:185][0m |          -0.0139 |          60.2138 |           9.4411 |
[32m[20221214 00:08:24 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:08:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 463.22
[32m[20221214 00:08:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 564.82
[32m[20221214 00:08:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 453.75
[32m[20221214 00:08:24 @agent_ppo2.py:143][0m Total time:      10.89 min
[32m[20221214 00:08:24 @agent_ppo2.py:145][0m 1005568 total steps have happened
[32m[20221214 00:08:24 @agent_ppo2.py:121][0m #------------------------ Iteration 4491 --------------------------#
[32m[20221214 00:08:24 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221214 00:08:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:08:24 @agent_ppo2.py:185][0m |           0.0021 |          53.7688 |           9.3030 |
[32m[20221214 00:08:24 @agent_ppo2.py:185][0m |          -0.0006 |          42.5684 |           9.3859 |
[32m[20221214 00:08:24 @agent_ppo2.py:185][0m |           0.0042 |          46.3413 |           9.4017 |
[32m[20221214 00:08:24 @agent_ppo2.py:185][0m |          -0.0095 |          40.7060 |           9.4812 |
[32m[20221214 00:08:25 @agent_ppo2.py:185][0m |           0.0028 |          41.5296 |           9.4704 |
[32m[20221214 00:08:25 @agent_ppo2.py:185][0m |          -0.0092 |          39.8435 |           9.5528 |
[32m[20221214 00:08:25 @agent_ppo2.py:185][0m |          -0.0124 |          39.5503 |           9.5945 |
[32m[20221214 00:08:25 @agent_ppo2.py:185][0m |          -0.0115 |          39.4058 |           9.6187 |
[32m[20221214 00:08:25 @agent_ppo2.py:185][0m |          -0.0147 |          39.1391 |           9.6544 |
[32m[20221214 00:08:25 @agent_ppo2.py:185][0m |          -0.0127 |          39.0223 |           9.6747 |
[32m[20221214 00:08:25 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:08:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 517.50
[32m[20221214 00:08:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 531.19
[32m[20221214 00:08:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 331.91
[32m[20221214 00:08:25 @agent_ppo2.py:143][0m Total time:      10.91 min
[32m[20221214 00:08:25 @agent_ppo2.py:145][0m 1007616 total steps have happened
[32m[20221214 00:08:25 @agent_ppo2.py:121][0m #------------------------ Iteration 4492 --------------------------#
[32m[20221214 00:08:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:08:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:08:25 @agent_ppo2.py:185][0m |          -0.0028 |          65.8170 |          10.1104 |
[32m[20221214 00:08:26 @agent_ppo2.py:185][0m |          -0.0104 |          60.3309 |          10.1012 |
[32m[20221214 00:08:26 @agent_ppo2.py:185][0m |          -0.0132 |          58.9408 |          10.0923 |
[32m[20221214 00:08:26 @agent_ppo2.py:185][0m |          -0.0107 |          58.4012 |          10.0815 |
[32m[20221214 00:08:26 @agent_ppo2.py:185][0m |          -0.0195 |          57.8366 |          10.0390 |
[32m[20221214 00:08:26 @agent_ppo2.py:185][0m |          -0.0167 |          57.1953 |          10.1050 |
[32m[20221214 00:08:26 @agent_ppo2.py:185][0m |          -0.0150 |          57.0203 |          10.0625 |
[32m[20221214 00:08:26 @agent_ppo2.py:185][0m |          -0.0194 |          56.9589 |          10.0484 |
[32m[20221214 00:08:26 @agent_ppo2.py:185][0m |          -0.0205 |          56.5858 |          10.0316 |
[32m[20221214 00:08:26 @agent_ppo2.py:185][0m |          -0.0196 |          56.4010 |           9.9979 |
[32m[20221214 00:08:26 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:08:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 415.58
[32m[20221214 00:08:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 458.08
[32m[20221214 00:08:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 528.15
[32m[20221214 00:08:26 @agent_ppo2.py:143][0m Total time:      10.93 min
[32m[20221214 00:08:26 @agent_ppo2.py:145][0m 1009664 total steps have happened
[32m[20221214 00:08:26 @agent_ppo2.py:121][0m #------------------------ Iteration 4493 --------------------------#
[32m[20221214 00:08:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:08:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:08:27 @agent_ppo2.py:185][0m |          -0.0015 |          62.0835 |           9.2519 |
[32m[20221214 00:08:27 @agent_ppo2.py:185][0m |          -0.0017 |          58.2207 |           9.2880 |
[32m[20221214 00:08:27 @agent_ppo2.py:185][0m |          -0.0123 |          56.0586 |           9.2957 |
[32m[20221214 00:08:27 @agent_ppo2.py:185][0m |          -0.0130 |          54.6225 |           9.3355 |
[32m[20221214 00:08:27 @agent_ppo2.py:185][0m |          -0.0127 |          54.3764 |           9.3400 |
[32m[20221214 00:08:27 @agent_ppo2.py:185][0m |          -0.0187 |          53.0261 |           9.3331 |
[32m[20221214 00:08:27 @agent_ppo2.py:185][0m |          -0.0202 |          52.6796 |           9.3198 |
[32m[20221214 00:08:27 @agent_ppo2.py:185][0m |          -0.0173 |          51.7676 |           9.3267 |
[32m[20221214 00:08:28 @agent_ppo2.py:185][0m |          -0.0190 |          51.1792 |           9.3594 |
[32m[20221214 00:08:28 @agent_ppo2.py:185][0m |          -0.0193 |          50.9249 |           9.3567 |
[32m[20221214 00:08:28 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:08:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 390.08
[32m[20221214 00:08:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 423.03
[32m[20221214 00:08:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 420.98
[32m[20221214 00:08:28 @agent_ppo2.py:143][0m Total time:      10.96 min
[32m[20221214 00:08:28 @agent_ppo2.py:145][0m 1011712 total steps have happened
[32m[20221214 00:08:28 @agent_ppo2.py:121][0m #------------------------ Iteration 4494 --------------------------#
[32m[20221214 00:08:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:08:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:08:28 @agent_ppo2.py:185][0m |          -0.0002 |          81.5225 |           9.4678 |
[32m[20221214 00:08:28 @agent_ppo2.py:185][0m |          -0.0078 |          76.4853 |           9.3936 |
[32m[20221214 00:08:28 @agent_ppo2.py:185][0m |          -0.0158 |          74.2065 |           9.4417 |
[32m[20221214 00:08:28 @agent_ppo2.py:185][0m |          -0.0143 |          73.0920 |           9.3882 |
[32m[20221214 00:08:28 @agent_ppo2.py:185][0m |          -0.0124 |          72.7201 |           9.3703 |
[32m[20221214 00:08:29 @agent_ppo2.py:185][0m |          -0.0165 |          71.7017 |           9.4184 |
[32m[20221214 00:08:29 @agent_ppo2.py:185][0m |          -0.0074 |          75.0155 |           9.3565 |
[32m[20221214 00:08:29 @agent_ppo2.py:185][0m |          -0.0197 |          70.9784 |           9.3607 |
[32m[20221214 00:08:29 @agent_ppo2.py:185][0m |          -0.0099 |          71.4429 |           9.3272 |
[32m[20221214 00:08:29 @agent_ppo2.py:185][0m |          -0.0160 |          70.3149 |           9.3100 |
[32m[20221214 00:08:29 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:08:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 505.41
[32m[20221214 00:08:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 529.50
[32m[20221214 00:08:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 502.49
[32m[20221214 00:08:29 @agent_ppo2.py:143][0m Total time:      10.98 min
[32m[20221214 00:08:29 @agent_ppo2.py:145][0m 1013760 total steps have happened
[32m[20221214 00:08:29 @agent_ppo2.py:121][0m #------------------------ Iteration 4495 --------------------------#
[32m[20221214 00:08:29 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:08:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:08:29 @agent_ppo2.py:185][0m |          -0.0004 |          64.6595 |           9.4125 |
[32m[20221214 00:08:29 @agent_ppo2.py:185][0m |          -0.0064 |          60.8009 |           9.4448 |
[32m[20221214 00:08:30 @agent_ppo2.py:185][0m |          -0.0087 |          59.7074 |           9.4521 |
[32m[20221214 00:08:30 @agent_ppo2.py:185][0m |          -0.0117 |          59.0551 |           9.4332 |
[32m[20221214 00:08:30 @agent_ppo2.py:185][0m |          -0.0104 |          58.5073 |           9.4300 |
[32m[20221214 00:08:30 @agent_ppo2.py:185][0m |          -0.0123 |          58.2704 |           9.4181 |
[32m[20221214 00:08:30 @agent_ppo2.py:185][0m |          -0.0076 |          58.3843 |           9.4201 |
[32m[20221214 00:08:30 @agent_ppo2.py:185][0m |          -0.0114 |          58.1547 |           9.3965 |
[32m[20221214 00:08:30 @agent_ppo2.py:185][0m |          -0.0141 |          57.5696 |           9.3662 |
[32m[20221214 00:08:30 @agent_ppo2.py:185][0m |          -0.0147 |          57.3032 |           9.4050 |
[32m[20221214 00:08:30 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:08:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 436.59
[32m[20221214 00:08:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 461.86
[32m[20221214 00:08:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 443.79
[32m[20221214 00:08:30 @agent_ppo2.py:143][0m Total time:      11.00 min
[32m[20221214 00:08:30 @agent_ppo2.py:145][0m 1015808 total steps have happened
[32m[20221214 00:08:30 @agent_ppo2.py:121][0m #------------------------ Iteration 4496 --------------------------#
[32m[20221214 00:08:31 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:08:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:08:31 @agent_ppo2.py:185][0m |          -0.0008 |          63.0292 |           9.3488 |
[32m[20221214 00:08:31 @agent_ppo2.py:185][0m |          -0.0067 |          59.8810 |           9.4164 |
[32m[20221214 00:08:31 @agent_ppo2.py:185][0m |          -0.0112 |          58.5122 |           9.3983 |
[32m[20221214 00:08:31 @agent_ppo2.py:185][0m |          -0.0172 |          58.0543 |           9.4143 |
[32m[20221214 00:08:31 @agent_ppo2.py:185][0m |          -0.0157 |          57.4461 |           9.3977 |
[32m[20221214 00:08:31 @agent_ppo2.py:185][0m |          -0.0210 |          57.2948 |           9.4736 |
[32m[20221214 00:08:31 @agent_ppo2.py:185][0m |          -0.0201 |          56.8348 |           9.4100 |
[32m[20221214 00:08:31 @agent_ppo2.py:185][0m |          -0.0190 |          56.4382 |           9.4273 |
[32m[20221214 00:08:32 @agent_ppo2.py:185][0m |          -0.0223 |          56.1562 |           9.4033 |
[32m[20221214 00:08:32 @agent_ppo2.py:185][0m |          -0.0204 |          56.5780 |           9.4015 |
[32m[20221214 00:08:32 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:08:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 418.73
[32m[20221214 00:08:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 432.79
[32m[20221214 00:08:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 497.62
[32m[20221214 00:08:32 @agent_ppo2.py:143][0m Total time:      11.02 min
[32m[20221214 00:08:32 @agent_ppo2.py:145][0m 1017856 total steps have happened
[32m[20221214 00:08:32 @agent_ppo2.py:121][0m #------------------------ Iteration 4497 --------------------------#
[32m[20221214 00:08:32 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:08:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:08:32 @agent_ppo2.py:185][0m |           0.0013 |          65.7034 |           9.3076 |
[32m[20221214 00:08:32 @agent_ppo2.py:185][0m |          -0.0047 |          62.5558 |           9.2669 |
[32m[20221214 00:08:32 @agent_ppo2.py:185][0m |           0.0090 |          73.1980 |           9.2895 |
[32m[20221214 00:08:32 @agent_ppo2.py:185][0m |          -0.0067 |          61.2429 |           9.2493 |
[32m[20221214 00:08:33 @agent_ppo2.py:185][0m |          -0.0074 |          60.9464 |           9.2079 |
[32m[20221214 00:08:33 @agent_ppo2.py:185][0m |           0.0071 |          67.2637 |           9.2232 |
[32m[20221214 00:08:33 @agent_ppo2.py:185][0m |          -0.0100 |          60.1666 |           9.1449 |
[32m[20221214 00:08:33 @agent_ppo2.py:185][0m |          -0.0136 |          59.8429 |           9.1873 |
[32m[20221214 00:08:33 @agent_ppo2.py:185][0m |          -0.0124 |          59.6657 |           9.1838 |
[32m[20221214 00:08:33 @agent_ppo2.py:185][0m |          -0.0060 |          65.2787 |           9.1678 |
[32m[20221214 00:08:33 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:08:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 487.14
[32m[20221214 00:08:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 501.16
[32m[20221214 00:08:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 509.11
[32m[20221214 00:08:33 @agent_ppo2.py:143][0m Total time:      11.05 min
[32m[20221214 00:08:33 @agent_ppo2.py:145][0m 1019904 total steps have happened
[32m[20221214 00:08:33 @agent_ppo2.py:121][0m #------------------------ Iteration 4498 --------------------------#
[32m[20221214 00:08:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:08:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:08:33 @agent_ppo2.py:185][0m |          -0.0008 |          70.5258 |           9.0088 |
[32m[20221214 00:08:34 @agent_ppo2.py:185][0m |          -0.0039 |          65.6835 |           9.0359 |
[32m[20221214 00:08:34 @agent_ppo2.py:185][0m |          -0.0072 |          62.9545 |           9.0563 |
[32m[20221214 00:08:34 @agent_ppo2.py:185][0m |          -0.0107 |          61.7904 |           9.0034 |
[32m[20221214 00:08:34 @agent_ppo2.py:185][0m |          -0.0133 |          61.1135 |           9.0366 |
[32m[20221214 00:08:34 @agent_ppo2.py:185][0m |          -0.0015 |          68.7055 |           9.0252 |
[32m[20221214 00:08:34 @agent_ppo2.py:185][0m |          -0.0115 |          60.6605 |           9.0309 |
[32m[20221214 00:08:34 @agent_ppo2.py:185][0m |          -0.0128 |          60.3807 |           9.0312 |
[32m[20221214 00:08:34 @agent_ppo2.py:185][0m |          -0.0092 |          61.6890 |           9.0288 |
[32m[20221214 00:08:34 @agent_ppo2.py:185][0m |          -0.0168 |          59.8308 |           9.0288 |
[32m[20221214 00:08:34 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:08:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 459.96
[32m[20221214 00:08:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 496.81
[32m[20221214 00:08:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 469.35
[32m[20221214 00:08:34 @agent_ppo2.py:143][0m Total time:      11.07 min
[32m[20221214 00:08:34 @agent_ppo2.py:145][0m 1021952 total steps have happened
[32m[20221214 00:08:34 @agent_ppo2.py:121][0m #------------------------ Iteration 4499 --------------------------#
[32m[20221214 00:08:35 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:08:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:08:35 @agent_ppo2.py:185][0m |          -0.0025 |          65.9529 |           9.4401 |
[32m[20221214 00:08:35 @agent_ppo2.py:185][0m |          -0.0046 |          61.4696 |           9.4748 |
[32m[20221214 00:08:35 @agent_ppo2.py:185][0m |          -0.0068 |          60.1722 |           9.3909 |
[32m[20221214 00:08:35 @agent_ppo2.py:185][0m |          -0.0062 |          59.0373 |           9.3875 |
[32m[20221214 00:08:35 @agent_ppo2.py:185][0m |          -0.0089 |          58.1481 |           9.3791 |
[32m[20221214 00:08:35 @agent_ppo2.py:185][0m |          -0.0086 |          57.5193 |           9.3993 |
[32m[20221214 00:08:35 @agent_ppo2.py:185][0m |          -0.0039 |          59.2563 |           9.3036 |
[32m[20221214 00:08:35 @agent_ppo2.py:185][0m |          -0.0110 |          56.7078 |           9.3461 |
[32m[20221214 00:08:36 @agent_ppo2.py:185][0m |          -0.0070 |          58.1763 |           9.3319 |
[32m[20221214 00:08:36 @agent_ppo2.py:185][0m |          -0.0071 |          60.6830 |           9.3294 |
[32m[20221214 00:08:36 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:08:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 492.95
[32m[20221214 00:08:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 515.38
[32m[20221214 00:08:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 496.87
[32m[20221214 00:08:36 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Saving the interval checkpoint with rewards 610.54
[32m[20221214 00:08:36 @agent_ppo2.py:143][0m Total time:      11.09 min
[32m[20221214 00:08:36 @agent_ppo2.py:145][0m 1024000 total steps have happened
[32m[20221214 00:08:36 @agent_ppo2.py:121][0m #------------------------ Iteration 4500 --------------------------#
[32m[20221214 00:08:36 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:08:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:08:36 @agent_ppo2.py:185][0m |          -0.0012 |          66.6512 |           8.9559 |
[32m[20221214 00:08:36 @agent_ppo2.py:185][0m |          -0.0046 |          61.6869 |           8.9526 |
[32m[20221214 00:08:36 @agent_ppo2.py:185][0m |          -0.0105 |          60.4553 |           8.9845 |
[32m[20221214 00:08:36 @agent_ppo2.py:185][0m |          -0.0129 |          59.3710 |           9.0084 |
[32m[20221214 00:08:36 @agent_ppo2.py:185][0m |          -0.0148 |          58.6348 |           9.0374 |
[32m[20221214 00:08:37 @agent_ppo2.py:185][0m |          -0.0120 |          59.6786 |           9.0206 |
[32m[20221214 00:08:37 @agent_ppo2.py:185][0m |          -0.0078 |          58.4989 |           9.0372 |
[32m[20221214 00:08:37 @agent_ppo2.py:185][0m |          -0.0178 |          57.4855 |           9.0801 |
[32m[20221214 00:08:37 @agent_ppo2.py:185][0m |          -0.0182 |          57.4703 |           9.0523 |
[32m[20221214 00:08:37 @agent_ppo2.py:185][0m |          -0.0119 |          58.1785 |           9.0565 |
[32m[20221214 00:08:37 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:08:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 404.61
[32m[20221214 00:08:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 445.86
[32m[20221214 00:08:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 371.29
[32m[20221214 00:08:37 @agent_ppo2.py:143][0m Total time:      11.11 min
[32m[20221214 00:08:37 @agent_ppo2.py:145][0m 1026048 total steps have happened
[32m[20221214 00:08:37 @agent_ppo2.py:121][0m #------------------------ Iteration 4501 --------------------------#
[32m[20221214 00:08:37 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:08:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:08:37 @agent_ppo2.py:185][0m |           0.0007 |          87.1852 |           8.9462 |
[32m[20221214 00:08:37 @agent_ppo2.py:185][0m |          -0.0094 |          83.0187 |           8.9178 |
[32m[20221214 00:08:38 @agent_ppo2.py:185][0m |          -0.0092 |          82.2428 |           8.8907 |
[32m[20221214 00:08:38 @agent_ppo2.py:185][0m |          -0.0128 |          81.3692 |           8.8672 |
[32m[20221214 00:08:38 @agent_ppo2.py:185][0m |          -0.0178 |          81.1940 |           8.8817 |
[32m[20221214 00:08:38 @agent_ppo2.py:185][0m |          -0.0172 |          80.5056 |           8.8472 |
[32m[20221214 00:08:38 @agent_ppo2.py:185][0m |          -0.0137 |          80.3788 |           8.8020 |
[32m[20221214 00:08:38 @agent_ppo2.py:185][0m |          -0.0162 |          80.4222 |           8.8206 |
[32m[20221214 00:08:38 @agent_ppo2.py:185][0m |          -0.0170 |          79.5849 |           8.7775 |
[32m[20221214 00:08:38 @agent_ppo2.py:185][0m |          -0.0197 |          79.4443 |           8.8323 |
[32m[20221214 00:08:38 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:08:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 498.74
[32m[20221214 00:08:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 518.61
[32m[20221214 00:08:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 513.41
[32m[20221214 00:08:38 @agent_ppo2.py:143][0m Total time:      11.13 min
[32m[20221214 00:08:38 @agent_ppo2.py:145][0m 1028096 total steps have happened
[32m[20221214 00:08:38 @agent_ppo2.py:121][0m #------------------------ Iteration 4502 --------------------------#
[32m[20221214 00:08:38 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:08:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:08:39 @agent_ppo2.py:185][0m |           0.0009 |          52.6611 |           8.2479 |
[32m[20221214 00:08:39 @agent_ppo2.py:185][0m |           0.0003 |          51.8733 |           8.2671 |
[32m[20221214 00:08:39 @agent_ppo2.py:185][0m |          -0.0109 |          47.9514 |           8.3156 |
[32m[20221214 00:08:39 @agent_ppo2.py:185][0m |          -0.0148 |          47.3980 |           8.3542 |
[32m[20221214 00:08:39 @agent_ppo2.py:185][0m |          -0.0178 |          46.8347 |           8.2889 |
[32m[20221214 00:08:39 @agent_ppo2.py:185][0m |          -0.0158 |          46.3796 |           8.3245 |
[32m[20221214 00:08:39 @agent_ppo2.py:185][0m |          -0.0170 |          46.3169 |           8.3267 |
[32m[20221214 00:08:39 @agent_ppo2.py:185][0m |          -0.0216 |          46.0792 |           8.3589 |
[32m[20221214 00:08:39 @agent_ppo2.py:185][0m |          -0.0222 |          45.7219 |           8.3982 |
[32m[20221214 00:08:39 @agent_ppo2.py:185][0m |          -0.0173 |          45.4720 |           8.4176 |
[32m[20221214 00:08:39 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:08:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 442.89
[32m[20221214 00:08:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 465.44
[32m[20221214 00:08:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221214 00:08:40 @agent_ppo2.py:143][0m Total time:      11.15 min
[32m[20221214 00:08:40 @agent_ppo2.py:145][0m 1030144 total steps have happened
[32m[20221214 00:08:40 @agent_ppo2.py:121][0m #------------------------ Iteration 4503 --------------------------#
[32m[20221214 00:08:40 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:08:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:08:40 @agent_ppo2.py:185][0m |           0.0069 |          79.1813 |           9.1096 |
[32m[20221214 00:08:40 @agent_ppo2.py:185][0m |          -0.0080 |          70.7384 |           9.1716 |
[32m[20221214 00:08:40 @agent_ppo2.py:185][0m |          -0.0052 |          69.9986 |           9.1851 |
[32m[20221214 00:08:40 @agent_ppo2.py:185][0m |          -0.0097 |          69.4146 |           9.2290 |
[32m[20221214 00:08:40 @agent_ppo2.py:185][0m |          -0.0107 |          69.0554 |           9.2697 |
[32m[20221214 00:08:40 @agent_ppo2.py:185][0m |          -0.0064 |          69.4474 |           9.2913 |
[32m[20221214 00:08:41 @agent_ppo2.py:185][0m |          -0.0104 |          68.6559 |           9.3113 |
[32m[20221214 00:08:41 @agent_ppo2.py:185][0m |          -0.0126 |          68.8130 |           9.3533 |
[32m[20221214 00:08:41 @agent_ppo2.py:185][0m |          -0.0096 |          68.3499 |           9.3836 |
[32m[20221214 00:08:41 @agent_ppo2.py:185][0m |          -0.0154 |          68.5404 |           9.3948 |
[32m[20221214 00:08:41 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:08:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 503.93
[32m[20221214 00:08:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 529.03
[32m[20221214 00:08:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 460.11
[32m[20221214 00:08:41 @agent_ppo2.py:143][0m Total time:      11.18 min
[32m[20221214 00:08:41 @agent_ppo2.py:145][0m 1032192 total steps have happened
[32m[20221214 00:08:41 @agent_ppo2.py:121][0m #------------------------ Iteration 4504 --------------------------#
[32m[20221214 00:08:41 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:08:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:08:41 @agent_ppo2.py:185][0m |           0.0012 |          53.2010 |           9.1764 |
[32m[20221214 00:08:41 @agent_ppo2.py:185][0m |          -0.0104 |          50.7032 |           9.2522 |
[32m[20221214 00:08:42 @agent_ppo2.py:185][0m |          -0.0130 |          49.6500 |           9.1913 |
[32m[20221214 00:08:42 @agent_ppo2.py:185][0m |          -0.0133 |          49.0076 |           9.1905 |
[32m[20221214 00:08:42 @agent_ppo2.py:185][0m |          -0.0155 |          48.6271 |           9.2124 |
[32m[20221214 00:08:42 @agent_ppo2.py:185][0m |          -0.0166 |          48.1899 |           9.1491 |
[32m[20221214 00:08:42 @agent_ppo2.py:185][0m |          -0.0079 |          48.4316 |           9.1543 |
[32m[20221214 00:08:42 @agent_ppo2.py:185][0m |          -0.0122 |          48.0042 |           9.1516 |
[32m[20221214 00:08:42 @agent_ppo2.py:185][0m |          -0.0164 |          47.1873 |           9.1702 |
[32m[20221214 00:08:42 @agent_ppo2.py:185][0m |          -0.0164 |          46.7819 |           9.1431 |
[32m[20221214 00:08:42 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:08:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 499.56
[32m[20221214 00:08:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 510.42
[32m[20221214 00:08:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 553.75
[32m[20221214 00:08:42 @agent_ppo2.py:143][0m Total time:      11.20 min
[32m[20221214 00:08:42 @agent_ppo2.py:145][0m 1034240 total steps have happened
[32m[20221214 00:08:42 @agent_ppo2.py:121][0m #------------------------ Iteration 4505 --------------------------#
[32m[20221214 00:08:42 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:08:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:08:43 @agent_ppo2.py:185][0m |           0.0027 |          75.8638 |           8.9943 |
[32m[20221214 00:08:43 @agent_ppo2.py:185][0m |          -0.0061 |          71.0871 |           8.9183 |
[32m[20221214 00:08:43 @agent_ppo2.py:185][0m |          -0.0042 |          70.1420 |           9.0067 |
[32m[20221214 00:08:43 @agent_ppo2.py:185][0m |          -0.0095 |          69.3122 |           8.9187 |
[32m[20221214 00:08:43 @agent_ppo2.py:185][0m |          -0.0006 |          71.3150 |           8.8775 |
[32m[20221214 00:08:43 @agent_ppo2.py:185][0m |          -0.0068 |          68.7947 |           8.8889 |
[32m[20221214 00:08:43 @agent_ppo2.py:185][0m |          -0.0126 |          67.9917 |           8.8296 |
[32m[20221214 00:08:43 @agent_ppo2.py:185][0m |          -0.0140 |          67.7261 |           8.7881 |
[32m[20221214 00:08:43 @agent_ppo2.py:185][0m |          -0.0118 |          67.6937 |           8.7587 |
[32m[20221214 00:08:43 @agent_ppo2.py:185][0m |          -0.0138 |          67.4095 |           8.7851 |
[32m[20221214 00:08:43 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:08:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 429.92
[32m[20221214 00:08:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 495.77
[32m[20221214 00:08:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 504.16
[32m[20221214 00:08:44 @agent_ppo2.py:143][0m Total time:      11.22 min
[32m[20221214 00:08:44 @agent_ppo2.py:145][0m 1036288 total steps have happened
[32m[20221214 00:08:44 @agent_ppo2.py:121][0m #------------------------ Iteration 4506 --------------------------#
[32m[20221214 00:08:44 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:08:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:08:44 @agent_ppo2.py:185][0m |           0.0019 |          84.9866 |           8.0518 |
[32m[20221214 00:08:44 @agent_ppo2.py:185][0m |          -0.0054 |          79.5342 |           7.9971 |
[32m[20221214 00:08:44 @agent_ppo2.py:185][0m |          -0.0059 |          78.0804 |           8.0363 |
[32m[20221214 00:08:44 @agent_ppo2.py:185][0m |          -0.0050 |          77.8142 |           7.9959 |
[32m[20221214 00:08:44 @agent_ppo2.py:185][0m |          -0.0098 |          77.1149 |           8.0029 |
[32m[20221214 00:08:44 @agent_ppo2.py:185][0m |          -0.0080 |          77.5207 |           8.0340 |
[32m[20221214 00:08:44 @agent_ppo2.py:185][0m |          -0.0071 |          75.8483 |           7.9788 |
[32m[20221214 00:08:45 @agent_ppo2.py:185][0m |          -0.0122 |          75.2035 |           7.9995 |
[32m[20221214 00:08:45 @agent_ppo2.py:185][0m |          -0.0121 |          74.7289 |           7.9667 |
[32m[20221214 00:08:45 @agent_ppo2.py:185][0m |          -0.0064 |          78.8674 |           7.9591 |
[32m[20221214 00:08:45 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:08:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 504.63
[32m[20221214 00:08:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 548.96
[32m[20221214 00:08:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 483.88
[32m[20221214 00:08:45 @agent_ppo2.py:143][0m Total time:      11.24 min
[32m[20221214 00:08:45 @agent_ppo2.py:145][0m 1038336 total steps have happened
[32m[20221214 00:08:45 @agent_ppo2.py:121][0m #------------------------ Iteration 4507 --------------------------#
[32m[20221214 00:08:45 @agent_ppo2.py:127][0m Sampling time: 0.32 s by 5 slaves
[32m[20221214 00:08:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:08:45 @agent_ppo2.py:185][0m |          -0.0029 |          66.3282 |           8.6290 |
[32m[20221214 00:08:46 @agent_ppo2.py:185][0m |          -0.0045 |          61.2225 |           8.6207 |
[32m[20221214 00:08:46 @agent_ppo2.py:185][0m |          -0.0072 |          58.7403 |           8.6396 |
[32m[20221214 00:08:46 @agent_ppo2.py:185][0m |          -0.0034 |          60.5188 |           8.6489 |
[32m[20221214 00:08:46 @agent_ppo2.py:185][0m |          -0.0123 |          55.9664 |           8.6938 |
[32m[20221214 00:08:46 @agent_ppo2.py:185][0m |          -0.0153 |          55.1299 |           8.7143 |
[32m[20221214 00:08:46 @agent_ppo2.py:185][0m |          -0.0140 |          54.4813 |           8.7448 |
[32m[20221214 00:08:46 @agent_ppo2.py:185][0m |          -0.0145 |          54.0010 |           8.7652 |
[32m[20221214 00:08:46 @agent_ppo2.py:185][0m |          -0.0162 |          53.6643 |           8.7577 |
[32m[20221214 00:08:46 @agent_ppo2.py:185][0m |          -0.0166 |          53.3152 |           8.8047 |
[32m[20221214 00:08:46 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:08:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 477.23
[32m[20221214 00:08:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 529.40
[32m[20221214 00:08:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 444.43
[32m[20221214 00:08:46 @agent_ppo2.py:143][0m Total time:      11.27 min
[32m[20221214 00:08:46 @agent_ppo2.py:145][0m 1040384 total steps have happened
[32m[20221214 00:08:46 @agent_ppo2.py:121][0m #------------------------ Iteration 4508 --------------------------#
[32m[20221214 00:08:47 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:08:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:08:47 @agent_ppo2.py:185][0m |          -0.0004 |          69.4840 |           8.7028 |
[32m[20221214 00:08:47 @agent_ppo2.py:185][0m |           0.0091 |          64.7636 |           8.6768 |
[32m[20221214 00:08:47 @agent_ppo2.py:185][0m |          -0.0090 |          58.9409 |           8.5412 |
[32m[20221214 00:08:47 @agent_ppo2.py:185][0m |          -0.0111 |          57.5280 |           8.5764 |
[32m[20221214 00:08:47 @agent_ppo2.py:185][0m |          -0.0096 |          56.4848 |           8.5038 |
[32m[20221214 00:08:47 @agent_ppo2.py:185][0m |          -0.0097 |          56.1250 |           8.4566 |
[32m[20221214 00:08:47 @agent_ppo2.py:185][0m |          -0.0091 |          55.8686 |           8.4064 |
[32m[20221214 00:08:47 @agent_ppo2.py:185][0m |          -0.0109 |          54.9772 |           8.4354 |
[32m[20221214 00:08:47 @agent_ppo2.py:185][0m |          -0.0135 |          54.7579 |           8.4151 |
[32m[20221214 00:08:48 @agent_ppo2.py:185][0m |          -0.0166 |          54.7277 |           8.3968 |
[32m[20221214 00:08:48 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:08:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 451.14
[32m[20221214 00:08:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 492.46
[32m[20221214 00:08:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 525.43
[32m[20221214 00:08:48 @agent_ppo2.py:143][0m Total time:      11.29 min
[32m[20221214 00:08:48 @agent_ppo2.py:145][0m 1042432 total steps have happened
[32m[20221214 00:08:48 @agent_ppo2.py:121][0m #------------------------ Iteration 4509 --------------------------#
[32m[20221214 00:08:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:08:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:08:48 @agent_ppo2.py:185][0m |          -0.0014 |          75.4308 |           8.9209 |
[32m[20221214 00:08:48 @agent_ppo2.py:185][0m |          -0.0080 |          66.6721 |           8.8462 |
[32m[20221214 00:08:48 @agent_ppo2.py:185][0m |          -0.0109 |          64.6420 |           8.8688 |
[32m[20221214 00:08:48 @agent_ppo2.py:185][0m |          -0.0113 |          63.5764 |           8.8563 |
[32m[20221214 00:08:48 @agent_ppo2.py:185][0m |          -0.0171 |          62.9313 |           8.9195 |
[32m[20221214 00:08:49 @agent_ppo2.py:185][0m |          -0.0103 |          65.4954 |           8.9213 |
[32m[20221214 00:08:49 @agent_ppo2.py:185][0m |          -0.0169 |          62.0005 |           8.8827 |
[32m[20221214 00:08:49 @agent_ppo2.py:185][0m |          -0.0176 |          61.6139 |           8.9092 |
[32m[20221214 00:08:49 @agent_ppo2.py:185][0m |          -0.0179 |          61.0062 |           8.9489 |
[32m[20221214 00:08:49 @agent_ppo2.py:185][0m |          -0.0196 |          61.0046 |           8.8326 |
[32m[20221214 00:08:49 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:08:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 412.84
[32m[20221214 00:08:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 459.85
[32m[20221214 00:08:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 525.27
[32m[20221214 00:08:49 @agent_ppo2.py:143][0m Total time:      11.31 min
[32m[20221214 00:08:49 @agent_ppo2.py:145][0m 1044480 total steps have happened
[32m[20221214 00:08:49 @agent_ppo2.py:121][0m #------------------------ Iteration 4510 --------------------------#
[32m[20221214 00:08:49 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:08:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:08:49 @agent_ppo2.py:185][0m |          -0.0012 |          69.4650 |           8.3071 |
[32m[20221214 00:08:50 @agent_ppo2.py:185][0m |          -0.0067 |          66.9667 |           8.3256 |
[32m[20221214 00:08:50 @agent_ppo2.py:185][0m |          -0.0065 |          66.2686 |           8.3169 |
[32m[20221214 00:08:50 @agent_ppo2.py:185][0m |          -0.0044 |          66.1237 |           8.2935 |
[32m[20221214 00:08:50 @agent_ppo2.py:185][0m |           0.0013 |          72.4211 |           8.3039 |
[32m[20221214 00:08:50 @agent_ppo2.py:185][0m |          -0.0017 |          68.4273 |           8.3430 |
[32m[20221214 00:08:50 @agent_ppo2.py:185][0m |          -0.0101 |          65.0681 |           8.2898 |
[32m[20221214 00:08:50 @agent_ppo2.py:185][0m |          -0.0116 |          64.7763 |           8.2868 |
[32m[20221214 00:08:50 @agent_ppo2.py:185][0m |          -0.0104 |          64.6922 |           8.2991 |
[32m[20221214 00:08:50 @agent_ppo2.py:185][0m |          -0.0112 |          64.4668 |           8.2820 |
[32m[20221214 00:08:50 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:08:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 507.39
[32m[20221214 00:08:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 519.31
[32m[20221214 00:08:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 481.88
[32m[20221214 00:08:50 @agent_ppo2.py:143][0m Total time:      11.33 min
[32m[20221214 00:08:50 @agent_ppo2.py:145][0m 1046528 total steps have happened
[32m[20221214 00:08:50 @agent_ppo2.py:121][0m #------------------------ Iteration 4511 --------------------------#
[32m[20221214 00:08:51 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:08:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:08:51 @agent_ppo2.py:185][0m |          -0.0001 |          62.3169 |           8.7229 |
[32m[20221214 00:08:51 @agent_ppo2.py:185][0m |          -0.0065 |          56.5879 |           8.6910 |
[32m[20221214 00:08:51 @agent_ppo2.py:185][0m |          -0.0090 |          54.1407 |           8.6980 |
[32m[20221214 00:08:51 @agent_ppo2.py:185][0m |          -0.0000 |          54.8209 |           8.6459 |
[32m[20221214 00:08:51 @agent_ppo2.py:185][0m |          -0.0136 |          52.0214 |           8.6736 |
[32m[20221214 00:08:51 @agent_ppo2.py:185][0m |          -0.0141 |          51.1737 |           8.7178 |
[32m[20221214 00:08:51 @agent_ppo2.py:185][0m |          -0.0156 |          50.3457 |           8.7049 |
[32m[20221214 00:08:51 @agent_ppo2.py:185][0m |          -0.0096 |          50.0837 |           8.6971 |
[32m[20221214 00:08:51 @agent_ppo2.py:185][0m |          -0.0150 |          49.3620 |           8.7273 |
[32m[20221214 00:08:52 @agent_ppo2.py:185][0m |          -0.0132 |          48.8394 |           8.7381 |
[32m[20221214 00:08:52 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:08:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 463.37
[32m[20221214 00:08:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 515.10
[32m[20221214 00:08:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 464.06
[32m[20221214 00:08:52 @agent_ppo2.py:143][0m Total time:      11.36 min
[32m[20221214 00:08:52 @agent_ppo2.py:145][0m 1048576 total steps have happened
[32m[20221214 00:08:52 @agent_ppo2.py:121][0m #------------------------ Iteration 4512 --------------------------#
[32m[20221214 00:08:52 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:08:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:08:52 @agent_ppo2.py:185][0m |           0.0012 |          66.2085 |           8.3148 |
[32m[20221214 00:08:52 @agent_ppo2.py:185][0m |          -0.0079 |          51.0607 |           8.3295 |
[32m[20221214 00:08:52 @agent_ppo2.py:185][0m |          -0.0020 |          49.6668 |           8.3194 |
[32m[20221214 00:08:52 @agent_ppo2.py:185][0m |          -0.0055 |          48.6900 |           8.3415 |
[32m[20221214 00:08:52 @agent_ppo2.py:185][0m |          -0.0108 |          48.3554 |           8.2961 |
[32m[20221214 00:08:53 @agent_ppo2.py:185][0m |          -0.0127 |          47.7824 |           8.2331 |
[32m[20221214 00:08:53 @agent_ppo2.py:185][0m |          -0.0118 |          47.3666 |           8.3138 |
[32m[20221214 00:08:53 @agent_ppo2.py:185][0m |          -0.0082 |          47.2539 |           8.2749 |
[32m[20221214 00:08:53 @agent_ppo2.py:185][0m |          -0.0158 |          47.1844 |           8.2752 |
[32m[20221214 00:08:53 @agent_ppo2.py:185][0m |          -0.0179 |          46.8283 |           8.2692 |
[32m[20221214 00:08:53 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:08:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 510.74
[32m[20221214 00:08:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 526.08
[32m[20221214 00:08:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 543.16
[32m[20221214 00:08:53 @agent_ppo2.py:143][0m Total time:      11.38 min
[32m[20221214 00:08:53 @agent_ppo2.py:145][0m 1050624 total steps have happened
[32m[20221214 00:08:53 @agent_ppo2.py:121][0m #------------------------ Iteration 4513 --------------------------#
[32m[20221214 00:08:53 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:08:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:08:53 @agent_ppo2.py:185][0m |          -0.0043 |          49.8307 |           7.7639 |
[32m[20221214 00:08:54 @agent_ppo2.py:185][0m |          -0.0089 |          39.9197 |           7.7835 |
[32m[20221214 00:08:54 @agent_ppo2.py:185][0m |          -0.0119 |          37.8499 |           7.8798 |
[32m[20221214 00:08:54 @agent_ppo2.py:185][0m |          -0.0185 |          36.5804 |           7.8466 |
[32m[20221214 00:08:54 @agent_ppo2.py:185][0m |          -0.0104 |          35.8112 |           7.8394 |
[32m[20221214 00:08:54 @agent_ppo2.py:185][0m |          -0.0180 |          35.2345 |           7.8811 |
[32m[20221214 00:08:54 @agent_ppo2.py:185][0m |          -0.0229 |          34.6681 |           7.9366 |
[32m[20221214 00:08:54 @agent_ppo2.py:185][0m |          -0.0141 |          34.5294 |           7.9692 |
[32m[20221214 00:08:54 @agent_ppo2.py:185][0m |          -0.0126 |          35.4760 |           7.9602 |
[32m[20221214 00:08:54 @agent_ppo2.py:185][0m |          -0.0228 |          33.5771 |           7.9877 |
[32m[20221214 00:08:54 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:08:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 405.06
[32m[20221214 00:08:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 456.46
[32m[20221214 00:08:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 494.58
[32m[20221214 00:08:54 @agent_ppo2.py:143][0m Total time:      11.40 min
[32m[20221214 00:08:54 @agent_ppo2.py:145][0m 1052672 total steps have happened
[32m[20221214 00:08:54 @agent_ppo2.py:121][0m #------------------------ Iteration 4514 --------------------------#
[32m[20221214 00:08:55 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:08:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:08:55 @agent_ppo2.py:185][0m |           0.0061 |          43.7490 |           8.1129 |
[32m[20221214 00:08:55 @agent_ppo2.py:185][0m |          -0.0039 |          37.3578 |           8.1963 |
[32m[20221214 00:08:55 @agent_ppo2.py:185][0m |          -0.0078 |          35.2131 |           8.1645 |
[32m[20221214 00:08:55 @agent_ppo2.py:185][0m |          -0.0125 |          33.9580 |           8.1393 |
[32m[20221214 00:08:55 @agent_ppo2.py:185][0m |          -0.0116 |          33.3635 |           8.2282 |
[32m[20221214 00:08:55 @agent_ppo2.py:185][0m |          -0.0156 |          32.7274 |           8.1825 |
[32m[20221214 00:08:55 @agent_ppo2.py:185][0m |          -0.0152 |          32.0799 |           8.1681 |
[32m[20221214 00:08:55 @agent_ppo2.py:185][0m |          -0.0115 |          31.8009 |           8.2023 |
[32m[20221214 00:08:55 @agent_ppo2.py:185][0m |          -0.0194 |          31.2805 |           8.1772 |
[32m[20221214 00:08:56 @agent_ppo2.py:185][0m |          -0.0194 |          30.9947 |           8.1607 |
[32m[20221214 00:08:56 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:08:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 481.55
[32m[20221214 00:08:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 531.45
[32m[20221214 00:08:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 198.80
[32m[20221214 00:08:56 @agent_ppo2.py:143][0m Total time:      11.42 min
[32m[20221214 00:08:56 @agent_ppo2.py:145][0m 1054720 total steps have happened
[32m[20221214 00:08:56 @agent_ppo2.py:121][0m #------------------------ Iteration 4515 --------------------------#
[32m[20221214 00:08:56 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:08:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:08:56 @agent_ppo2.py:185][0m |           0.0039 |          53.0225 |           7.9311 |
[32m[20221214 00:08:56 @agent_ppo2.py:185][0m |          -0.0054 |          47.1925 |           7.9388 |
[32m[20221214 00:08:56 @agent_ppo2.py:185][0m |          -0.0031 |          45.9579 |           7.9429 |
[32m[20221214 00:08:56 @agent_ppo2.py:185][0m |          -0.0081 |          44.5406 |           7.9057 |
[32m[20221214 00:08:56 @agent_ppo2.py:185][0m |          -0.0181 |          43.9780 |           7.9305 |
[32m[20221214 00:08:57 @agent_ppo2.py:185][0m |          -0.0143 |          43.4980 |           7.9479 |
[32m[20221214 00:08:57 @agent_ppo2.py:185][0m |          -0.0201 |          43.0997 |           7.9214 |
[32m[20221214 00:08:57 @agent_ppo2.py:185][0m |          -0.0147 |          43.2169 |           7.9338 |
[32m[20221214 00:08:57 @agent_ppo2.py:185][0m |          -0.0240 |          42.7097 |           7.8877 |
[32m[20221214 00:08:57 @agent_ppo2.py:185][0m |          -0.0186 |          42.4636 |           7.9311 |
[32m[20221214 00:08:57 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:08:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 427.35
[32m[20221214 00:08:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 497.99
[32m[20221214 00:08:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 457.79
[32m[20221214 00:08:57 @agent_ppo2.py:143][0m Total time:      11.44 min
[32m[20221214 00:08:57 @agent_ppo2.py:145][0m 1056768 total steps have happened
[32m[20221214 00:08:57 @agent_ppo2.py:121][0m #------------------------ Iteration 4516 --------------------------#
[32m[20221214 00:08:57 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:08:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:08:57 @agent_ppo2.py:185][0m |           0.0035 |          53.5970 |           8.6976 |
[32m[20221214 00:08:57 @agent_ppo2.py:185][0m |          -0.0084 |          50.3795 |           8.6916 |
[32m[20221214 00:08:58 @agent_ppo2.py:185][0m |          -0.0117 |          48.9301 |           8.7352 |
[32m[20221214 00:08:58 @agent_ppo2.py:185][0m |          -0.0145 |          47.9434 |           8.7397 |
[32m[20221214 00:08:58 @agent_ppo2.py:185][0m |          -0.0144 |          47.1007 |           8.7915 |
[32m[20221214 00:08:58 @agent_ppo2.py:185][0m |          -0.0170 |          46.5238 |           8.8172 |
[32m[20221214 00:08:58 @agent_ppo2.py:185][0m |          -0.0196 |          46.1211 |           8.8147 |
[32m[20221214 00:08:58 @agent_ppo2.py:185][0m |          -0.0190 |          45.5301 |           8.8259 |
[32m[20221214 00:08:58 @agent_ppo2.py:185][0m |          -0.0170 |          45.1261 |           8.8799 |
[32m[20221214 00:08:58 @agent_ppo2.py:185][0m |          -0.0183 |          44.5177 |           8.8666 |
[32m[20221214 00:08:58 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:08:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 350.17
[32m[20221214 00:08:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 461.54
[32m[20221214 00:08:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 337.92
[32m[20221214 00:08:58 @agent_ppo2.py:143][0m Total time:      11.47 min
[32m[20221214 00:08:58 @agent_ppo2.py:145][0m 1058816 total steps have happened
[32m[20221214 00:08:58 @agent_ppo2.py:121][0m #------------------------ Iteration 4517 --------------------------#
[32m[20221214 00:08:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:08:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:08:59 @agent_ppo2.py:185][0m |           0.0015 |          66.1935 |           8.4690 |
[32m[20221214 00:08:59 @agent_ppo2.py:185][0m |          -0.0104 |          61.7910 |           8.4830 |
[32m[20221214 00:08:59 @agent_ppo2.py:185][0m |          -0.0131 |          60.3958 |           8.5069 |
[32m[20221214 00:08:59 @agent_ppo2.py:185][0m |          -0.0146 |          59.4144 |           8.4998 |
[32m[20221214 00:08:59 @agent_ppo2.py:185][0m |          -0.0171 |          58.8150 |           8.5459 |
[32m[20221214 00:08:59 @agent_ppo2.py:185][0m |          -0.0176 |          57.9549 |           8.5016 |
[32m[20221214 00:08:59 @agent_ppo2.py:185][0m |          -0.0196 |          57.4218 |           8.5007 |
[32m[20221214 00:08:59 @agent_ppo2.py:185][0m |          -0.0212 |          57.1878 |           8.4800 |
[32m[20221214 00:08:59 @agent_ppo2.py:185][0m |          -0.0139 |          56.6974 |           8.4899 |
[32m[20221214 00:09:00 @agent_ppo2.py:185][0m |          -0.0126 |          58.5197 |           8.4504 |
[32m[20221214 00:09:00 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:09:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 402.01
[32m[20221214 00:09:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 475.62
[32m[20221214 00:09:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 608.01
[32m[20221214 00:09:00 @agent_ppo2.py:143][0m Total time:      11.49 min
[32m[20221214 00:09:00 @agent_ppo2.py:145][0m 1060864 total steps have happened
[32m[20221214 00:09:00 @agent_ppo2.py:121][0m #------------------------ Iteration 4518 --------------------------#
[32m[20221214 00:09:00 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:09:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:09:00 @agent_ppo2.py:185][0m |           0.0011 |          46.4129 |           8.3152 |
[32m[20221214 00:09:00 @agent_ppo2.py:185][0m |          -0.0040 |          36.8197 |           8.3705 |
[32m[20221214 00:09:00 @agent_ppo2.py:185][0m |          -0.0077 |          33.2020 |           8.3080 |
[32m[20221214 00:09:00 @agent_ppo2.py:185][0m |          -0.0093 |          31.3706 |           8.3019 |
[32m[20221214 00:09:00 @agent_ppo2.py:185][0m |          -0.0137 |          30.2746 |           8.2512 |
[32m[20221214 00:09:01 @agent_ppo2.py:185][0m |          -0.0058 |          29.8820 |           8.3165 |
[32m[20221214 00:09:01 @agent_ppo2.py:185][0m |          -0.0202 |          28.5787 |           8.3175 |
[32m[20221214 00:09:01 @agent_ppo2.py:185][0m |          -0.0214 |          28.1319 |           8.3050 |
[32m[20221214 00:09:01 @agent_ppo2.py:185][0m |          -0.0139 |          27.3150 |           8.2910 |
[32m[20221214 00:09:01 @agent_ppo2.py:185][0m |          -0.0206 |          26.9061 |           8.2371 |
[32m[20221214 00:09:01 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:09:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 344.57
[32m[20221214 00:09:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 416.89
[32m[20221214 00:09:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 500.53
[32m[20221214 00:09:01 @agent_ppo2.py:143][0m Total time:      11.51 min
[32m[20221214 00:09:01 @agent_ppo2.py:145][0m 1062912 total steps have happened
[32m[20221214 00:09:01 @agent_ppo2.py:121][0m #------------------------ Iteration 4519 --------------------------#
[32m[20221214 00:09:01 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:09:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:09:01 @agent_ppo2.py:185][0m |          -0.0058 |          52.8311 |           8.2935 |
[32m[20221214 00:09:02 @agent_ppo2.py:185][0m |          -0.0114 |          47.8215 |           8.4183 |
[32m[20221214 00:09:02 @agent_ppo2.py:185][0m |          -0.0107 |          45.5972 |           8.4076 |
[32m[20221214 00:09:02 @agent_ppo2.py:185][0m |          -0.0175 |          44.5843 |           8.4379 |
[32m[20221214 00:09:02 @agent_ppo2.py:185][0m |          -0.0158 |          43.8970 |           8.4400 |
[32m[20221214 00:09:02 @agent_ppo2.py:185][0m |          -0.0166 |          43.0050 |           8.4892 |
[32m[20221214 00:09:02 @agent_ppo2.py:185][0m |          -0.0151 |          42.4508 |           8.5162 |
[32m[20221214 00:09:02 @agent_ppo2.py:185][0m |          -0.0216 |          42.1892 |           8.5178 |
[32m[20221214 00:09:02 @agent_ppo2.py:185][0m |          -0.0228 |          41.6072 |           8.4951 |
[32m[20221214 00:09:02 @agent_ppo2.py:185][0m |          -0.0183 |          41.3358 |           8.4784 |
[32m[20221214 00:09:02 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:09:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 405.34
[32m[20221214 00:09:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 486.42
[32m[20221214 00:09:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 352.50
[32m[20221214 00:09:02 @agent_ppo2.py:143][0m Total time:      11.53 min
[32m[20221214 00:09:02 @agent_ppo2.py:145][0m 1064960 total steps have happened
[32m[20221214 00:09:02 @agent_ppo2.py:121][0m #------------------------ Iteration 4520 --------------------------#
[32m[20221214 00:09:03 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:09:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:09:03 @agent_ppo2.py:185][0m |          -0.0005 |          63.5483 |           8.2520 |
[32m[20221214 00:09:03 @agent_ppo2.py:185][0m |          -0.0049 |          58.4897 |           8.2446 |
[32m[20221214 00:09:03 @agent_ppo2.py:185][0m |           0.0023 |          59.5589 |           8.2403 |
[32m[20221214 00:09:03 @agent_ppo2.py:185][0m |          -0.0038 |          55.6452 |           8.2803 |
[32m[20221214 00:09:03 @agent_ppo2.py:185][0m |          -0.0092 |          56.2558 |           8.2960 |
[32m[20221214 00:09:03 @agent_ppo2.py:185][0m |          -0.0094 |          53.9413 |           8.2282 |
[32m[20221214 00:09:03 @agent_ppo2.py:185][0m |          -0.0092 |          53.2582 |           8.2659 |
[32m[20221214 00:09:03 @agent_ppo2.py:185][0m |          -0.0069 |          53.7970 |           8.2597 |
[32m[20221214 00:09:03 @agent_ppo2.py:185][0m |          -0.0126 |          52.1413 |           8.2080 |
[32m[20221214 00:09:04 @agent_ppo2.py:185][0m |          -0.0156 |          51.5982 |           8.1850 |
[32m[20221214 00:09:04 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:09:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 392.76
[32m[20221214 00:09:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 463.89
[32m[20221214 00:09:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 241.46
[32m[20221214 00:09:04 @agent_ppo2.py:143][0m Total time:      11.56 min
[32m[20221214 00:09:04 @agent_ppo2.py:145][0m 1067008 total steps have happened
[32m[20221214 00:09:04 @agent_ppo2.py:121][0m #------------------------ Iteration 4521 --------------------------#
[32m[20221214 00:09:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:09:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:09:04 @agent_ppo2.py:185][0m |          -0.0017 |          55.4005 |           7.7708 |
[32m[20221214 00:09:04 @agent_ppo2.py:185][0m |          -0.0043 |          50.9748 |           7.7712 |
[32m[20221214 00:09:04 @agent_ppo2.py:185][0m |          -0.0021 |          50.9529 |           7.7591 |
[32m[20221214 00:09:04 @agent_ppo2.py:185][0m |          -0.0088 |          48.4903 |           7.8513 |
[32m[20221214 00:09:04 @agent_ppo2.py:185][0m |          -0.0110 |          47.8065 |           7.7829 |
[32m[20221214 00:09:05 @agent_ppo2.py:185][0m |          -0.0150 |          47.2855 |           7.8402 |
[32m[20221214 00:09:05 @agent_ppo2.py:185][0m |          -0.0167 |          47.0517 |           7.8474 |
[32m[20221214 00:09:05 @agent_ppo2.py:185][0m |          -0.0152 |          46.6584 |           7.8515 |
[32m[20221214 00:09:05 @agent_ppo2.py:185][0m |          -0.0156 |          46.4041 |           7.8333 |
[32m[20221214 00:09:05 @agent_ppo2.py:185][0m |          -0.0161 |          45.9417 |           7.8411 |
[32m[20221214 00:09:05 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:09:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 425.37
[32m[20221214 00:09:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 495.56
[32m[20221214 00:09:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 461.81
[32m[20221214 00:09:05 @agent_ppo2.py:143][0m Total time:      11.58 min
[32m[20221214 00:09:05 @agent_ppo2.py:145][0m 1069056 total steps have happened
[32m[20221214 00:09:05 @agent_ppo2.py:121][0m #------------------------ Iteration 4522 --------------------------#
[32m[20221214 00:09:05 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:09:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:09:05 @agent_ppo2.py:185][0m |          -0.0015 |          52.5208 |           8.8658 |
[32m[20221214 00:09:05 @agent_ppo2.py:185][0m |          -0.0028 |          47.3598 |           8.9289 |
[32m[20221214 00:09:06 @agent_ppo2.py:185][0m |          -0.0103 |          45.1692 |           8.9170 |
[32m[20221214 00:09:06 @agent_ppo2.py:185][0m |          -0.0125 |          43.8485 |           8.9536 |
[32m[20221214 00:09:06 @agent_ppo2.py:185][0m |           0.0039 |          51.6324 |           8.9525 |
[32m[20221214 00:09:06 @agent_ppo2.py:185][0m |          -0.0088 |          46.5763 |           8.9987 |
[32m[20221214 00:09:06 @agent_ppo2.py:185][0m |          -0.0168 |          41.7584 |           8.9753 |
[32m[20221214 00:09:06 @agent_ppo2.py:185][0m |          -0.0119 |          40.8021 |           9.0048 |
[32m[20221214 00:09:06 @agent_ppo2.py:185][0m |          -0.0190 |          40.3934 |           8.9976 |
[32m[20221214 00:09:06 @agent_ppo2.py:185][0m |          -0.0176 |          39.9118 |           8.9998 |
[32m[20221214 00:09:06 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:09:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 423.18
[32m[20221214 00:09:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 461.66
[32m[20221214 00:09:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 505.95
[32m[20221214 00:09:06 @agent_ppo2.py:143][0m Total time:      11.60 min
[32m[20221214 00:09:06 @agent_ppo2.py:145][0m 1071104 total steps have happened
[32m[20221214 00:09:06 @agent_ppo2.py:121][0m #------------------------ Iteration 4523 --------------------------#
[32m[20221214 00:09:06 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:09:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:09:07 @agent_ppo2.py:185][0m |          -0.0006 |          71.1629 |           7.6878 |
[32m[20221214 00:09:07 @agent_ppo2.py:185][0m |          -0.0053 |          65.6668 |           7.6618 |
[32m[20221214 00:09:07 @agent_ppo2.py:185][0m |          -0.0086 |          62.9646 |           7.6175 |
[32m[20221214 00:09:07 @agent_ppo2.py:185][0m |          -0.0082 |          62.1627 |           7.6217 |
[32m[20221214 00:09:07 @agent_ppo2.py:185][0m |          -0.0128 |          61.1340 |           7.6108 |
[32m[20221214 00:09:07 @agent_ppo2.py:185][0m |          -0.0117 |          61.0236 |           7.5463 |
[32m[20221214 00:09:07 @agent_ppo2.py:185][0m |          -0.0141 |          59.9786 |           7.5413 |
[32m[20221214 00:09:07 @agent_ppo2.py:185][0m |          -0.0053 |          62.6651 |           7.5258 |
[32m[20221214 00:09:07 @agent_ppo2.py:185][0m |          -0.0035 |          62.3616 |           7.5460 |
[32m[20221214 00:09:07 @agent_ppo2.py:185][0m |          -0.0179 |          57.5964 |           7.4895 |
[32m[20221214 00:09:07 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:09:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 471.02
[32m[20221214 00:09:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 504.00
[32m[20221214 00:09:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 555.95
[32m[20221214 00:09:08 @agent_ppo2.py:143][0m Total time:      11.62 min
[32m[20221214 00:09:08 @agent_ppo2.py:145][0m 1073152 total steps have happened
[32m[20221214 00:09:08 @agent_ppo2.py:121][0m #------------------------ Iteration 4524 --------------------------#
[32m[20221214 00:09:08 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:09:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:09:08 @agent_ppo2.py:185][0m |          -0.0014 |          56.1063 |           7.8709 |
[32m[20221214 00:09:08 @agent_ppo2.py:185][0m |          -0.0076 |          51.7287 |           7.9573 |
[32m[20221214 00:09:08 @agent_ppo2.py:185][0m |          -0.0085 |          50.3306 |           7.9415 |
[32m[20221214 00:09:08 @agent_ppo2.py:185][0m |          -0.0092 |          49.7417 |           7.9535 |
[32m[20221214 00:09:08 @agent_ppo2.py:185][0m |          -0.0097 |          48.5431 |           7.9868 |
[32m[20221214 00:09:08 @agent_ppo2.py:185][0m |          -0.0130 |          48.1769 |           7.9863 |
[32m[20221214 00:09:08 @agent_ppo2.py:185][0m |          -0.0135 |          47.7400 |           8.0426 |
[32m[20221214 00:09:09 @agent_ppo2.py:185][0m |          -0.0134 |          47.2689 |           8.0133 |
[32m[20221214 00:09:09 @agent_ppo2.py:185][0m |          -0.0142 |          47.3466 |           8.0772 |
[32m[20221214 00:09:09 @agent_ppo2.py:185][0m |          -0.0192 |          46.8637 |           8.1247 |
[32m[20221214 00:09:09 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:09:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 449.18
[32m[20221214 00:09:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 501.13
[32m[20221214 00:09:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 512.42
[32m[20221214 00:09:09 @agent_ppo2.py:143][0m Total time:      11.64 min
[32m[20221214 00:09:09 @agent_ppo2.py:145][0m 1075200 total steps have happened
[32m[20221214 00:09:09 @agent_ppo2.py:121][0m #------------------------ Iteration 4525 --------------------------#
[32m[20221214 00:09:09 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:09:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:09:09 @agent_ppo2.py:185][0m |           0.0053 |          52.2424 |           8.4877 |
[32m[20221214 00:09:09 @agent_ppo2.py:185][0m |          -0.0072 |          43.4065 |           8.5534 |
[32m[20221214 00:09:09 @agent_ppo2.py:185][0m |          -0.0090 |          42.1479 |           8.5656 |
[32m[20221214 00:09:10 @agent_ppo2.py:185][0m |          -0.0083 |          41.1284 |           8.5437 |
[32m[20221214 00:09:10 @agent_ppo2.py:185][0m |          -0.0104 |          40.2192 |           8.5584 |
[32m[20221214 00:09:10 @agent_ppo2.py:185][0m |          -0.0213 |          39.9769 |           8.5234 |
[32m[20221214 00:09:10 @agent_ppo2.py:185][0m |          -0.0153 |          39.4631 |           8.4804 |
[32m[20221214 00:09:10 @agent_ppo2.py:185][0m |          -0.0164 |          39.1530 |           8.5072 |
[32m[20221214 00:09:10 @agent_ppo2.py:185][0m |          -0.0181 |          38.9109 |           8.5511 |
[32m[20221214 00:09:10 @agent_ppo2.py:185][0m |          -0.0084 |          38.6541 |           8.5510 |
[32m[20221214 00:09:10 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:09:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 483.03
[32m[20221214 00:09:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 551.58
[32m[20221214 00:09:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 539.69
[32m[20221214 00:09:10 @agent_ppo2.py:143][0m Total time:      11.66 min
[32m[20221214 00:09:10 @agent_ppo2.py:145][0m 1077248 total steps have happened
[32m[20221214 00:09:10 @agent_ppo2.py:121][0m #------------------------ Iteration 4526 --------------------------#
[32m[20221214 00:09:10 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:09:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:09:11 @agent_ppo2.py:185][0m |          -0.0014 |          71.6547 |           8.8294 |
[32m[20221214 00:09:11 @agent_ppo2.py:185][0m |          -0.0023 |          67.6679 |           8.8017 |
[32m[20221214 00:09:11 @agent_ppo2.py:185][0m |          -0.0004 |          66.2839 |           8.7422 |
[32m[20221214 00:09:11 @agent_ppo2.py:185][0m |          -0.0063 |          65.2892 |           8.7215 |
[32m[20221214 00:09:11 @agent_ppo2.py:185][0m |          -0.0071 |          64.5841 |           8.7255 |
[32m[20221214 00:09:11 @agent_ppo2.py:185][0m |          -0.0101 |          63.8833 |           8.6990 |
[32m[20221214 00:09:11 @agent_ppo2.py:185][0m |          -0.0081 |          63.4931 |           8.7121 |
[32m[20221214 00:09:11 @agent_ppo2.py:185][0m |          -0.0124 |          62.8609 |           8.6639 |
[32m[20221214 00:09:11 @agent_ppo2.py:185][0m |          -0.0053 |          64.2661 |           8.6586 |
[32m[20221214 00:09:11 @agent_ppo2.py:185][0m |          -0.0134 |          62.5848 |           8.6385 |
[32m[20221214 00:09:11 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:09:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 496.95
[32m[20221214 00:09:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 532.63
[32m[20221214 00:09:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 613.71
[32m[20221214 00:09:12 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 613.71
[32m[20221214 00:09:12 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 613.71
[32m[20221214 00:09:12 @agent_ppo2.py:143][0m Total time:      11.69 min
[32m[20221214 00:09:12 @agent_ppo2.py:145][0m 1079296 total steps have happened
[32m[20221214 00:09:12 @agent_ppo2.py:121][0m #------------------------ Iteration 4527 --------------------------#
[32m[20221214 00:09:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:09:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:09:12 @agent_ppo2.py:185][0m |           0.0124 |          79.1953 |           7.9315 |
[32m[20221214 00:09:12 @agent_ppo2.py:185][0m |          -0.0047 |          65.6056 |           7.9394 |
[32m[20221214 00:09:12 @agent_ppo2.py:185][0m |           0.0000 |          62.3517 |           7.9063 |
[32m[20221214 00:09:12 @agent_ppo2.py:185][0m |          -0.0057 |          60.8863 |           7.9243 |
[32m[20221214 00:09:12 @agent_ppo2.py:185][0m |          -0.0127 |          58.6353 |           7.9046 |
[32m[20221214 00:09:12 @agent_ppo2.py:185][0m |          -0.0133 |          57.7404 |           7.9360 |
[32m[20221214 00:09:12 @agent_ppo2.py:185][0m |          -0.0135 |          57.3353 |           7.9137 |
[32m[20221214 00:09:12 @agent_ppo2.py:185][0m |          -0.0176 |          56.4502 |           7.9704 |
[32m[20221214 00:09:13 @agent_ppo2.py:185][0m |          -0.0198 |          56.2609 |           7.9610 |
[32m[20221214 00:09:13 @agent_ppo2.py:185][0m |          -0.0141 |          55.3841 |           7.9038 |
[32m[20221214 00:09:13 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:09:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 412.30
[32m[20221214 00:09:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 479.08
[32m[20221214 00:09:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 539.86
[32m[20221214 00:09:13 @agent_ppo2.py:143][0m Total time:      11.71 min
[32m[20221214 00:09:13 @agent_ppo2.py:145][0m 1081344 total steps have happened
[32m[20221214 00:09:13 @agent_ppo2.py:121][0m #------------------------ Iteration 4528 --------------------------#
[32m[20221214 00:09:13 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:09:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:09:13 @agent_ppo2.py:185][0m |          -0.0014 |          49.0437 |           7.8280 |
[32m[20221214 00:09:13 @agent_ppo2.py:185][0m |          -0.0052 |          42.4583 |           7.8160 |
[32m[20221214 00:09:13 @agent_ppo2.py:185][0m |          -0.0070 |          42.5133 |           7.7971 |
[32m[20221214 00:09:13 @agent_ppo2.py:185][0m |          -0.0115 |          40.2648 |           7.8892 |
[32m[20221214 00:09:13 @agent_ppo2.py:185][0m |          -0.0134 |          39.5630 |           7.8265 |
[32m[20221214 00:09:14 @agent_ppo2.py:185][0m |          -0.0123 |          39.0796 |           7.8248 |
[32m[20221214 00:09:14 @agent_ppo2.py:185][0m |          -0.0107 |          38.9081 |           7.8298 |
[32m[20221214 00:09:14 @agent_ppo2.py:185][0m |          -0.0110 |          38.3079 |           7.8114 |
[32m[20221214 00:09:14 @agent_ppo2.py:185][0m |          -0.0153 |          37.8813 |           7.8538 |
[32m[20221214 00:09:14 @agent_ppo2.py:185][0m |          -0.0150 |          37.8996 |           7.7852 |
[32m[20221214 00:09:14 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:09:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 468.85
[32m[20221214 00:09:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 523.44
[32m[20221214 00:09:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 549.72
[32m[20221214 00:09:14 @agent_ppo2.py:143][0m Total time:      11.73 min
[32m[20221214 00:09:14 @agent_ppo2.py:145][0m 1083392 total steps have happened
[32m[20221214 00:09:14 @agent_ppo2.py:121][0m #------------------------ Iteration 4529 --------------------------#
[32m[20221214 00:09:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:09:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:09:14 @agent_ppo2.py:185][0m |           0.0046 |          33.7916 |           7.6774 |
[32m[20221214 00:09:14 @agent_ppo2.py:185][0m |          -0.0053 |          26.7047 |           7.7324 |
[32m[20221214 00:09:15 @agent_ppo2.py:185][0m |          -0.0101 |          24.4851 |           7.7369 |
[32m[20221214 00:09:15 @agent_ppo2.py:185][0m |          -0.0138 |          22.5634 |           7.7668 |
[32m[20221214 00:09:15 @agent_ppo2.py:185][0m |          -0.0080 |          21.3250 |           7.8186 |
[32m[20221214 00:09:15 @agent_ppo2.py:185][0m |          -0.0115 |          20.3821 |           7.8001 |
[32m[20221214 00:09:15 @agent_ppo2.py:185][0m |          -0.0219 |          19.7873 |           7.8342 |
[32m[20221214 00:09:15 @agent_ppo2.py:185][0m |          -0.0237 |          19.3010 |           7.8328 |
[32m[20221214 00:09:15 @agent_ppo2.py:185][0m |          -0.0238 |          18.7384 |           7.8571 |
[32m[20221214 00:09:15 @agent_ppo2.py:185][0m |          -0.0229 |          18.3594 |           7.8643 |
[32m[20221214 00:09:15 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:09:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 326.32
[32m[20221214 00:09:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 378.12
[32m[20221214 00:09:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 368.43
[32m[20221214 00:09:15 @agent_ppo2.py:143][0m Total time:      11.75 min
[32m[20221214 00:09:15 @agent_ppo2.py:145][0m 1085440 total steps have happened
[32m[20221214 00:09:15 @agent_ppo2.py:121][0m #------------------------ Iteration 4530 --------------------------#
[32m[20221214 00:09:16 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:09:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:09:16 @agent_ppo2.py:185][0m |           0.0012 |          82.5561 |           7.8538 |
[32m[20221214 00:09:16 @agent_ppo2.py:185][0m |          -0.0053 |          79.5771 |           7.8810 |
[32m[20221214 00:09:16 @agent_ppo2.py:185][0m |          -0.0074 |          78.8778 |           7.8682 |
[32m[20221214 00:09:16 @agent_ppo2.py:185][0m |          -0.0098 |          77.8484 |           7.8266 |
[32m[20221214 00:09:16 @agent_ppo2.py:185][0m |          -0.0130 |          77.7515 |           7.8570 |
[32m[20221214 00:09:16 @agent_ppo2.py:185][0m |          -0.0098 |          76.7668 |           7.8567 |
[32m[20221214 00:09:16 @agent_ppo2.py:185][0m |          -0.0115 |          76.6100 |           7.8199 |
[32m[20221214 00:09:16 @agent_ppo2.py:185][0m |          -0.0064 |          81.8630 |           7.8280 |
[32m[20221214 00:09:16 @agent_ppo2.py:185][0m |          -0.0109 |          76.0229 |           7.8351 |
[32m[20221214 00:09:17 @agent_ppo2.py:185][0m |          -0.0127 |          75.6545 |           7.8005 |
[32m[20221214 00:09:17 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:09:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 513.85
[32m[20221214 00:09:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 553.76
[32m[20221214 00:09:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 566.86
[32m[20221214 00:09:17 @agent_ppo2.py:143][0m Total time:      11.77 min
[32m[20221214 00:09:17 @agent_ppo2.py:145][0m 1087488 total steps have happened
[32m[20221214 00:09:17 @agent_ppo2.py:121][0m #------------------------ Iteration 4531 --------------------------#
[32m[20221214 00:09:17 @agent_ppo2.py:127][0m Sampling time: 0.28 s by 5 slaves
[32m[20221214 00:09:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:09:17 @agent_ppo2.py:185][0m |          -0.0009 |          49.9841 |           7.9427 |
[32m[20221214 00:09:17 @agent_ppo2.py:185][0m |          -0.0001 |          45.1781 |           7.9157 |
[32m[20221214 00:09:17 @agent_ppo2.py:185][0m |          -0.0084 |          41.9945 |           7.9223 |
[32m[20221214 00:09:17 @agent_ppo2.py:185][0m |          -0.0151 |          40.4800 |           7.9647 |
[32m[20221214 00:09:18 @agent_ppo2.py:185][0m |          -0.0103 |          39.3773 |           7.9607 |
[32m[20221214 00:09:18 @agent_ppo2.py:185][0m |          -0.0124 |          38.4593 |           7.9516 |
[32m[20221214 00:09:18 @agent_ppo2.py:185][0m |          -0.0150 |          37.6972 |           7.9688 |
[32m[20221214 00:09:18 @agent_ppo2.py:185][0m |          -0.0096 |          38.3935 |           7.9487 |
[32m[20221214 00:09:18 @agent_ppo2.py:185][0m |          -0.0137 |          36.5661 |           7.9532 |
[32m[20221214 00:09:18 @agent_ppo2.py:185][0m |          -0.0153 |          36.1049 |           7.9411 |
[32m[20221214 00:09:18 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:09:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 493.24
[32m[20221214 00:09:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 559.72
[32m[20221214 00:09:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 359.68
[32m[20221214 00:09:18 @agent_ppo2.py:143][0m Total time:      11.80 min
[32m[20221214 00:09:18 @agent_ppo2.py:145][0m 1089536 total steps have happened
[32m[20221214 00:09:18 @agent_ppo2.py:121][0m #------------------------ Iteration 4532 --------------------------#
[32m[20221214 00:09:18 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:09:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:09:19 @agent_ppo2.py:185][0m |           0.0132 |          72.0679 |           7.7257 |
[32m[20221214 00:09:19 @agent_ppo2.py:185][0m |          -0.0038 |          59.4867 |           7.6731 |
[32m[20221214 00:09:19 @agent_ppo2.py:185][0m |          -0.0082 |          57.1871 |           7.6796 |
[32m[20221214 00:09:19 @agent_ppo2.py:185][0m |          -0.0108 |          56.1555 |           7.6661 |
[32m[20221214 00:09:19 @agent_ppo2.py:185][0m |          -0.0034 |          57.7418 |           7.6426 |
[32m[20221214 00:09:19 @agent_ppo2.py:185][0m |          -0.0160 |          55.2092 |           7.6020 |
[32m[20221214 00:09:19 @agent_ppo2.py:185][0m |          -0.0112 |          54.5359 |           7.6513 |
[32m[20221214 00:09:19 @agent_ppo2.py:185][0m |          -0.0110 |          54.6734 |           7.6298 |
[32m[20221214 00:09:19 @agent_ppo2.py:185][0m |          -0.0111 |          53.9019 |           7.6354 |
[32m[20221214 00:09:19 @agent_ppo2.py:185][0m |          -0.0144 |          53.6705 |           7.5634 |
[32m[20221214 00:09:19 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:09:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 478.39
[32m[20221214 00:09:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 537.96
[32m[20221214 00:09:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 529.83
[32m[20221214 00:09:19 @agent_ppo2.py:143][0m Total time:      11.82 min
[32m[20221214 00:09:19 @agent_ppo2.py:145][0m 1091584 total steps have happened
[32m[20221214 00:09:19 @agent_ppo2.py:121][0m #------------------------ Iteration 4533 --------------------------#
[32m[20221214 00:09:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:09:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:09:20 @agent_ppo2.py:185][0m |          -0.0009 |          67.1308 |           7.0263 |
[32m[20221214 00:09:20 @agent_ppo2.py:185][0m |          -0.0050 |          61.7331 |           7.0189 |
[32m[20221214 00:09:20 @agent_ppo2.py:185][0m |          -0.0087 |          60.5781 |           7.0458 |
[32m[20221214 00:09:20 @agent_ppo2.py:185][0m |          -0.0015 |          62.1683 |           7.0577 |
[32m[20221214 00:09:20 @agent_ppo2.py:185][0m |          -0.0115 |          59.1722 |           7.0687 |
[32m[20221214 00:09:20 @agent_ppo2.py:185][0m |          -0.0080 |          59.6210 |           6.9655 |
[32m[20221214 00:09:20 @agent_ppo2.py:185][0m |          -0.0093 |          58.4088 |           7.0563 |
[32m[20221214 00:09:20 @agent_ppo2.py:185][0m |          -0.0115 |          57.6843 |           7.0652 |
[32m[20221214 00:09:21 @agent_ppo2.py:185][0m |          -0.0136 |          57.5770 |           7.0315 |
[32m[20221214 00:09:21 @agent_ppo2.py:185][0m |          -0.0126 |          57.2554 |           7.0708 |
[32m[20221214 00:09:21 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:09:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 499.44
[32m[20221214 00:09:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 551.08
[32m[20221214 00:09:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 522.68
[32m[20221214 00:09:21 @agent_ppo2.py:143][0m Total time:      11.84 min
[32m[20221214 00:09:21 @agent_ppo2.py:145][0m 1093632 total steps have happened
[32m[20221214 00:09:21 @agent_ppo2.py:121][0m #------------------------ Iteration 4534 --------------------------#
[32m[20221214 00:09:21 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:09:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:09:21 @agent_ppo2.py:185][0m |           0.0000 |          33.2350 |           7.9703 |
[32m[20221214 00:09:21 @agent_ppo2.py:185][0m |          -0.0030 |          24.1724 |           8.0387 |
[32m[20221214 00:09:21 @agent_ppo2.py:185][0m |          -0.0097 |          22.5706 |           8.0298 |
[32m[20221214 00:09:21 @agent_ppo2.py:185][0m |          -0.0081 |          21.6063 |           8.0439 |
[32m[20221214 00:09:21 @agent_ppo2.py:185][0m |          -0.0121 |          20.8665 |           8.0736 |
[32m[20221214 00:09:22 @agent_ppo2.py:185][0m |          -0.0215 |          20.5025 |           8.1059 |
[32m[20221214 00:09:22 @agent_ppo2.py:185][0m |          -0.0244 |          20.1259 |           8.1199 |
[32m[20221214 00:09:22 @agent_ppo2.py:185][0m |          -0.0136 |          19.6887 |           8.1487 |
[32m[20221214 00:09:22 @agent_ppo2.py:185][0m |          -0.0229 |          19.4942 |           8.1333 |
[32m[20221214 00:09:22 @agent_ppo2.py:185][0m |          -0.0182 |          19.1285 |           8.1885 |
[32m[20221214 00:09:22 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:09:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 367.07
[32m[20221214 00:09:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 436.03
[32m[20221214 00:09:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 548.50
[32m[20221214 00:09:22 @agent_ppo2.py:143][0m Total time:      11.86 min
[32m[20221214 00:09:22 @agent_ppo2.py:145][0m 1095680 total steps have happened
[32m[20221214 00:09:22 @agent_ppo2.py:121][0m #------------------------ Iteration 4535 --------------------------#
[32m[20221214 00:09:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:09:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:09:22 @agent_ppo2.py:185][0m |           0.0054 |          57.2421 |           7.8146 |
[32m[20221214 00:09:23 @agent_ppo2.py:185][0m |          -0.0039 |          51.4654 |           7.7662 |
[32m[20221214 00:09:23 @agent_ppo2.py:185][0m |          -0.0015 |          50.7550 |           7.7373 |
[32m[20221214 00:09:23 @agent_ppo2.py:185][0m |          -0.0083 |          49.0379 |           7.7273 |
[32m[20221214 00:09:23 @agent_ppo2.py:185][0m |          -0.0078 |          48.2944 |           7.7052 |
[32m[20221214 00:09:23 @agent_ppo2.py:185][0m |          -0.0094 |          47.9113 |           7.6119 |
[32m[20221214 00:09:23 @agent_ppo2.py:185][0m |          -0.0073 |          48.6430 |           7.5927 |
[32m[20221214 00:09:23 @agent_ppo2.py:185][0m |          -0.0122 |          47.3235 |           7.5844 |
[32m[20221214 00:09:23 @agent_ppo2.py:185][0m |          -0.0115 |          47.2454 |           7.5218 |
[32m[20221214 00:09:23 @agent_ppo2.py:185][0m |          -0.0117 |          47.1188 |           7.5196 |
[32m[20221214 00:09:23 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:09:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 503.85
[32m[20221214 00:09:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 540.54
[32m[20221214 00:09:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 286.71
[32m[20221214 00:09:23 @agent_ppo2.py:143][0m Total time:      11.88 min
[32m[20221214 00:09:23 @agent_ppo2.py:145][0m 1097728 total steps have happened
[32m[20221214 00:09:23 @agent_ppo2.py:121][0m #------------------------ Iteration 4536 --------------------------#
[32m[20221214 00:09:24 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:09:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:09:24 @agent_ppo2.py:185][0m |           0.0029 |          27.6587 |           7.7093 |
[32m[20221214 00:09:24 @agent_ppo2.py:185][0m |          -0.0053 |          21.7282 |           7.7688 |
[32m[20221214 00:09:24 @agent_ppo2.py:185][0m |          -0.0088 |          20.1951 |           7.7298 |
[32m[20221214 00:09:24 @agent_ppo2.py:185][0m |          -0.0209 |          18.9974 |           7.7433 |
[32m[20221214 00:09:24 @agent_ppo2.py:185][0m |          -0.0120 |          18.3684 |           7.7042 |
[32m[20221214 00:09:24 @agent_ppo2.py:185][0m |          -0.0186 |          17.9283 |           7.7254 |
[32m[20221214 00:09:24 @agent_ppo2.py:185][0m |          -0.0233 |          17.6299 |           7.7809 |
[32m[20221214 00:09:24 @agent_ppo2.py:185][0m |          -0.0166 |          17.1595 |           7.7326 |
[32m[20221214 00:09:24 @agent_ppo2.py:185][0m |          -0.0236 |          16.8937 |           7.7017 |
[32m[20221214 00:09:25 @agent_ppo2.py:185][0m |          -0.0212 |          16.7160 |           7.6948 |
[32m[20221214 00:09:25 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:09:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 302.24
[32m[20221214 00:09:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 325.87
[32m[20221214 00:09:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 427.31
[32m[20221214 00:09:25 @agent_ppo2.py:143][0m Total time:      11.91 min
[32m[20221214 00:09:25 @agent_ppo2.py:145][0m 1099776 total steps have happened
[32m[20221214 00:09:25 @agent_ppo2.py:121][0m #------------------------ Iteration 4537 --------------------------#
[32m[20221214 00:09:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:09:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:09:25 @agent_ppo2.py:185][0m |          -0.0010 |          78.7932 |           7.6388 |
[32m[20221214 00:09:25 @agent_ppo2.py:185][0m |          -0.0075 |          75.6980 |           7.7343 |
[32m[20221214 00:09:25 @agent_ppo2.py:185][0m |          -0.0080 |          73.6722 |           7.7392 |
[32m[20221214 00:09:25 @agent_ppo2.py:185][0m |          -0.0080 |          72.4959 |           7.6876 |
[32m[20221214 00:09:25 @agent_ppo2.py:185][0m |          -0.0117 |          71.9898 |           7.6994 |
[32m[20221214 00:09:26 @agent_ppo2.py:185][0m |          -0.0141 |          71.4425 |           7.6894 |
[32m[20221214 00:09:26 @agent_ppo2.py:185][0m |          -0.0124 |          71.2227 |           7.7069 |
[32m[20221214 00:09:26 @agent_ppo2.py:185][0m |          -0.0056 |          73.5764 |           7.6857 |
[32m[20221214 00:09:26 @agent_ppo2.py:185][0m |          -0.0114 |          70.5397 |           7.7686 |
[32m[20221214 00:09:26 @agent_ppo2.py:185][0m |          -0.0138 |          70.2980 |           7.7497 |
[32m[20221214 00:09:26 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:09:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 511.77
[32m[20221214 00:09:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 533.45
[32m[20221214 00:09:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 498.49
[32m[20221214 00:09:26 @agent_ppo2.py:143][0m Total time:      11.93 min
[32m[20221214 00:09:26 @agent_ppo2.py:145][0m 1101824 total steps have happened
[32m[20221214 00:09:26 @agent_ppo2.py:121][0m #------------------------ Iteration 4538 --------------------------#
[32m[20221214 00:09:26 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:09:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:09:26 @agent_ppo2.py:185][0m |          -0.0021 |          48.5946 |           7.4337 |
[32m[20221214 00:09:26 @agent_ppo2.py:185][0m |          -0.0112 |          44.1390 |           7.4964 |
[32m[20221214 00:09:27 @agent_ppo2.py:185][0m |          -0.0131 |          42.6819 |           7.4751 |
[32m[20221214 00:09:27 @agent_ppo2.py:185][0m |          -0.0141 |          41.9886 |           7.4867 |
[32m[20221214 00:09:27 @agent_ppo2.py:185][0m |          -0.0177 |          41.4218 |           7.5250 |
[32m[20221214 00:09:27 @agent_ppo2.py:185][0m |          -0.0162 |          41.1351 |           7.5230 |
[32m[20221214 00:09:27 @agent_ppo2.py:185][0m |          -0.0077 |          45.0949 |           7.5077 |
[32m[20221214 00:09:27 @agent_ppo2.py:185][0m |          -0.0171 |          40.3808 |           7.5038 |
[32m[20221214 00:09:27 @agent_ppo2.py:185][0m |          -0.0224 |          40.1208 |           7.5173 |
[32m[20221214 00:09:27 @agent_ppo2.py:185][0m |          -0.0214 |          39.8281 |           7.5395 |
[32m[20221214 00:09:27 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:09:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 402.00
[32m[20221214 00:09:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 441.89
[32m[20221214 00:09:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 566.28
[32m[20221214 00:09:27 @agent_ppo2.py:143][0m Total time:      11.95 min
[32m[20221214 00:09:27 @agent_ppo2.py:145][0m 1103872 total steps have happened
[32m[20221214 00:09:27 @agent_ppo2.py:121][0m #------------------------ Iteration 4539 --------------------------#
[32m[20221214 00:09:28 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:09:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:09:28 @agent_ppo2.py:185][0m |           0.0020 |          83.2572 |           7.3356 |
[32m[20221214 00:09:28 @agent_ppo2.py:185][0m |          -0.0064 |          77.7209 |           7.3423 |
[32m[20221214 00:09:28 @agent_ppo2.py:185][0m |          -0.0093 |          75.7878 |           7.3398 |
[32m[20221214 00:09:28 @agent_ppo2.py:185][0m |          -0.0028 |          76.9505 |           7.3221 |
[32m[20221214 00:09:28 @agent_ppo2.py:185][0m |          -0.0075 |          75.1630 |           7.3541 |
[32m[20221214 00:09:28 @agent_ppo2.py:185][0m |          -0.0146 |          73.7847 |           7.2893 |
[32m[20221214 00:09:28 @agent_ppo2.py:185][0m |          -0.0139 |          73.2532 |           7.3087 |
[32m[20221214 00:09:28 @agent_ppo2.py:185][0m |          -0.0113 |          72.9195 |           7.3294 |
[32m[20221214 00:09:29 @agent_ppo2.py:185][0m |          -0.0153 |          72.7044 |           7.3090 |
[32m[20221214 00:09:29 @agent_ppo2.py:185][0m |          -0.0109 |          72.3842 |           7.3394 |
[32m[20221214 00:09:29 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:09:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 398.42
[32m[20221214 00:09:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 474.23
[32m[20221214 00:09:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 387.12
[32m[20221214 00:09:29 @agent_ppo2.py:143][0m Total time:      11.97 min
[32m[20221214 00:09:29 @agent_ppo2.py:145][0m 1105920 total steps have happened
[32m[20221214 00:09:29 @agent_ppo2.py:121][0m #------------------------ Iteration 4540 --------------------------#
[32m[20221214 00:09:29 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:09:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:09:29 @agent_ppo2.py:185][0m |          -0.0017 |          71.8358 |           7.4573 |
[32m[20221214 00:09:29 @agent_ppo2.py:185][0m |          -0.0059 |          66.4230 |           7.4547 |
[32m[20221214 00:09:29 @agent_ppo2.py:185][0m |          -0.0049 |          64.3703 |           7.4426 |
[32m[20221214 00:09:29 @agent_ppo2.py:185][0m |          -0.0073 |          63.0556 |           7.4701 |
[32m[20221214 00:09:30 @agent_ppo2.py:185][0m |          -0.0077 |          62.0161 |           7.4542 |
[32m[20221214 00:09:30 @agent_ppo2.py:185][0m |          -0.0088 |          61.4563 |           7.5231 |
[32m[20221214 00:09:30 @agent_ppo2.py:185][0m |          -0.0123 |          60.9224 |           7.5027 |
[32m[20221214 00:09:30 @agent_ppo2.py:185][0m |           0.0108 |          66.6356 |           7.5378 |
[32m[20221214 00:09:30 @agent_ppo2.py:185][0m |          -0.0027 |          61.0906 |           7.5484 |
[32m[20221214 00:09:30 @agent_ppo2.py:185][0m |          -0.0141 |          59.7294 |           7.5575 |
[32m[20221214 00:09:30 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:09:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 498.61
[32m[20221214 00:09:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 559.81
[32m[20221214 00:09:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 461.62
[32m[20221214 00:09:30 @agent_ppo2.py:143][0m Total time:      12.00 min
[32m[20221214 00:09:30 @agent_ppo2.py:145][0m 1107968 total steps have happened
[32m[20221214 00:09:30 @agent_ppo2.py:121][0m #------------------------ Iteration 4541 --------------------------#
[32m[20221214 00:09:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:09:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:09:30 @agent_ppo2.py:185][0m |          -0.0024 |          68.6369 |           7.9680 |
[32m[20221214 00:09:31 @agent_ppo2.py:185][0m |          -0.0041 |          58.4630 |           7.9431 |
[32m[20221214 00:09:31 @agent_ppo2.py:185][0m |          -0.0090 |          55.5901 |           7.9610 |
[32m[20221214 00:09:31 @agent_ppo2.py:185][0m |          -0.0073 |          54.7988 |           8.0253 |
[32m[20221214 00:09:31 @agent_ppo2.py:185][0m |          -0.0157 |          53.3635 |           8.0461 |
[32m[20221214 00:09:31 @agent_ppo2.py:185][0m |          -0.0150 |          52.4698 |           8.0547 |
[32m[20221214 00:09:31 @agent_ppo2.py:185][0m |          -0.0170 |          51.7826 |           8.1208 |
[32m[20221214 00:09:31 @agent_ppo2.py:185][0m |          -0.0165 |          51.2355 |           8.1221 |
[32m[20221214 00:09:31 @agent_ppo2.py:185][0m |          -0.0145 |          51.2613 |           8.1202 |
[32m[20221214 00:09:31 @agent_ppo2.py:185][0m |          -0.0192 |          50.6091 |           8.2015 |
[32m[20221214 00:09:31 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:09:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 475.91
[32m[20221214 00:09:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 523.64
[32m[20221214 00:09:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 488.09
[32m[20221214 00:09:31 @agent_ppo2.py:143][0m Total time:      12.02 min
[32m[20221214 00:09:31 @agent_ppo2.py:145][0m 1110016 total steps have happened
[32m[20221214 00:09:31 @agent_ppo2.py:121][0m #------------------------ Iteration 4542 --------------------------#
[32m[20221214 00:09:32 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:09:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:09:32 @agent_ppo2.py:185][0m |           0.0014 |          56.3794 |           7.6681 |
[32m[20221214 00:09:32 @agent_ppo2.py:185][0m |          -0.0063 |          50.3835 |           7.6282 |
[32m[20221214 00:09:32 @agent_ppo2.py:185][0m |          -0.0092 |          48.5226 |           7.6937 |
[32m[20221214 00:09:32 @agent_ppo2.py:185][0m |          -0.0095 |          46.7952 |           7.6374 |
[32m[20221214 00:09:32 @agent_ppo2.py:185][0m |          -0.0116 |          45.7424 |           7.5978 |
[32m[20221214 00:09:32 @agent_ppo2.py:185][0m |          -0.0053 |          45.2734 |           7.5736 |
[32m[20221214 00:09:32 @agent_ppo2.py:185][0m |          -0.0120 |          43.9677 |           7.6316 |
[32m[20221214 00:09:32 @agent_ppo2.py:185][0m |          -0.0157 |          43.4263 |           7.5889 |
[32m[20221214 00:09:32 @agent_ppo2.py:185][0m |          -0.0142 |          42.9031 |           7.6747 |
[32m[20221214 00:09:32 @agent_ppo2.py:185][0m |          -0.0146 |          42.5092 |           7.6047 |
[32m[20221214 00:09:32 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:09:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 473.45
[32m[20221214 00:09:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 531.83
[32m[20221214 00:09:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 543.78
[32m[20221214 00:09:33 @agent_ppo2.py:143][0m Total time:      12.04 min
[32m[20221214 00:09:33 @agent_ppo2.py:145][0m 1112064 total steps have happened
[32m[20221214 00:09:33 @agent_ppo2.py:121][0m #------------------------ Iteration 4543 --------------------------#
[32m[20221214 00:09:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:09:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:09:33 @agent_ppo2.py:185][0m |          -0.0014 |          60.1431 |           7.7055 |
[32m[20221214 00:09:33 @agent_ppo2.py:185][0m |          -0.0058 |          43.2396 |           7.8164 |
[32m[20221214 00:09:33 @agent_ppo2.py:185][0m |          -0.0036 |          39.4284 |           7.7849 |
[32m[20221214 00:09:33 @agent_ppo2.py:185][0m |          -0.0074 |          37.7991 |           7.7337 |
[32m[20221214 00:09:33 @agent_ppo2.py:185][0m |          -0.0142 |          36.5695 |           7.7588 |
[32m[20221214 00:09:33 @agent_ppo2.py:185][0m |          -0.0062 |          36.8003 |           7.7719 |
[32m[20221214 00:09:33 @agent_ppo2.py:185][0m |          -0.0050 |          36.9713 |           7.7542 |
[32m[20221214 00:09:34 @agent_ppo2.py:185][0m |          -0.0151 |          34.3561 |           7.7638 |
[32m[20221214 00:09:34 @agent_ppo2.py:185][0m |          -0.0156 |          33.6744 |           7.7921 |
[32m[20221214 00:09:34 @agent_ppo2.py:185][0m |          -0.0243 |          33.4172 |           7.7254 |
[32m[20221214 00:09:34 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:09:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 357.78
[32m[20221214 00:09:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 502.57
[32m[20221214 00:09:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 562.87
[32m[20221214 00:09:34 @agent_ppo2.py:143][0m Total time:      12.06 min
[32m[20221214 00:09:34 @agent_ppo2.py:145][0m 1114112 total steps have happened
[32m[20221214 00:09:34 @agent_ppo2.py:121][0m #------------------------ Iteration 4544 --------------------------#
[32m[20221214 00:09:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:09:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:09:34 @agent_ppo2.py:185][0m |           0.0103 |          87.2921 |           7.2971 |
[32m[20221214 00:09:34 @agent_ppo2.py:185][0m |          -0.0023 |          79.6620 |           7.4264 |
[32m[20221214 00:09:34 @agent_ppo2.py:185][0m |          -0.0075 |          77.9436 |           7.3842 |
[32m[20221214 00:09:35 @agent_ppo2.py:185][0m |          -0.0038 |          76.3055 |           7.3954 |
[32m[20221214 00:09:35 @agent_ppo2.py:185][0m |          -0.0038 |          77.4513 |           7.4237 |
[32m[20221214 00:09:35 @agent_ppo2.py:185][0m |          -0.0091 |          74.8875 |           7.4194 |
[32m[20221214 00:09:35 @agent_ppo2.py:185][0m |          -0.0108 |          73.8059 |           7.4474 |
[32m[20221214 00:09:35 @agent_ppo2.py:185][0m |          -0.0056 |          81.3707 |           7.3910 |
[32m[20221214 00:09:35 @agent_ppo2.py:185][0m |          -0.0109 |          73.7886 |           7.4505 |
[32m[20221214 00:09:35 @agent_ppo2.py:185][0m |          -0.0063 |          75.8482 |           7.4887 |
[32m[20221214 00:09:35 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:09:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 450.35
[32m[20221214 00:09:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 512.27
[32m[20221214 00:09:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 541.46
[32m[20221214 00:09:35 @agent_ppo2.py:143][0m Total time:      12.08 min
[32m[20221214 00:09:35 @agent_ppo2.py:145][0m 1116160 total steps have happened
[32m[20221214 00:09:35 @agent_ppo2.py:121][0m #------------------------ Iteration 4545 --------------------------#
[32m[20221214 00:09:35 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:09:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:09:36 @agent_ppo2.py:185][0m |          -0.0016 |          77.2648 |           7.6485 |
[32m[20221214 00:09:36 @agent_ppo2.py:185][0m |          -0.0047 |          72.8691 |           7.7481 |
[32m[20221214 00:09:36 @agent_ppo2.py:185][0m |          -0.0087 |          71.1843 |           7.6802 |
[32m[20221214 00:09:36 @agent_ppo2.py:185][0m |          -0.0133 |          70.0268 |           7.6463 |
[32m[20221214 00:09:36 @agent_ppo2.py:185][0m |          -0.0100 |          69.7575 |           7.7943 |
[32m[20221214 00:09:36 @agent_ppo2.py:185][0m |          -0.0160 |          68.7738 |           7.7201 |
[32m[20221214 00:09:36 @agent_ppo2.py:185][0m |          -0.0163 |          68.3299 |           7.7087 |
[32m[20221214 00:09:36 @agent_ppo2.py:185][0m |          -0.0061 |          76.6515 |           7.7434 |
[32m[20221214 00:09:36 @agent_ppo2.py:185][0m |          -0.0183 |          68.1450 |           7.7213 |
[32m[20221214 00:09:36 @agent_ppo2.py:185][0m |          -0.0179 |          67.6594 |           7.6946 |
[32m[20221214 00:09:36 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:09:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 522.39
[32m[20221214 00:09:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 534.24
[32m[20221214 00:09:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 499.99
[32m[20221214 00:09:36 @agent_ppo2.py:143][0m Total time:      12.10 min
[32m[20221214 00:09:36 @agent_ppo2.py:145][0m 1118208 total steps have happened
[32m[20221214 00:09:36 @agent_ppo2.py:121][0m #------------------------ Iteration 4546 --------------------------#
[32m[20221214 00:09:37 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:09:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:09:37 @agent_ppo2.py:185][0m |          -0.0077 |          42.0846 |           7.7177 |
[32m[20221214 00:09:37 @agent_ppo2.py:185][0m |          -0.0073 |          29.2241 |           7.7113 |
[32m[20221214 00:09:37 @agent_ppo2.py:185][0m |          -0.0050 |          26.9388 |           7.7461 |
[32m[20221214 00:09:37 @agent_ppo2.py:185][0m |          -0.0059 |          25.6466 |           7.7306 |
[32m[20221214 00:09:37 @agent_ppo2.py:185][0m |          -0.0151 |          24.6093 |           7.7277 |
[32m[20221214 00:09:37 @agent_ppo2.py:185][0m |          -0.0091 |          23.8723 |           7.7571 |
[32m[20221214 00:09:37 @agent_ppo2.py:185][0m |          -0.0151 |          23.4598 |           7.7763 |
[32m[20221214 00:09:37 @agent_ppo2.py:185][0m |          -0.0126 |          23.1185 |           7.8093 |
[32m[20221214 00:09:37 @agent_ppo2.py:185][0m |          -0.0216 |          22.6326 |           7.7676 |
[32m[20221214 00:09:38 @agent_ppo2.py:185][0m |          -0.0178 |          22.4035 |           7.8189 |
[32m[20221214 00:09:38 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:09:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 336.33
[32m[20221214 00:09:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 423.01
[32m[20221214 00:09:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 503.29
[32m[20221214 00:09:38 @agent_ppo2.py:143][0m Total time:      12.12 min
[32m[20221214 00:09:38 @agent_ppo2.py:145][0m 1120256 total steps have happened
[32m[20221214 00:09:38 @agent_ppo2.py:121][0m #------------------------ Iteration 4547 --------------------------#
[32m[20221214 00:09:38 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:09:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:09:38 @agent_ppo2.py:185][0m |           0.0030 |          40.3576 |           7.7355 |
[32m[20221214 00:09:38 @agent_ppo2.py:185][0m |          -0.0060 |          33.3357 |           7.7743 |
[32m[20221214 00:09:38 @agent_ppo2.py:185][0m |           0.0023 |          33.0315 |           7.7771 |
[32m[20221214 00:09:38 @agent_ppo2.py:185][0m |          -0.0095 |          30.8426 |           7.7803 |
[32m[20221214 00:09:38 @agent_ppo2.py:185][0m |          -0.0142 |          30.2096 |           7.7874 |
[32m[20221214 00:09:38 @agent_ppo2.py:185][0m |          -0.0108 |          29.4561 |           7.7634 |
[32m[20221214 00:09:39 @agent_ppo2.py:185][0m |          -0.0149 |          28.9433 |           7.7684 |
[32m[20221214 00:09:39 @agent_ppo2.py:185][0m |          -0.0138 |          28.5766 |           7.7602 |
[32m[20221214 00:09:39 @agent_ppo2.py:185][0m |          -0.0202 |          28.3968 |           7.7713 |
[32m[20221214 00:09:39 @agent_ppo2.py:185][0m |          -0.0145 |          28.1366 |           7.7283 |
[32m[20221214 00:09:39 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:09:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 413.42
[32m[20221214 00:09:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 488.26
[32m[20221214 00:09:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 470.39
[32m[20221214 00:09:39 @agent_ppo2.py:143][0m Total time:      12.14 min
[32m[20221214 00:09:39 @agent_ppo2.py:145][0m 1122304 total steps have happened
[32m[20221214 00:09:39 @agent_ppo2.py:121][0m #------------------------ Iteration 4548 --------------------------#
[32m[20221214 00:09:39 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:09:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:09:39 @agent_ppo2.py:185][0m |          -0.0014 |          56.6667 |           7.0984 |
[32m[20221214 00:09:39 @agent_ppo2.py:185][0m |          -0.0057 |          53.8606 |           7.0072 |
[32m[20221214 00:09:40 @agent_ppo2.py:185][0m |          -0.0082 |          52.7030 |           6.9987 |
[32m[20221214 00:09:40 @agent_ppo2.py:185][0m |          -0.0080 |          52.0092 |           6.9959 |
[32m[20221214 00:09:40 @agent_ppo2.py:185][0m |          -0.0108 |          51.4847 |           6.9361 |
[32m[20221214 00:09:40 @agent_ppo2.py:185][0m |          -0.0099 |          51.0160 |           6.9805 |
[32m[20221214 00:09:40 @agent_ppo2.py:185][0m |          -0.0130 |          50.8630 |           6.9513 |
[32m[20221214 00:09:40 @agent_ppo2.py:185][0m |          -0.0143 |          50.2978 |           6.8569 |
[32m[20221214 00:09:40 @agent_ppo2.py:185][0m |          -0.0102 |          50.0181 |           6.8627 |
[32m[20221214 00:09:40 @agent_ppo2.py:185][0m |          -0.0122 |          49.6253 |           6.8655 |
[32m[20221214 00:09:40 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:09:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 489.42
[32m[20221214 00:09:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 513.30
[32m[20221214 00:09:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 529.81
[32m[20221214 00:09:40 @agent_ppo2.py:143][0m Total time:      12.17 min
[32m[20221214 00:09:40 @agent_ppo2.py:145][0m 1124352 total steps have happened
[32m[20221214 00:09:40 @agent_ppo2.py:121][0m #------------------------ Iteration 4549 --------------------------#
[32m[20221214 00:09:40 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:09:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:09:41 @agent_ppo2.py:185][0m |          -0.0004 |          75.9121 |           7.6130 |
[32m[20221214 00:09:41 @agent_ppo2.py:185][0m |          -0.0013 |          76.4616 |           7.7158 |
[32m[20221214 00:09:41 @agent_ppo2.py:185][0m |          -0.0106 |          73.1349 |           7.7778 |
[32m[20221214 00:09:41 @agent_ppo2.py:185][0m |          -0.0134 |          72.7553 |           7.7405 |
[32m[20221214 00:09:41 @agent_ppo2.py:185][0m |          -0.0138 |          72.5946 |           7.7655 |
[32m[20221214 00:09:41 @agent_ppo2.py:185][0m |          -0.0056 |          76.8522 |           7.7650 |
[32m[20221214 00:09:41 @agent_ppo2.py:185][0m |          -0.0067 |          75.6355 |           7.8444 |
[32m[20221214 00:09:41 @agent_ppo2.py:185][0m |          -0.0153 |          71.8874 |           7.7937 |
[32m[20221214 00:09:41 @agent_ppo2.py:185][0m |          -0.0150 |          71.6654 |           7.8468 |
[32m[20221214 00:09:41 @agent_ppo2.py:185][0m |          -0.0120 |          75.6145 |           7.8671 |
[32m[20221214 00:09:41 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:09:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 519.79
[32m[20221214 00:09:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 551.97
[32m[20221214 00:09:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 458.03
[32m[20221214 00:09:42 @agent_ppo2.py:143][0m Total time:      12.19 min
[32m[20221214 00:09:42 @agent_ppo2.py:145][0m 1126400 total steps have happened
[32m[20221214 00:09:42 @agent_ppo2.py:121][0m #------------------------ Iteration 4550 --------------------------#
[32m[20221214 00:09:42 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:09:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:09:42 @agent_ppo2.py:185][0m |           0.0095 |          43.3535 |           7.7400 |
[32m[20221214 00:09:42 @agent_ppo2.py:185][0m |          -0.0089 |          35.5238 |           7.7410 |
[32m[20221214 00:09:42 @agent_ppo2.py:185][0m |          -0.0048 |          33.8124 |           7.7239 |
[32m[20221214 00:09:42 @agent_ppo2.py:185][0m |          -0.0041 |          33.2411 |           7.6820 |
[32m[20221214 00:09:42 @agent_ppo2.py:185][0m |          -0.0119 |          32.2468 |           7.6731 |
[32m[20221214 00:09:42 @agent_ppo2.py:185][0m |          -0.0137 |          31.4618 |           7.6995 |
[32m[20221214 00:09:43 @agent_ppo2.py:185][0m |          -0.0169 |          31.0821 |           7.6465 |
[32m[20221214 00:09:43 @agent_ppo2.py:185][0m |          -0.0183 |          30.7812 |           7.7141 |
[32m[20221214 00:09:43 @agent_ppo2.py:185][0m |          -0.0141 |          30.4713 |           7.6835 |
[32m[20221214 00:09:43 @agent_ppo2.py:185][0m |          -0.0236 |          30.1216 |           7.6765 |
[32m[20221214 00:09:43 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:09:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 339.24
[32m[20221214 00:09:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 484.60
[32m[20221214 00:09:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 400.39
[32m[20221214 00:09:43 @agent_ppo2.py:143][0m Total time:      12.21 min
[32m[20221214 00:09:43 @agent_ppo2.py:145][0m 1128448 total steps have happened
[32m[20221214 00:09:43 @agent_ppo2.py:121][0m #------------------------ Iteration 4551 --------------------------#
[32m[20221214 00:09:43 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:09:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:09:43 @agent_ppo2.py:185][0m |          -0.0027 |          29.4511 |           7.5470 |
[32m[20221214 00:09:43 @agent_ppo2.py:185][0m |          -0.0028 |          23.0361 |           7.5555 |
[32m[20221214 00:09:43 @agent_ppo2.py:185][0m |          -0.0092 |          21.4481 |           7.5315 |
[32m[20221214 00:09:44 @agent_ppo2.py:185][0m |          -0.0155 |          20.6436 |           7.5356 |
[32m[20221214 00:09:44 @agent_ppo2.py:185][0m |          -0.0181 |          19.9256 |           7.4940 |
[32m[20221214 00:09:44 @agent_ppo2.py:185][0m |          -0.0195 |          19.2318 |           7.4738 |
[32m[20221214 00:09:44 @agent_ppo2.py:185][0m |          -0.0209 |          18.6484 |           7.4417 |
[32m[20221214 00:09:44 @agent_ppo2.py:185][0m |          -0.0205 |          18.1837 |           7.4754 |
[32m[20221214 00:09:44 @agent_ppo2.py:185][0m |          -0.0258 |          17.8667 |           7.4489 |
[32m[20221214 00:09:44 @agent_ppo2.py:185][0m |          -0.0226 |          17.4138 |           7.4303 |
[32m[20221214 00:09:44 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:09:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 402.50
[32m[20221214 00:09:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 495.61
[32m[20221214 00:09:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 424.67
[32m[20221214 00:09:44 @agent_ppo2.py:143][0m Total time:      12.23 min
[32m[20221214 00:09:44 @agent_ppo2.py:145][0m 1130496 total steps have happened
[32m[20221214 00:09:44 @agent_ppo2.py:121][0m #------------------------ Iteration 4552 --------------------------#
[32m[20221214 00:09:44 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:09:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:09:45 @agent_ppo2.py:185][0m |           0.0049 |          61.1446 |           7.6577 |
[32m[20221214 00:09:45 @agent_ppo2.py:185][0m |          -0.0079 |          54.4339 |           7.6967 |
[32m[20221214 00:09:45 @agent_ppo2.py:185][0m |           0.0012 |          51.8217 |           7.6967 |
[32m[20221214 00:09:45 @agent_ppo2.py:185][0m |          -0.0071 |          49.5871 |           7.6744 |
[32m[20221214 00:09:45 @agent_ppo2.py:185][0m |          -0.0136 |          48.4091 |           7.6935 |
[32m[20221214 00:09:45 @agent_ppo2.py:185][0m |          -0.0133 |          47.5808 |           7.7088 |
[32m[20221214 00:09:45 @agent_ppo2.py:185][0m |          -0.0131 |          46.9405 |           7.7563 |
[32m[20221214 00:09:45 @agent_ppo2.py:185][0m |          -0.0149 |          46.4731 |           7.7447 |
[32m[20221214 00:09:45 @agent_ppo2.py:185][0m |          -0.0120 |          46.0723 |           7.7638 |
[32m[20221214 00:09:45 @agent_ppo2.py:185][0m |          -0.0056 |          48.6761 |           7.7605 |
[32m[20221214 00:09:45 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:09:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 455.11
[32m[20221214 00:09:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 512.20
[32m[20221214 00:09:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 410.09
[32m[20221214 00:09:46 @agent_ppo2.py:143][0m Total time:      12.25 min
[32m[20221214 00:09:46 @agent_ppo2.py:145][0m 1132544 total steps have happened
[32m[20221214 00:09:46 @agent_ppo2.py:121][0m #------------------------ Iteration 4553 --------------------------#
[32m[20221214 00:09:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:09:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:09:46 @agent_ppo2.py:185][0m |           0.0043 |          53.9425 |           7.1722 |
[32m[20221214 00:09:46 @agent_ppo2.py:185][0m |          -0.0052 |          49.6535 |           7.1677 |
[32m[20221214 00:09:46 @agent_ppo2.py:185][0m |          -0.0112 |          48.7071 |           7.2602 |
[32m[20221214 00:09:46 @agent_ppo2.py:185][0m |          -0.0082 |          48.1759 |           7.2915 |
[32m[20221214 00:09:46 @agent_ppo2.py:185][0m |          -0.0133 |          47.0968 |           7.3637 |
[32m[20221214 00:09:46 @agent_ppo2.py:185][0m |          -0.0141 |          46.7089 |           7.3558 |
[32m[20221214 00:09:46 @agent_ppo2.py:185][0m |          -0.0155 |          46.3467 |           7.3833 |
[32m[20221214 00:09:47 @agent_ppo2.py:185][0m |          -0.0142 |          45.9577 |           7.4204 |
[32m[20221214 00:09:47 @agent_ppo2.py:185][0m |          -0.0190 |          45.5653 |           7.4541 |
[32m[20221214 00:09:47 @agent_ppo2.py:185][0m |          -0.0191 |          45.2047 |           7.4519 |
[32m[20221214 00:09:47 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:09:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 468.97
[32m[20221214 00:09:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 542.79
[32m[20221214 00:09:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 500.39
[32m[20221214 00:09:47 @agent_ppo2.py:143][0m Total time:      12.28 min
[32m[20221214 00:09:47 @agent_ppo2.py:145][0m 1134592 total steps have happened
[32m[20221214 00:09:47 @agent_ppo2.py:121][0m #------------------------ Iteration 4554 --------------------------#
[32m[20221214 00:09:47 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:09:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:09:47 @agent_ppo2.py:185][0m |           0.0002 |          53.9674 |           8.0889 |
[32m[20221214 00:09:47 @agent_ppo2.py:185][0m |          -0.0063 |          49.5037 |           8.1245 |
[32m[20221214 00:09:47 @agent_ppo2.py:185][0m |          -0.0092 |          48.0380 |           8.1444 |
[32m[20221214 00:09:48 @agent_ppo2.py:185][0m |          -0.0160 |          47.0016 |           8.1272 |
[32m[20221214 00:09:48 @agent_ppo2.py:185][0m |          -0.0107 |          46.6569 |           8.1587 |
[32m[20221214 00:09:48 @agent_ppo2.py:185][0m |          -0.0083 |          46.5320 |           8.1663 |
[32m[20221214 00:09:48 @agent_ppo2.py:185][0m |          -0.0008 |          49.7804 |           8.1587 |
[32m[20221214 00:09:48 @agent_ppo2.py:185][0m |          -0.0148 |          44.5391 |           8.1610 |
[32m[20221214 00:09:48 @agent_ppo2.py:185][0m |          -0.0155 |          44.0593 |           8.2064 |
[32m[20221214 00:09:48 @agent_ppo2.py:185][0m |          -0.0107 |          44.8921 |           8.2194 |
[32m[20221214 00:09:48 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:09:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 478.22
[32m[20221214 00:09:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 536.67
[32m[20221214 00:09:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 518.11
[32m[20221214 00:09:48 @agent_ppo2.py:143][0m Total time:      12.30 min
[32m[20221214 00:09:48 @agent_ppo2.py:145][0m 1136640 total steps have happened
[32m[20221214 00:09:48 @agent_ppo2.py:121][0m #------------------------ Iteration 4555 --------------------------#
[32m[20221214 00:09:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:09:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:09:48 @agent_ppo2.py:185][0m |          -0.0019 |          34.6871 |           7.8945 |
[32m[20221214 00:09:49 @agent_ppo2.py:185][0m |          -0.0080 |          30.2556 |           7.8685 |
[32m[20221214 00:09:49 @agent_ppo2.py:185][0m |          -0.0075 |          28.5995 |           7.8581 |
[32m[20221214 00:09:49 @agent_ppo2.py:185][0m |          -0.0182 |          27.5789 |           7.8582 |
[32m[20221214 00:09:49 @agent_ppo2.py:185][0m |          -0.0171 |          26.5911 |           7.8665 |
[32m[20221214 00:09:49 @agent_ppo2.py:185][0m |          -0.0153 |          25.9129 |           7.8158 |
[32m[20221214 00:09:49 @agent_ppo2.py:185][0m |          -0.0215 |          25.3148 |           7.7796 |
[32m[20221214 00:09:49 @agent_ppo2.py:185][0m |          -0.0179 |          24.9336 |           7.7657 |
[32m[20221214 00:09:49 @agent_ppo2.py:185][0m |          -0.0203 |          24.7723 |           7.7828 |
[32m[20221214 00:09:49 @agent_ppo2.py:185][0m |          -0.0203 |          24.3497 |           7.7352 |
[32m[20221214 00:09:49 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:09:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 444.67
[32m[20221214 00:09:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 556.70
[32m[20221214 00:09:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 495.90
[32m[20221214 00:09:49 @agent_ppo2.py:143][0m Total time:      12.32 min
[32m[20221214 00:09:49 @agent_ppo2.py:145][0m 1138688 total steps have happened
[32m[20221214 00:09:49 @agent_ppo2.py:121][0m #------------------------ Iteration 4556 --------------------------#
[32m[20221214 00:09:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:09:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:09:50 @agent_ppo2.py:185][0m |           0.0154 |          31.7936 |           7.6459 |
[32m[20221214 00:09:50 @agent_ppo2.py:185][0m |          -0.0033 |          23.0482 |           7.5917 |
[32m[20221214 00:09:50 @agent_ppo2.py:185][0m |          -0.0096 |          21.8199 |           7.7149 |
[32m[20221214 00:09:50 @agent_ppo2.py:185][0m |          -0.0109 |          21.2550 |           7.7584 |
[32m[20221214 00:09:50 @agent_ppo2.py:185][0m |          -0.0147 |          20.6558 |           7.7936 |
[32m[20221214 00:09:50 @agent_ppo2.py:185][0m |          -0.0194 |          20.2856 |           7.8351 |
[32m[20221214 00:09:50 @agent_ppo2.py:185][0m |          -0.0173 |          19.8832 |           7.8217 |
[32m[20221214 00:09:50 @agent_ppo2.py:185][0m |          -0.0176 |          19.5366 |           7.8832 |
[32m[20221214 00:09:50 @agent_ppo2.py:185][0m |          -0.0200 |          19.4337 |           7.8636 |
[32m[20221214 00:09:51 @agent_ppo2.py:185][0m |          -0.0247 |          19.1539 |           7.8855 |
[32m[20221214 00:09:51 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:09:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 416.63
[32m[20221214 00:09:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 489.27
[32m[20221214 00:09:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 548.05
[32m[20221214 00:09:51 @agent_ppo2.py:143][0m Total time:      12.34 min
[32m[20221214 00:09:51 @agent_ppo2.py:145][0m 1140736 total steps have happened
[32m[20221214 00:09:51 @agent_ppo2.py:121][0m #------------------------ Iteration 4557 --------------------------#
[32m[20221214 00:09:51 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:09:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:09:51 @agent_ppo2.py:185][0m |          -0.0063 |          46.9740 |           7.6003 |
[32m[20221214 00:09:51 @agent_ppo2.py:185][0m |          -0.0101 |          41.2767 |           7.6234 |
[32m[20221214 00:09:51 @agent_ppo2.py:185][0m |          -0.0043 |          40.9815 |           7.6222 |
[32m[20221214 00:09:51 @agent_ppo2.py:185][0m |          -0.0119 |          38.8381 |           7.5866 |
[32m[20221214 00:09:51 @agent_ppo2.py:185][0m |          -0.0139 |          37.8046 |           7.5253 |
[32m[20221214 00:09:51 @agent_ppo2.py:185][0m |          -0.0158 |          37.0013 |           7.5461 |
[32m[20221214 00:09:52 @agent_ppo2.py:185][0m |          -0.0156 |          36.2606 |           7.5336 |
[32m[20221214 00:09:52 @agent_ppo2.py:185][0m |          -0.0124 |          36.8650 |           7.4672 |
[32m[20221214 00:09:52 @agent_ppo2.py:185][0m |          -0.0209 |          35.7377 |           7.4392 |
[32m[20221214 00:09:52 @agent_ppo2.py:185][0m |          -0.0185 |          35.1223 |           7.4158 |
[32m[20221214 00:09:52 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:09:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 463.84
[32m[20221214 00:09:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 517.80
[32m[20221214 00:09:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 477.80
[32m[20221214 00:09:52 @agent_ppo2.py:143][0m Total time:      12.36 min
[32m[20221214 00:09:52 @agent_ppo2.py:145][0m 1142784 total steps have happened
[32m[20221214 00:09:52 @agent_ppo2.py:121][0m #------------------------ Iteration 4558 --------------------------#
[32m[20221214 00:09:52 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:09:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:09:52 @agent_ppo2.py:185][0m |          -0.0025 |          46.4975 |           7.3070 |
[32m[20221214 00:09:52 @agent_ppo2.py:185][0m |           0.0047 |          42.3741 |           7.2499 |
[32m[20221214 00:09:53 @agent_ppo2.py:185][0m |          -0.0051 |          38.2971 |           7.2764 |
[32m[20221214 00:09:53 @agent_ppo2.py:185][0m |           0.0040 |          42.3314 |           7.2157 |
[32m[20221214 00:09:53 @agent_ppo2.py:185][0m |          -0.0107 |          36.4162 |           7.2646 |
[32m[20221214 00:09:53 @agent_ppo2.py:185][0m |          -0.0146 |          35.0020 |           7.1879 |
[32m[20221214 00:09:53 @agent_ppo2.py:185][0m |          -0.0134 |          34.4720 |           7.1606 |
[32m[20221214 00:09:53 @agent_ppo2.py:185][0m |          -0.0172 |          34.1442 |           7.1480 |
[32m[20221214 00:09:53 @agent_ppo2.py:185][0m |          -0.0163 |          33.6200 |           7.1197 |
[32m[20221214 00:09:53 @agent_ppo2.py:185][0m |          -0.0181 |          33.5685 |           7.0462 |
[32m[20221214 00:09:53 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:09:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 437.23
[32m[20221214 00:09:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 519.83
[32m[20221214 00:09:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 357.40
[32m[20221214 00:09:53 @agent_ppo2.py:143][0m Total time:      12.38 min
[32m[20221214 00:09:53 @agent_ppo2.py:145][0m 1144832 total steps have happened
[32m[20221214 00:09:53 @agent_ppo2.py:121][0m #------------------------ Iteration 4559 --------------------------#
[32m[20221214 00:09:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:09:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:09:54 @agent_ppo2.py:185][0m |           0.0025 |          73.5674 |           7.7090 |
[32m[20221214 00:09:54 @agent_ppo2.py:185][0m |          -0.0035 |          70.0541 |           7.7147 |
[32m[20221214 00:09:54 @agent_ppo2.py:185][0m |          -0.0105 |          69.1914 |           7.6677 |
[32m[20221214 00:09:54 @agent_ppo2.py:185][0m |          -0.0094 |          68.7851 |           7.7177 |
[32m[20221214 00:09:54 @agent_ppo2.py:185][0m |          -0.0129 |          68.4775 |           7.6936 |
[32m[20221214 00:09:54 @agent_ppo2.py:185][0m |          -0.0040 |          69.4886 |           7.6911 |
[32m[20221214 00:09:54 @agent_ppo2.py:185][0m |          -0.0134 |          68.1362 |           7.6518 |
[32m[20221214 00:09:54 @agent_ppo2.py:185][0m |          -0.0160 |          67.9275 |           7.7077 |
[32m[20221214 00:09:54 @agent_ppo2.py:185][0m |          -0.0158 |          67.5403 |           7.6769 |
[32m[20221214 00:09:54 @agent_ppo2.py:185][0m |          -0.0158 |          67.3504 |           7.7402 |
[32m[20221214 00:09:54 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:09:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 543.33
[32m[20221214 00:09:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 576.75
[32m[20221214 00:09:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 501.80
[32m[20221214 00:09:55 @agent_ppo2.py:143][0m Total time:      12.40 min
[32m[20221214 00:09:55 @agent_ppo2.py:145][0m 1146880 total steps have happened
[32m[20221214 00:09:55 @agent_ppo2.py:121][0m #------------------------ Iteration 4560 --------------------------#
[32m[20221214 00:09:55 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 00:09:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:09:55 @agent_ppo2.py:185][0m |           0.0063 |          33.4442 |           7.5124 |
[32m[20221214 00:09:55 @agent_ppo2.py:185][0m |           0.0000 |          29.5202 |           7.6144 |
[32m[20221214 00:09:55 @agent_ppo2.py:185][0m |          -0.0095 |          27.0746 |           7.5931 |
[32m[20221214 00:09:55 @agent_ppo2.py:185][0m |          -0.0070 |          26.1052 |           7.6573 |
[32m[20221214 00:09:56 @agent_ppo2.py:185][0m |          -0.0001 |          25.4460 |           7.7061 |
[32m[20221214 00:09:56 @agent_ppo2.py:185][0m |          -0.0142 |          24.6068 |           7.6868 |
[32m[20221214 00:09:56 @agent_ppo2.py:185][0m |          -0.0132 |          24.7303 |           7.6633 |
[32m[20221214 00:09:56 @agent_ppo2.py:185][0m |          -0.0189 |          23.9728 |           7.6862 |
[32m[20221214 00:09:56 @agent_ppo2.py:185][0m |          -0.0229 |          23.5385 |           7.7528 |
[32m[20221214 00:09:56 @agent_ppo2.py:185][0m |          -0.0176 |          23.2227 |           7.7584 |
[32m[20221214 00:09:56 @agent_ppo2.py:130][0m Policy update time: 1.25 s
[32m[20221214 00:09:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 340.03
[32m[20221214 00:09:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 472.46
[32m[20221214 00:09:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 455.31
[32m[20221214 00:09:56 @agent_ppo2.py:143][0m Total time:      12.43 min
[32m[20221214 00:09:56 @agent_ppo2.py:145][0m 1148928 total steps have happened
[32m[20221214 00:09:56 @agent_ppo2.py:121][0m #------------------------ Iteration 4561 --------------------------#
[32m[20221214 00:09:56 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:09:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:09:57 @agent_ppo2.py:185][0m |          -0.0010 |          73.2321 |           7.3314 |
[32m[20221214 00:09:57 @agent_ppo2.py:185][0m |          -0.0067 |          68.1409 |           7.3599 |
[32m[20221214 00:09:57 @agent_ppo2.py:185][0m |          -0.0144 |          65.8423 |           7.3877 |
[32m[20221214 00:09:57 @agent_ppo2.py:185][0m |          -0.0081 |          64.0037 |           7.4035 |
[32m[20221214 00:09:57 @agent_ppo2.py:185][0m |          -0.0156 |          63.2035 |           7.3901 |
[32m[20221214 00:09:57 @agent_ppo2.py:185][0m |          -0.0165 |          62.8387 |           7.4574 |
[32m[20221214 00:09:57 @agent_ppo2.py:185][0m |          -0.0175 |          62.1929 |           7.4428 |
[32m[20221214 00:09:57 @agent_ppo2.py:185][0m |          -0.0153 |          61.7080 |           7.4262 |
[32m[20221214 00:09:57 @agent_ppo2.py:185][0m |          -0.0147 |          61.2985 |           7.5196 |
[32m[20221214 00:09:57 @agent_ppo2.py:185][0m |          -0.0166 |          60.8899 |           7.4712 |
[32m[20221214 00:09:57 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:09:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 483.18
[32m[20221214 00:09:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 534.41
[32m[20221214 00:09:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 282.17
[32m[20221214 00:09:58 @agent_ppo2.py:143][0m Total time:      12.45 min
[32m[20221214 00:09:58 @agent_ppo2.py:145][0m 1150976 total steps have happened
[32m[20221214 00:09:58 @agent_ppo2.py:121][0m #------------------------ Iteration 4562 --------------------------#
[32m[20221214 00:09:58 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:09:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:09:58 @agent_ppo2.py:185][0m |           0.0007 |          44.5066 |           7.4221 |
[32m[20221214 00:09:58 @agent_ppo2.py:185][0m |          -0.0134 |          36.2779 |           7.3820 |
[32m[20221214 00:09:58 @agent_ppo2.py:185][0m |          -0.0057 |          34.3975 |           7.3869 |
[32m[20221214 00:09:58 @agent_ppo2.py:185][0m |          -0.0189 |          33.2736 |           7.3727 |
[32m[20221214 00:09:58 @agent_ppo2.py:185][0m |          -0.0175 |          32.4259 |           7.3996 |
[32m[20221214 00:09:58 @agent_ppo2.py:185][0m |          -0.0160 |          32.1025 |           7.3772 |
[32m[20221214 00:09:58 @agent_ppo2.py:185][0m |          -0.0211 |          31.6904 |           7.3386 |
[32m[20221214 00:09:59 @agent_ppo2.py:185][0m |          -0.0129 |          31.0841 |           7.4211 |
[32m[20221214 00:09:59 @agent_ppo2.py:185][0m |          -0.0173 |          30.5890 |           7.3961 |
[32m[20221214 00:09:59 @agent_ppo2.py:185][0m |          -0.0195 |          30.2913 |           7.4102 |
[32m[20221214 00:09:59 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:09:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 400.11
[32m[20221214 00:09:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 489.55
[32m[20221214 00:09:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 546.17
[32m[20221214 00:09:59 @agent_ppo2.py:143][0m Total time:      12.47 min
[32m[20221214 00:09:59 @agent_ppo2.py:145][0m 1153024 total steps have happened
[32m[20221214 00:09:59 @agent_ppo2.py:121][0m #------------------------ Iteration 4563 --------------------------#
[32m[20221214 00:09:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:09:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:09:59 @agent_ppo2.py:185][0m |          -0.0018 |          69.9749 |           7.8776 |
[32m[20221214 00:09:59 @agent_ppo2.py:185][0m |          -0.0038 |          66.0305 |           7.8327 |
[32m[20221214 00:09:59 @agent_ppo2.py:185][0m |          -0.0089 |          65.2459 |           7.8825 |
[32m[20221214 00:09:59 @agent_ppo2.py:185][0m |          -0.0088 |          64.6803 |           7.8957 |
[32m[20221214 00:10:00 @agent_ppo2.py:185][0m |          -0.0120 |          64.5931 |           7.9058 |
[32m[20221214 00:10:00 @agent_ppo2.py:185][0m |          -0.0072 |          64.1104 |           7.8656 |
[32m[20221214 00:10:00 @agent_ppo2.py:185][0m |          -0.0040 |          65.0982 |           7.8709 |
[32m[20221214 00:10:00 @agent_ppo2.py:185][0m |          -0.0129 |          63.7825 |           7.8687 |
[32m[20221214 00:10:00 @agent_ppo2.py:185][0m |          -0.0110 |          63.5805 |           7.9025 |
[32m[20221214 00:10:00 @agent_ppo2.py:185][0m |          -0.0114 |          63.2677 |           7.8209 |
[32m[20221214 00:10:00 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:10:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 513.06
[32m[20221214 00:10:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 545.14
[32m[20221214 00:10:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 526.40
[32m[20221214 00:10:00 @agent_ppo2.py:143][0m Total time:      12.50 min
[32m[20221214 00:10:00 @agent_ppo2.py:145][0m 1155072 total steps have happened
[32m[20221214 00:10:00 @agent_ppo2.py:121][0m #------------------------ Iteration 4564 --------------------------#
[32m[20221214 00:10:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:10:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:10:01 @agent_ppo2.py:185][0m |          -0.0017 |          46.6304 |           7.2209 |
[32m[20221214 00:10:01 @agent_ppo2.py:185][0m |          -0.0121 |          40.7593 |           7.2309 |
[32m[20221214 00:10:01 @agent_ppo2.py:185][0m |          -0.0106 |          39.2044 |           7.2140 |
[32m[20221214 00:10:01 @agent_ppo2.py:185][0m |          -0.0137 |          38.4707 |           7.2031 |
[32m[20221214 00:10:01 @agent_ppo2.py:185][0m |          -0.0134 |          37.6748 |           7.2681 |
[32m[20221214 00:10:01 @agent_ppo2.py:185][0m |          -0.0108 |          37.3051 |           7.2289 |
[32m[20221214 00:10:01 @agent_ppo2.py:185][0m |          -0.0188 |          36.9134 |           7.1783 |
[32m[20221214 00:10:01 @agent_ppo2.py:185][0m |          -0.0109 |          37.0932 |           7.1731 |
[32m[20221214 00:10:01 @agent_ppo2.py:185][0m |          -0.0166 |          36.3657 |           7.1358 |
[32m[20221214 00:10:01 @agent_ppo2.py:185][0m |          -0.0155 |          36.0762 |           7.1333 |
[32m[20221214 00:10:01 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:10:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 386.47
[32m[20221214 00:10:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 516.75
[32m[20221214 00:10:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 439.35
[32m[20221214 00:10:01 @agent_ppo2.py:143][0m Total time:      12.52 min
[32m[20221214 00:10:01 @agent_ppo2.py:145][0m 1157120 total steps have happened
[32m[20221214 00:10:01 @agent_ppo2.py:121][0m #------------------------ Iteration 4565 --------------------------#
[32m[20221214 00:10:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:10:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:10:02 @agent_ppo2.py:185][0m |           0.0094 |          28.4434 |           7.2166 |
[32m[20221214 00:10:02 @agent_ppo2.py:185][0m |          -0.0076 |          22.9368 |           7.2025 |
[32m[20221214 00:10:02 @agent_ppo2.py:185][0m |          -0.0076 |          21.4841 |           7.1253 |
[32m[20221214 00:10:02 @agent_ppo2.py:185][0m |          -0.0060 |          20.7342 |           7.1516 |
[32m[20221214 00:10:02 @agent_ppo2.py:185][0m |          -0.0100 |          20.3800 |           7.1009 |
[32m[20221214 00:10:02 @agent_ppo2.py:185][0m |          -0.0108 |          19.6972 |           7.0751 |
[32m[20221214 00:10:02 @agent_ppo2.py:185][0m |          -0.0142 |          19.2542 |           7.0257 |
[32m[20221214 00:10:02 @agent_ppo2.py:185][0m |          -0.0153 |          19.1014 |           7.0366 |
[32m[20221214 00:10:02 @agent_ppo2.py:185][0m |          -0.0201 |          18.7119 |           7.0328 |
[32m[20221214 00:10:03 @agent_ppo2.py:185][0m |          -0.0207 |          18.3482 |           7.0180 |
[32m[20221214 00:10:03 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:10:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 396.62
[32m[20221214 00:10:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 585.29
[32m[20221214 00:10:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 470.43
[32m[20221214 00:10:03 @agent_ppo2.py:143][0m Total time:      12.54 min
[32m[20221214 00:10:03 @agent_ppo2.py:145][0m 1159168 total steps have happened
[32m[20221214 00:10:03 @agent_ppo2.py:121][0m #------------------------ Iteration 4566 --------------------------#
[32m[20221214 00:10:03 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:10:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:10:03 @agent_ppo2.py:185][0m |           0.0098 |          79.8138 |           6.8854 |
[32m[20221214 00:10:03 @agent_ppo2.py:185][0m |           0.0197 |          81.8497 |           6.8551 |
[32m[20221214 00:10:03 @agent_ppo2.py:185][0m |          -0.0045 |          70.6549 |           6.9625 |
[32m[20221214 00:10:03 @agent_ppo2.py:185][0m |          -0.0066 |          69.3788 |           6.8654 |
[32m[20221214 00:10:03 @agent_ppo2.py:185][0m |          -0.0097 |          69.0155 |           6.8398 |
[32m[20221214 00:10:03 @agent_ppo2.py:185][0m |          -0.0083 |          68.4878 |           6.8494 |
[32m[20221214 00:10:04 @agent_ppo2.py:185][0m |          -0.0122 |          67.9690 |           6.7960 |
[32m[20221214 00:10:04 @agent_ppo2.py:185][0m |          -0.0091 |          67.9618 |           6.8052 |
[32m[20221214 00:10:04 @agent_ppo2.py:185][0m |          -0.0114 |          67.5395 |           6.8885 |
[32m[20221214 00:10:04 @agent_ppo2.py:185][0m |          -0.0108 |          67.8499 |           6.8069 |
[32m[20221214 00:10:04 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:10:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 515.91
[32m[20221214 00:10:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 578.49
[32m[20221214 00:10:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 520.83
[32m[20221214 00:10:04 @agent_ppo2.py:143][0m Total time:      12.56 min
[32m[20221214 00:10:04 @agent_ppo2.py:145][0m 1161216 total steps have happened
[32m[20221214 00:10:04 @agent_ppo2.py:121][0m #------------------------ Iteration 4567 --------------------------#
[32m[20221214 00:10:04 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:10:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:10:04 @agent_ppo2.py:185][0m |          -0.0002 |          55.8838 |           6.3155 |
[32m[20221214 00:10:04 @agent_ppo2.py:185][0m |          -0.0039 |          48.5744 |           6.3552 |
[32m[20221214 00:10:05 @agent_ppo2.py:185][0m |          -0.0088 |          46.0863 |           6.3136 |
[32m[20221214 00:10:05 @agent_ppo2.py:185][0m |          -0.0104 |          44.7963 |           6.3435 |
[32m[20221214 00:10:05 @agent_ppo2.py:185][0m |          -0.0147 |          43.8014 |           6.3336 |
[32m[20221214 00:10:05 @agent_ppo2.py:185][0m |          -0.0120 |          43.5413 |           6.3278 |
[32m[20221214 00:10:05 @agent_ppo2.py:185][0m |          -0.0104 |          43.9563 |           6.3030 |
[32m[20221214 00:10:05 @agent_ppo2.py:185][0m |          -0.0093 |          46.8082 |           6.3223 |
[32m[20221214 00:10:05 @agent_ppo2.py:185][0m |          -0.0166 |          41.9512 |           6.3019 |
[32m[20221214 00:10:05 @agent_ppo2.py:185][0m |          -0.0220 |          41.4970 |           6.3634 |
[32m[20221214 00:10:05 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:10:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 410.19
[32m[20221214 00:10:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 499.25
[32m[20221214 00:10:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 490.24
[32m[20221214 00:10:05 @agent_ppo2.py:143][0m Total time:      12.58 min
[32m[20221214 00:10:05 @agent_ppo2.py:145][0m 1163264 total steps have happened
[32m[20221214 00:10:05 @agent_ppo2.py:121][0m #------------------------ Iteration 4568 --------------------------#
[32m[20221214 00:10:06 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:10:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:10:06 @agent_ppo2.py:185][0m |           0.0061 |          29.8366 |           6.6307 |
[32m[20221214 00:10:06 @agent_ppo2.py:185][0m |          -0.0034 |          22.9810 |           6.6774 |
[32m[20221214 00:10:06 @agent_ppo2.py:185][0m |          -0.0090 |          21.1964 |           6.6631 |
[32m[20221214 00:10:06 @agent_ppo2.py:185][0m |          -0.0149 |          20.4160 |           6.6107 |
[32m[20221214 00:10:06 @agent_ppo2.py:185][0m |          -0.0089 |          19.5194 |           6.6542 |
[32m[20221214 00:10:06 @agent_ppo2.py:185][0m |          -0.0063 |          18.6950 |           6.6242 |
[32m[20221214 00:10:06 @agent_ppo2.py:185][0m |          -0.0171 |          18.3484 |           6.6578 |
[32m[20221214 00:10:06 @agent_ppo2.py:185][0m |          -0.0201 |          17.8785 |           6.7141 |
[32m[20221214 00:10:06 @agent_ppo2.py:185][0m |          -0.0217 |          17.5551 |           6.6356 |
[32m[20221214 00:10:06 @agent_ppo2.py:185][0m |          -0.0217 |          17.1959 |           6.6351 |
[32m[20221214 00:10:06 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:10:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 304.23
[32m[20221214 00:10:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 469.32
[32m[20221214 00:10:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 506.61
[32m[20221214 00:10:07 @agent_ppo2.py:143][0m Total time:      12.60 min
[32m[20221214 00:10:07 @agent_ppo2.py:145][0m 1165312 total steps have happened
[32m[20221214 00:10:07 @agent_ppo2.py:121][0m #------------------------ Iteration 4569 --------------------------#
[32m[20221214 00:10:07 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:10:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:10:07 @agent_ppo2.py:185][0m |           0.0168 |          86.0053 |           6.2359 |
[32m[20221214 00:10:07 @agent_ppo2.py:185][0m |           0.0002 |          72.4073 |           6.3879 |
[32m[20221214 00:10:07 @agent_ppo2.py:185][0m |          -0.0109 |          67.9315 |           6.4275 |
[32m[20221214 00:10:07 @agent_ppo2.py:185][0m |          -0.0110 |          67.4473 |           6.4522 |
[32m[20221214 00:10:07 @agent_ppo2.py:185][0m |          -0.0127 |          66.9242 |           6.4578 |
[32m[20221214 00:10:07 @agent_ppo2.py:185][0m |          -0.0173 |          66.6653 |           6.5128 |
[32m[20221214 00:10:08 @agent_ppo2.py:185][0m |          -0.0184 |          66.6405 |           6.5159 |
[32m[20221214 00:10:08 @agent_ppo2.py:185][0m |          -0.0204 |          66.3949 |           6.5061 |
[32m[20221214 00:10:08 @agent_ppo2.py:185][0m |          -0.0189 |          66.2429 |           6.5181 |
[32m[20221214 00:10:08 @agent_ppo2.py:185][0m |          -0.0218 |          66.1853 |           6.6343 |
[32m[20221214 00:10:08 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:10:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 504.64
[32m[20221214 00:10:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 534.88
[32m[20221214 00:10:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 531.60
[32m[20221214 00:10:08 @agent_ppo2.py:143][0m Total time:      12.63 min
[32m[20221214 00:10:08 @agent_ppo2.py:145][0m 1167360 total steps have happened
[32m[20221214 00:10:08 @agent_ppo2.py:121][0m #------------------------ Iteration 4570 --------------------------#
[32m[20221214 00:10:08 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:10:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:10:08 @agent_ppo2.py:185][0m |          -0.0012 |          65.2826 |           7.1976 |
[32m[20221214 00:10:08 @agent_ppo2.py:185][0m |          -0.0011 |          63.2969 |           7.1369 |
[32m[20221214 00:10:09 @agent_ppo2.py:185][0m |           0.0041 |          66.6229 |           7.1965 |
[32m[20221214 00:10:09 @agent_ppo2.py:185][0m |          -0.0050 |          62.2772 |           7.2195 |
[32m[20221214 00:10:09 @agent_ppo2.py:185][0m |          -0.0108 |          61.0033 |           7.1815 |
[32m[20221214 00:10:09 @agent_ppo2.py:185][0m |          -0.0122 |          60.5701 |           7.2430 |
[32m[20221214 00:10:09 @agent_ppo2.py:185][0m |          -0.0138 |          60.3844 |           7.2208 |
[32m[20221214 00:10:09 @agent_ppo2.py:185][0m |          -0.0123 |          59.9706 |           7.2340 |
[32m[20221214 00:10:09 @agent_ppo2.py:185][0m |           0.0004 |          61.8802 |           7.2693 |
[32m[20221214 00:10:09 @agent_ppo2.py:185][0m |          -0.0149 |          59.8208 |           7.2583 |
[32m[20221214 00:10:09 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:10:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 427.99
[32m[20221214 00:10:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 453.51
[32m[20221214 00:10:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 441.86
[32m[20221214 00:10:09 @agent_ppo2.py:143][0m Total time:      12.65 min
[32m[20221214 00:10:09 @agent_ppo2.py:145][0m 1169408 total steps have happened
[32m[20221214 00:10:09 @agent_ppo2.py:121][0m #------------------------ Iteration 4571 --------------------------#
[32m[20221214 00:10:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:10:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:10:10 @agent_ppo2.py:185][0m |           0.0003 |          36.7230 |           7.0993 |
[32m[20221214 00:10:10 @agent_ppo2.py:185][0m |          -0.0042 |          32.4283 |           7.0680 |
[32m[20221214 00:10:10 @agent_ppo2.py:185][0m |          -0.0069 |          31.5941 |           7.1228 |
[32m[20221214 00:10:10 @agent_ppo2.py:185][0m |          -0.0148 |          30.7748 |           7.0833 |
[32m[20221214 00:10:10 @agent_ppo2.py:185][0m |          -0.0125 |          30.4028 |           7.0905 |
[32m[20221214 00:10:10 @agent_ppo2.py:185][0m |          -0.0165 |          29.7066 |           7.0522 |
[32m[20221214 00:10:10 @agent_ppo2.py:185][0m |          -0.0160 |          29.4218 |           7.0221 |
[32m[20221214 00:10:10 @agent_ppo2.py:185][0m |          -0.0130 |          29.0600 |           7.0032 |
[32m[20221214 00:10:10 @agent_ppo2.py:185][0m |          -0.0170 |          28.8664 |           6.9922 |
[32m[20221214 00:10:10 @agent_ppo2.py:185][0m |          -0.0200 |          28.7653 |           6.9013 |
[32m[20221214 00:10:10 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:10:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 417.87
[32m[20221214 00:10:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 507.48
[32m[20221214 00:10:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 290.95
[32m[20221214 00:10:11 @agent_ppo2.py:143][0m Total time:      12.67 min
[32m[20221214 00:10:11 @agent_ppo2.py:145][0m 1171456 total steps have happened
[32m[20221214 00:10:11 @agent_ppo2.py:121][0m #------------------------ Iteration 4572 --------------------------#
[32m[20221214 00:10:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:10:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:10:11 @agent_ppo2.py:185][0m |           0.0011 |          48.3295 |           6.8982 |
[32m[20221214 00:10:11 @agent_ppo2.py:185][0m |          -0.0082 |          45.8856 |           6.9439 |
[32m[20221214 00:10:11 @agent_ppo2.py:185][0m |          -0.0066 |          44.6171 |           6.9482 |
[32m[20221214 00:10:11 @agent_ppo2.py:185][0m |          -0.0081 |          43.5614 |           6.9421 |
[32m[20221214 00:10:11 @agent_ppo2.py:185][0m |          -0.0124 |          42.8720 |           6.9492 |
[32m[20221214 00:10:11 @agent_ppo2.py:185][0m |          -0.0142 |          42.4101 |           6.9577 |
[32m[20221214 00:10:12 @agent_ppo2.py:185][0m |          -0.0097 |          42.1388 |           6.9196 |
[32m[20221214 00:10:12 @agent_ppo2.py:185][0m |          -0.0167 |          41.7700 |           6.8746 |
[32m[20221214 00:10:12 @agent_ppo2.py:185][0m |          -0.0092 |          43.1808 |           6.8973 |
[32m[20221214 00:10:12 @agent_ppo2.py:185][0m |          -0.0171 |          41.1559 |           6.8400 |
[32m[20221214 00:10:12 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:10:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 408.11
[32m[20221214 00:10:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 494.71
[32m[20221214 00:10:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221214 00:10:12 @agent_ppo2.py:143][0m Total time:      12.69 min
[32m[20221214 00:10:12 @agent_ppo2.py:145][0m 1173504 total steps have happened
[32m[20221214 00:10:12 @agent_ppo2.py:121][0m #------------------------ Iteration 4573 --------------------------#
[32m[20221214 00:10:12 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:10:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:10:12 @agent_ppo2.py:185][0m |           0.0046 |          94.7905 |           5.7657 |
[32m[20221214 00:10:12 @agent_ppo2.py:185][0m |          -0.0013 |          92.4117 |           5.6829 |
[32m[20221214 00:10:13 @agent_ppo2.py:185][0m |          -0.0042 |          90.8174 |           5.5943 |
[32m[20221214 00:10:13 @agent_ppo2.py:185][0m |          -0.0045 |          90.5799 |           5.5711 |
[32m[20221214 00:10:13 @agent_ppo2.py:185][0m |          -0.0003 |          90.9646 |           5.5173 |
[32m[20221214 00:10:13 @agent_ppo2.py:185][0m |          -0.0061 |          89.8145 |           5.4802 |
[32m[20221214 00:10:13 @agent_ppo2.py:185][0m |          -0.0070 |          89.8998 |           5.4496 |
[32m[20221214 00:10:13 @agent_ppo2.py:185][0m |          -0.0066 |          89.3830 |           5.3886 |
[32m[20221214 00:10:13 @agent_ppo2.py:185][0m |          -0.0079 |          89.4255 |           5.3446 |
[32m[20221214 00:10:13 @agent_ppo2.py:185][0m |          -0.0058 |          90.8324 |           5.2479 |
[32m[20221214 00:10:13 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:10:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 505.68
[32m[20221214 00:10:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 530.99
[32m[20221214 00:10:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 569.58
[32m[20221214 00:10:13 @agent_ppo2.py:143][0m Total time:      12.72 min
[32m[20221214 00:10:13 @agent_ppo2.py:145][0m 1175552 total steps have happened
[32m[20221214 00:10:13 @agent_ppo2.py:121][0m #------------------------ Iteration 4574 --------------------------#
[32m[20221214 00:10:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:10:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:10:14 @agent_ppo2.py:185][0m |          -0.0003 |          85.7476 |           6.1077 |
[32m[20221214 00:10:14 @agent_ppo2.py:185][0m |          -0.0100 |          83.2141 |           6.1386 |
[32m[20221214 00:10:14 @agent_ppo2.py:185][0m |          -0.0097 |          82.1603 |           6.1574 |
[32m[20221214 00:10:14 @agent_ppo2.py:185][0m |          -0.0099 |          82.2730 |           6.1287 |
[32m[20221214 00:10:14 @agent_ppo2.py:185][0m |          -0.0133 |          81.5421 |           6.1917 |
[32m[20221214 00:10:14 @agent_ppo2.py:185][0m |          -0.0113 |          81.2337 |           6.2797 |
[32m[20221214 00:10:14 @agent_ppo2.py:185][0m |          -0.0129 |          80.7939 |           6.1783 |
[32m[20221214 00:10:14 @agent_ppo2.py:185][0m |          -0.0166 |          80.8233 |           6.1733 |
[32m[20221214 00:10:14 @agent_ppo2.py:185][0m |          -0.0164 |          80.7581 |           6.2637 |
[32m[20221214 00:10:14 @agent_ppo2.py:185][0m |          -0.0155 |          80.2931 |           6.2509 |
[32m[20221214 00:10:14 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:10:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 422.97
[32m[20221214 00:10:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 461.53
[32m[20221214 00:10:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 540.19
[32m[20221214 00:10:15 @agent_ppo2.py:143][0m Total time:      12.74 min
[32m[20221214 00:10:15 @agent_ppo2.py:145][0m 1177600 total steps have happened
[32m[20221214 00:10:15 @agent_ppo2.py:121][0m #------------------------ Iteration 4575 --------------------------#
[32m[20221214 00:10:15 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:10:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:10:15 @agent_ppo2.py:185][0m |          -0.0026 |          89.7084 |           6.5015 |
[32m[20221214 00:10:15 @agent_ppo2.py:185][0m |          -0.0083 |          86.7878 |           6.4459 |
[32m[20221214 00:10:15 @agent_ppo2.py:185][0m |          -0.0031 |          91.5022 |           6.5267 |
[32m[20221214 00:10:15 @agent_ppo2.py:185][0m |          -0.0130 |          84.9436 |           6.5284 |
[32m[20221214 00:10:15 @agent_ppo2.py:185][0m |          -0.0114 |          84.3217 |           6.5202 |
[32m[20221214 00:10:15 @agent_ppo2.py:185][0m |          -0.0175 |          83.5988 |           6.4244 |
[32m[20221214 00:10:15 @agent_ppo2.py:185][0m |          -0.0160 |          83.1507 |           6.4573 |
[32m[20221214 00:10:16 @agent_ppo2.py:185][0m |          -0.0170 |          82.3205 |           6.4735 |
[32m[20221214 00:10:16 @agent_ppo2.py:185][0m |          -0.0202 |          81.6056 |           6.4930 |
[32m[20221214 00:10:16 @agent_ppo2.py:185][0m |          -0.0198 |          80.7727 |           6.5245 |
[32m[20221214 00:10:16 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:10:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 400.26
[32m[20221214 00:10:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 460.34
[32m[20221214 00:10:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 489.16
[32m[20221214 00:10:16 @agent_ppo2.py:143][0m Total time:      12.76 min
[32m[20221214 00:10:16 @agent_ppo2.py:145][0m 1179648 total steps have happened
[32m[20221214 00:10:16 @agent_ppo2.py:121][0m #------------------------ Iteration 4576 --------------------------#
[32m[20221214 00:10:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:10:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:10:16 @agent_ppo2.py:185][0m |           0.0016 |          55.3466 |           5.7934 |
[32m[20221214 00:10:16 @agent_ppo2.py:185][0m |          -0.0023 |          50.5502 |           5.8395 |
[32m[20221214 00:10:16 @agent_ppo2.py:185][0m |          -0.0092 |          49.1211 |           5.8470 |
[32m[20221214 00:10:16 @agent_ppo2.py:185][0m |          -0.0057 |          48.2260 |           5.8723 |
[32m[20221214 00:10:17 @agent_ppo2.py:185][0m |          -0.0073 |          47.7667 |           5.8799 |
[32m[20221214 00:10:17 @agent_ppo2.py:185][0m |          -0.0079 |          47.2619 |           5.9358 |
[32m[20221214 00:10:17 @agent_ppo2.py:185][0m |          -0.0093 |          47.1580 |           5.9238 |
[32m[20221214 00:10:17 @agent_ppo2.py:185][0m |           0.0016 |          49.3512 |           5.8862 |
[32m[20221214 00:10:17 @agent_ppo2.py:185][0m |          -0.0056 |          47.0292 |           5.9115 |
[32m[20221214 00:10:17 @agent_ppo2.py:185][0m |          -0.0078 |          46.5099 |           5.9198 |
[32m[20221214 00:10:17 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:10:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 534.20
[32m[20221214 00:10:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 585.55
[32m[20221214 00:10:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 521.46
[32m[20221214 00:10:17 @agent_ppo2.py:143][0m Total time:      12.78 min
[32m[20221214 00:10:17 @agent_ppo2.py:145][0m 1181696 total steps have happened
[32m[20221214 00:10:17 @agent_ppo2.py:121][0m #------------------------ Iteration 4577 --------------------------#
[32m[20221214 00:10:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:10:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:10:18 @agent_ppo2.py:185][0m |           0.0001 |          64.6002 |           6.6353 |
[32m[20221214 00:10:18 @agent_ppo2.py:185][0m |          -0.0068 |          54.1701 |           6.7483 |
[32m[20221214 00:10:18 @agent_ppo2.py:185][0m |          -0.0093 |          52.0520 |           6.7451 |
[32m[20221214 00:10:18 @agent_ppo2.py:185][0m |          -0.0121 |          51.2863 |           6.7596 |
[32m[20221214 00:10:18 @agent_ppo2.py:185][0m |          -0.0127 |          50.4308 |           6.8434 |
[32m[20221214 00:10:18 @agent_ppo2.py:185][0m |           0.0027 |          53.8959 |           6.8109 |
[32m[20221214 00:10:18 @agent_ppo2.py:185][0m |          -0.0053 |          51.7137 |           6.8923 |
[32m[20221214 00:10:18 @agent_ppo2.py:185][0m |          -0.0138 |          49.1660 |           6.9366 |
[32m[20221214 00:10:18 @agent_ppo2.py:185][0m |          -0.0097 |          50.3803 |           6.9422 |
[32m[20221214 00:10:18 @agent_ppo2.py:185][0m |          -0.0165 |          48.5750 |           7.0120 |
[32m[20221214 00:10:18 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:10:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 476.80
[32m[20221214 00:10:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 527.97
[32m[20221214 00:10:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 365.68
[32m[20221214 00:10:19 @agent_ppo2.py:143][0m Total time:      12.80 min
[32m[20221214 00:10:19 @agent_ppo2.py:145][0m 1183744 total steps have happened
[32m[20221214 00:10:19 @agent_ppo2.py:121][0m #------------------------ Iteration 4578 --------------------------#
[32m[20221214 00:10:19 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:10:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:10:19 @agent_ppo2.py:185][0m |           0.0076 |          39.5268 |           5.7105 |
[32m[20221214 00:10:19 @agent_ppo2.py:185][0m |           0.0008 |          27.8393 |           5.6223 |
[32m[20221214 00:10:19 @agent_ppo2.py:185][0m |          -0.0078 |          25.3248 |           5.6717 |
[32m[20221214 00:10:19 @agent_ppo2.py:185][0m |          -0.0147 |          24.7797 |           5.6882 |
[32m[20221214 00:10:19 @agent_ppo2.py:185][0m |          -0.0125 |          23.0510 |           5.7264 |
[32m[20221214 00:10:19 @agent_ppo2.py:185][0m |          -0.0219 |          22.5892 |           5.7893 |
[32m[20221214 00:10:19 @agent_ppo2.py:185][0m |          -0.0149 |          22.1543 |           5.7875 |
[32m[20221214 00:10:20 @agent_ppo2.py:185][0m |          -0.0121 |          21.6403 |           5.8449 |
[32m[20221214 00:10:20 @agent_ppo2.py:185][0m |          -0.0175 |          21.2059 |           5.8357 |
[32m[20221214 00:10:20 @agent_ppo2.py:185][0m |          -0.0138 |          20.8490 |           5.8621 |
[32m[20221214 00:10:20 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:10:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 367.23
[32m[20221214 00:10:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 451.38
[32m[20221214 00:10:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 523.33
[32m[20221214 00:10:20 @agent_ppo2.py:143][0m Total time:      12.83 min
[32m[20221214 00:10:20 @agent_ppo2.py:145][0m 1185792 total steps have happened
[32m[20221214 00:10:20 @agent_ppo2.py:121][0m #------------------------ Iteration 4579 --------------------------#
[32m[20221214 00:10:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:10:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:10:20 @agent_ppo2.py:185][0m |          -0.0005 |          83.0821 |           7.1369 |
[32m[20221214 00:10:20 @agent_ppo2.py:185][0m |          -0.0013 |          80.4421 |           7.1644 |
[32m[20221214 00:10:20 @agent_ppo2.py:185][0m |          -0.0022 |          80.4365 |           7.2059 |
[32m[20221214 00:10:21 @agent_ppo2.py:185][0m |          -0.0058 |          77.3332 |           7.2287 |
[32m[20221214 00:10:21 @agent_ppo2.py:185][0m |          -0.0104 |          76.3547 |           7.2069 |
[32m[20221214 00:10:21 @agent_ppo2.py:185][0m |          -0.0104 |          75.9344 |           7.2267 |
[32m[20221214 00:10:21 @agent_ppo2.py:185][0m |          -0.0105 |          75.5616 |           7.2601 |
[32m[20221214 00:10:21 @agent_ppo2.py:185][0m |          -0.0130 |          75.5784 |           7.2505 |
[32m[20221214 00:10:21 @agent_ppo2.py:185][0m |          -0.0143 |          75.0609 |           7.2485 |
[32m[20221214 00:10:21 @agent_ppo2.py:185][0m |          -0.0114 |          74.7440 |           7.2689 |
[32m[20221214 00:10:21 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:10:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 486.82
[32m[20221214 00:10:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 500.85
[32m[20221214 00:10:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 264.00
[32m[20221214 00:10:21 @agent_ppo2.py:143][0m Total time:      12.85 min
[32m[20221214 00:10:21 @agent_ppo2.py:145][0m 1187840 total steps have happened
[32m[20221214 00:10:21 @agent_ppo2.py:121][0m #------------------------ Iteration 4580 --------------------------#
[32m[20221214 00:10:21 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:10:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:10:22 @agent_ppo2.py:185][0m |           0.0052 |          57.4356 |           7.0272 |
[32m[20221214 00:10:22 @agent_ppo2.py:185][0m |          -0.0016 |          48.8665 |           7.1613 |
[32m[20221214 00:10:22 @agent_ppo2.py:185][0m |          -0.0119 |          45.8278 |           7.2008 |
[32m[20221214 00:10:22 @agent_ppo2.py:185][0m |          -0.0128 |          44.4900 |           7.1310 |
[32m[20221214 00:10:22 @agent_ppo2.py:185][0m |          -0.0144 |          43.3755 |           7.1575 |
[32m[20221214 00:10:22 @agent_ppo2.py:185][0m |          -0.0109 |          43.9427 |           7.1247 |
[32m[20221214 00:10:22 @agent_ppo2.py:185][0m |          -0.0159 |          41.9625 |           7.1519 |
[32m[20221214 00:10:22 @agent_ppo2.py:185][0m |          -0.0128 |          42.1969 |           7.1954 |
[32m[20221214 00:10:22 @agent_ppo2.py:185][0m |          -0.0192 |          41.1237 |           7.2170 |
[32m[20221214 00:10:23 @agent_ppo2.py:185][0m |          -0.0258 |          40.5536 |           7.2256 |
[32m[20221214 00:10:23 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 00:10:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 377.35
[32m[20221214 00:10:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 439.28
[32m[20221214 00:10:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 286.59
[32m[20221214 00:10:23 @agent_ppo2.py:143][0m Total time:      12.87 min
[32m[20221214 00:10:23 @agent_ppo2.py:145][0m 1189888 total steps have happened
[32m[20221214 00:10:23 @agent_ppo2.py:121][0m #------------------------ Iteration 4581 --------------------------#
[32m[20221214 00:10:23 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:10:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:10:23 @agent_ppo2.py:185][0m |           0.0046 |          34.0821 |           6.8050 |
[32m[20221214 00:10:23 @agent_ppo2.py:185][0m |          -0.0125 |          28.5845 |           6.8476 |
[32m[20221214 00:10:23 @agent_ppo2.py:185][0m |          -0.0123 |          26.8484 |           6.8978 |
[32m[20221214 00:10:23 @agent_ppo2.py:185][0m |          -0.0087 |          25.7848 |           6.8183 |
[32m[20221214 00:10:23 @agent_ppo2.py:185][0m |          -0.0083 |          24.9916 |           6.8331 |
[32m[20221214 00:10:23 @agent_ppo2.py:185][0m |          -0.0146 |          24.4253 |           6.8339 |
[32m[20221214 00:10:24 @agent_ppo2.py:185][0m |          -0.0184 |          23.9982 |           6.8774 |
[32m[20221214 00:10:24 @agent_ppo2.py:185][0m |          -0.0189 |          23.5699 |           6.8777 |
[32m[20221214 00:10:24 @agent_ppo2.py:185][0m |          -0.0221 |          23.0942 |           6.8600 |
[32m[20221214 00:10:24 @agent_ppo2.py:185][0m |          -0.0259 |          23.0566 |           6.8422 |
[32m[20221214 00:10:24 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:10:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 349.50
[32m[20221214 00:10:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 508.03
[32m[20221214 00:10:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 551.77
[32m[20221214 00:10:24 @agent_ppo2.py:143][0m Total time:      12.89 min
[32m[20221214 00:10:24 @agent_ppo2.py:145][0m 1191936 total steps have happened
[32m[20221214 00:10:24 @agent_ppo2.py:121][0m #------------------------ Iteration 4582 --------------------------#
[32m[20221214 00:10:24 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:10:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:10:24 @agent_ppo2.py:185][0m |           0.0021 |          18.4162 |           6.6517 |
[32m[20221214 00:10:24 @agent_ppo2.py:185][0m |          -0.0038 |          14.5456 |           6.5880 |
[32m[20221214 00:10:24 @agent_ppo2.py:185][0m |          -0.0110 |          13.6238 |           6.5363 |
[32m[20221214 00:10:25 @agent_ppo2.py:185][0m |          -0.0111 |          12.8665 |           6.5497 |
[32m[20221214 00:10:25 @agent_ppo2.py:185][0m |          -0.0135 |          12.2100 |           6.4656 |
[32m[20221214 00:10:25 @agent_ppo2.py:185][0m |          -0.0181 |          12.0944 |           6.4656 |
[32m[20221214 00:10:25 @agent_ppo2.py:185][0m |          -0.0193 |          11.5303 |           6.4281 |
[32m[20221214 00:10:25 @agent_ppo2.py:185][0m |          -0.0187 |          11.2490 |           6.4220 |
[32m[20221214 00:10:25 @agent_ppo2.py:185][0m |          -0.0208 |          11.1100 |           6.3081 |
[32m[20221214 00:10:25 @agent_ppo2.py:185][0m |          -0.0224 |          10.8057 |           6.3832 |
[32m[20221214 00:10:25 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:10:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 343.17
[32m[20221214 00:10:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 477.70
[32m[20221214 00:10:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 510.05
[32m[20221214 00:10:25 @agent_ppo2.py:143][0m Total time:      12.92 min
[32m[20221214 00:10:25 @agent_ppo2.py:145][0m 1193984 total steps have happened
[32m[20221214 00:10:25 @agent_ppo2.py:121][0m #------------------------ Iteration 4583 --------------------------#
[32m[20221214 00:10:25 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:10:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:10:26 @agent_ppo2.py:185][0m |          -0.0003 |          55.7316 |           6.3801 |
[32m[20221214 00:10:26 @agent_ppo2.py:185][0m |          -0.0089 |          50.4252 |           6.3755 |
[32m[20221214 00:10:26 @agent_ppo2.py:185][0m |          -0.0072 |          49.1373 |           6.3592 |
[32m[20221214 00:10:26 @agent_ppo2.py:185][0m |          -0.0134 |          48.3465 |           6.3152 |
[32m[20221214 00:10:26 @agent_ppo2.py:185][0m |          -0.0142 |          47.7013 |           6.3262 |
[32m[20221214 00:10:26 @agent_ppo2.py:185][0m |          -0.0197 |          47.3335 |           6.2539 |
[32m[20221214 00:10:26 @agent_ppo2.py:185][0m |          -0.0163 |          47.2075 |           6.2658 |
[32m[20221214 00:10:26 @agent_ppo2.py:185][0m |          -0.0177 |          46.5667 |           6.2672 |
[32m[20221214 00:10:26 @agent_ppo2.py:185][0m |          -0.0201 |          46.2957 |           6.2128 |
[32m[20221214 00:10:26 @agent_ppo2.py:185][0m |          -0.0168 |          46.1066 |           6.1913 |
[32m[20221214 00:10:26 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:10:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 452.96
[32m[20221214 00:10:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 512.09
[32m[20221214 00:10:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 515.08
[32m[20221214 00:10:27 @agent_ppo2.py:143][0m Total time:      12.94 min
[32m[20221214 00:10:27 @agent_ppo2.py:145][0m 1196032 total steps have happened
[32m[20221214 00:10:27 @agent_ppo2.py:121][0m #------------------------ Iteration 4584 --------------------------#
[32m[20221214 00:10:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:10:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:10:27 @agent_ppo2.py:185][0m |           0.0140 |          53.3379 |           5.8190 |
[32m[20221214 00:10:27 @agent_ppo2.py:185][0m |          -0.0063 |          46.3434 |           5.9382 |
[32m[20221214 00:10:27 @agent_ppo2.py:185][0m |          -0.0075 |          44.2668 |           5.9683 |
[32m[20221214 00:10:27 @agent_ppo2.py:185][0m |          -0.0136 |          43.1035 |           5.9148 |
[32m[20221214 00:10:27 @agent_ppo2.py:185][0m |          -0.0066 |          42.4020 |           6.0210 |
[32m[20221214 00:10:27 @agent_ppo2.py:185][0m |          -0.0034 |          46.0150 |           6.0358 |
[32m[20221214 00:10:28 @agent_ppo2.py:185][0m |          -0.0166 |          41.4701 |           5.9798 |
[32m[20221214 00:10:28 @agent_ppo2.py:185][0m |          -0.0160 |          41.0502 |           6.0226 |
[32m[20221214 00:10:28 @agent_ppo2.py:185][0m |          -0.0168 |          41.0208 |           6.0132 |
[32m[20221214 00:10:28 @agent_ppo2.py:185][0m |          -0.0135 |          40.9545 |           6.0170 |
[32m[20221214 00:10:28 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:10:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 366.06
[32m[20221214 00:10:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 542.54
[32m[20221214 00:10:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 414.74
[32m[20221214 00:10:28 @agent_ppo2.py:143][0m Total time:      12.96 min
[32m[20221214 00:10:28 @agent_ppo2.py:145][0m 1198080 total steps have happened
[32m[20221214 00:10:28 @agent_ppo2.py:121][0m #------------------------ Iteration 4585 --------------------------#
[32m[20221214 00:10:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:10:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:10:28 @agent_ppo2.py:185][0m |           0.0022 |          44.3358 |           6.6125 |
[32m[20221214 00:10:28 @agent_ppo2.py:185][0m |          -0.0101 |          39.0406 |           6.5548 |
[32m[20221214 00:10:28 @agent_ppo2.py:185][0m |          -0.0068 |          37.9485 |           6.6173 |
[32m[20221214 00:10:29 @agent_ppo2.py:185][0m |          -0.0119 |          37.1266 |           6.5330 |
[32m[20221214 00:10:29 @agent_ppo2.py:185][0m |          -0.0128 |          36.5993 |           6.5644 |
[32m[20221214 00:10:29 @agent_ppo2.py:185][0m |          -0.0178 |          36.1962 |           6.5012 |
[32m[20221214 00:10:29 @agent_ppo2.py:185][0m |          -0.0144 |          35.7241 |           6.5269 |
[32m[20221214 00:10:29 @agent_ppo2.py:185][0m |          -0.0154 |          35.6218 |           6.4332 |
[32m[20221214 00:10:29 @agent_ppo2.py:185][0m |          -0.0178 |          35.2845 |           6.4342 |
[32m[20221214 00:10:29 @agent_ppo2.py:185][0m |          -0.0173 |          35.1461 |           6.4346 |
[32m[20221214 00:10:29 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:10:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 376.98
[32m[20221214 00:10:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 472.96
[32m[20221214 00:10:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 513.73
[32m[20221214 00:10:29 @agent_ppo2.py:143][0m Total time:      12.98 min
[32m[20221214 00:10:29 @agent_ppo2.py:145][0m 1200128 total steps have happened
[32m[20221214 00:10:29 @agent_ppo2.py:121][0m #------------------------ Iteration 4586 --------------------------#
[32m[20221214 00:10:29 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:10:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:10:30 @agent_ppo2.py:185][0m |           0.0023 |          75.2117 |           5.6429 |
[32m[20221214 00:10:30 @agent_ppo2.py:185][0m |          -0.0073 |          70.4529 |           5.6347 |
[32m[20221214 00:10:30 @agent_ppo2.py:185][0m |          -0.0121 |          68.5596 |           5.6715 |
[32m[20221214 00:10:30 @agent_ppo2.py:185][0m |          -0.0081 |          68.2649 |           5.5619 |
[32m[20221214 00:10:30 @agent_ppo2.py:185][0m |          -0.0148 |          66.7898 |           5.6537 |
[32m[20221214 00:10:30 @agent_ppo2.py:185][0m |          -0.0105 |          66.3765 |           5.5803 |
[32m[20221214 00:10:30 @agent_ppo2.py:185][0m |          -0.0149 |          65.5016 |           5.5820 |
[32m[20221214 00:10:30 @agent_ppo2.py:185][0m |          -0.0180 |          64.9057 |           5.5810 |
[32m[20221214 00:10:30 @agent_ppo2.py:185][0m |          -0.0149 |          65.2027 |           5.5116 |
[32m[20221214 00:10:30 @agent_ppo2.py:185][0m |          -0.0152 |          64.8059 |           5.5297 |
[32m[20221214 00:10:30 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:10:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 407.74
[32m[20221214 00:10:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 481.74
[32m[20221214 00:10:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 412.01
[32m[20221214 00:10:30 @agent_ppo2.py:143][0m Total time:      13.00 min
[32m[20221214 00:10:30 @agent_ppo2.py:145][0m 1202176 total steps have happened
[32m[20221214 00:10:30 @agent_ppo2.py:121][0m #------------------------ Iteration 4587 --------------------------#
[32m[20221214 00:10:31 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:10:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:10:31 @agent_ppo2.py:185][0m |          -0.0001 |          79.6171 |           5.6333 |
[32m[20221214 00:10:31 @agent_ppo2.py:185][0m |          -0.0040 |          77.1009 |           5.6356 |
[32m[20221214 00:10:31 @agent_ppo2.py:185][0m |          -0.0072 |          76.1713 |           5.6063 |
[32m[20221214 00:10:31 @agent_ppo2.py:185][0m |          -0.0071 |          76.3864 |           5.6468 |
[32m[20221214 00:10:31 @agent_ppo2.py:185][0m |          -0.0069 |          75.5970 |           5.6547 |
[32m[20221214 00:10:31 @agent_ppo2.py:185][0m |          -0.0097 |          75.2465 |           5.6037 |
[32m[20221214 00:10:31 @agent_ppo2.py:185][0m |          -0.0149 |          74.8571 |           5.5493 |
[32m[20221214 00:10:31 @agent_ppo2.py:185][0m |          -0.0124 |          74.8147 |           5.5410 |
[32m[20221214 00:10:31 @agent_ppo2.py:185][0m |          -0.0125 |          74.4838 |           5.4742 |
[32m[20221214 00:10:32 @agent_ppo2.py:185][0m |          -0.0020 |          81.1147 |           5.5189 |
[32m[20221214 00:10:32 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:10:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 489.70
[32m[20221214 00:10:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 517.05
[32m[20221214 00:10:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 523.47
[32m[20221214 00:10:32 @agent_ppo2.py:143][0m Total time:      13.02 min
[32m[20221214 00:10:32 @agent_ppo2.py:145][0m 1204224 total steps have happened
[32m[20221214 00:10:32 @agent_ppo2.py:121][0m #------------------------ Iteration 4588 --------------------------#
[32m[20221214 00:10:32 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:10:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:10:32 @agent_ppo2.py:185][0m |           0.0047 |          51.1316 |           5.1139 |
[32m[20221214 00:10:32 @agent_ppo2.py:185][0m |          -0.0084 |          41.8211 |           5.0351 |
[32m[20221214 00:10:32 @agent_ppo2.py:185][0m |          -0.0117 |          39.9466 |           5.0420 |
[32m[20221214 00:10:32 @agent_ppo2.py:185][0m |          -0.0137 |          38.6501 |           5.0009 |
[32m[20221214 00:10:32 @agent_ppo2.py:185][0m |          -0.0150 |          37.9081 |           4.9898 |
[32m[20221214 00:10:32 @agent_ppo2.py:185][0m |          -0.0128 |          37.6831 |           5.0190 |
[32m[20221214 00:10:33 @agent_ppo2.py:185][0m |          -0.0152 |          36.8579 |           5.0455 |
[32m[20221214 00:10:33 @agent_ppo2.py:185][0m |          -0.0141 |          36.4257 |           5.0290 |
[32m[20221214 00:10:33 @agent_ppo2.py:185][0m |          -0.0174 |          36.0815 |           5.0104 |
[32m[20221214 00:10:33 @agent_ppo2.py:185][0m |          -0.0032 |          41.7023 |           4.9925 |
[32m[20221214 00:10:33 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:10:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 477.82
[32m[20221214 00:10:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 535.09
[32m[20221214 00:10:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 513.88
[32m[20221214 00:10:33 @agent_ppo2.py:143][0m Total time:      13.04 min
[32m[20221214 00:10:33 @agent_ppo2.py:145][0m 1206272 total steps have happened
[32m[20221214 00:10:33 @agent_ppo2.py:121][0m #------------------------ Iteration 4589 --------------------------#
[32m[20221214 00:10:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:10:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:10:33 @agent_ppo2.py:185][0m |          -0.0018 |          84.3393 |           5.5272 |
[32m[20221214 00:10:33 @agent_ppo2.py:185][0m |          -0.0099 |          82.4005 |           5.6561 |
[32m[20221214 00:10:33 @agent_ppo2.py:185][0m |          -0.0079 |          81.4468 |           5.5499 |
[32m[20221214 00:10:34 @agent_ppo2.py:185][0m |           0.0057 |          91.4607 |           5.6475 |
[32m[20221214 00:10:34 @agent_ppo2.py:185][0m |          -0.0115 |          80.6181 |           5.6030 |
[32m[20221214 00:10:34 @agent_ppo2.py:185][0m |          -0.0132 |          80.6374 |           5.6466 |
[32m[20221214 00:10:34 @agent_ppo2.py:185][0m |          -0.0121 |          80.8302 |           5.6181 |
[32m[20221214 00:10:34 @agent_ppo2.py:185][0m |          -0.0104 |          80.3117 |           5.6431 |
[32m[20221214 00:10:34 @agent_ppo2.py:185][0m |          -0.0124 |          80.2593 |           5.6410 |
[32m[20221214 00:10:34 @agent_ppo2.py:185][0m |          -0.0159 |          80.1829 |           5.6702 |
[32m[20221214 00:10:34 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:10:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 501.21
[32m[20221214 00:10:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 549.76
[32m[20221214 00:10:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 532.81
[32m[20221214 00:10:34 @agent_ppo2.py:143][0m Total time:      13.06 min
[32m[20221214 00:10:34 @agent_ppo2.py:145][0m 1208320 total steps have happened
[32m[20221214 00:10:34 @agent_ppo2.py:121][0m #------------------------ Iteration 4590 --------------------------#
[32m[20221214 00:10:34 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:10:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:10:35 @agent_ppo2.py:185][0m |           0.0018 |          67.7901 |           4.7167 |
[32m[20221214 00:10:35 @agent_ppo2.py:185][0m |          -0.0010 |          62.9238 |           4.7630 |
[32m[20221214 00:10:35 @agent_ppo2.py:185][0m |          -0.0027 |          61.6541 |           4.8450 |
[32m[20221214 00:10:35 @agent_ppo2.py:185][0m |          -0.0009 |          61.1307 |           4.9095 |
[32m[20221214 00:10:35 @agent_ppo2.py:185][0m |          -0.0115 |          59.4769 |           4.8405 |
[32m[20221214 00:10:35 @agent_ppo2.py:185][0m |          -0.0079 |          58.8520 |           4.9216 |
[32m[20221214 00:10:35 @agent_ppo2.py:185][0m |          -0.0096 |          58.6119 |           4.8771 |
[32m[20221214 00:10:35 @agent_ppo2.py:185][0m |          -0.0155 |          58.2411 |           4.8902 |
[32m[20221214 00:10:35 @agent_ppo2.py:185][0m |          -0.0149 |          57.7286 |           4.9405 |
[32m[20221214 00:10:35 @agent_ppo2.py:185][0m |          -0.0175 |          57.5933 |           4.9747 |
[32m[20221214 00:10:35 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:10:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 443.14
[32m[20221214 00:10:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 478.05
[32m[20221214 00:10:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 503.20
[32m[20221214 00:10:35 @agent_ppo2.py:143][0m Total time:      13.09 min
[32m[20221214 00:10:35 @agent_ppo2.py:145][0m 1210368 total steps have happened
[32m[20221214 00:10:35 @agent_ppo2.py:121][0m #------------------------ Iteration 4591 --------------------------#
[32m[20221214 00:10:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:10:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:10:36 @agent_ppo2.py:185][0m |          -0.0006 |          70.5957 |           5.3337 |
[32m[20221214 00:10:36 @agent_ppo2.py:185][0m |          -0.0052 |          66.3398 |           5.3245 |
[32m[20221214 00:10:36 @agent_ppo2.py:185][0m |          -0.0098 |          65.4190 |           5.4050 |
[32m[20221214 00:10:36 @agent_ppo2.py:185][0m |          -0.0081 |          64.5311 |           5.5129 |
[32m[20221214 00:10:36 @agent_ppo2.py:185][0m |          -0.0099 |          64.0416 |           5.4842 |
[32m[20221214 00:10:36 @agent_ppo2.py:185][0m |           0.0009 |          68.2860 |           5.5081 |
[32m[20221214 00:10:36 @agent_ppo2.py:185][0m |           0.0063 |          66.4020 |           5.5719 |
[32m[20221214 00:10:36 @agent_ppo2.py:185][0m |          -0.0108 |          62.7177 |           5.6071 |
[32m[20221214 00:10:37 @agent_ppo2.py:185][0m |          -0.0115 |          62.4991 |           5.5897 |
[32m[20221214 00:10:37 @agent_ppo2.py:185][0m |           0.0011 |          67.5385 |           5.5978 |
[32m[20221214 00:10:37 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:10:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 465.49
[32m[20221214 00:10:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 498.93
[32m[20221214 00:10:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 229.86
[32m[20221214 00:10:37 @agent_ppo2.py:143][0m Total time:      13.11 min
[32m[20221214 00:10:37 @agent_ppo2.py:145][0m 1212416 total steps have happened
[32m[20221214 00:10:37 @agent_ppo2.py:121][0m #------------------------ Iteration 4592 --------------------------#
[32m[20221214 00:10:37 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:10:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:10:37 @agent_ppo2.py:185][0m |           0.0024 |          72.5355 |           5.6591 |
[32m[20221214 00:10:37 @agent_ppo2.py:185][0m |          -0.0111 |          68.0678 |           5.6404 |
[32m[20221214 00:10:37 @agent_ppo2.py:185][0m |           0.0012 |          71.1663 |           5.6771 |
[32m[20221214 00:10:37 @agent_ppo2.py:185][0m |          -0.0119 |          65.2737 |           5.6141 |
[32m[20221214 00:10:37 @agent_ppo2.py:185][0m |          -0.0131 |          64.6891 |           5.6158 |
[32m[20221214 00:10:38 @agent_ppo2.py:185][0m |          -0.0164 |          64.3084 |           5.6300 |
[32m[20221214 00:10:38 @agent_ppo2.py:185][0m |          -0.0200 |          64.0719 |           5.6359 |
[32m[20221214 00:10:38 @agent_ppo2.py:185][0m |          -0.0211 |          63.5694 |           5.6650 |
[32m[20221214 00:10:38 @agent_ppo2.py:185][0m |          -0.0169 |          63.1324 |           5.6886 |
[32m[20221214 00:10:38 @agent_ppo2.py:185][0m |          -0.0183 |          62.9698 |           5.7000 |
[32m[20221214 00:10:38 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:10:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 454.64
[32m[20221214 00:10:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 504.62
[32m[20221214 00:10:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 519.86
[32m[20221214 00:10:38 @agent_ppo2.py:143][0m Total time:      13.13 min
[32m[20221214 00:10:38 @agent_ppo2.py:145][0m 1214464 total steps have happened
[32m[20221214 00:10:38 @agent_ppo2.py:121][0m #------------------------ Iteration 4593 --------------------------#
[32m[20221214 00:10:38 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:10:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:10:38 @agent_ppo2.py:185][0m |           0.0050 |          46.6124 |           5.1961 |
[32m[20221214 00:10:38 @agent_ppo2.py:185][0m |           0.0107 |          46.0587 |           5.1361 |
[32m[20221214 00:10:39 @agent_ppo2.py:185][0m |          -0.0063 |          40.3993 |           5.2359 |
[32m[20221214 00:10:39 @agent_ppo2.py:185][0m |          -0.0069 |          38.7712 |           5.2027 |
[32m[20221214 00:10:39 @agent_ppo2.py:185][0m |          -0.0080 |          37.9499 |           5.2040 |
[32m[20221214 00:10:39 @agent_ppo2.py:185][0m |          -0.0126 |          37.1456 |           5.1684 |
[32m[20221214 00:10:39 @agent_ppo2.py:185][0m |          -0.0042 |          36.7688 |           5.1147 |
[32m[20221214 00:10:39 @agent_ppo2.py:185][0m |          -0.0128 |          36.3227 |           5.0964 |
[32m[20221214 00:10:39 @agent_ppo2.py:185][0m |          -0.0091 |          36.0051 |           5.0395 |
[32m[20221214 00:10:39 @agent_ppo2.py:185][0m |          -0.0111 |          35.7613 |           5.0806 |
[32m[20221214 00:10:39 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:10:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 446.26
[32m[20221214 00:10:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 533.54
[32m[20221214 00:10:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 470.94
[32m[20221214 00:10:39 @agent_ppo2.py:143][0m Total time:      13.15 min
[32m[20221214 00:10:39 @agent_ppo2.py:145][0m 1216512 total steps have happened
[32m[20221214 00:10:39 @agent_ppo2.py:121][0m #------------------------ Iteration 4594 --------------------------#
[32m[20221214 00:10:40 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:10:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:10:40 @agent_ppo2.py:185][0m |           0.0004 |          16.6788 |           4.5382 |
[32m[20221214 00:10:40 @agent_ppo2.py:185][0m |           0.0038 |          15.7519 |           4.4494 |
[32m[20221214 00:10:40 @agent_ppo2.py:185][0m |          -0.0009 |          15.4475 |           4.4917 |
[32m[20221214 00:10:40 @agent_ppo2.py:185][0m |           0.0097 |          16.1374 |           4.4290 |
[32m[20221214 00:10:40 @agent_ppo2.py:185][0m |          -0.0036 |          15.3445 |           4.3500 |
[32m[20221214 00:10:40 @agent_ppo2.py:185][0m |          -0.0041 |          15.3264 |           4.4312 |
[32m[20221214 00:10:40 @agent_ppo2.py:185][0m |           0.0065 |          16.0865 |           4.4175 |
[32m[20221214 00:10:40 @agent_ppo2.py:185][0m |          -0.0036 |          15.3061 |           4.4112 |
[32m[20221214 00:10:40 @agent_ppo2.py:185][0m |          -0.0003 |          15.3084 |           4.3788 |
[32m[20221214 00:10:40 @agent_ppo2.py:185][0m |          -0.0019 |          15.2551 |           4.3168 |
[32m[20221214 00:10:40 @agent_ppo2.py:130][0m Policy update time: 0.92 s
[32m[20221214 00:10:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221214 00:10:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221214 00:10:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 495.36
[32m[20221214 00:10:41 @agent_ppo2.py:143][0m Total time:      13.17 min
[32m[20221214 00:10:41 @agent_ppo2.py:145][0m 1218560 total steps have happened
[32m[20221214 00:10:41 @agent_ppo2.py:121][0m #------------------------ Iteration 4595 --------------------------#
[32m[20221214 00:10:41 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:10:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:10:41 @agent_ppo2.py:185][0m |          -0.0004 |          77.0708 |           4.6666 |
[32m[20221214 00:10:41 @agent_ppo2.py:185][0m |          -0.0050 |          73.3888 |           4.6577 |
[32m[20221214 00:10:41 @agent_ppo2.py:185][0m |          -0.0080 |          72.3799 |           4.6163 |
[32m[20221214 00:10:41 @agent_ppo2.py:185][0m |          -0.0064 |          71.9761 |           4.6544 |
[32m[20221214 00:10:41 @agent_ppo2.py:185][0m |          -0.0119 |          70.9289 |           4.6199 |
[32m[20221214 00:10:41 @agent_ppo2.py:185][0m |          -0.0044 |          72.2237 |           4.6856 |
[32m[20221214 00:10:41 @agent_ppo2.py:185][0m |          -0.0121 |          69.8123 |           4.6517 |
[32m[20221214 00:10:42 @agent_ppo2.py:185][0m |          -0.0142 |          69.3118 |           4.5781 |
[32m[20221214 00:10:42 @agent_ppo2.py:185][0m |          -0.0127 |          69.0520 |           4.6623 |
[32m[20221214 00:10:42 @agent_ppo2.py:185][0m |          -0.0155 |          68.7462 |           4.6554 |
[32m[20221214 00:10:42 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:10:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 500.15
[32m[20221214 00:10:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 532.93
[32m[20221214 00:10:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 537.83
[32m[20221214 00:10:42 @agent_ppo2.py:143][0m Total time:      13.19 min
[32m[20221214 00:10:42 @agent_ppo2.py:145][0m 1220608 total steps have happened
[32m[20221214 00:10:42 @agent_ppo2.py:121][0m #------------------------ Iteration 4596 --------------------------#
[32m[20221214 00:10:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:10:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:10:42 @agent_ppo2.py:185][0m |           0.0054 |          85.4244 |           4.6330 |
[32m[20221214 00:10:42 @agent_ppo2.py:185][0m |          -0.0044 |          83.3193 |           4.6895 |
[32m[20221214 00:10:42 @agent_ppo2.py:185][0m |          -0.0028 |          83.0844 |           4.6017 |
[32m[20221214 00:10:42 @agent_ppo2.py:185][0m |          -0.0030 |          82.4385 |           4.7019 |
[32m[20221214 00:10:42 @agent_ppo2.py:185][0m |          -0.0084 |          82.0159 |           4.6456 |
[32m[20221214 00:10:43 @agent_ppo2.py:185][0m |          -0.0052 |          81.6703 |           4.7360 |
[32m[20221214 00:10:43 @agent_ppo2.py:185][0m |          -0.0099 |          81.7138 |           4.7016 |
[32m[20221214 00:10:43 @agent_ppo2.py:185][0m |          -0.0073 |          82.6891 |           4.6117 |
[32m[20221214 00:10:43 @agent_ppo2.py:185][0m |          -0.0119 |          81.1987 |           4.6349 |
[32m[20221214 00:10:43 @agent_ppo2.py:185][0m |          -0.0126 |          80.9836 |           4.5636 |
[32m[20221214 00:10:43 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:10:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 505.44
[32m[20221214 00:10:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 553.91
[32m[20221214 00:10:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 586.61
[32m[20221214 00:10:43 @agent_ppo2.py:143][0m Total time:      13.21 min
[32m[20221214 00:10:43 @agent_ppo2.py:145][0m 1222656 total steps have happened
[32m[20221214 00:10:43 @agent_ppo2.py:121][0m #------------------------ Iteration 4597 --------------------------#
[32m[20221214 00:10:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:10:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:10:43 @agent_ppo2.py:185][0m |           0.0015 |          38.0156 |           4.6863 |
[32m[20221214 00:10:44 @agent_ppo2.py:185][0m |          -0.0069 |          33.0024 |           4.5922 |
[32m[20221214 00:10:44 @agent_ppo2.py:185][0m |          -0.0105 |          31.3333 |           4.6336 |
[32m[20221214 00:10:44 @agent_ppo2.py:185][0m |          -0.0153 |          30.5421 |           4.6142 |
[32m[20221214 00:10:44 @agent_ppo2.py:185][0m |          -0.0128 |          29.9389 |           4.6631 |
[32m[20221214 00:10:44 @agent_ppo2.py:185][0m |          -0.0146 |          29.4553 |           4.6910 |
[32m[20221214 00:10:44 @agent_ppo2.py:185][0m |          -0.0146 |          28.8442 |           4.6709 |
[32m[20221214 00:10:44 @agent_ppo2.py:185][0m |          -0.0194 |          28.4859 |           4.6722 |
[32m[20221214 00:10:44 @agent_ppo2.py:185][0m |          -0.0298 |          28.2442 |           4.6849 |
[32m[20221214 00:10:44 @agent_ppo2.py:185][0m |          -0.0182 |          27.8796 |           4.7283 |
[32m[20221214 00:10:44 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:10:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 381.67
[32m[20221214 00:10:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 528.07
[32m[20221214 00:10:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 552.19
[32m[20221214 00:10:44 @agent_ppo2.py:143][0m Total time:      13.23 min
[32m[20221214 00:10:44 @agent_ppo2.py:145][0m 1224704 total steps have happened
[32m[20221214 00:10:44 @agent_ppo2.py:121][0m #------------------------ Iteration 4598 --------------------------#
[32m[20221214 00:10:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:10:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:10:45 @agent_ppo2.py:185][0m |           0.0054 |          89.0821 |           4.9161 |
[32m[20221214 00:10:45 @agent_ppo2.py:185][0m |          -0.0023 |          86.5434 |           4.8903 |
[32m[20221214 00:10:45 @agent_ppo2.py:185][0m |          -0.0066 |          85.4207 |           4.8456 |
[32m[20221214 00:10:45 @agent_ppo2.py:185][0m |           0.0057 |          94.1418 |           4.9026 |
[32m[20221214 00:10:45 @agent_ppo2.py:185][0m |           0.0122 |          95.0147 |           4.9218 |
[32m[20221214 00:10:45 @agent_ppo2.py:185][0m |          -0.0096 |          85.9147 |           4.9554 |
[32m[20221214 00:10:45 @agent_ppo2.py:185][0m |          -0.0097 |          84.3200 |           4.9459 |
[32m[20221214 00:10:45 @agent_ppo2.py:185][0m |          -0.0126 |          84.2241 |           4.9554 |
[32m[20221214 00:10:45 @agent_ppo2.py:185][0m |          -0.0124 |          83.9666 |           5.0069 |
[32m[20221214 00:10:45 @agent_ppo2.py:185][0m |          -0.0139 |          83.7912 |           4.9740 |
[32m[20221214 00:10:45 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:10:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 489.91
[32m[20221214 00:10:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 538.39
[32m[20221214 00:10:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 507.97
[32m[20221214 00:10:46 @agent_ppo2.py:143][0m Total time:      13.25 min
[32m[20221214 00:10:46 @agent_ppo2.py:145][0m 1226752 total steps have happened
[32m[20221214 00:10:46 @agent_ppo2.py:121][0m #------------------------ Iteration 4599 --------------------------#
[32m[20221214 00:10:46 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:10:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:10:46 @agent_ppo2.py:185][0m |          -0.0011 |          29.0944 |           5.4037 |
[32m[20221214 00:10:46 @agent_ppo2.py:185][0m |          -0.0051 |          25.5494 |           5.4375 |
[32m[20221214 00:10:46 @agent_ppo2.py:185][0m |          -0.0057 |          24.3558 |           5.3950 |
[32m[20221214 00:10:46 @agent_ppo2.py:185][0m |          -0.0071 |          23.6304 |           5.4302 |
[32m[20221214 00:10:46 @agent_ppo2.py:185][0m |          -0.0137 |          23.0687 |           5.3837 |
[32m[20221214 00:10:46 @agent_ppo2.py:185][0m |          -0.0103 |          23.3180 |           5.4359 |
[32m[20221214 00:10:46 @agent_ppo2.py:185][0m |          -0.0169 |          22.4204 |           5.4025 |
[32m[20221214 00:10:47 @agent_ppo2.py:185][0m |          -0.0178 |          22.2673 |           5.3910 |
[32m[20221214 00:10:47 @agent_ppo2.py:185][0m |          -0.0164 |          22.0525 |           5.4267 |
[32m[20221214 00:10:47 @agent_ppo2.py:185][0m |          -0.0181 |          22.2354 |           5.4478 |
[32m[20221214 00:10:47 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:10:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 377.29
[32m[20221214 00:10:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 492.48
[32m[20221214 00:10:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 520.82
[32m[20221214 00:10:47 @agent_ppo2.py:143][0m Total time:      13.27 min
[32m[20221214 00:10:47 @agent_ppo2.py:145][0m 1228800 total steps have happened
[32m[20221214 00:10:47 @agent_ppo2.py:121][0m #------------------------ Iteration 4600 --------------------------#
[32m[20221214 00:10:47 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:10:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:10:47 @agent_ppo2.py:185][0m |           0.0034 |          39.2727 |           4.2084 |
[32m[20221214 00:10:47 @agent_ppo2.py:185][0m |          -0.0034 |          35.5181 |           4.3429 |
[32m[20221214 00:10:47 @agent_ppo2.py:185][0m |          -0.0040 |          33.7156 |           4.2427 |
[32m[20221214 00:10:47 @agent_ppo2.py:185][0m |          -0.0100 |          32.7746 |           4.2170 |
[32m[20221214 00:10:48 @agent_ppo2.py:185][0m |          -0.0071 |          31.8252 |           4.1172 |
[32m[20221214 00:10:48 @agent_ppo2.py:185][0m |          -0.0141 |          31.2133 |           4.1543 |
[32m[20221214 00:10:48 @agent_ppo2.py:185][0m |          -0.0135 |          30.6162 |           4.1165 |
[32m[20221214 00:10:48 @agent_ppo2.py:185][0m |          -0.0138 |          30.6017 |           4.0532 |
[32m[20221214 00:10:48 @agent_ppo2.py:185][0m |          -0.0168 |          29.9229 |           4.0621 |
[32m[20221214 00:10:48 @agent_ppo2.py:185][0m |          -0.0183 |          29.8265 |           4.0135 |
[32m[20221214 00:10:48 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:10:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 390.84
[32m[20221214 00:10:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 484.43
[32m[20221214 00:10:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 587.74
[32m[20221214 00:10:48 @agent_ppo2.py:143][0m Total time:      13.30 min
[32m[20221214 00:10:48 @agent_ppo2.py:145][0m 1230848 total steps have happened
[32m[20221214 00:10:48 @agent_ppo2.py:121][0m #------------------------ Iteration 4601 --------------------------#
[32m[20221214 00:10:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:10:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:10:48 @agent_ppo2.py:185][0m |           0.0014 |          80.0075 |           3.9481 |
[32m[20221214 00:10:49 @agent_ppo2.py:185][0m |          -0.0067 |          76.9540 |           4.0197 |
[32m[20221214 00:10:49 @agent_ppo2.py:185][0m |          -0.0065 |          76.0097 |           3.9883 |
[32m[20221214 00:10:49 @agent_ppo2.py:185][0m |          -0.0085 |          75.4881 |           4.0330 |
[32m[20221214 00:10:49 @agent_ppo2.py:185][0m |          -0.0131 |          74.7324 |           3.9442 |
[32m[20221214 00:10:49 @agent_ppo2.py:185][0m |          -0.0116 |          74.9007 |           4.0206 |
[32m[20221214 00:10:49 @agent_ppo2.py:185][0m |          -0.0118 |          73.8432 |           4.1106 |
[32m[20221214 00:10:49 @agent_ppo2.py:185][0m |          -0.0060 |          78.4979 |           4.0876 |
[32m[20221214 00:10:49 @agent_ppo2.py:185][0m |          -0.0144 |          73.8073 |           4.1029 |
[32m[20221214 00:10:49 @agent_ppo2.py:185][0m |           0.0098 |          83.6069 |           4.1345 |
[32m[20221214 00:10:49 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:10:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 490.99
[32m[20221214 00:10:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 534.49
[32m[20221214 00:10:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 527.39
[32m[20221214 00:10:49 @agent_ppo2.py:143][0m Total time:      13.32 min
[32m[20221214 00:10:49 @agent_ppo2.py:145][0m 1232896 total steps have happened
[32m[20221214 00:10:49 @agent_ppo2.py:121][0m #------------------------ Iteration 4602 --------------------------#
[32m[20221214 00:10:50 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:10:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:10:50 @agent_ppo2.py:185][0m |          -0.0004 |          49.4767 |           4.0914 |
[32m[20221214 00:10:50 @agent_ppo2.py:185][0m |          -0.0054 |          45.2534 |           4.1054 |
[32m[20221214 00:10:50 @agent_ppo2.py:185][0m |          -0.0099 |          42.8239 |           4.1426 |
[32m[20221214 00:10:50 @agent_ppo2.py:185][0m |           0.0004 |          46.1683 |           4.1278 |
[32m[20221214 00:10:50 @agent_ppo2.py:185][0m |          -0.0155 |          40.5736 |           4.0692 |
[32m[20221214 00:10:50 @agent_ppo2.py:185][0m |           0.0011 |          50.2342 |           4.0688 |
[32m[20221214 00:10:50 @agent_ppo2.py:185][0m |          -0.0164 |          39.5453 |           4.0839 |
[32m[20221214 00:10:50 @agent_ppo2.py:185][0m |          -0.0206 |          38.2591 |           4.0739 |
[32m[20221214 00:10:50 @agent_ppo2.py:185][0m |          -0.0225 |          37.4731 |           4.1322 |
[32m[20221214 00:10:50 @agent_ppo2.py:185][0m |          -0.0217 |          37.0036 |           4.0908 |
[32m[20221214 00:10:50 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:10:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 381.95
[32m[20221214 00:10:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 506.89
[32m[20221214 00:10:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 448.81
[32m[20221214 00:10:51 @agent_ppo2.py:143][0m Total time:      13.34 min
[32m[20221214 00:10:51 @agent_ppo2.py:145][0m 1234944 total steps have happened
[32m[20221214 00:10:51 @agent_ppo2.py:121][0m #------------------------ Iteration 4603 --------------------------#
[32m[20221214 00:10:51 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:10:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:10:51 @agent_ppo2.py:185][0m |           0.0075 |          34.2561 |           4.1146 |
[32m[20221214 00:10:51 @agent_ppo2.py:185][0m |          -0.0018 |          29.3865 |           4.0639 |
[32m[20221214 00:10:51 @agent_ppo2.py:185][0m |          -0.0096 |          27.7639 |           4.1291 |
[32m[20221214 00:10:51 @agent_ppo2.py:185][0m |          -0.0096 |          26.8995 |           4.1263 |
[32m[20221214 00:10:51 @agent_ppo2.py:185][0m |          -0.0020 |          30.4999 |           4.1489 |
[32m[20221214 00:10:51 @agent_ppo2.py:185][0m |          -0.0136 |          25.9288 |           4.1456 |
[32m[20221214 00:10:52 @agent_ppo2.py:185][0m |          -0.0170 |          25.6566 |           4.1247 |
[32m[20221214 00:10:52 @agent_ppo2.py:185][0m |          -0.0221 |          25.2906 |           4.1746 |
[32m[20221214 00:10:52 @agent_ppo2.py:185][0m |          -0.0067 |          26.2229 |           4.1324 |
[32m[20221214 00:10:52 @agent_ppo2.py:185][0m |          -0.0208 |          24.6229 |           4.0709 |
[32m[20221214 00:10:52 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:10:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 408.61
[32m[20221214 00:10:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 474.46
[32m[20221214 00:10:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 477.63
[32m[20221214 00:10:52 @agent_ppo2.py:143][0m Total time:      13.36 min
[32m[20221214 00:10:52 @agent_ppo2.py:145][0m 1236992 total steps have happened
[32m[20221214 00:10:52 @agent_ppo2.py:121][0m #------------------------ Iteration 4604 --------------------------#
[32m[20221214 00:10:52 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:10:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:10:52 @agent_ppo2.py:185][0m |           0.0022 |          71.4099 |           4.6732 |
[32m[20221214 00:10:52 @agent_ppo2.py:185][0m |          -0.0080 |          67.4397 |           4.7199 |
[32m[20221214 00:10:52 @agent_ppo2.py:185][0m |          -0.0108 |          66.6664 |           4.6287 |
[32m[20221214 00:10:53 @agent_ppo2.py:185][0m |          -0.0077 |          66.1003 |           4.6711 |
[32m[20221214 00:10:53 @agent_ppo2.py:185][0m |          -0.0118 |          65.9159 |           4.6358 |
[32m[20221214 00:10:53 @agent_ppo2.py:185][0m |          -0.0071 |          65.5717 |           4.6723 |
[32m[20221214 00:10:53 @agent_ppo2.py:185][0m |          -0.0103 |          65.3040 |           4.6742 |
[32m[20221214 00:10:53 @agent_ppo2.py:185][0m |          -0.0097 |          65.0666 |           4.6594 |
[32m[20221214 00:10:53 @agent_ppo2.py:185][0m |          -0.0057 |          68.0749 |           4.6426 |
[32m[20221214 00:10:53 @agent_ppo2.py:185][0m |          -0.0093 |          64.7430 |           4.6500 |
[32m[20221214 00:10:53 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:10:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 496.84
[32m[20221214 00:10:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 510.05
[32m[20221214 00:10:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 489.69
[32m[20221214 00:10:53 @agent_ppo2.py:143][0m Total time:      13.38 min
[32m[20221214 00:10:53 @agent_ppo2.py:145][0m 1239040 total steps have happened
[32m[20221214 00:10:53 @agent_ppo2.py:121][0m #------------------------ Iteration 4605 --------------------------#
[32m[20221214 00:10:53 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:10:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:10:54 @agent_ppo2.py:185][0m |          -0.0012 |          44.7959 |           4.0229 |
[32m[20221214 00:10:54 @agent_ppo2.py:185][0m |          -0.0024 |          39.8962 |           4.1092 |
[32m[20221214 00:10:54 @agent_ppo2.py:185][0m |          -0.0055 |          38.4592 |           4.1029 |
[32m[20221214 00:10:54 @agent_ppo2.py:185][0m |          -0.0071 |          38.0638 |           4.0947 |
[32m[20221214 00:10:54 @agent_ppo2.py:185][0m |          -0.0131 |          36.9161 |           4.0680 |
[32m[20221214 00:10:54 @agent_ppo2.py:185][0m |          -0.0122 |          36.7420 |           4.0650 |
[32m[20221214 00:10:54 @agent_ppo2.py:185][0m |          -0.0119 |          36.1074 |           4.0285 |
[32m[20221214 00:10:54 @agent_ppo2.py:185][0m |          -0.0165 |          36.0289 |           3.8778 |
[32m[20221214 00:10:54 @agent_ppo2.py:185][0m |          -0.0153 |          35.5923 |           3.9052 |
[32m[20221214 00:10:54 @agent_ppo2.py:185][0m |          -0.0168 |          35.3227 |           3.9484 |
[32m[20221214 00:10:54 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:10:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 388.83
[32m[20221214 00:10:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 404.66
[32m[20221214 00:10:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 518.95
[32m[20221214 00:10:55 @agent_ppo2.py:143][0m Total time:      13.40 min
[32m[20221214 00:10:55 @agent_ppo2.py:145][0m 1241088 total steps have happened
[32m[20221214 00:10:55 @agent_ppo2.py:121][0m #------------------------ Iteration 4606 --------------------------#
[32m[20221214 00:10:55 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221214 00:10:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:10:55 @agent_ppo2.py:185][0m |           0.0036 |          72.2332 |           3.8845 |
[32m[20221214 00:10:55 @agent_ppo2.py:185][0m |           0.0106 |          75.9380 |           3.9022 |
[32m[20221214 00:10:55 @agent_ppo2.py:185][0m |          -0.0058 |          68.4021 |           3.9329 |
[32m[20221214 00:10:55 @agent_ppo2.py:185][0m |          -0.0100 |          67.3027 |           3.9960 |
[32m[20221214 00:10:55 @agent_ppo2.py:185][0m |          -0.0101 |          66.6784 |           3.9438 |
[32m[20221214 00:10:55 @agent_ppo2.py:185][0m |          -0.0095 |          66.2077 |           3.9721 |
[32m[20221214 00:10:55 @agent_ppo2.py:185][0m |          -0.0131 |          65.7218 |           3.9249 |
[32m[20221214 00:10:56 @agent_ppo2.py:185][0m |          -0.0118 |          65.5722 |           4.0065 |
[32m[20221214 00:10:56 @agent_ppo2.py:185][0m |          -0.0130 |          65.3491 |           4.0079 |
[32m[20221214 00:10:56 @agent_ppo2.py:185][0m |          -0.0113 |          64.9565 |           3.9782 |
[32m[20221214 00:10:56 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:10:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 469.78
[32m[20221214 00:10:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 482.95
[32m[20221214 00:10:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 397.47
[32m[20221214 00:10:56 @agent_ppo2.py:143][0m Total time:      13.43 min
[32m[20221214 00:10:56 @agent_ppo2.py:145][0m 1243136 total steps have happened
[32m[20221214 00:10:56 @agent_ppo2.py:121][0m #------------------------ Iteration 4607 --------------------------#
[32m[20221214 00:10:56 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:10:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:10:56 @agent_ppo2.py:185][0m |           0.0004 |          81.5882 |           3.7594 |
[32m[20221214 00:10:56 @agent_ppo2.py:185][0m |          -0.0089 |          79.0528 |           3.7310 |
[32m[20221214 00:10:57 @agent_ppo2.py:185][0m |          -0.0074 |          77.9595 |           3.6549 |
[32m[20221214 00:10:57 @agent_ppo2.py:185][0m |          -0.0116 |          77.6518 |           3.6283 |
[32m[20221214 00:10:57 @agent_ppo2.py:185][0m |          -0.0028 |          85.6197 |           3.5459 |
[32m[20221214 00:10:57 @agent_ppo2.py:185][0m |          -0.0042 |          79.2121 |           3.5693 |
[32m[20221214 00:10:57 @agent_ppo2.py:185][0m |          -0.0118 |          76.1540 |           3.6152 |
[32m[20221214 00:10:57 @agent_ppo2.py:185][0m |          -0.0108 |          76.0689 |           3.5668 |
[32m[20221214 00:10:57 @agent_ppo2.py:185][0m |          -0.0169 |          75.7548 |           3.5752 |
[32m[20221214 00:10:57 @agent_ppo2.py:185][0m |          -0.0157 |          75.4439 |           3.5603 |
[32m[20221214 00:10:57 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:10:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 427.12
[32m[20221214 00:10:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 449.44
[32m[20221214 00:10:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 405.81
[32m[20221214 00:10:57 @agent_ppo2.py:143][0m Total time:      13.45 min
[32m[20221214 00:10:57 @agent_ppo2.py:145][0m 1245184 total steps have happened
[32m[20221214 00:10:57 @agent_ppo2.py:121][0m #------------------------ Iteration 4608 --------------------------#
[32m[20221214 00:10:58 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:10:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:10:58 @agent_ppo2.py:185][0m |          -0.0002 |          68.2174 |           4.1965 |
[32m[20221214 00:10:58 @agent_ppo2.py:185][0m |          -0.0072 |          65.4234 |           4.2105 |
[32m[20221214 00:10:58 @agent_ppo2.py:185][0m |          -0.0093 |          64.3307 |           4.1644 |
[32m[20221214 00:10:58 @agent_ppo2.py:185][0m |          -0.0098 |          63.8079 |           4.0808 |
[32m[20221214 00:10:58 @agent_ppo2.py:185][0m |          -0.0141 |          63.4310 |           4.1356 |
[32m[20221214 00:10:59 @agent_ppo2.py:185][0m |          -0.0042 |          64.8231 |           4.1468 |
[32m[20221214 00:10:59 @agent_ppo2.py:185][0m |          -0.0135 |          62.6548 |           4.1438 |
[32m[20221214 00:10:59 @agent_ppo2.py:185][0m |          -0.0144 |          62.4935 |           4.2013 |
[32m[20221214 00:10:59 @agent_ppo2.py:185][0m |          -0.0145 |          62.3404 |           4.1964 |
[32m[20221214 00:10:59 @agent_ppo2.py:185][0m |          -0.0137 |          61.9828 |           4.1804 |
[32m[20221214 00:10:59 @agent_ppo2.py:130][0m Policy update time: 1.36 s
[32m[20221214 00:10:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 443.58
[32m[20221214 00:10:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 487.83
[32m[20221214 00:10:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 491.57
[32m[20221214 00:10:59 @agent_ppo2.py:143][0m Total time:      13.48 min
[32m[20221214 00:10:59 @agent_ppo2.py:145][0m 1247232 total steps have happened
[32m[20221214 00:10:59 @agent_ppo2.py:121][0m #------------------------ Iteration 4609 --------------------------#
[32m[20221214 00:10:59 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:10:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:11:00 @agent_ppo2.py:185][0m |          -0.0011 |          73.9494 |           4.1497 |
[32m[20221214 00:11:00 @agent_ppo2.py:185][0m |          -0.0033 |          70.8447 |           4.3581 |
[32m[20221214 00:11:00 @agent_ppo2.py:185][0m |          -0.0092 |          69.8518 |           4.3535 |
[32m[20221214 00:11:00 @agent_ppo2.py:185][0m |          -0.0090 |          69.2151 |           4.4055 |
[32m[20221214 00:11:00 @agent_ppo2.py:185][0m |          -0.0076 |          68.7214 |           4.5075 |
[32m[20221214 00:11:00 @agent_ppo2.py:185][0m |          -0.0133 |          68.6131 |           4.4771 |
[32m[20221214 00:11:00 @agent_ppo2.py:185][0m |          -0.0166 |          68.4371 |           4.5511 |
[32m[20221214 00:11:00 @agent_ppo2.py:185][0m |          -0.0155 |          68.0086 |           4.6001 |
[32m[20221214 00:11:00 @agent_ppo2.py:185][0m |          -0.0183 |          67.8267 |           4.5807 |
[32m[20221214 00:11:00 @agent_ppo2.py:185][0m |          -0.0157 |          68.1715 |           4.6380 |
[32m[20221214 00:11:00 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:11:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 450.63
[32m[20221214 00:11:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 510.65
[32m[20221214 00:11:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 499.57
[32m[20221214 00:11:01 @agent_ppo2.py:143][0m Total time:      13.50 min
[32m[20221214 00:11:01 @agent_ppo2.py:145][0m 1249280 total steps have happened
[32m[20221214 00:11:01 @agent_ppo2.py:121][0m #------------------------ Iteration 4610 --------------------------#
[32m[20221214 00:11:01 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:11:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:11:01 @agent_ppo2.py:185][0m |           0.0045 |          78.9438 |           4.1483 |
[32m[20221214 00:11:01 @agent_ppo2.py:185][0m |          -0.0069 |          73.7661 |           4.2289 |
[32m[20221214 00:11:01 @agent_ppo2.py:185][0m |          -0.0066 |          74.2284 |           4.1736 |
[32m[20221214 00:11:01 @agent_ppo2.py:185][0m |          -0.0114 |          72.4972 |           4.3067 |
[32m[20221214 00:11:01 @agent_ppo2.py:185][0m |          -0.0064 |          73.7534 |           4.2160 |
[32m[20221214 00:11:01 @agent_ppo2.py:185][0m |          -0.0147 |          72.0772 |           4.1893 |
[32m[20221214 00:11:01 @agent_ppo2.py:185][0m |          -0.0120 |          71.9105 |           4.2149 |
[32m[20221214 00:11:01 @agent_ppo2.py:185][0m |          -0.0168 |          71.7355 |           4.2509 |
[32m[20221214 00:11:02 @agent_ppo2.py:185][0m |          -0.0134 |          72.2022 |           4.2430 |
[32m[20221214 00:11:02 @agent_ppo2.py:185][0m |          -0.0165 |          71.4411 |           4.2747 |
[32m[20221214 00:11:02 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:11:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 496.18
[32m[20221214 00:11:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 522.43
[32m[20221214 00:11:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 498.42
[32m[20221214 00:11:02 @agent_ppo2.py:143][0m Total time:      13.52 min
[32m[20221214 00:11:02 @agent_ppo2.py:145][0m 1251328 total steps have happened
[32m[20221214 00:11:02 @agent_ppo2.py:121][0m #------------------------ Iteration 4611 --------------------------#
[32m[20221214 00:11:02 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:11:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:11:02 @agent_ppo2.py:185][0m |          -0.0011 |          83.2988 |           4.2255 |
[32m[20221214 00:11:02 @agent_ppo2.py:185][0m |          -0.0089 |          79.5875 |           4.2089 |
[32m[20221214 00:11:02 @agent_ppo2.py:185][0m |          -0.0089 |          78.1478 |           4.2149 |
[32m[20221214 00:11:02 @agent_ppo2.py:185][0m |          -0.0118 |          77.2515 |           4.2227 |
[32m[20221214 00:11:02 @agent_ppo2.py:185][0m |          -0.0129 |          76.2696 |           4.2908 |
[32m[20221214 00:11:03 @agent_ppo2.py:185][0m |          -0.0115 |          75.6720 |           4.3323 |
[32m[20221214 00:11:03 @agent_ppo2.py:185][0m |          -0.0144 |          75.2591 |           4.3142 |
[32m[20221214 00:11:03 @agent_ppo2.py:185][0m |          -0.0147 |          74.8147 |           4.3815 |
[32m[20221214 00:11:03 @agent_ppo2.py:185][0m |          -0.0162 |          74.6797 |           4.3875 |
[32m[20221214 00:11:03 @agent_ppo2.py:185][0m |          -0.0163 |          74.1453 |           4.4135 |
[32m[20221214 00:11:03 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:11:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 385.17
[32m[20221214 00:11:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 448.62
[32m[20221214 00:11:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 507.56
[32m[20221214 00:11:03 @agent_ppo2.py:143][0m Total time:      13.55 min
[32m[20221214 00:11:03 @agent_ppo2.py:145][0m 1253376 total steps have happened
[32m[20221214 00:11:03 @agent_ppo2.py:121][0m #------------------------ Iteration 4612 --------------------------#
[32m[20221214 00:11:03 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:11:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:11:03 @agent_ppo2.py:185][0m |           0.0052 |          85.1888 |           5.0160 |
[32m[20221214 00:11:04 @agent_ppo2.py:185][0m |           0.0016 |          76.6242 |           4.9811 |
[32m[20221214 00:11:04 @agent_ppo2.py:185][0m |          -0.0078 |          74.0674 |           4.9863 |
[32m[20221214 00:11:04 @agent_ppo2.py:185][0m |          -0.0069 |          73.0164 |           4.9193 |
[32m[20221214 00:11:04 @agent_ppo2.py:185][0m |          -0.0080 |          71.8920 |           4.8978 |
[32m[20221214 00:11:04 @agent_ppo2.py:185][0m |          -0.0073 |          71.1823 |           4.8703 |
[32m[20221214 00:11:04 @agent_ppo2.py:185][0m |          -0.0123 |          70.8777 |           4.8451 |
[32m[20221214 00:11:04 @agent_ppo2.py:185][0m |          -0.0124 |          69.7433 |           4.8332 |
[32m[20221214 00:11:04 @agent_ppo2.py:185][0m |          -0.0032 |          75.8371 |           4.8079 |
[32m[20221214 00:11:04 @agent_ppo2.py:185][0m |          -0.0122 |          69.2940 |           4.7253 |
[32m[20221214 00:11:04 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:11:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 430.21
[32m[20221214 00:11:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 489.23
[32m[20221214 00:11:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 496.80
[32m[20221214 00:11:04 @agent_ppo2.py:143][0m Total time:      13.57 min
[32m[20221214 00:11:04 @agent_ppo2.py:145][0m 1255424 total steps have happened
[32m[20221214 00:11:04 @agent_ppo2.py:121][0m #------------------------ Iteration 4613 --------------------------#
[32m[20221214 00:11:05 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:11:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:11:05 @agent_ppo2.py:185][0m |           0.0051 |          81.7950 |           3.8938 |
[32m[20221214 00:11:05 @agent_ppo2.py:185][0m |           0.0029 |          73.9987 |           3.9864 |
[32m[20221214 00:11:05 @agent_ppo2.py:185][0m |          -0.0076 |          71.4163 |           3.9878 |
[32m[20221214 00:11:05 @agent_ppo2.py:185][0m |          -0.0086 |          71.0702 |           4.1074 |
[32m[20221214 00:11:05 @agent_ppo2.py:185][0m |          -0.0103 |          69.0382 |           4.0516 |
[32m[20221214 00:11:05 @agent_ppo2.py:185][0m |          -0.0061 |          72.0309 |           4.1530 |
[32m[20221214 00:11:05 @agent_ppo2.py:185][0m |          -0.0140 |          67.9015 |           4.1349 |
[32m[20221214 00:11:05 @agent_ppo2.py:185][0m |          -0.0124 |          67.1925 |           4.1870 |
[32m[20221214 00:11:06 @agent_ppo2.py:185][0m |          -0.0156 |          67.0110 |           4.1903 |
[32m[20221214 00:11:06 @agent_ppo2.py:185][0m |          -0.0181 |          66.3400 |           4.1680 |
[32m[20221214 00:11:06 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:11:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 414.45
[32m[20221214 00:11:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 495.99
[32m[20221214 00:11:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 557.59
[32m[20221214 00:11:06 @agent_ppo2.py:143][0m Total time:      13.59 min
[32m[20221214 00:11:06 @agent_ppo2.py:145][0m 1257472 total steps have happened
[32m[20221214 00:11:06 @agent_ppo2.py:121][0m #------------------------ Iteration 4614 --------------------------#
[32m[20221214 00:11:06 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:11:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:11:06 @agent_ppo2.py:185][0m |          -0.0000 |          65.1574 |           4.5425 |
[32m[20221214 00:11:06 @agent_ppo2.py:185][0m |          -0.0031 |          60.3637 |           4.5035 |
[32m[20221214 00:11:06 @agent_ppo2.py:185][0m |          -0.0046 |          59.3875 |           4.5681 |
[32m[20221214 00:11:06 @agent_ppo2.py:185][0m |           0.0079 |          68.8650 |           4.6181 |
[32m[20221214 00:11:07 @agent_ppo2.py:185][0m |          -0.0128 |          56.1401 |           4.6471 |
[32m[20221214 00:11:07 @agent_ppo2.py:185][0m |          -0.0137 |          54.8442 |           4.6448 |
[32m[20221214 00:11:07 @agent_ppo2.py:185][0m |          -0.0169 |          54.3281 |           4.6666 |
[32m[20221214 00:11:07 @agent_ppo2.py:185][0m |          -0.0188 |          53.6645 |           4.6525 |
[32m[20221214 00:11:07 @agent_ppo2.py:185][0m |          -0.0176 |          53.2813 |           4.6241 |
[32m[20221214 00:11:07 @agent_ppo2.py:185][0m |          -0.0169 |          52.7863 |           4.6385 |
[32m[20221214 00:11:07 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:11:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 371.85
[32m[20221214 00:11:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 492.50
[32m[20221214 00:11:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 536.49
[32m[20221214 00:11:07 @agent_ppo2.py:143][0m Total time:      13.61 min
[32m[20221214 00:11:07 @agent_ppo2.py:145][0m 1259520 total steps have happened
[32m[20221214 00:11:07 @agent_ppo2.py:121][0m #------------------------ Iteration 4615 --------------------------#
[32m[20221214 00:11:07 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:11:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:11:08 @agent_ppo2.py:185][0m |           0.0002 |          78.8505 |           5.3595 |
[32m[20221214 00:11:08 @agent_ppo2.py:185][0m |          -0.0049 |          75.7459 |           5.4356 |
[32m[20221214 00:11:08 @agent_ppo2.py:185][0m |          -0.0113 |          74.4364 |           5.5294 |
[32m[20221214 00:11:08 @agent_ppo2.py:185][0m |          -0.0146 |          73.9713 |           5.5268 |
[32m[20221214 00:11:08 @agent_ppo2.py:185][0m |          -0.0134 |          73.6408 |           5.4817 |
[32m[20221214 00:11:08 @agent_ppo2.py:185][0m |          -0.0134 |          73.2988 |           5.5551 |
[32m[20221214 00:11:08 @agent_ppo2.py:185][0m |          -0.0153 |          73.1978 |           5.5353 |
[32m[20221214 00:11:08 @agent_ppo2.py:185][0m |          -0.0146 |          73.1037 |           5.5887 |
[32m[20221214 00:11:08 @agent_ppo2.py:185][0m |          -0.0162 |          72.8119 |           5.5627 |
[32m[20221214 00:11:08 @agent_ppo2.py:185][0m |          -0.0055 |          75.8096 |           5.5829 |
[32m[20221214 00:11:08 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:11:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 528.17
[32m[20221214 00:11:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 557.19
[32m[20221214 00:11:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 578.69
[32m[20221214 00:11:08 @agent_ppo2.py:143][0m Total time:      13.63 min
[32m[20221214 00:11:08 @agent_ppo2.py:145][0m 1261568 total steps have happened
[32m[20221214 00:11:08 @agent_ppo2.py:121][0m #------------------------ Iteration 4616 --------------------------#
[32m[20221214 00:11:09 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:11:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:11:09 @agent_ppo2.py:185][0m |           0.0109 |          63.4079 |           4.7615 |
[32m[20221214 00:11:09 @agent_ppo2.py:185][0m |          -0.0042 |          54.4389 |           4.8544 |
[32m[20221214 00:11:09 @agent_ppo2.py:185][0m |          -0.0043 |          52.7439 |           4.8395 |
[32m[20221214 00:11:09 @agent_ppo2.py:185][0m |          -0.0103 |          50.8715 |           4.9227 |
[32m[20221214 00:11:09 @agent_ppo2.py:185][0m |          -0.0143 |          50.0769 |           5.0057 |
[32m[20221214 00:11:09 @agent_ppo2.py:185][0m |          -0.0173 |          49.1785 |           4.9565 |
[32m[20221214 00:11:09 @agent_ppo2.py:185][0m |          -0.0103 |          48.6076 |           4.9954 |
[32m[20221214 00:11:09 @agent_ppo2.py:185][0m |          -0.0085 |          50.6823 |           5.0043 |
[32m[20221214 00:11:09 @agent_ppo2.py:185][0m |          -0.0164 |          48.0204 |           5.0916 |
[32m[20221214 00:11:10 @agent_ppo2.py:185][0m |          -0.0152 |          47.1581 |           5.1000 |
[32m[20221214 00:11:10 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:11:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 409.21
[32m[20221214 00:11:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 483.88
[32m[20221214 00:11:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 508.06
[32m[20221214 00:11:10 @agent_ppo2.py:143][0m Total time:      13.66 min
[32m[20221214 00:11:10 @agent_ppo2.py:145][0m 1263616 total steps have happened
[32m[20221214 00:11:10 @agent_ppo2.py:121][0m #------------------------ Iteration 4617 --------------------------#
[32m[20221214 00:11:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:11:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:11:10 @agent_ppo2.py:185][0m |           0.0022 |          77.7217 |           5.0093 |
[32m[20221214 00:11:10 @agent_ppo2.py:185][0m |          -0.0077 |          71.6265 |           5.0748 |
[32m[20221214 00:11:10 @agent_ppo2.py:185][0m |          -0.0102 |          69.9248 |           5.0153 |
[32m[20221214 00:11:10 @agent_ppo2.py:185][0m |          -0.0112 |          68.3075 |           5.0256 |
[32m[20221214 00:11:10 @agent_ppo2.py:185][0m |          -0.0098 |          67.0788 |           4.9674 |
[32m[20221214 00:11:11 @agent_ppo2.py:185][0m |           0.0013 |          73.9291 |           4.9238 |
[32m[20221214 00:11:11 @agent_ppo2.py:185][0m |          -0.0154 |          65.1182 |           4.9487 |
[32m[20221214 00:11:11 @agent_ppo2.py:185][0m |          -0.0122 |          63.8135 |           4.9224 |
[32m[20221214 00:11:11 @agent_ppo2.py:185][0m |          -0.0107 |          63.7795 |           4.8638 |
[32m[20221214 00:11:11 @agent_ppo2.py:185][0m |          -0.0120 |          62.9801 |           4.8819 |
[32m[20221214 00:11:11 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:11:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 433.41
[32m[20221214 00:11:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 491.31
[32m[20221214 00:11:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 328.95
[32m[20221214 00:11:11 @agent_ppo2.py:143][0m Total time:      13.68 min
[32m[20221214 00:11:11 @agent_ppo2.py:145][0m 1265664 total steps have happened
[32m[20221214 00:11:11 @agent_ppo2.py:121][0m #------------------------ Iteration 4618 --------------------------#
[32m[20221214 00:11:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:11:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:11:11 @agent_ppo2.py:185][0m |           0.0011 |          83.0794 |           4.9136 |
[32m[20221214 00:11:11 @agent_ppo2.py:185][0m |           0.0092 |          80.9702 |           4.9384 |
[32m[20221214 00:11:12 @agent_ppo2.py:185][0m |          -0.0060 |          69.8735 |           4.9763 |
[32m[20221214 00:11:12 @agent_ppo2.py:185][0m |          -0.0086 |          66.1018 |           5.0881 |
[32m[20221214 00:11:12 @agent_ppo2.py:185][0m |          -0.0101 |          63.3874 |           5.0982 |
[32m[20221214 00:11:12 @agent_ppo2.py:185][0m |          -0.0095 |          60.9763 |           5.1105 |
[32m[20221214 00:11:12 @agent_ppo2.py:185][0m |          -0.0107 |          59.5674 |           5.1369 |
[32m[20221214 00:11:12 @agent_ppo2.py:185][0m |          -0.0120 |          58.6938 |           5.1639 |
[32m[20221214 00:11:12 @agent_ppo2.py:185][0m |          -0.0099 |          58.2777 |           5.1740 |
[32m[20221214 00:11:12 @agent_ppo2.py:185][0m |          -0.0094 |          57.7026 |           5.2599 |
[32m[20221214 00:11:12 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:11:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 515.84
[32m[20221214 00:11:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 549.28
[32m[20221214 00:11:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 513.19
[32m[20221214 00:11:12 @agent_ppo2.py:143][0m Total time:      13.70 min
[32m[20221214 00:11:12 @agent_ppo2.py:145][0m 1267712 total steps have happened
[32m[20221214 00:11:12 @agent_ppo2.py:121][0m #------------------------ Iteration 4619 --------------------------#
[32m[20221214 00:11:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:11:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:11:13 @agent_ppo2.py:185][0m |          -0.0018 |          72.3540 |           5.2603 |
[32m[20221214 00:11:13 @agent_ppo2.py:185][0m |          -0.0027 |          69.9488 |           5.2906 |
[32m[20221214 00:11:13 @agent_ppo2.py:185][0m |          -0.0073 |          68.7330 |           5.2595 |
[32m[20221214 00:11:13 @agent_ppo2.py:185][0m |          -0.0100 |          68.1322 |           5.2209 |
[32m[20221214 00:11:13 @agent_ppo2.py:185][0m |          -0.0098 |          67.8109 |           5.2712 |
[32m[20221214 00:11:13 @agent_ppo2.py:185][0m |          -0.0132 |          67.4491 |           5.2282 |
[32m[20221214 00:11:13 @agent_ppo2.py:185][0m |          -0.0137 |          67.1900 |           5.2046 |
[32m[20221214 00:11:13 @agent_ppo2.py:185][0m |          -0.0125 |          67.0617 |           5.2557 |
[32m[20221214 00:11:13 @agent_ppo2.py:185][0m |          -0.0139 |          66.9611 |           5.2133 |
[32m[20221214 00:11:13 @agent_ppo2.py:185][0m |          -0.0097 |          68.1556 |           5.2104 |
[32m[20221214 00:11:13 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:11:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 521.16
[32m[20221214 00:11:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 548.33
[32m[20221214 00:11:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 559.32
[32m[20221214 00:11:13 @agent_ppo2.py:143][0m Total time:      13.72 min
[32m[20221214 00:11:13 @agent_ppo2.py:145][0m 1269760 total steps have happened
[32m[20221214 00:11:13 @agent_ppo2.py:121][0m #------------------------ Iteration 4620 --------------------------#
[32m[20221214 00:11:14 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:11:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:11:14 @agent_ppo2.py:185][0m |           0.0041 |          60.9912 |           5.4282 |
[32m[20221214 00:11:14 @agent_ppo2.py:185][0m |          -0.0026 |          48.8670 |           5.5195 |
[32m[20221214 00:11:14 @agent_ppo2.py:185][0m |          -0.0046 |          46.9150 |           5.5872 |
[32m[20221214 00:11:14 @agent_ppo2.py:185][0m |          -0.0028 |          45.6402 |           5.5229 |
[32m[20221214 00:11:14 @agent_ppo2.py:185][0m |          -0.0087 |          45.1563 |           5.5945 |
[32m[20221214 00:11:14 @agent_ppo2.py:185][0m |          -0.0082 |          44.7285 |           5.5610 |
[32m[20221214 00:11:14 @agent_ppo2.py:185][0m |          -0.0070 |          45.4858 |           5.5624 |
[32m[20221214 00:11:14 @agent_ppo2.py:185][0m |          -0.0065 |          43.8639 |           5.6061 |
[32m[20221214 00:11:15 @agent_ppo2.py:185][0m |          -0.0122 |          43.5755 |           5.6561 |
[32m[20221214 00:11:15 @agent_ppo2.py:185][0m |          -0.0083 |          43.3475 |           5.6628 |
[32m[20221214 00:11:15 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:11:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 477.63
[32m[20221214 00:11:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 498.50
[32m[20221214 00:11:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 444.06
[32m[20221214 00:11:15 @agent_ppo2.py:143][0m Total time:      13.74 min
[32m[20221214 00:11:15 @agent_ppo2.py:145][0m 1271808 total steps have happened
[32m[20221214 00:11:15 @agent_ppo2.py:121][0m #------------------------ Iteration 4621 --------------------------#
[32m[20221214 00:11:15 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:11:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:11:15 @agent_ppo2.py:185][0m |          -0.0015 |          42.8243 |           5.3143 |
[32m[20221214 00:11:15 @agent_ppo2.py:185][0m |          -0.0070 |          35.3735 |           5.3717 |
[32m[20221214 00:11:15 @agent_ppo2.py:185][0m |          -0.0086 |          33.5433 |           5.4625 |
[32m[20221214 00:11:15 @agent_ppo2.py:185][0m |          -0.0070 |          32.8686 |           5.4064 |
[32m[20221214 00:11:15 @agent_ppo2.py:185][0m |          -0.0128 |          31.9673 |           5.5027 |
[32m[20221214 00:11:16 @agent_ppo2.py:185][0m |          -0.0112 |          31.4830 |           5.5305 |
[32m[20221214 00:11:16 @agent_ppo2.py:185][0m |          -0.0180 |          30.8561 |           5.6079 |
[32m[20221214 00:11:16 @agent_ppo2.py:185][0m |          -0.0160 |          30.7752 |           5.6577 |
[32m[20221214 00:11:16 @agent_ppo2.py:185][0m |          -0.0181 |          30.1405 |           5.6864 |
[32m[20221214 00:11:16 @agent_ppo2.py:185][0m |          -0.0129 |          29.9419 |           5.6407 |
[32m[20221214 00:11:16 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:11:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 413.25
[32m[20221214 00:11:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 471.23
[32m[20221214 00:11:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 551.11
[32m[20221214 00:11:16 @agent_ppo2.py:143][0m Total time:      13.76 min
[32m[20221214 00:11:16 @agent_ppo2.py:145][0m 1273856 total steps have happened
[32m[20221214 00:11:16 @agent_ppo2.py:121][0m #------------------------ Iteration 4622 --------------------------#
[32m[20221214 00:11:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:11:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:11:16 @agent_ppo2.py:185][0m |          -0.0039 |          43.3203 |           5.0630 |
[32m[20221214 00:11:16 @agent_ppo2.py:185][0m |          -0.0059 |          37.7762 |           5.1156 |
[32m[20221214 00:11:17 @agent_ppo2.py:185][0m |          -0.0097 |          36.8782 |           5.1917 |
[32m[20221214 00:11:17 @agent_ppo2.py:185][0m |          -0.0071 |          36.1327 |           5.1701 |
[32m[20221214 00:11:17 @agent_ppo2.py:185][0m |          -0.0108 |          35.8234 |           5.2862 |
[32m[20221214 00:11:17 @agent_ppo2.py:185][0m |          -0.0143 |          35.5428 |           5.2790 |
[32m[20221214 00:11:17 @agent_ppo2.py:185][0m |          -0.0118 |          35.1953 |           5.4036 |
[32m[20221214 00:11:17 @agent_ppo2.py:185][0m |          -0.0106 |          34.9458 |           5.4199 |
[32m[20221214 00:11:17 @agent_ppo2.py:185][0m |          -0.0152 |          34.9413 |           5.4448 |
[32m[20221214 00:11:17 @agent_ppo2.py:185][0m |          -0.0177 |          34.7500 |           5.4441 |
[32m[20221214 00:11:17 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:11:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 495.01
[32m[20221214 00:11:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 524.73
[32m[20221214 00:11:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 501.24
[32m[20221214 00:11:17 @agent_ppo2.py:143][0m Total time:      13.78 min
[32m[20221214 00:11:17 @agent_ppo2.py:145][0m 1275904 total steps have happened
[32m[20221214 00:11:17 @agent_ppo2.py:121][0m #------------------------ Iteration 4623 --------------------------#
[32m[20221214 00:11:17 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:11:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:11:18 @agent_ppo2.py:185][0m |           0.0060 |          43.0084 |           6.0772 |
[32m[20221214 00:11:18 @agent_ppo2.py:185][0m |          -0.0050 |          37.0173 |           6.1127 |
[32m[20221214 00:11:18 @agent_ppo2.py:185][0m |          -0.0064 |          35.4962 |           6.0432 |
[32m[20221214 00:11:18 @agent_ppo2.py:185][0m |          -0.0158 |          34.3172 |           6.0986 |
[32m[20221214 00:11:18 @agent_ppo2.py:185][0m |          -0.0126 |          33.8307 |           6.0804 |
[32m[20221214 00:11:18 @agent_ppo2.py:185][0m |          -0.0132 |          33.2559 |           6.1547 |
[32m[20221214 00:11:18 @agent_ppo2.py:185][0m |          -0.0130 |          32.7520 |           6.2117 |
[32m[20221214 00:11:18 @agent_ppo2.py:185][0m |          -0.0146 |          32.5109 |           6.1493 |
[32m[20221214 00:11:18 @agent_ppo2.py:185][0m |          -0.0189 |          32.1355 |           6.1797 |
[32m[20221214 00:11:18 @agent_ppo2.py:185][0m |          -0.0121 |          32.4990 |           6.2727 |
[32m[20221214 00:11:18 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:11:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 368.59
[32m[20221214 00:11:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 513.18
[32m[20221214 00:11:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 524.55
[32m[20221214 00:11:19 @agent_ppo2.py:143][0m Total time:      13.80 min
[32m[20221214 00:11:19 @agent_ppo2.py:145][0m 1277952 total steps have happened
[32m[20221214 00:11:19 @agent_ppo2.py:121][0m #------------------------ Iteration 4624 --------------------------#
[32m[20221214 00:11:19 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:11:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:11:19 @agent_ppo2.py:185][0m |          -0.0030 |          65.5593 |           6.3270 |
[32m[20221214 00:11:19 @agent_ppo2.py:185][0m |          -0.0129 |          57.5894 |           6.2956 |
[32m[20221214 00:11:19 @agent_ppo2.py:185][0m |          -0.0172 |          54.0843 |           6.3053 |
[32m[20221214 00:11:19 @agent_ppo2.py:185][0m |          -0.0148 |          52.0066 |           6.3008 |
[32m[20221214 00:11:19 @agent_ppo2.py:185][0m |          -0.0178 |          50.7769 |           6.2680 |
[32m[20221214 00:11:19 @agent_ppo2.py:185][0m |          -0.0169 |          49.5081 |           6.2230 |
[32m[20221214 00:11:19 @agent_ppo2.py:185][0m |          -0.0214 |          48.5830 |           6.1693 |
[32m[20221214 00:11:20 @agent_ppo2.py:185][0m |          -0.0226 |          47.8384 |           6.1731 |
[32m[20221214 00:11:20 @agent_ppo2.py:185][0m |          -0.0209 |          47.3687 |           6.1351 |
[32m[20221214 00:11:20 @agent_ppo2.py:185][0m |          -0.0197 |          47.3476 |           6.1035 |
[32m[20221214 00:11:20 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:11:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 489.72
[32m[20221214 00:11:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 542.06
[32m[20221214 00:11:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 504.40
[32m[20221214 00:11:20 @agent_ppo2.py:143][0m Total time:      13.82 min
[32m[20221214 00:11:20 @agent_ppo2.py:145][0m 1280000 total steps have happened
[32m[20221214 00:11:20 @agent_ppo2.py:121][0m #------------------------ Iteration 4625 --------------------------#
[32m[20221214 00:11:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:11:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:11:20 @agent_ppo2.py:185][0m |          -0.0034 |          78.6450 |           5.0598 |
[32m[20221214 00:11:20 @agent_ppo2.py:185][0m |          -0.0079 |          76.2849 |           5.1096 |
[32m[20221214 00:11:20 @agent_ppo2.py:185][0m |          -0.0098 |          75.5608 |           5.1385 |
[32m[20221214 00:11:20 @agent_ppo2.py:185][0m |          -0.0063 |          75.6156 |           5.1085 |
[32m[20221214 00:11:21 @agent_ppo2.py:185][0m |           0.0008 |          78.3681 |           5.1719 |
[32m[20221214 00:11:21 @agent_ppo2.py:185][0m |          -0.0055 |          76.1411 |           5.1850 |
[32m[20221214 00:11:21 @agent_ppo2.py:185][0m |          -0.0140 |          74.0129 |           5.1531 |
[32m[20221214 00:11:21 @agent_ppo2.py:185][0m |          -0.0132 |          73.7856 |           5.1291 |
[32m[20221214 00:11:21 @agent_ppo2.py:185][0m |          -0.0149 |          73.3434 |           5.2043 |
[32m[20221214 00:11:21 @agent_ppo2.py:185][0m |          -0.0127 |          73.1628 |           5.1834 |
[32m[20221214 00:11:21 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:11:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 509.92
[32m[20221214 00:11:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 549.67
[32m[20221214 00:11:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 350.20
[32m[20221214 00:11:21 @agent_ppo2.py:143][0m Total time:      13.85 min
[32m[20221214 00:11:21 @agent_ppo2.py:145][0m 1282048 total steps have happened
[32m[20221214 00:11:21 @agent_ppo2.py:121][0m #------------------------ Iteration 4626 --------------------------#
[32m[20221214 00:11:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:11:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:11:21 @agent_ppo2.py:185][0m |          -0.0008 |          63.5209 |           5.7671 |
[32m[20221214 00:11:22 @agent_ppo2.py:185][0m |          -0.0064 |          59.6688 |           5.7644 |
[32m[20221214 00:11:22 @agent_ppo2.py:185][0m |          -0.0125 |          57.9296 |           5.8211 |
[32m[20221214 00:11:22 @agent_ppo2.py:185][0m |          -0.0183 |          56.7449 |           5.8232 |
[32m[20221214 00:11:22 @agent_ppo2.py:185][0m |          -0.0165 |          55.7275 |           5.8044 |
[32m[20221214 00:11:22 @agent_ppo2.py:185][0m |          -0.0188 |          55.2025 |           5.9337 |
[32m[20221214 00:11:22 @agent_ppo2.py:185][0m |          -0.0174 |          54.6687 |           5.8952 |
[32m[20221214 00:11:22 @agent_ppo2.py:185][0m |          -0.0189 |          54.1140 |           5.9209 |
[32m[20221214 00:11:22 @agent_ppo2.py:185][0m |          -0.0192 |          53.8116 |           5.9052 |
[32m[20221214 00:11:22 @agent_ppo2.py:185][0m |          -0.0201 |          53.4556 |           5.9764 |
[32m[20221214 00:11:22 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:11:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 491.38
[32m[20221214 00:11:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 540.68
[32m[20221214 00:11:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 554.06
[32m[20221214 00:11:22 @agent_ppo2.py:143][0m Total time:      13.87 min
[32m[20221214 00:11:22 @agent_ppo2.py:145][0m 1284096 total steps have happened
[32m[20221214 00:11:22 @agent_ppo2.py:121][0m #------------------------ Iteration 4627 --------------------------#
[32m[20221214 00:11:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:11:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:11:23 @agent_ppo2.py:185][0m |           0.0046 |          83.3364 |           6.0743 |
[32m[20221214 00:11:23 @agent_ppo2.py:185][0m |          -0.0087 |          74.4841 |           6.0328 |
[32m[20221214 00:11:23 @agent_ppo2.py:185][0m |          -0.0085 |          72.0463 |           6.1017 |
[32m[20221214 00:11:23 @agent_ppo2.py:185][0m |          -0.0117 |          71.1734 |           6.0536 |
[32m[20221214 00:11:23 @agent_ppo2.py:185][0m |          -0.0115 |          70.7182 |           6.0555 |
[32m[20221214 00:11:23 @agent_ppo2.py:185][0m |          -0.0119 |          70.0179 |           6.0958 |
[32m[20221214 00:11:23 @agent_ppo2.py:185][0m |          -0.0135 |          69.3482 |           6.0805 |
[32m[20221214 00:11:23 @agent_ppo2.py:185][0m |          -0.0134 |          69.2917 |           6.0369 |
[32m[20221214 00:11:23 @agent_ppo2.py:185][0m |          -0.0162 |          68.6585 |           6.0127 |
[32m[20221214 00:11:24 @agent_ppo2.py:185][0m |          -0.0066 |          70.5037 |           6.0948 |
[32m[20221214 00:11:24 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:11:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 512.47
[32m[20221214 00:11:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 546.20
[32m[20221214 00:11:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 471.03
[32m[20221214 00:11:24 @agent_ppo2.py:143][0m Total time:      13.89 min
[32m[20221214 00:11:24 @agent_ppo2.py:145][0m 1286144 total steps have happened
[32m[20221214 00:11:24 @agent_ppo2.py:121][0m #------------------------ Iteration 4628 --------------------------#
[32m[20221214 00:11:24 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:11:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:11:24 @agent_ppo2.py:185][0m |           0.0069 |          65.3876 |           5.6583 |
[32m[20221214 00:11:24 @agent_ppo2.py:185][0m |          -0.0072 |          59.3942 |           5.7052 |
[32m[20221214 00:11:24 @agent_ppo2.py:185][0m |          -0.0073 |          57.3501 |           5.7324 |
[32m[20221214 00:11:24 @agent_ppo2.py:185][0m |          -0.0118 |          56.1548 |           5.6854 |
[32m[20221214 00:11:24 @agent_ppo2.py:185][0m |          -0.0067 |          55.6194 |           5.6988 |
[32m[20221214 00:11:24 @agent_ppo2.py:185][0m |          -0.0072 |          54.7108 |           5.7270 |
[32m[20221214 00:11:25 @agent_ppo2.py:185][0m |          -0.0149 |          54.5568 |           5.7339 |
[32m[20221214 00:11:25 @agent_ppo2.py:185][0m |          -0.0177 |          54.2197 |           5.7894 |
[32m[20221214 00:11:25 @agent_ppo2.py:185][0m |          -0.0172 |          53.7398 |           5.7853 |
[32m[20221214 00:11:25 @agent_ppo2.py:185][0m |          -0.0206 |          53.5616 |           5.7955 |
[32m[20221214 00:11:25 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:11:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 460.95
[32m[20221214 00:11:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 519.40
[32m[20221214 00:11:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 490.85
[32m[20221214 00:11:25 @agent_ppo2.py:143][0m Total time:      13.91 min
[32m[20221214 00:11:25 @agent_ppo2.py:145][0m 1288192 total steps have happened
[32m[20221214 00:11:25 @agent_ppo2.py:121][0m #------------------------ Iteration 4629 --------------------------#
[32m[20221214 00:11:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:11:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:11:25 @agent_ppo2.py:185][0m |          -0.0025 |          69.5982 |           5.4942 |
[32m[20221214 00:11:25 @agent_ppo2.py:185][0m |           0.0010 |          70.4393 |           5.4291 |
[32m[20221214 00:11:25 @agent_ppo2.py:185][0m |          -0.0121 |          64.3244 |           5.3432 |
[32m[20221214 00:11:26 @agent_ppo2.py:185][0m |          -0.0179 |          63.6566 |           5.2880 |
[32m[20221214 00:11:26 @agent_ppo2.py:185][0m |           0.0003 |          64.7799 |           5.1894 |
[32m[20221214 00:11:26 @agent_ppo2.py:185][0m |          -0.0158 |          62.4112 |           5.1285 |
[32m[20221214 00:11:26 @agent_ppo2.py:185][0m |          -0.0206 |          62.1078 |           5.0688 |
[32m[20221214 00:11:26 @agent_ppo2.py:185][0m |          -0.0181 |          61.8198 |           5.0437 |
[32m[20221214 00:11:26 @agent_ppo2.py:185][0m |          -0.0165 |          61.6606 |           5.0069 |
[32m[20221214 00:11:26 @agent_ppo2.py:185][0m |          -0.0186 |          61.3995 |           4.9957 |
[32m[20221214 00:11:26 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:11:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 425.37
[32m[20221214 00:11:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 483.83
[32m[20221214 00:11:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 338.93
[32m[20221214 00:11:26 @agent_ppo2.py:143][0m Total time:      13.93 min
[32m[20221214 00:11:26 @agent_ppo2.py:145][0m 1290240 total steps have happened
[32m[20221214 00:11:26 @agent_ppo2.py:121][0m #------------------------ Iteration 4630 --------------------------#
[32m[20221214 00:11:26 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:11:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:11:27 @agent_ppo2.py:185][0m |           0.0082 |          72.3876 |           4.6748 |
[32m[20221214 00:11:27 @agent_ppo2.py:185][0m |          -0.0027 |          63.0849 |           4.7216 |
[32m[20221214 00:11:27 @agent_ppo2.py:185][0m |          -0.0091 |          60.2698 |           4.7832 |
[32m[20221214 00:11:27 @agent_ppo2.py:185][0m |          -0.0113 |          58.7148 |           4.7669 |
[32m[20221214 00:11:27 @agent_ppo2.py:185][0m |          -0.0093 |          57.6803 |           4.7825 |
[32m[20221214 00:11:27 @agent_ppo2.py:185][0m |          -0.0119 |          56.9222 |           4.8408 |
[32m[20221214 00:11:27 @agent_ppo2.py:185][0m |          -0.0178 |          55.8131 |           4.8996 |
[32m[20221214 00:11:27 @agent_ppo2.py:185][0m |          -0.0138 |          55.0033 |           4.8998 |
[32m[20221214 00:11:27 @agent_ppo2.py:185][0m |          -0.0142 |          54.5369 |           4.9728 |
[32m[20221214 00:11:27 @agent_ppo2.py:185][0m |          -0.0157 |          54.3270 |           4.9613 |
[32m[20221214 00:11:27 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:11:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 493.99
[32m[20221214 00:11:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 526.49
[32m[20221214 00:11:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 384.68
[32m[20221214 00:11:28 @agent_ppo2.py:143][0m Total time:      13.95 min
[32m[20221214 00:11:28 @agent_ppo2.py:145][0m 1292288 total steps have happened
[32m[20221214 00:11:28 @agent_ppo2.py:121][0m #------------------------ Iteration 4631 --------------------------#
[32m[20221214 00:11:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:11:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:11:28 @agent_ppo2.py:185][0m |           0.0048 |          62.2842 |           5.6952 |
[32m[20221214 00:11:28 @agent_ppo2.py:185][0m |          -0.0129 |          43.5138 |           5.8645 |
[32m[20221214 00:11:28 @agent_ppo2.py:185][0m |          -0.0039 |          41.6490 |           5.8932 |
[32m[20221214 00:11:28 @agent_ppo2.py:185][0m |          -0.0129 |          40.5314 |           5.9027 |
[32m[20221214 00:11:28 @agent_ppo2.py:185][0m |          -0.0137 |          39.9816 |           5.9485 |
[32m[20221214 00:11:28 @agent_ppo2.py:185][0m |          -0.0122 |          39.6449 |           6.0035 |
[32m[20221214 00:11:28 @agent_ppo2.py:185][0m |          -0.0134 |          39.3077 |           6.0720 |
[32m[20221214 00:11:28 @agent_ppo2.py:185][0m |          -0.0168 |          40.4118 |           6.0528 |
[32m[20221214 00:11:29 @agent_ppo2.py:185][0m |          -0.0127 |          38.6211 |           6.1009 |
[32m[20221214 00:11:29 @agent_ppo2.py:185][0m |          -0.0185 |          38.6482 |           6.1030 |
[32m[20221214 00:11:29 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:11:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 399.02
[32m[20221214 00:11:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 458.77
[32m[20221214 00:11:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 562.71
[32m[20221214 00:11:29 @agent_ppo2.py:143][0m Total time:      13.97 min
[32m[20221214 00:11:29 @agent_ppo2.py:145][0m 1294336 total steps have happened
[32m[20221214 00:11:29 @agent_ppo2.py:121][0m #------------------------ Iteration 4632 --------------------------#
[32m[20221214 00:11:29 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:11:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:11:29 @agent_ppo2.py:185][0m |           0.0016 |          54.9929 |           6.1378 |
[32m[20221214 00:11:29 @agent_ppo2.py:185][0m |          -0.0101 |          47.5139 |           6.0983 |
[32m[20221214 00:11:29 @agent_ppo2.py:185][0m |          -0.0053 |          45.8651 |           6.0492 |
[32m[20221214 00:11:29 @agent_ppo2.py:185][0m |          -0.0135 |          44.7715 |           6.0564 |
[32m[20221214 00:11:29 @agent_ppo2.py:185][0m |          -0.0174 |          44.0424 |           5.9811 |
[32m[20221214 00:11:30 @agent_ppo2.py:185][0m |          -0.0144 |          43.3928 |           5.9371 |
[32m[20221214 00:11:30 @agent_ppo2.py:185][0m |          -0.0156 |          42.9879 |           5.9220 |
[32m[20221214 00:11:30 @agent_ppo2.py:185][0m |          -0.0184 |          42.8077 |           5.9330 |
[32m[20221214 00:11:30 @agent_ppo2.py:185][0m |          -0.0236 |          42.4131 |           5.8513 |
[32m[20221214 00:11:30 @agent_ppo2.py:185][0m |          -0.0164 |          43.7264 |           5.7928 |
[32m[20221214 00:11:30 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:11:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 421.84
[32m[20221214 00:11:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 503.63
[32m[20221214 00:11:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 521.58
[32m[20221214 00:11:30 @agent_ppo2.py:143][0m Total time:      13.99 min
[32m[20221214 00:11:30 @agent_ppo2.py:145][0m 1296384 total steps have happened
[32m[20221214 00:11:30 @agent_ppo2.py:121][0m #------------------------ Iteration 4633 --------------------------#
[32m[20221214 00:11:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:11:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:11:30 @agent_ppo2.py:185][0m |           0.0026 |          83.2627 |           4.9959 |
[32m[20221214 00:11:30 @agent_ppo2.py:185][0m |          -0.0012 |          75.4625 |           5.0680 |
[32m[20221214 00:11:31 @agent_ppo2.py:185][0m |          -0.0056 |          72.2774 |           5.0865 |
[32m[20221214 00:11:31 @agent_ppo2.py:185][0m |          -0.0046 |          70.2951 |           4.9968 |
[32m[20221214 00:11:31 @agent_ppo2.py:185][0m |          -0.0078 |          68.7225 |           5.0475 |
[32m[20221214 00:11:31 @agent_ppo2.py:185][0m |          -0.0142 |          67.9135 |           5.0525 |
[32m[20221214 00:11:31 @agent_ppo2.py:185][0m |          -0.0099 |          66.6742 |           5.0524 |
[32m[20221214 00:11:31 @agent_ppo2.py:185][0m |          -0.0147 |          66.1819 |           5.0749 |
[32m[20221214 00:11:31 @agent_ppo2.py:185][0m |          -0.0145 |          65.2061 |           5.0140 |
[32m[20221214 00:11:31 @agent_ppo2.py:185][0m |          -0.0119 |          64.7906 |           5.0771 |
[32m[20221214 00:11:31 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:11:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 493.25
[32m[20221214 00:11:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 546.69
[32m[20221214 00:11:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 503.58
[32m[20221214 00:11:31 @agent_ppo2.py:143][0m Total time:      14.02 min
[32m[20221214 00:11:31 @agent_ppo2.py:145][0m 1298432 total steps have happened
[32m[20221214 00:11:31 @agent_ppo2.py:121][0m #------------------------ Iteration 4634 --------------------------#
[32m[20221214 00:11:31 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:11:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:11:32 @agent_ppo2.py:185][0m |           0.0012 |          72.0728 |           5.3935 |
[32m[20221214 00:11:32 @agent_ppo2.py:185][0m |          -0.0018 |          65.1842 |           5.3544 |
[32m[20221214 00:11:32 @agent_ppo2.py:185][0m |          -0.0086 |          62.5387 |           5.3520 |
[32m[20221214 00:11:32 @agent_ppo2.py:185][0m |          -0.0185 |          61.1660 |           5.3703 |
[32m[20221214 00:11:32 @agent_ppo2.py:185][0m |          -0.0182 |          60.2453 |           5.2879 |
[32m[20221214 00:11:32 @agent_ppo2.py:185][0m |          -0.0156 |          59.8181 |           5.2708 |
[32m[20221214 00:11:32 @agent_ppo2.py:185][0m |          -0.0170 |          59.3602 |           5.2580 |
[32m[20221214 00:11:32 @agent_ppo2.py:185][0m |          -0.0170 |          58.5211 |           5.2200 |
[32m[20221214 00:11:32 @agent_ppo2.py:185][0m |          -0.0158 |          58.1289 |           5.2189 |
[32m[20221214 00:11:32 @agent_ppo2.py:185][0m |          -0.0161 |          57.7383 |           5.2006 |
[32m[20221214 00:11:32 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:11:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 427.58
[32m[20221214 00:11:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 463.09
[32m[20221214 00:11:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 580.23
[32m[20221214 00:11:33 @agent_ppo2.py:143][0m Total time:      14.04 min
[32m[20221214 00:11:33 @agent_ppo2.py:145][0m 1300480 total steps have happened
[32m[20221214 00:11:33 @agent_ppo2.py:121][0m #------------------------ Iteration 4635 --------------------------#
[32m[20221214 00:11:33 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:11:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:11:33 @agent_ppo2.py:185][0m |          -0.0042 |          60.3839 |           5.1628 |
[32m[20221214 00:11:33 @agent_ppo2.py:185][0m |           0.0061 |          60.0476 |           5.1314 |
[32m[20221214 00:11:33 @agent_ppo2.py:185][0m |          -0.0123 |          54.1353 |           5.1184 |
[32m[20221214 00:11:33 @agent_ppo2.py:185][0m |          -0.0121 |          52.1647 |           5.1393 |
[32m[20221214 00:11:33 @agent_ppo2.py:185][0m |          -0.0130 |          51.1871 |           5.0774 |
[32m[20221214 00:11:33 @agent_ppo2.py:185][0m |          -0.0156 |          50.3444 |           5.1070 |
[32m[20221214 00:11:33 @agent_ppo2.py:185][0m |          -0.0133 |          49.5737 |           5.0357 |
[32m[20221214 00:11:33 @agent_ppo2.py:185][0m |          -0.0182 |          48.9796 |           5.0641 |
[32m[20221214 00:11:34 @agent_ppo2.py:185][0m |          -0.0167 |          48.5206 |           5.0864 |
[32m[20221214 00:11:34 @agent_ppo2.py:185][0m |          -0.0141 |          48.3237 |           5.0236 |
[32m[20221214 00:11:34 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:11:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 473.29
[32m[20221214 00:11:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 537.04
[32m[20221214 00:11:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 555.67
[32m[20221214 00:11:34 @agent_ppo2.py:143][0m Total time:      14.06 min
[32m[20221214 00:11:34 @agent_ppo2.py:145][0m 1302528 total steps have happened
[32m[20221214 00:11:34 @agent_ppo2.py:121][0m #------------------------ Iteration 4636 --------------------------#
[32m[20221214 00:11:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:11:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:11:34 @agent_ppo2.py:185][0m |           0.0244 |          71.3699 |           5.0588 |
[32m[20221214 00:11:34 @agent_ppo2.py:185][0m |          -0.0034 |          56.8767 |           5.0448 |
[32m[20221214 00:11:34 @agent_ppo2.py:185][0m |          -0.0079 |          53.2263 |           4.8805 |
[32m[20221214 00:11:34 @agent_ppo2.py:185][0m |          -0.0077 |          50.7419 |           4.8369 |
[32m[20221214 00:11:34 @agent_ppo2.py:185][0m |          -0.0081 |          48.8224 |           4.8388 |
[32m[20221214 00:11:35 @agent_ppo2.py:185][0m |          -0.0092 |          47.0381 |           4.8391 |
[32m[20221214 00:11:35 @agent_ppo2.py:185][0m |          -0.0113 |          45.9997 |           4.7349 |
[32m[20221214 00:11:35 @agent_ppo2.py:185][0m |          -0.0105 |          45.3322 |           4.6346 |
[32m[20221214 00:11:35 @agent_ppo2.py:185][0m |          -0.0145 |          44.4923 |           4.7019 |
[32m[20221214 00:11:35 @agent_ppo2.py:185][0m |          -0.0095 |          43.5964 |           4.7028 |
[32m[20221214 00:11:35 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:11:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 485.12
[32m[20221214 00:11:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 507.99
[32m[20221214 00:11:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 310.44
[32m[20221214 00:11:35 @agent_ppo2.py:143][0m Total time:      14.08 min
[32m[20221214 00:11:35 @agent_ppo2.py:145][0m 1304576 total steps have happened
[32m[20221214 00:11:35 @agent_ppo2.py:121][0m #------------------------ Iteration 4637 --------------------------#
[32m[20221214 00:11:35 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:11:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:11:35 @agent_ppo2.py:185][0m |           0.0036 |          62.9265 |           4.8012 |
[32m[20221214 00:11:35 @agent_ppo2.py:185][0m |          -0.0098 |          53.4790 |           4.9241 |
[32m[20221214 00:11:36 @agent_ppo2.py:185][0m |          -0.0020 |          51.8925 |           4.8787 |
[32m[20221214 00:11:36 @agent_ppo2.py:185][0m |          -0.0102 |          51.1428 |           4.9008 |
[32m[20221214 00:11:36 @agent_ppo2.py:185][0m |          -0.0183 |          49.8255 |           4.9302 |
[32m[20221214 00:11:36 @agent_ppo2.py:185][0m |          -0.0142 |          49.0801 |           5.0436 |
[32m[20221214 00:11:36 @agent_ppo2.py:185][0m |          -0.0156 |          48.5629 |           5.0534 |
[32m[20221214 00:11:36 @agent_ppo2.py:185][0m |          -0.0029 |          51.1684 |           5.0784 |
[32m[20221214 00:11:36 @agent_ppo2.py:185][0m |          -0.0162 |          48.2680 |           5.1980 |
[32m[20221214 00:11:36 @agent_ppo2.py:185][0m |          -0.0059 |          51.1328 |           5.1522 |
[32m[20221214 00:11:36 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:11:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 424.85
[32m[20221214 00:11:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 473.21
[32m[20221214 00:11:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 513.50
[32m[20221214 00:11:36 @agent_ppo2.py:143][0m Total time:      14.10 min
[32m[20221214 00:11:36 @agent_ppo2.py:145][0m 1306624 total steps have happened
[32m[20221214 00:11:36 @agent_ppo2.py:121][0m #------------------------ Iteration 4638 --------------------------#
[32m[20221214 00:11:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:11:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:11:37 @agent_ppo2.py:185][0m |           0.0078 |          59.7531 |           4.0495 |
[32m[20221214 00:11:37 @agent_ppo2.py:185][0m |          -0.0012 |          51.1968 |           4.1527 |
[32m[20221214 00:11:37 @agent_ppo2.py:185][0m |          -0.0067 |          53.0018 |           4.1614 |
[32m[20221214 00:11:37 @agent_ppo2.py:185][0m |          -0.0118 |          48.7563 |           4.2615 |
[32m[20221214 00:11:37 @agent_ppo2.py:185][0m |          -0.0104 |          47.9806 |           4.2582 |
[32m[20221214 00:11:37 @agent_ppo2.py:185][0m |          -0.0168 |          47.2039 |           4.3489 |
[32m[20221214 00:11:37 @agent_ppo2.py:185][0m |          -0.0170 |          46.8723 |           4.3882 |
[32m[20221214 00:11:37 @agent_ppo2.py:185][0m |          -0.0181 |          46.5529 |           4.3788 |
[32m[20221214 00:11:37 @agent_ppo2.py:185][0m |          -0.0138 |          49.3606 |           4.4133 |
[32m[20221214 00:11:37 @agent_ppo2.py:185][0m |          -0.0219 |          45.8323 |           4.4629 |
[32m[20221214 00:11:37 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:11:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 451.87
[32m[20221214 00:11:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 502.10
[32m[20221214 00:11:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 507.74
[32m[20221214 00:11:38 @agent_ppo2.py:143][0m Total time:      14.12 min
[32m[20221214 00:11:38 @agent_ppo2.py:145][0m 1308672 total steps have happened
[32m[20221214 00:11:38 @agent_ppo2.py:121][0m #------------------------ Iteration 4639 --------------------------#
[32m[20221214 00:11:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:11:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:11:38 @agent_ppo2.py:185][0m |          -0.0007 |          68.6046 |           5.0307 |
[32m[20221214 00:11:38 @agent_ppo2.py:185][0m |          -0.0068 |          67.5978 |           5.1364 |
[32m[20221214 00:11:38 @agent_ppo2.py:185][0m |          -0.0084 |          66.9901 |           5.1863 |
[32m[20221214 00:11:38 @agent_ppo2.py:185][0m |          -0.0019 |          70.6173 |           5.2589 |
[32m[20221214 00:11:38 @agent_ppo2.py:185][0m |          -0.0104 |          67.0397 |           5.2675 |
[32m[20221214 00:11:38 @agent_ppo2.py:185][0m |          -0.0116 |          66.2637 |           5.2224 |
[32m[20221214 00:11:38 @agent_ppo2.py:185][0m |          -0.0120 |          66.0360 |           5.2479 |
[32m[20221214 00:11:39 @agent_ppo2.py:185][0m |          -0.0131 |          65.9007 |           5.2413 |
[32m[20221214 00:11:39 @agent_ppo2.py:185][0m |          -0.0133 |          65.6081 |           5.2147 |
[32m[20221214 00:11:39 @agent_ppo2.py:185][0m |          -0.0121 |          65.6783 |           5.2864 |
[32m[20221214 00:11:39 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:11:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 478.37
[32m[20221214 00:11:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 501.61
[32m[20221214 00:11:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 399.59
[32m[20221214 00:11:39 @agent_ppo2.py:143][0m Total time:      14.14 min
[32m[20221214 00:11:39 @agent_ppo2.py:145][0m 1310720 total steps have happened
[32m[20221214 00:11:39 @agent_ppo2.py:121][0m #------------------------ Iteration 4640 --------------------------#
[32m[20221214 00:11:39 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:11:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:11:39 @agent_ppo2.py:185][0m |          -0.0019 |          59.9996 |           5.1203 |
[32m[20221214 00:11:39 @agent_ppo2.py:185][0m |          -0.0082 |          55.6681 |           5.1694 |
[32m[20221214 00:11:39 @agent_ppo2.py:185][0m |          -0.0038 |          54.4947 |           5.1815 |
[32m[20221214 00:11:39 @agent_ppo2.py:185][0m |          -0.0072 |          53.5725 |           5.2172 |
[32m[20221214 00:11:40 @agent_ppo2.py:185][0m |          -0.0063 |          52.9747 |           5.2257 |
[32m[20221214 00:11:40 @agent_ppo2.py:185][0m |          -0.0098 |          52.6329 |           5.2599 |
[32m[20221214 00:11:40 @agent_ppo2.py:185][0m |          -0.0120 |          52.2361 |           5.3141 |
[32m[20221214 00:11:40 @agent_ppo2.py:185][0m |          -0.0094 |          52.0000 |           5.3737 |
[32m[20221214 00:11:40 @agent_ppo2.py:185][0m |          -0.0079 |          52.2947 |           5.3786 |
[32m[20221214 00:11:40 @agent_ppo2.py:185][0m |          -0.0098 |          51.7657 |           5.4102 |
[32m[20221214 00:11:40 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:11:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 420.74
[32m[20221214 00:11:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 494.21
[32m[20221214 00:11:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 405.62
[32m[20221214 00:11:40 @agent_ppo2.py:143][0m Total time:      14.16 min
[32m[20221214 00:11:40 @agent_ppo2.py:145][0m 1312768 total steps have happened
[32m[20221214 00:11:40 @agent_ppo2.py:121][0m #------------------------ Iteration 4641 --------------------------#
[32m[20221214 00:11:40 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:11:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:11:40 @agent_ppo2.py:185][0m |           0.0020 |          55.1026 |           5.9985 |
[32m[20221214 00:11:41 @agent_ppo2.py:185][0m |          -0.0005 |          52.5633 |           5.9906 |
[32m[20221214 00:11:41 @agent_ppo2.py:185][0m |          -0.0103 |          51.1209 |           6.0172 |
[32m[20221214 00:11:41 @agent_ppo2.py:185][0m |          -0.0085 |          50.7177 |           6.0012 |
[32m[20221214 00:11:41 @agent_ppo2.py:185][0m |          -0.0093 |          50.0347 |           6.0340 |
[32m[20221214 00:11:41 @agent_ppo2.py:185][0m |          -0.0098 |          49.9084 |           6.0414 |
[32m[20221214 00:11:41 @agent_ppo2.py:185][0m |          -0.0000 |          54.9564 |           6.0324 |
[32m[20221214 00:11:41 @agent_ppo2.py:185][0m |          -0.0160 |          49.5468 |           6.0060 |
[32m[20221214 00:11:41 @agent_ppo2.py:185][0m |          -0.0134 |          49.1621 |           6.0641 |
[32m[20221214 00:11:41 @agent_ppo2.py:185][0m |          -0.0137 |          49.1265 |           6.0027 |
[32m[20221214 00:11:41 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:11:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 428.58
[32m[20221214 00:11:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 490.08
[32m[20221214 00:11:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 440.15
[32m[20221214 00:11:41 @agent_ppo2.py:143][0m Total time:      14.18 min
[32m[20221214 00:11:41 @agent_ppo2.py:145][0m 1314816 total steps have happened
[32m[20221214 00:11:41 @agent_ppo2.py:121][0m #------------------------ Iteration 4642 --------------------------#
[32m[20221214 00:11:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:11:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:11:42 @agent_ppo2.py:185][0m |          -0.0003 |          57.1454 |           6.6384 |
[32m[20221214 00:11:42 @agent_ppo2.py:185][0m |          -0.0091 |          50.3166 |           6.7923 |
[32m[20221214 00:11:42 @agent_ppo2.py:185][0m |          -0.0088 |          47.7690 |           6.7878 |
[32m[20221214 00:11:42 @agent_ppo2.py:185][0m |          -0.0111 |          46.3263 |           6.7321 |
[32m[20221214 00:11:42 @agent_ppo2.py:185][0m |          -0.0160 |          45.7817 |           6.7137 |
[32m[20221214 00:11:42 @agent_ppo2.py:185][0m |          -0.0114 |          44.8552 |           6.7378 |
[32m[20221214 00:11:42 @agent_ppo2.py:185][0m |          -0.0180 |          43.9424 |           6.7548 |
[32m[20221214 00:11:42 @agent_ppo2.py:185][0m |          -0.0192 |          43.8987 |           6.7389 |
[32m[20221214 00:11:42 @agent_ppo2.py:185][0m |          -0.0115 |          44.2051 |           6.7276 |
[32m[20221214 00:11:42 @agent_ppo2.py:185][0m |          -0.0181 |          42.6168 |           6.7150 |
[32m[20221214 00:11:42 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:11:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 435.00
[32m[20221214 00:11:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 486.07
[32m[20221214 00:11:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 511.24
[32m[20221214 00:11:43 @agent_ppo2.py:143][0m Total time:      14.20 min
[32m[20221214 00:11:43 @agent_ppo2.py:145][0m 1316864 total steps have happened
[32m[20221214 00:11:43 @agent_ppo2.py:121][0m #------------------------ Iteration 4643 --------------------------#
[32m[20221214 00:11:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:11:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:11:43 @agent_ppo2.py:185][0m |           0.0031 |          58.5228 |           5.3092 |
[32m[20221214 00:11:43 @agent_ppo2.py:185][0m |          -0.0031 |          55.0396 |           5.3914 |
[32m[20221214 00:11:43 @agent_ppo2.py:185][0m |          -0.0076 |          53.4164 |           5.4137 |
[32m[20221214 00:11:43 @agent_ppo2.py:185][0m |          -0.0096 |          52.5864 |           5.4535 |
[32m[20221214 00:11:43 @agent_ppo2.py:185][0m |          -0.0140 |          51.8197 |           5.4570 |
[32m[20221214 00:11:43 @agent_ppo2.py:185][0m |          -0.0137 |          51.1872 |           5.4753 |
[32m[20221214 00:11:43 @agent_ppo2.py:185][0m |          -0.0088 |          50.9412 |           5.4994 |
[32m[20221214 00:11:44 @agent_ppo2.py:185][0m |          -0.0137 |          50.3623 |           5.5740 |
[32m[20221214 00:11:44 @agent_ppo2.py:185][0m |          -0.0117 |          50.0964 |           5.5624 |
[32m[20221214 00:11:44 @agent_ppo2.py:185][0m |          -0.0131 |          49.7670 |           5.6268 |
[32m[20221214 00:11:44 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:11:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 428.16
[32m[20221214 00:11:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 495.11
[32m[20221214 00:11:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 465.34
[32m[20221214 00:11:44 @agent_ppo2.py:143][0m Total time:      14.23 min
[32m[20221214 00:11:44 @agent_ppo2.py:145][0m 1318912 total steps have happened
[32m[20221214 00:11:44 @agent_ppo2.py:121][0m #------------------------ Iteration 4644 --------------------------#
[32m[20221214 00:11:44 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:11:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:11:44 @agent_ppo2.py:185][0m |           0.0046 |          68.4052 |           5.5917 |
[32m[20221214 00:11:44 @agent_ppo2.py:185][0m |          -0.0071 |          63.1204 |           5.6088 |
[32m[20221214 00:11:44 @agent_ppo2.py:185][0m |           0.0009 |          66.7807 |           5.5866 |
[32m[20221214 00:11:45 @agent_ppo2.py:185][0m |          -0.0078 |          60.6849 |           5.6277 |
[32m[20221214 00:11:45 @agent_ppo2.py:185][0m |          -0.0156 |          59.9809 |           5.6279 |
[32m[20221214 00:11:45 @agent_ppo2.py:185][0m |          -0.0187 |          59.5357 |           5.6132 |
[32m[20221214 00:11:45 @agent_ppo2.py:185][0m |          -0.0141 |          59.1075 |           5.5903 |
[32m[20221214 00:11:45 @agent_ppo2.py:185][0m |          -0.0156 |          58.8072 |           5.6465 |
[32m[20221214 00:11:45 @agent_ppo2.py:185][0m |          -0.0177 |          58.7622 |           5.6643 |
[32m[20221214 00:11:45 @agent_ppo2.py:185][0m |          -0.0160 |          58.2152 |           5.6905 |
[32m[20221214 00:11:45 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:11:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 409.86
[32m[20221214 00:11:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 440.28
[32m[20221214 00:11:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 431.30
[32m[20221214 00:11:45 @agent_ppo2.py:143][0m Total time:      14.25 min
[32m[20221214 00:11:45 @agent_ppo2.py:145][0m 1320960 total steps have happened
[32m[20221214 00:11:45 @agent_ppo2.py:121][0m #------------------------ Iteration 4645 --------------------------#
[32m[20221214 00:11:45 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:11:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:11:46 @agent_ppo2.py:185][0m |           0.0037 |          65.7399 |           6.3684 |
[32m[20221214 00:11:46 @agent_ppo2.py:185][0m |          -0.0032 |          62.2098 |           6.3551 |
[32m[20221214 00:11:46 @agent_ppo2.py:185][0m |          -0.0060 |          60.9390 |           6.3121 |
[32m[20221214 00:11:46 @agent_ppo2.py:185][0m |          -0.0067 |          60.2900 |           6.3405 |
[32m[20221214 00:11:46 @agent_ppo2.py:185][0m |          -0.0076 |          59.4307 |           6.2936 |
[32m[20221214 00:11:46 @agent_ppo2.py:185][0m |          -0.0071 |          58.7892 |           6.2841 |
[32m[20221214 00:11:46 @agent_ppo2.py:185][0m |          -0.0097 |          58.4875 |           6.3007 |
[32m[20221214 00:11:46 @agent_ppo2.py:185][0m |          -0.0091 |          58.1875 |           6.1964 |
[32m[20221214 00:11:46 @agent_ppo2.py:185][0m |          -0.0111 |          57.7744 |           6.2188 |
[32m[20221214 00:11:46 @agent_ppo2.py:185][0m |          -0.0044 |          61.4386 |           6.2197 |
[32m[20221214 00:11:46 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:11:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 410.90
[32m[20221214 00:11:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 517.74
[32m[20221214 00:11:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 535.22
[32m[20221214 00:11:47 @agent_ppo2.py:143][0m Total time:      14.27 min
[32m[20221214 00:11:47 @agent_ppo2.py:145][0m 1323008 total steps have happened
[32m[20221214 00:11:47 @agent_ppo2.py:121][0m #------------------------ Iteration 4646 --------------------------#
[32m[20221214 00:11:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:11:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:11:47 @agent_ppo2.py:185][0m |          -0.0001 |          71.4305 |           6.0499 |
[32m[20221214 00:11:47 @agent_ppo2.py:185][0m |          -0.0028 |          64.1165 |           6.0443 |
[32m[20221214 00:11:47 @agent_ppo2.py:185][0m |          -0.0070 |          60.2816 |           6.0136 |
[32m[20221214 00:11:47 @agent_ppo2.py:185][0m |          -0.0080 |          58.4332 |           5.9879 |
[32m[20221214 00:11:47 @agent_ppo2.py:185][0m |          -0.0036 |          59.7840 |           5.9307 |
[32m[20221214 00:11:47 @agent_ppo2.py:185][0m |          -0.0121 |          56.6206 |           5.9443 |
[32m[20221214 00:11:47 @agent_ppo2.py:185][0m |          -0.0101 |          55.3877 |           5.9162 |
[32m[20221214 00:11:48 @agent_ppo2.py:185][0m |          -0.0073 |          56.2983 |           5.9327 |
[32m[20221214 00:11:48 @agent_ppo2.py:185][0m |          -0.0136 |          54.5993 |           5.9757 |
[32m[20221214 00:11:48 @agent_ppo2.py:185][0m |          -0.0167 |          54.0285 |           5.9339 |
[32m[20221214 00:11:48 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:11:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 487.65
[32m[20221214 00:11:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 527.65
[32m[20221214 00:11:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 145.31
[32m[20221214 00:11:48 @agent_ppo2.py:143][0m Total time:      14.29 min
[32m[20221214 00:11:48 @agent_ppo2.py:145][0m 1325056 total steps have happened
[32m[20221214 00:11:48 @agent_ppo2.py:121][0m #------------------------ Iteration 4647 --------------------------#
[32m[20221214 00:11:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:11:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:11:48 @agent_ppo2.py:185][0m |          -0.0010 |          73.5669 |           5.7593 |
[32m[20221214 00:11:48 @agent_ppo2.py:185][0m |           0.0075 |          74.1551 |           5.7677 |
[32m[20221214 00:11:48 @agent_ppo2.py:185][0m |          -0.0071 |          68.2842 |           5.7659 |
[32m[20221214 00:11:49 @agent_ppo2.py:185][0m |          -0.0096 |          67.3089 |           5.7334 |
[32m[20221214 00:11:49 @agent_ppo2.py:185][0m |          -0.0101 |          66.7892 |           5.7208 |
[32m[20221214 00:11:49 @agent_ppo2.py:185][0m |          -0.0059 |          76.1125 |           5.7616 |
[32m[20221214 00:11:49 @agent_ppo2.py:185][0m |          -0.0035 |          70.1903 |           5.7509 |
[32m[20221214 00:11:49 @agent_ppo2.py:185][0m |          -0.0169 |          65.5690 |           5.7068 |
[32m[20221214 00:11:49 @agent_ppo2.py:185][0m |          -0.0157 |          65.5175 |           5.7739 |
[32m[20221214 00:11:49 @agent_ppo2.py:185][0m |          -0.0192 |          65.0659 |           5.7020 |
[32m[20221214 00:11:49 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:11:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 472.59
[32m[20221214 00:11:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 496.63
[32m[20221214 00:11:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 515.82
[32m[20221214 00:11:49 @agent_ppo2.py:143][0m Total time:      14.31 min
[32m[20221214 00:11:49 @agent_ppo2.py:145][0m 1327104 total steps have happened
[32m[20221214 00:11:49 @agent_ppo2.py:121][0m #------------------------ Iteration 4648 --------------------------#
[32m[20221214 00:11:49 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:11:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:11:50 @agent_ppo2.py:185][0m |           0.0053 |          58.9356 |           5.4274 |
[32m[20221214 00:11:50 @agent_ppo2.py:185][0m |          -0.0002 |          48.6740 |           5.4743 |
[32m[20221214 00:11:50 @agent_ppo2.py:185][0m |          -0.0130 |          46.7329 |           5.4125 |
[32m[20221214 00:11:50 @agent_ppo2.py:185][0m |          -0.0110 |          45.6122 |           5.4264 |
[32m[20221214 00:11:50 @agent_ppo2.py:185][0m |          -0.0130 |          44.8058 |           5.4307 |
[32m[20221214 00:11:50 @agent_ppo2.py:185][0m |          -0.0134 |          44.3351 |           5.3835 |
[32m[20221214 00:11:50 @agent_ppo2.py:185][0m |          -0.0100 |          43.8016 |           5.4547 |
[32m[20221214 00:11:50 @agent_ppo2.py:185][0m |          -0.0134 |          43.6443 |           5.3914 |
[32m[20221214 00:11:50 @agent_ppo2.py:185][0m |          -0.0057 |          43.8139 |           5.4234 |
[32m[20221214 00:11:50 @agent_ppo2.py:185][0m |          -0.0142 |          43.2883 |           5.3920 |
[32m[20221214 00:11:50 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:11:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 365.95
[32m[20221214 00:11:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 491.10
[32m[20221214 00:11:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 538.59
[32m[20221214 00:11:50 @agent_ppo2.py:143][0m Total time:      14.34 min
[32m[20221214 00:11:50 @agent_ppo2.py:145][0m 1329152 total steps have happened
[32m[20221214 00:11:50 @agent_ppo2.py:121][0m #------------------------ Iteration 4649 --------------------------#
[32m[20221214 00:11:51 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:11:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:11:51 @agent_ppo2.py:185][0m |           0.0089 |          72.3685 |           4.8212 |
[32m[20221214 00:11:51 @agent_ppo2.py:185][0m |          -0.0034 |          63.6200 |           4.7925 |
[32m[20221214 00:11:51 @agent_ppo2.py:185][0m |          -0.0046 |          62.2609 |           4.8539 |
[32m[20221214 00:11:51 @agent_ppo2.py:185][0m |          -0.0070 |          61.2893 |           4.8192 |
[32m[20221214 00:11:51 @agent_ppo2.py:185][0m |          -0.0130 |          61.1238 |           4.8427 |
[32m[20221214 00:11:51 @agent_ppo2.py:185][0m |          -0.0123 |          60.3290 |           4.9013 |
[32m[20221214 00:11:51 @agent_ppo2.py:185][0m |           0.0018 |          65.2255 |           4.8873 |
[32m[20221214 00:11:51 @agent_ppo2.py:185][0m |          -0.0143 |          59.6969 |           4.9134 |
[32m[20221214 00:11:52 @agent_ppo2.py:185][0m |          -0.0098 |          59.3380 |           4.8945 |
[32m[20221214 00:11:52 @agent_ppo2.py:185][0m |          -0.0164 |          59.2801 |           4.8825 |
[32m[20221214 00:11:52 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:11:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 520.16
[32m[20221214 00:11:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 560.77
[32m[20221214 00:11:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 428.95
[32m[20221214 00:11:52 @agent_ppo2.py:143][0m Total time:      14.36 min
[32m[20221214 00:11:52 @agent_ppo2.py:145][0m 1331200 total steps have happened
[32m[20221214 00:11:52 @agent_ppo2.py:121][0m #------------------------ Iteration 4650 --------------------------#
[32m[20221214 00:11:52 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:11:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:11:52 @agent_ppo2.py:185][0m |           0.0098 |          71.7733 |           5.3674 |
[32m[20221214 00:11:52 @agent_ppo2.py:185][0m |          -0.0069 |          64.3076 |           5.4705 |
[32m[20221214 00:11:52 @agent_ppo2.py:185][0m |          -0.0011 |          63.7086 |           5.4833 |
[32m[20221214 00:11:52 @agent_ppo2.py:185][0m |          -0.0069 |          61.7919 |           5.5016 |
[32m[20221214 00:11:52 @agent_ppo2.py:185][0m |          -0.0105 |          60.9994 |           5.5236 |
[32m[20221214 00:11:53 @agent_ppo2.py:185][0m |          -0.0094 |          61.5967 |           5.4857 |
[32m[20221214 00:11:53 @agent_ppo2.py:185][0m |          -0.0168 |          59.9797 |           5.6337 |
[32m[20221214 00:11:53 @agent_ppo2.py:185][0m |          -0.0142 |          59.3695 |           5.6015 |
[32m[20221214 00:11:53 @agent_ppo2.py:185][0m |          -0.0144 |          58.7630 |           5.6474 |
[32m[20221214 00:11:53 @agent_ppo2.py:185][0m |          -0.0152 |          58.4884 |           5.6414 |
[32m[20221214 00:11:53 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:11:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 467.57
[32m[20221214 00:11:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 498.85
[32m[20221214 00:11:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 485.27
[32m[20221214 00:11:53 @agent_ppo2.py:143][0m Total time:      14.38 min
[32m[20221214 00:11:53 @agent_ppo2.py:145][0m 1333248 total steps have happened
[32m[20221214 00:11:53 @agent_ppo2.py:121][0m #------------------------ Iteration 4651 --------------------------#
[32m[20221214 00:11:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:11:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:11:53 @agent_ppo2.py:185][0m |          -0.0015 |          76.1516 |           5.4662 |
[32m[20221214 00:11:53 @agent_ppo2.py:185][0m |          -0.0073 |          72.9173 |           5.4176 |
[32m[20221214 00:11:54 @agent_ppo2.py:185][0m |           0.0014 |          74.9933 |           5.5150 |
[32m[20221214 00:11:54 @agent_ppo2.py:185][0m |          -0.0016 |          72.2409 |           5.5117 |
[32m[20221214 00:11:54 @agent_ppo2.py:185][0m |          -0.0101 |          70.6044 |           5.5563 |
[32m[20221214 00:11:54 @agent_ppo2.py:185][0m |          -0.0107 |          69.6409 |           5.5742 |
[32m[20221214 00:11:54 @agent_ppo2.py:185][0m |          -0.0149 |          69.4799 |           5.5885 |
[32m[20221214 00:11:54 @agent_ppo2.py:185][0m |          -0.0137 |          69.0278 |           5.6198 |
[32m[20221214 00:11:54 @agent_ppo2.py:185][0m |          -0.0135 |          68.9705 |           5.6547 |
[32m[20221214 00:11:54 @agent_ppo2.py:185][0m |          -0.0005 |          77.3390 |           5.6968 |
[32m[20221214 00:11:54 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:11:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 485.72
[32m[20221214 00:11:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 524.54
[32m[20221214 00:11:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 484.38
[32m[20221214 00:11:54 @agent_ppo2.py:143][0m Total time:      14.40 min
[32m[20221214 00:11:54 @agent_ppo2.py:145][0m 1335296 total steps have happened
[32m[20221214 00:11:54 @agent_ppo2.py:121][0m #------------------------ Iteration 4652 --------------------------#
[32m[20221214 00:11:54 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:11:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:11:55 @agent_ppo2.py:185][0m |           0.0010 |          52.0980 |           5.5703 |
[32m[20221214 00:11:55 @agent_ppo2.py:185][0m |          -0.0063 |          45.3004 |           5.6615 |
[32m[20221214 00:11:55 @agent_ppo2.py:185][0m |          -0.0062 |          43.1634 |           5.6421 |
[32m[20221214 00:11:55 @agent_ppo2.py:185][0m |          -0.0015 |          51.1097 |           5.6434 |
[32m[20221214 00:11:55 @agent_ppo2.py:185][0m |          -0.0094 |          42.6837 |           5.6710 |
[32m[20221214 00:11:55 @agent_ppo2.py:185][0m |          -0.0107 |          40.0906 |           5.6704 |
[32m[20221214 00:11:55 @agent_ppo2.py:185][0m |          -0.0168 |          39.5540 |           5.7326 |
[32m[20221214 00:11:55 @agent_ppo2.py:185][0m |          -0.0199 |          39.0026 |           5.7786 |
[32m[20221214 00:11:55 @agent_ppo2.py:185][0m |          -0.0198 |          38.5624 |           5.7906 |
[32m[20221214 00:11:55 @agent_ppo2.py:185][0m |          -0.0230 |          38.2767 |           5.7881 |
[32m[20221214 00:11:55 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:11:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 401.56
[32m[20221214 00:11:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 492.96
[32m[20221214 00:11:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 584.94
[32m[20221214 00:11:56 @agent_ppo2.py:143][0m Total time:      14.42 min
[32m[20221214 00:11:56 @agent_ppo2.py:145][0m 1337344 total steps have happened
[32m[20221214 00:11:56 @agent_ppo2.py:121][0m #------------------------ Iteration 4653 --------------------------#
[32m[20221214 00:11:56 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:11:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:11:56 @agent_ppo2.py:185][0m |           0.0083 |          73.2282 |           6.1255 |
[32m[20221214 00:11:56 @agent_ppo2.py:185][0m |          -0.0072 |          63.7467 |           6.1043 |
[32m[20221214 00:11:56 @agent_ppo2.py:185][0m |          -0.0069 |          62.2756 |           6.1745 |
[32m[20221214 00:11:56 @agent_ppo2.py:185][0m |          -0.0116 |          60.7194 |           6.1966 |
[32m[20221214 00:11:56 @agent_ppo2.py:185][0m |          -0.0132 |          59.3627 |           6.2689 |
[32m[20221214 00:11:56 @agent_ppo2.py:185][0m |          -0.0161 |          58.6172 |           6.2153 |
[32m[20221214 00:11:56 @agent_ppo2.py:185][0m |          -0.0206 |          57.7736 |           6.2924 |
[32m[20221214 00:11:57 @agent_ppo2.py:185][0m |          -0.0165 |          57.3343 |           6.2631 |
[32m[20221214 00:11:57 @agent_ppo2.py:185][0m |          -0.0156 |          56.6512 |           6.2513 |
[32m[20221214 00:11:57 @agent_ppo2.py:185][0m |          -0.0197 |          56.4651 |           6.3586 |
[32m[20221214 00:11:57 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:11:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 451.06
[32m[20221214 00:11:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 556.29
[32m[20221214 00:11:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 558.11
[32m[20221214 00:11:57 @agent_ppo2.py:143][0m Total time:      14.44 min
[32m[20221214 00:11:57 @agent_ppo2.py:145][0m 1339392 total steps have happened
[32m[20221214 00:11:57 @agent_ppo2.py:121][0m #------------------------ Iteration 4654 --------------------------#
[32m[20221214 00:11:57 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:11:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:11:57 @agent_ppo2.py:185][0m |          -0.0030 |          57.6284 |           6.4619 |
[32m[20221214 00:11:57 @agent_ppo2.py:185][0m |          -0.0046 |          50.5164 |           6.4646 |
[32m[20221214 00:11:57 @agent_ppo2.py:185][0m |          -0.0086 |          47.4576 |           6.5423 |
[32m[20221214 00:11:57 @agent_ppo2.py:185][0m |          -0.0182 |          46.0569 |           6.5727 |
[32m[20221214 00:11:58 @agent_ppo2.py:185][0m |          -0.0145 |          44.6281 |           6.5858 |
[32m[20221214 00:11:58 @agent_ppo2.py:185][0m |          -0.0082 |          44.5030 |           6.6355 |
[32m[20221214 00:11:58 @agent_ppo2.py:185][0m |          -0.0159 |          42.8476 |           6.6508 |
[32m[20221214 00:11:58 @agent_ppo2.py:185][0m |          -0.0167 |          42.2060 |           6.7158 |
[32m[20221214 00:11:58 @agent_ppo2.py:185][0m |          -0.0130 |          42.2716 |           6.7090 |
[32m[20221214 00:11:58 @agent_ppo2.py:185][0m |          -0.0218 |          40.9664 |           6.6919 |
[32m[20221214 00:11:58 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:11:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 308.06
[32m[20221214 00:11:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 489.70
[32m[20221214 00:11:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 568.53
[32m[20221214 00:11:58 @agent_ppo2.py:143][0m Total time:      14.46 min
[32m[20221214 00:11:58 @agent_ppo2.py:145][0m 1341440 total steps have happened
[32m[20221214 00:11:58 @agent_ppo2.py:121][0m #------------------------ Iteration 4655 --------------------------#
[32m[20221214 00:11:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:11:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:11:58 @agent_ppo2.py:185][0m |           0.0115 |          86.5603 |           6.7212 |
[32m[20221214 00:11:59 @agent_ppo2.py:185][0m |          -0.0062 |          71.1585 |           6.7603 |
[32m[20221214 00:11:59 @agent_ppo2.py:185][0m |          -0.0106 |          68.5161 |           6.7328 |
[32m[20221214 00:11:59 @agent_ppo2.py:185][0m |          -0.0098 |          66.8012 |           6.7403 |
[32m[20221214 00:11:59 @agent_ppo2.py:185][0m |          -0.0097 |          65.8767 |           6.7081 |
[32m[20221214 00:11:59 @agent_ppo2.py:185][0m |          -0.0166 |          64.9343 |           6.7676 |
[32m[20221214 00:11:59 @agent_ppo2.py:185][0m |          -0.0157 |          64.2409 |           6.7370 |
[32m[20221214 00:11:59 @agent_ppo2.py:185][0m |          -0.0171 |          63.7220 |           6.7521 |
[32m[20221214 00:11:59 @agent_ppo2.py:185][0m |          -0.0231 |          63.3101 |           6.7482 |
[32m[20221214 00:11:59 @agent_ppo2.py:185][0m |          -0.0134 |          63.3880 |           6.7026 |
[32m[20221214 00:11:59 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:11:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 477.97
[32m[20221214 00:11:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 528.86
[32m[20221214 00:11:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 359.38
[32m[20221214 00:11:59 @agent_ppo2.py:143][0m Total time:      14.48 min
[32m[20221214 00:11:59 @agent_ppo2.py:145][0m 1343488 total steps have happened
[32m[20221214 00:11:59 @agent_ppo2.py:121][0m #------------------------ Iteration 4656 --------------------------#
[32m[20221214 00:12:00 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:12:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:12:00 @agent_ppo2.py:185][0m |          -0.0022 |          76.0571 |           6.9893 |
[32m[20221214 00:12:00 @agent_ppo2.py:185][0m |          -0.0133 |          68.7865 |           7.0865 |
[32m[20221214 00:12:00 @agent_ppo2.py:185][0m |          -0.0171 |          65.6847 |           7.0400 |
[32m[20221214 00:12:00 @agent_ppo2.py:185][0m |          -0.0150 |          63.5876 |           7.0798 |
[32m[20221214 00:12:00 @agent_ppo2.py:185][0m |          -0.0137 |          65.4000 |           7.0621 |
[32m[20221214 00:12:00 @agent_ppo2.py:185][0m |          -0.0066 |          64.8752 |           7.0580 |
[32m[20221214 00:12:00 @agent_ppo2.py:185][0m |          -0.0191 |          59.7612 |           7.1800 |
[32m[20221214 00:12:00 @agent_ppo2.py:185][0m |          -0.0180 |          58.7828 |           7.1109 |
[32m[20221214 00:12:00 @agent_ppo2.py:185][0m |          -0.0192 |          58.3657 |           7.1658 |
[32m[20221214 00:12:01 @agent_ppo2.py:185][0m |          -0.0221 |          57.6332 |           7.1437 |
[32m[20221214 00:12:01 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:12:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 498.89
[32m[20221214 00:12:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 557.89
[32m[20221214 00:12:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 539.39
[32m[20221214 00:12:01 @agent_ppo2.py:143][0m Total time:      14.51 min
[32m[20221214 00:12:01 @agent_ppo2.py:145][0m 1345536 total steps have happened
[32m[20221214 00:12:01 @agent_ppo2.py:121][0m #------------------------ Iteration 4657 --------------------------#
[32m[20221214 00:12:01 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:12:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:12:01 @agent_ppo2.py:185][0m |          -0.0089 |          76.5940 |           6.6268 |
[32m[20221214 00:12:01 @agent_ppo2.py:185][0m |          -0.0068 |          64.8652 |           6.6924 |
[32m[20221214 00:12:01 @agent_ppo2.py:185][0m |          -0.0023 |          61.2697 |           6.6691 |
[32m[20221214 00:12:01 @agent_ppo2.py:185][0m |          -0.0100 |          59.0439 |           6.6565 |
[32m[20221214 00:12:01 @agent_ppo2.py:185][0m |          -0.0129 |          57.7097 |           6.5934 |
[32m[20221214 00:12:01 @agent_ppo2.py:185][0m |          -0.0156 |          56.1662 |           6.6390 |
[32m[20221214 00:12:02 @agent_ppo2.py:185][0m |          -0.0140 |          55.0749 |           6.5959 |
[32m[20221214 00:12:02 @agent_ppo2.py:185][0m |          -0.0100 |          54.2426 |           6.6338 |
[32m[20221214 00:12:02 @agent_ppo2.py:185][0m |          -0.0170 |          53.8556 |           6.5713 |
[32m[20221214 00:12:02 @agent_ppo2.py:185][0m |          -0.0200 |          53.3557 |           6.5580 |
[32m[20221214 00:12:02 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:12:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 397.91
[32m[20221214 00:12:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 451.77
[32m[20221214 00:12:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 611.26
[32m[20221214 00:12:02 @agent_ppo2.py:143][0m Total time:      14.53 min
[32m[20221214 00:12:02 @agent_ppo2.py:145][0m 1347584 total steps have happened
[32m[20221214 00:12:02 @agent_ppo2.py:121][0m #------------------------ Iteration 4658 --------------------------#
[32m[20221214 00:12:02 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:12:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:12:02 @agent_ppo2.py:185][0m |          -0.0030 |          66.1282 |           6.4618 |
[32m[20221214 00:12:02 @agent_ppo2.py:185][0m |          -0.0099 |          56.7300 |           6.5264 |
[32m[20221214 00:12:02 @agent_ppo2.py:185][0m |          -0.0071 |          52.8922 |           6.4789 |
[32m[20221214 00:12:03 @agent_ppo2.py:185][0m |          -0.0014 |          55.0361 |           6.4082 |
[32m[20221214 00:12:03 @agent_ppo2.py:185][0m |          -0.0056 |          48.4726 |           6.4265 |
[32m[20221214 00:12:03 @agent_ppo2.py:185][0m |          -0.0091 |          47.6005 |           6.4704 |
[32m[20221214 00:12:03 @agent_ppo2.py:185][0m |          -0.0078 |          47.1323 |           6.3954 |
[32m[20221214 00:12:03 @agent_ppo2.py:185][0m |          -0.0019 |          48.9703 |           6.3153 |
[32m[20221214 00:12:03 @agent_ppo2.py:185][0m |          -0.0109 |          46.1047 |           6.4008 |
[32m[20221214 00:12:03 @agent_ppo2.py:185][0m |          -0.0157 |          44.8830 |           6.3086 |
[32m[20221214 00:12:03 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:12:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 478.16
[32m[20221214 00:12:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 527.48
[32m[20221214 00:12:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 552.27
[32m[20221214 00:12:03 @agent_ppo2.py:143][0m Total time:      14.55 min
[32m[20221214 00:12:03 @agent_ppo2.py:145][0m 1349632 total steps have happened
[32m[20221214 00:12:03 @agent_ppo2.py:121][0m #------------------------ Iteration 4659 --------------------------#
[32m[20221214 00:12:03 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:12:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:12:04 @agent_ppo2.py:185][0m |           0.0031 |          74.0881 |           6.4037 |
[32m[20221214 00:12:04 @agent_ppo2.py:185][0m |          -0.0100 |          65.8012 |           6.3746 |
[32m[20221214 00:12:04 @agent_ppo2.py:185][0m |          -0.0095 |          62.7137 |           6.4640 |
[32m[20221214 00:12:04 @agent_ppo2.py:185][0m |          -0.0135 |          60.7613 |           6.4962 |
[32m[20221214 00:12:04 @agent_ppo2.py:185][0m |          -0.0083 |          60.3059 |           6.4355 |
[32m[20221214 00:12:04 @agent_ppo2.py:185][0m |          -0.0149 |          58.5518 |           6.4853 |
[32m[20221214 00:12:04 @agent_ppo2.py:185][0m |          -0.0160 |          57.4509 |           6.4840 |
[32m[20221214 00:12:04 @agent_ppo2.py:185][0m |          -0.0112 |          61.7902 |           6.5441 |
[32m[20221214 00:12:04 @agent_ppo2.py:185][0m |          -0.0192 |          56.9093 |           6.5775 |
[32m[20221214 00:12:04 @agent_ppo2.py:185][0m |          -0.0129 |          56.6449 |           6.5807 |
[32m[20221214 00:12:04 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:12:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 475.84
[32m[20221214 00:12:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 534.93
[32m[20221214 00:12:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 364.26
[32m[20221214 00:12:04 @agent_ppo2.py:143][0m Total time:      14.57 min
[32m[20221214 00:12:04 @agent_ppo2.py:145][0m 1351680 total steps have happened
[32m[20221214 00:12:04 @agent_ppo2.py:121][0m #------------------------ Iteration 4660 --------------------------#
[32m[20221214 00:12:05 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:12:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:12:05 @agent_ppo2.py:185][0m |           0.0025 |          44.5283 |           6.1425 |
[32m[20221214 00:12:05 @agent_ppo2.py:185][0m |           0.0012 |          34.5326 |           6.2161 |
[32m[20221214 00:12:05 @agent_ppo2.py:185][0m |          -0.0111 |          31.8144 |           6.1570 |
[32m[20221214 00:12:05 @agent_ppo2.py:185][0m |          -0.0110 |          31.6169 |           6.2042 |
[32m[20221214 00:12:05 @agent_ppo2.py:185][0m |          -0.0092 |          28.9857 |           6.1875 |
[32m[20221214 00:12:05 @agent_ppo2.py:185][0m |          -0.0086 |          28.2285 |           6.1801 |
[32m[20221214 00:12:05 @agent_ppo2.py:185][0m |          -0.0139 |          27.6247 |           6.1838 |
[32m[20221214 00:12:05 @agent_ppo2.py:185][0m |          -0.0161 |          27.2258 |           6.1589 |
[32m[20221214 00:12:06 @agent_ppo2.py:185][0m |          -0.0216 |          26.7548 |           6.0620 |
[32m[20221214 00:12:06 @agent_ppo2.py:185][0m |          -0.0228 |          26.4731 |           6.0778 |
[32m[20221214 00:12:06 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:12:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 277.44
[32m[20221214 00:12:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 411.84
[32m[20221214 00:12:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 564.77
[32m[20221214 00:12:06 @agent_ppo2.py:143][0m Total time:      14.59 min
[32m[20221214 00:12:06 @agent_ppo2.py:145][0m 1353728 total steps have happened
[32m[20221214 00:12:06 @agent_ppo2.py:121][0m #------------------------ Iteration 4661 --------------------------#
[32m[20221214 00:12:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:12:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:12:06 @agent_ppo2.py:185][0m |           0.0032 |          42.1674 |           6.9958 |
[32m[20221214 00:12:06 @agent_ppo2.py:185][0m |          -0.0048 |          36.3183 |           6.9077 |
[32m[20221214 00:12:06 @agent_ppo2.py:185][0m |          -0.0092 |          34.8137 |           6.9816 |
[32m[20221214 00:12:06 @agent_ppo2.py:185][0m |          -0.0076 |          33.7423 |           6.9939 |
[32m[20221214 00:12:06 @agent_ppo2.py:185][0m |          -0.0065 |          33.2486 |           6.9702 |
[32m[20221214 00:12:06 @agent_ppo2.py:185][0m |          -0.0150 |          32.2426 |           6.8932 |
[32m[20221214 00:12:07 @agent_ppo2.py:185][0m |          -0.0181 |          31.9518 |           6.9510 |
[32m[20221214 00:12:07 @agent_ppo2.py:185][0m |          -0.0161 |          31.4635 |           6.9432 |
[32m[20221214 00:12:07 @agent_ppo2.py:185][0m |          -0.0208 |          30.8752 |           6.9011 |
[32m[20221214 00:12:07 @agent_ppo2.py:185][0m |          -0.0168 |          30.4631 |           6.8606 |
[32m[20221214 00:12:07 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:12:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 274.67
[32m[20221214 00:12:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 431.90
[32m[20221214 00:12:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 583.94
[32m[20221214 00:12:07 @agent_ppo2.py:143][0m Total time:      14.61 min
[32m[20221214 00:12:07 @agent_ppo2.py:145][0m 1355776 total steps have happened
[32m[20221214 00:12:07 @agent_ppo2.py:121][0m #------------------------ Iteration 4662 --------------------------#
[32m[20221214 00:12:07 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:12:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:12:07 @agent_ppo2.py:185][0m |           0.0030 |          44.8248 |           5.8888 |
[32m[20221214 00:12:07 @agent_ppo2.py:185][0m |          -0.0091 |          39.5653 |           5.8748 |
[32m[20221214 00:12:07 @agent_ppo2.py:185][0m |          -0.0079 |          37.5804 |           5.9091 |
[32m[20221214 00:12:08 @agent_ppo2.py:185][0m |          -0.0121 |          36.2438 |           5.9135 |
[32m[20221214 00:12:08 @agent_ppo2.py:185][0m |          -0.0107 |          35.5477 |           5.8899 |
[32m[20221214 00:12:08 @agent_ppo2.py:185][0m |          -0.0110 |          35.1496 |           5.9447 |
[32m[20221214 00:12:08 @agent_ppo2.py:185][0m |          -0.0116 |          34.5297 |           5.9238 |
[32m[20221214 00:12:08 @agent_ppo2.py:185][0m |          -0.0170 |          34.0476 |           5.9247 |
[32m[20221214 00:12:08 @agent_ppo2.py:185][0m |          -0.0160 |          33.7069 |           5.9700 |
[32m[20221214 00:12:08 @agent_ppo2.py:185][0m |          -0.0155 |          33.6708 |           5.9395 |
[32m[20221214 00:12:08 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:12:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 392.31
[32m[20221214 00:12:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 495.90
[32m[20221214 00:12:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 289.69
[32m[20221214 00:12:08 @agent_ppo2.py:143][0m Total time:      14.63 min
[32m[20221214 00:12:08 @agent_ppo2.py:145][0m 1357824 total steps have happened
[32m[20221214 00:12:08 @agent_ppo2.py:121][0m #------------------------ Iteration 4663 --------------------------#
[32m[20221214 00:12:08 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:12:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:12:09 @agent_ppo2.py:185][0m |           0.0078 |          51.5402 |           6.2298 |
[32m[20221214 00:12:09 @agent_ppo2.py:185][0m |          -0.0018 |          42.9876 |           6.1330 |
[32m[20221214 00:12:09 @agent_ppo2.py:185][0m |          -0.0100 |          40.3037 |           6.1854 |
[32m[20221214 00:12:09 @agent_ppo2.py:185][0m |          -0.0148 |          38.5527 |           6.1032 |
[32m[20221214 00:12:09 @agent_ppo2.py:185][0m |          -0.0189 |          37.2318 |           6.1030 |
[32m[20221214 00:12:09 @agent_ppo2.py:185][0m |          -0.0107 |          36.4591 |           6.0478 |
[32m[20221214 00:12:09 @agent_ppo2.py:185][0m |          -0.0092 |          36.7083 |           5.9951 |
[32m[20221214 00:12:09 @agent_ppo2.py:185][0m |          -0.0192 |          35.0730 |           5.9719 |
[32m[20221214 00:12:09 @agent_ppo2.py:185][0m |          -0.0183 |          34.4189 |           5.9526 |
[32m[20221214 00:12:09 @agent_ppo2.py:185][0m |          -0.0202 |          33.9385 |           5.9326 |
[32m[20221214 00:12:09 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:12:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 391.93
[32m[20221214 00:12:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 442.54
[32m[20221214 00:12:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 553.12
[32m[20221214 00:12:10 @agent_ppo2.py:143][0m Total time:      14.65 min
[32m[20221214 00:12:10 @agent_ppo2.py:145][0m 1359872 total steps have happened
[32m[20221214 00:12:10 @agent_ppo2.py:121][0m #------------------------ Iteration 4664 --------------------------#
[32m[20221214 00:12:10 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:12:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:12:10 @agent_ppo2.py:185][0m |           0.0051 |          73.4264 |           5.2691 |
[32m[20221214 00:12:10 @agent_ppo2.py:185][0m |          -0.0090 |          61.3538 |           5.2715 |
[32m[20221214 00:12:10 @agent_ppo2.py:185][0m |          -0.0155 |          57.4348 |           5.2525 |
[32m[20221214 00:12:10 @agent_ppo2.py:185][0m |          -0.0108 |          54.9339 |           5.2527 |
[32m[20221214 00:12:10 @agent_ppo2.py:185][0m |          -0.0112 |          53.3028 |           5.2575 |
[32m[20221214 00:12:10 @agent_ppo2.py:185][0m |          -0.0046 |          55.7835 |           5.2552 |
[32m[20221214 00:12:10 @agent_ppo2.py:185][0m |          -0.0158 |          51.7957 |           5.2781 |
[32m[20221214 00:12:11 @agent_ppo2.py:185][0m |          -0.0051 |          57.7660 |           5.2645 |
[32m[20221214 00:12:11 @agent_ppo2.py:185][0m |          -0.0005 |          52.6938 |           5.2385 |
[32m[20221214 00:12:11 @agent_ppo2.py:185][0m |          -0.0149 |          50.0389 |           5.2449 |
[32m[20221214 00:12:11 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:12:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 516.63
[32m[20221214 00:12:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 545.33
[32m[20221214 00:12:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 525.25
[32m[20221214 00:12:11 @agent_ppo2.py:143][0m Total time:      14.67 min
[32m[20221214 00:12:11 @agent_ppo2.py:145][0m 1361920 total steps have happened
[32m[20221214 00:12:11 @agent_ppo2.py:121][0m #------------------------ Iteration 4665 --------------------------#
[32m[20221214 00:12:11 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:12:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:12:11 @agent_ppo2.py:185][0m |          -0.0015 |          41.8163 |           5.9471 |
[32m[20221214 00:12:11 @agent_ppo2.py:185][0m |          -0.0060 |          31.4314 |           6.0470 |
[32m[20221214 00:12:11 @agent_ppo2.py:185][0m |          -0.0035 |          30.7517 |           6.0367 |
[32m[20221214 00:12:11 @agent_ppo2.py:185][0m |          -0.0120 |          29.3853 |           6.1043 |
[32m[20221214 00:12:12 @agent_ppo2.py:185][0m |          -0.0132 |          27.8210 |           6.2423 |
[32m[20221214 00:12:12 @agent_ppo2.py:185][0m |          -0.0093 |          27.6519 |           6.2257 |
[32m[20221214 00:12:12 @agent_ppo2.py:185][0m |          -0.0164 |          26.5911 |           6.2440 |
[32m[20221214 00:12:12 @agent_ppo2.py:185][0m |          -0.0124 |          26.3023 |           6.3219 |
[32m[20221214 00:12:12 @agent_ppo2.py:185][0m |          -0.0162 |          26.2876 |           6.3178 |
[32m[20221214 00:12:12 @agent_ppo2.py:185][0m |          -0.0100 |          25.7362 |           6.3514 |
[32m[20221214 00:12:12 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:12:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 516.12
[32m[20221214 00:12:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 592.65
[32m[20221214 00:12:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 493.66
[32m[20221214 00:12:12 @agent_ppo2.py:143][0m Total time:      14.70 min
[32m[20221214 00:12:12 @agent_ppo2.py:145][0m 1363968 total steps have happened
[32m[20221214 00:12:12 @agent_ppo2.py:121][0m #------------------------ Iteration 4666 --------------------------#
[32m[20221214 00:12:12 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:12:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:12:12 @agent_ppo2.py:185][0m |           0.0127 |          64.5574 |           6.3129 |
[32m[20221214 00:12:13 @agent_ppo2.py:185][0m |           0.0069 |          60.1619 |           6.3435 |
[32m[20221214 00:12:13 @agent_ppo2.py:185][0m |          -0.0058 |          54.2482 |           6.3652 |
[32m[20221214 00:12:13 @agent_ppo2.py:185][0m |          -0.0134 |          53.2919 |           6.2878 |
[32m[20221214 00:12:13 @agent_ppo2.py:185][0m |          -0.0140 |          52.7562 |           6.3270 |
[32m[20221214 00:12:13 @agent_ppo2.py:185][0m |          -0.0097 |          52.2822 |           6.2350 |
[32m[20221214 00:12:13 @agent_ppo2.py:185][0m |          -0.0117 |          52.0045 |           6.3217 |
[32m[20221214 00:12:13 @agent_ppo2.py:185][0m |          -0.0154 |          51.9077 |           6.2245 |
[32m[20221214 00:12:13 @agent_ppo2.py:185][0m |          -0.0165 |          51.5679 |           6.1877 |
[32m[20221214 00:12:13 @agent_ppo2.py:185][0m |          -0.0103 |          51.4917 |           6.1403 |
[32m[20221214 00:12:13 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:12:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 476.07
[32m[20221214 00:12:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 546.42
[32m[20221214 00:12:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 539.72
[32m[20221214 00:12:13 @agent_ppo2.py:143][0m Total time:      14.72 min
[32m[20221214 00:12:13 @agent_ppo2.py:145][0m 1366016 total steps have happened
[32m[20221214 00:12:13 @agent_ppo2.py:121][0m #------------------------ Iteration 4667 --------------------------#
[32m[20221214 00:12:14 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:12:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:12:14 @agent_ppo2.py:185][0m |           0.0069 |          40.6967 |           6.3303 |
[32m[20221214 00:12:14 @agent_ppo2.py:185][0m |          -0.0081 |          34.7333 |           6.3182 |
[32m[20221214 00:12:14 @agent_ppo2.py:185][0m |          -0.0108 |          33.2104 |           6.3273 |
[32m[20221214 00:12:14 @agent_ppo2.py:185][0m |          -0.0133 |          32.3880 |           6.3455 |
[32m[20221214 00:12:14 @agent_ppo2.py:185][0m |          -0.0074 |          31.9644 |           6.5108 |
[32m[20221214 00:12:14 @agent_ppo2.py:185][0m |          -0.0202 |          31.3578 |           6.5027 |
[32m[20221214 00:12:14 @agent_ppo2.py:185][0m |          -0.0117 |          31.1302 |           6.4622 |
[32m[20221214 00:12:14 @agent_ppo2.py:185][0m |           0.0005 |          36.5602 |           6.5144 |
[32m[20221214 00:12:14 @agent_ppo2.py:185][0m |          -0.0143 |          30.6947 |           6.5423 |
[32m[20221214 00:12:15 @agent_ppo2.py:185][0m |          -0.0176 |          30.0628 |           6.6142 |
[32m[20221214 00:12:15 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:12:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 428.95
[32m[20221214 00:12:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 552.41
[32m[20221214 00:12:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 549.46
[32m[20221214 00:12:15 @agent_ppo2.py:143][0m Total time:      14.74 min
[32m[20221214 00:12:15 @agent_ppo2.py:145][0m 1368064 total steps have happened
[32m[20221214 00:12:15 @agent_ppo2.py:121][0m #------------------------ Iteration 4668 --------------------------#
[32m[20221214 00:12:15 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:12:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:12:15 @agent_ppo2.py:185][0m |          -0.0036 |          65.3670 |           6.5298 |
[32m[20221214 00:12:15 @agent_ppo2.py:185][0m |          -0.0043 |          61.9057 |           6.4761 |
[32m[20221214 00:12:15 @agent_ppo2.py:185][0m |          -0.0083 |          60.8918 |           6.5451 |
[32m[20221214 00:12:15 @agent_ppo2.py:185][0m |          -0.0093 |          60.0036 |           6.4701 |
[32m[20221214 00:12:15 @agent_ppo2.py:185][0m |          -0.0102 |          59.1663 |           6.4397 |
[32m[20221214 00:12:15 @agent_ppo2.py:185][0m |          -0.0105 |          58.2304 |           6.4491 |
[32m[20221214 00:12:16 @agent_ppo2.py:185][0m |          -0.0133 |          57.6475 |           6.4591 |
[32m[20221214 00:12:16 @agent_ppo2.py:185][0m |          -0.0092 |          57.7967 |           6.4619 |
[32m[20221214 00:12:16 @agent_ppo2.py:185][0m |          -0.0163 |          57.1653 |           6.4651 |
[32m[20221214 00:12:16 @agent_ppo2.py:185][0m |          -0.0178 |          56.5103 |           6.3910 |
[32m[20221214 00:12:16 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:12:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 525.96
[32m[20221214 00:12:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 572.33
[32m[20221214 00:12:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 457.91
[32m[20221214 00:12:16 @agent_ppo2.py:143][0m Total time:      14.76 min
[32m[20221214 00:12:16 @agent_ppo2.py:145][0m 1370112 total steps have happened
[32m[20221214 00:12:16 @agent_ppo2.py:121][0m #------------------------ Iteration 4669 --------------------------#
[32m[20221214 00:12:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:12:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:12:16 @agent_ppo2.py:185][0m |          -0.0004 |          76.8621 |           5.6333 |
[32m[20221214 00:12:16 @agent_ppo2.py:185][0m |          -0.0033 |          72.0791 |           5.6708 |
[32m[20221214 00:12:16 @agent_ppo2.py:185][0m |          -0.0074 |          69.8907 |           5.7016 |
[32m[20221214 00:12:17 @agent_ppo2.py:185][0m |          -0.0004 |          72.9161 |           5.7375 |
[32m[20221214 00:12:17 @agent_ppo2.py:185][0m |           0.0040 |          79.2986 |           5.7882 |
[32m[20221214 00:12:17 @agent_ppo2.py:185][0m |          -0.0106 |          68.2586 |           5.8490 |
[32m[20221214 00:12:17 @agent_ppo2.py:185][0m |          -0.0131 |          66.0500 |           5.8889 |
[32m[20221214 00:12:17 @agent_ppo2.py:185][0m |          -0.0143 |          65.4844 |           5.8574 |
[32m[20221214 00:12:17 @agent_ppo2.py:185][0m |          -0.0097 |          65.1621 |           5.8688 |
[32m[20221214 00:12:17 @agent_ppo2.py:185][0m |          -0.0117 |          64.8599 |           5.9368 |
[32m[20221214 00:12:17 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:12:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 494.43
[32m[20221214 00:12:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 556.11
[32m[20221214 00:12:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 585.33
[32m[20221214 00:12:17 @agent_ppo2.py:143][0m Total time:      14.78 min
[32m[20221214 00:12:17 @agent_ppo2.py:145][0m 1372160 total steps have happened
[32m[20221214 00:12:17 @agent_ppo2.py:121][0m #------------------------ Iteration 4670 --------------------------#
[32m[20221214 00:12:17 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:12:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:12:18 @agent_ppo2.py:185][0m |          -0.0026 |          73.7652 |           6.7900 |
[32m[20221214 00:12:18 @agent_ppo2.py:185][0m |           0.0011 |          68.5089 |           6.7929 |
[32m[20221214 00:12:18 @agent_ppo2.py:185][0m |          -0.0079 |          62.9249 |           6.8638 |
[32m[20221214 00:12:18 @agent_ppo2.py:185][0m |          -0.0113 |          60.7319 |           6.8057 |
[32m[20221214 00:12:18 @agent_ppo2.py:185][0m |          -0.0087 |          59.7005 |           6.7819 |
[32m[20221214 00:12:18 @agent_ppo2.py:185][0m |          -0.0137 |          58.7705 |           6.8195 |
[32m[20221214 00:12:18 @agent_ppo2.py:185][0m |          -0.0076 |          58.0914 |           6.7899 |
[32m[20221214 00:12:18 @agent_ppo2.py:185][0m |          -0.0141 |          57.3410 |           6.7495 |
[32m[20221214 00:12:18 @agent_ppo2.py:185][0m |          -0.0163 |          56.9758 |           6.7669 |
[32m[20221214 00:12:18 @agent_ppo2.py:185][0m |          -0.0060 |          58.0840 |           6.8095 |
[32m[20221214 00:12:18 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:12:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 515.63
[32m[20221214 00:12:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 552.54
[32m[20221214 00:12:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 606.81
[32m[20221214 00:12:18 @agent_ppo2.py:143][0m Total time:      14.80 min
[32m[20221214 00:12:18 @agent_ppo2.py:145][0m 1374208 total steps have happened
[32m[20221214 00:12:18 @agent_ppo2.py:121][0m #------------------------ Iteration 4671 --------------------------#
[32m[20221214 00:12:19 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:12:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:12:19 @agent_ppo2.py:185][0m |          -0.0001 |          82.6704 |           6.7159 |
[32m[20221214 00:12:19 @agent_ppo2.py:185][0m |          -0.0031 |          79.9519 |           6.7224 |
[32m[20221214 00:12:19 @agent_ppo2.py:185][0m |          -0.0075 |          78.6352 |           6.7763 |
[32m[20221214 00:12:19 @agent_ppo2.py:185][0m |          -0.0086 |          78.1740 |           6.8146 |
[32m[20221214 00:12:19 @agent_ppo2.py:185][0m |          -0.0030 |          80.5085 |           6.8204 |
[32m[20221214 00:12:19 @agent_ppo2.py:185][0m |          -0.0058 |          78.7373 |           6.8346 |
[32m[20221214 00:12:19 @agent_ppo2.py:185][0m |          -0.0127 |          77.3311 |           6.9312 |
[32m[20221214 00:12:19 @agent_ppo2.py:185][0m |          -0.0118 |          76.9400 |           6.9545 |
[32m[20221214 00:12:19 @agent_ppo2.py:185][0m |           0.0014 |          81.6787 |           6.9622 |
[32m[20221214 00:12:20 @agent_ppo2.py:185][0m |          -0.0126 |          76.8495 |           6.9797 |
[32m[20221214 00:12:20 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:12:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 529.21
[32m[20221214 00:12:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 546.40
[32m[20221214 00:12:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 525.22
[32m[20221214 00:12:20 @agent_ppo2.py:143][0m Total time:      14.82 min
[32m[20221214 00:12:20 @agent_ppo2.py:145][0m 1376256 total steps have happened
[32m[20221214 00:12:20 @agent_ppo2.py:121][0m #------------------------ Iteration 4672 --------------------------#
[32m[20221214 00:12:20 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:12:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:12:20 @agent_ppo2.py:185][0m |          -0.0017 |          35.6788 |           6.8428 |
[32m[20221214 00:12:20 @agent_ppo2.py:185][0m |          -0.0041 |          28.7534 |           6.8674 |
[32m[20221214 00:12:20 @agent_ppo2.py:185][0m |          -0.0174 |          27.6824 |           6.9054 |
[32m[20221214 00:12:20 @agent_ppo2.py:185][0m |          -0.0097 |          26.6071 |           6.8406 |
[32m[20221214 00:12:20 @agent_ppo2.py:185][0m |          -0.0164 |          26.2152 |           6.8665 |
[32m[20221214 00:12:21 @agent_ppo2.py:185][0m |          -0.0096 |          26.0898 |           6.8947 |
[32m[20221214 00:12:21 @agent_ppo2.py:185][0m |          -0.0152 |          25.0870 |           6.8540 |
[32m[20221214 00:12:21 @agent_ppo2.py:185][0m |          -0.0141 |          25.1668 |           6.8458 |
[32m[20221214 00:12:21 @agent_ppo2.py:185][0m |          -0.0234 |          24.6803 |           6.8813 |
[32m[20221214 00:12:21 @agent_ppo2.py:185][0m |          -0.0209 |          24.0604 |           6.8801 |
[32m[20221214 00:12:21 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:12:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 364.77
[32m[20221214 00:12:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 424.14
[32m[20221214 00:12:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 578.09
[32m[20221214 00:12:21 @agent_ppo2.py:143][0m Total time:      14.84 min
[32m[20221214 00:12:21 @agent_ppo2.py:145][0m 1378304 total steps have happened
[32m[20221214 00:12:21 @agent_ppo2.py:121][0m #------------------------ Iteration 4673 --------------------------#
[32m[20221214 00:12:21 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:12:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:12:21 @agent_ppo2.py:185][0m |           0.0018 |          58.5493 |           6.7342 |
[32m[20221214 00:12:21 @agent_ppo2.py:185][0m |          -0.0070 |          53.5313 |           6.8227 |
[32m[20221214 00:12:22 @agent_ppo2.py:185][0m |          -0.0073 |          51.6663 |           6.8085 |
[32m[20221214 00:12:22 @agent_ppo2.py:185][0m |          -0.0084 |          50.3639 |           6.8185 |
[32m[20221214 00:12:22 @agent_ppo2.py:185][0m |          -0.0108 |          49.7413 |           6.8365 |
[32m[20221214 00:12:22 @agent_ppo2.py:185][0m |          -0.0059 |          50.6636 |           6.8664 |
[32m[20221214 00:12:22 @agent_ppo2.py:185][0m |          -0.0145 |          48.8539 |           6.8488 |
[32m[20221214 00:12:22 @agent_ppo2.py:185][0m |          -0.0208 |          47.9115 |           6.9573 |
[32m[20221214 00:12:22 @agent_ppo2.py:185][0m |          -0.0087 |          49.0314 |           6.9112 |
[32m[20221214 00:12:22 @agent_ppo2.py:185][0m |          -0.0198 |          46.9947 |           6.9742 |
[32m[20221214 00:12:22 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:12:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 478.15
[32m[20221214 00:12:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 533.21
[32m[20221214 00:12:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 145.24
[32m[20221214 00:12:22 @agent_ppo2.py:143][0m Total time:      14.87 min
[32m[20221214 00:12:22 @agent_ppo2.py:145][0m 1380352 total steps have happened
[32m[20221214 00:12:22 @agent_ppo2.py:121][0m #------------------------ Iteration 4674 --------------------------#
[32m[20221214 00:12:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:12:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:12:23 @agent_ppo2.py:185][0m |           0.0111 |          22.0455 |           6.9935 |
[32m[20221214 00:12:23 @agent_ppo2.py:185][0m |           0.0007 |          14.4701 |           7.0286 |
[32m[20221214 00:12:23 @agent_ppo2.py:185][0m |           0.0034 |          13.4967 |           6.9720 |
[32m[20221214 00:12:23 @agent_ppo2.py:185][0m |          -0.0103 |          12.4476 |           6.9948 |
[32m[20221214 00:12:23 @agent_ppo2.py:185][0m |          -0.0120 |          11.8370 |           6.9685 |
[32m[20221214 00:12:23 @agent_ppo2.py:185][0m |          -0.0054 |          11.4194 |           6.9835 |
[32m[20221214 00:12:23 @agent_ppo2.py:185][0m |          -0.0161 |          11.2080 |           6.9710 |
[32m[20221214 00:12:23 @agent_ppo2.py:185][0m |          -0.0176 |          10.7905 |           6.9447 |
[32m[20221214 00:12:23 @agent_ppo2.py:185][0m |          -0.0196 |          10.6996 |           6.9488 |
[32m[20221214 00:12:23 @agent_ppo2.py:185][0m |          -0.0164 |          10.4193 |           6.8917 |
[32m[20221214 00:12:23 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:12:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 302.90
[32m[20221214 00:12:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 434.39
[32m[20221214 00:12:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 476.60
[32m[20221214 00:12:24 @agent_ppo2.py:143][0m Total time:      14.89 min
[32m[20221214 00:12:24 @agent_ppo2.py:145][0m 1382400 total steps have happened
[32m[20221214 00:12:24 @agent_ppo2.py:121][0m #------------------------ Iteration 4675 --------------------------#
[32m[20221214 00:12:24 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:12:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:12:24 @agent_ppo2.py:185][0m |           0.0030 |          70.9871 |           6.8182 |
[32m[20221214 00:12:24 @agent_ppo2.py:185][0m |           0.0008 |          70.2971 |           6.8636 |
[32m[20221214 00:12:24 @agent_ppo2.py:185][0m |          -0.0081 |          65.0102 |           6.9080 |
[32m[20221214 00:12:24 @agent_ppo2.py:185][0m |          -0.0080 |          64.2326 |           6.9543 |
[32m[20221214 00:12:24 @agent_ppo2.py:185][0m |          -0.0085 |          63.9891 |           6.9942 |
[32m[20221214 00:12:24 @agent_ppo2.py:185][0m |          -0.0015 |          65.3116 |           7.0125 |
[32m[20221214 00:12:24 @agent_ppo2.py:185][0m |          -0.0112 |          63.4363 |           7.0696 |
[32m[20221214 00:12:25 @agent_ppo2.py:185][0m |          -0.0149 |          62.6258 |           7.1474 |
[32m[20221214 00:12:25 @agent_ppo2.py:185][0m |          -0.0143 |          62.5753 |           7.1531 |
[32m[20221214 00:12:25 @agent_ppo2.py:185][0m |          -0.0147 |          62.2095 |           7.2050 |
[32m[20221214 00:12:25 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:12:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 522.10
[32m[20221214 00:12:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 545.15
[32m[20221214 00:12:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 614.34
[32m[20221214 00:12:25 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 614.34
[32m[20221214 00:12:25 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 614.34
[32m[20221214 00:12:25 @agent_ppo2.py:143][0m Total time:      14.91 min
[32m[20221214 00:12:25 @agent_ppo2.py:145][0m 1384448 total steps have happened
[32m[20221214 00:12:25 @agent_ppo2.py:121][0m #------------------------ Iteration 4676 --------------------------#
[32m[20221214 00:12:25 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:12:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:12:25 @agent_ppo2.py:185][0m |           0.0013 |          68.0300 |           7.1853 |
[32m[20221214 00:12:25 @agent_ppo2.py:185][0m |          -0.0067 |          63.8535 |           7.1264 |
[32m[20221214 00:12:25 @agent_ppo2.py:185][0m |          -0.0092 |          62.7684 |           7.1522 |
[32m[20221214 00:12:26 @agent_ppo2.py:185][0m |          -0.0103 |          63.2642 |           7.1952 |
[32m[20221214 00:12:26 @agent_ppo2.py:185][0m |          -0.0087 |          61.8685 |           7.2092 |
[32m[20221214 00:12:26 @agent_ppo2.py:185][0m |          -0.0103 |          61.1148 |           7.2058 |
[32m[20221214 00:12:26 @agent_ppo2.py:185][0m |          -0.0116 |          60.8845 |           7.2763 |
[32m[20221214 00:12:26 @agent_ppo2.py:185][0m |          -0.0143 |          60.6889 |           7.2649 |
[32m[20221214 00:12:26 @agent_ppo2.py:185][0m |          -0.0096 |          60.5415 |           7.3255 |
[32m[20221214 00:12:26 @agent_ppo2.py:185][0m |          -0.0119 |          59.9657 |           7.3155 |
[32m[20221214 00:12:26 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 00:12:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 533.81
[32m[20221214 00:12:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 565.09
[32m[20221214 00:12:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 403.42
[32m[20221214 00:12:26 @agent_ppo2.py:143][0m Total time:      14.93 min
[32m[20221214 00:12:26 @agent_ppo2.py:145][0m 1386496 total steps have happened
[32m[20221214 00:12:26 @agent_ppo2.py:121][0m #------------------------ Iteration 4677 --------------------------#
[32m[20221214 00:12:27 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:12:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:12:27 @agent_ppo2.py:185][0m |           0.0039 |          46.8413 |           7.2367 |
[32m[20221214 00:12:27 @agent_ppo2.py:185][0m |          -0.0022 |          39.1382 |           7.1074 |
[32m[20221214 00:12:27 @agent_ppo2.py:185][0m |          -0.0046 |          36.6080 |           7.1866 |
[32m[20221214 00:12:27 @agent_ppo2.py:185][0m |          -0.0115 |          35.1291 |           7.0803 |
[32m[20221214 00:12:27 @agent_ppo2.py:185][0m |          -0.0115 |          34.2553 |           7.1774 |
[32m[20221214 00:12:27 @agent_ppo2.py:185][0m |          -0.0138 |          33.5284 |           7.1328 |
[32m[20221214 00:12:27 @agent_ppo2.py:185][0m |          -0.0143 |          33.0683 |           7.1335 |
[32m[20221214 00:12:27 @agent_ppo2.py:185][0m |          -0.0132 |          32.4622 |           7.1599 |
[32m[20221214 00:12:28 @agent_ppo2.py:185][0m |          -0.0228 |          32.0356 |           7.1965 |
[32m[20221214 00:12:28 @agent_ppo2.py:185][0m |          -0.0144 |          31.8164 |           7.1243 |
[32m[20221214 00:12:28 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:12:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 507.13
[32m[20221214 00:12:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 581.86
[32m[20221214 00:12:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 478.97
[32m[20221214 00:12:28 @agent_ppo2.py:143][0m Total time:      14.96 min
[32m[20221214 00:12:28 @agent_ppo2.py:145][0m 1388544 total steps have happened
[32m[20221214 00:12:28 @agent_ppo2.py:121][0m #------------------------ Iteration 4678 --------------------------#
[32m[20221214 00:12:28 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:12:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:12:28 @agent_ppo2.py:185][0m |          -0.0046 |          43.3579 |           7.1393 |
[32m[20221214 00:12:28 @agent_ppo2.py:185][0m |          -0.0090 |          36.0025 |           7.1043 |
[32m[20221214 00:12:28 @agent_ppo2.py:185][0m |          -0.0079 |          33.7079 |           7.1125 |
[32m[20221214 00:12:28 @agent_ppo2.py:185][0m |          -0.0123 |          32.1736 |           7.0763 |
[32m[20221214 00:12:28 @agent_ppo2.py:185][0m |          -0.0144 |          31.0800 |           7.0497 |
[32m[20221214 00:12:29 @agent_ppo2.py:185][0m |          -0.0157 |          30.1008 |           7.0233 |
[32m[20221214 00:12:29 @agent_ppo2.py:185][0m |          -0.0156 |          29.5132 |           6.9976 |
[32m[20221214 00:12:29 @agent_ppo2.py:185][0m |          -0.0213 |          28.7501 |           6.9927 |
[32m[20221214 00:12:29 @agent_ppo2.py:185][0m |          -0.0194 |          28.2595 |           6.9531 |
[32m[20221214 00:12:29 @agent_ppo2.py:185][0m |          -0.0210 |          27.7065 |           6.9122 |
[32m[20221214 00:12:29 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:12:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 405.89
[32m[20221214 00:12:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 496.56
[32m[20221214 00:12:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 573.33
[32m[20221214 00:12:29 @agent_ppo2.py:143][0m Total time:      14.98 min
[32m[20221214 00:12:29 @agent_ppo2.py:145][0m 1390592 total steps have happened
[32m[20221214 00:12:29 @agent_ppo2.py:121][0m #------------------------ Iteration 4679 --------------------------#
[32m[20221214 00:12:29 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:12:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:12:29 @agent_ppo2.py:185][0m |           0.0017 |          51.1951 |           7.3463 |
[32m[20221214 00:12:29 @agent_ppo2.py:185][0m |          -0.0074 |          42.2502 |           7.2900 |
[32m[20221214 00:12:30 @agent_ppo2.py:185][0m |          -0.0108 |          38.9049 |           7.2980 |
[32m[20221214 00:12:30 @agent_ppo2.py:185][0m |          -0.0096 |          37.4482 |           7.3679 |
[32m[20221214 00:12:30 @agent_ppo2.py:185][0m |          -0.0126 |          36.4590 |           7.3128 |
[32m[20221214 00:12:30 @agent_ppo2.py:185][0m |          -0.0145 |          34.9972 |           7.3693 |
[32m[20221214 00:12:30 @agent_ppo2.py:185][0m |          -0.0159 |          34.6188 |           7.3250 |
[32m[20221214 00:12:30 @agent_ppo2.py:185][0m |          -0.0219 |          34.0164 |           7.3454 |
[32m[20221214 00:12:30 @agent_ppo2.py:185][0m |          -0.0163 |          33.4421 |           7.3623 |
[32m[20221214 00:12:30 @agent_ppo2.py:185][0m |          -0.0075 |          33.7917 |           7.3508 |
[32m[20221214 00:12:30 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:12:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 378.21
[32m[20221214 00:12:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 540.02
[32m[20221214 00:12:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 453.03
[32m[20221214 00:12:30 @agent_ppo2.py:143][0m Total time:      15.00 min
[32m[20221214 00:12:30 @agent_ppo2.py:145][0m 1392640 total steps have happened
[32m[20221214 00:12:30 @agent_ppo2.py:121][0m #------------------------ Iteration 4680 --------------------------#
[32m[20221214 00:12:31 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:12:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:12:31 @agent_ppo2.py:185][0m |           0.0092 |          24.5334 |           7.6182 |
[32m[20221214 00:12:31 @agent_ppo2.py:185][0m |          -0.0059 |          17.5544 |           7.5688 |
[32m[20221214 00:12:31 @agent_ppo2.py:185][0m |          -0.0033 |          16.4087 |           7.5829 |
[32m[20221214 00:12:31 @agent_ppo2.py:185][0m |          -0.0046 |          15.2671 |           7.5741 |
[32m[20221214 00:12:31 @agent_ppo2.py:185][0m |          -0.0156 |          14.6377 |           7.5758 |
[32m[20221214 00:12:31 @agent_ppo2.py:185][0m |          -0.0115 |          14.0787 |           7.6055 |
[32m[20221214 00:12:31 @agent_ppo2.py:185][0m |          -0.0132 |          13.5601 |           7.5993 |
[32m[20221214 00:12:32 @agent_ppo2.py:185][0m |          -0.0194 |          13.1614 |           7.6321 |
[32m[20221214 00:12:32 @agent_ppo2.py:185][0m |          -0.0206 |          12.8567 |           7.5977 |
[32m[20221214 00:12:32 @agent_ppo2.py:185][0m |          -0.0189 |          12.5488 |           7.5048 |
[32m[20221214 00:12:32 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 00:12:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 379.73
[32m[20221214 00:12:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 477.01
[32m[20221214 00:12:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 207.26
[32m[20221214 00:12:32 @agent_ppo2.py:143][0m Total time:      15.02 min
[32m[20221214 00:12:32 @agent_ppo2.py:145][0m 1394688 total steps have happened
[32m[20221214 00:12:32 @agent_ppo2.py:121][0m #------------------------ Iteration 4681 --------------------------#
[32m[20221214 00:12:32 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:12:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:12:32 @agent_ppo2.py:185][0m |          -0.0012 |          26.7323 |           6.9723 |
[32m[20221214 00:12:32 @agent_ppo2.py:185][0m |          -0.0092 |          23.3094 |           6.8909 |
[32m[20221214 00:12:32 @agent_ppo2.py:185][0m |          -0.0109 |          21.7202 |           6.9116 |
[32m[20221214 00:12:32 @agent_ppo2.py:185][0m |          -0.0164 |          21.0057 |           6.8818 |
[32m[20221214 00:12:33 @agent_ppo2.py:185][0m |          -0.0140 |          20.2185 |           6.8055 |
[32m[20221214 00:12:33 @agent_ppo2.py:185][0m |          -0.0160 |          19.8784 |           6.8099 |
[32m[20221214 00:12:33 @agent_ppo2.py:185][0m |          -0.0136 |          19.6974 |           6.8563 |
[32m[20221214 00:12:33 @agent_ppo2.py:185][0m |          -0.0174 |          19.2728 |           6.7706 |
[32m[20221214 00:12:33 @agent_ppo2.py:185][0m |          -0.0255 |          19.1334 |           6.7746 |
[32m[20221214 00:12:33 @agent_ppo2.py:185][0m |          -0.0231 |          18.6910 |           6.7529 |
[32m[20221214 00:12:33 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:12:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 323.60
[32m[20221214 00:12:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 472.91
[32m[20221214 00:12:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 449.96
[32m[20221214 00:12:33 @agent_ppo2.py:143][0m Total time:      15.05 min
[32m[20221214 00:12:33 @agent_ppo2.py:145][0m 1396736 total steps have happened
[32m[20221214 00:12:33 @agent_ppo2.py:121][0m #------------------------ Iteration 4682 --------------------------#
[32m[20221214 00:12:33 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:12:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:12:33 @agent_ppo2.py:185][0m |           0.0013 |          63.8818 |           6.0371 |
[32m[20221214 00:12:34 @agent_ppo2.py:185][0m |          -0.0034 |          58.7383 |           6.1806 |
[32m[20221214 00:12:34 @agent_ppo2.py:185][0m |          -0.0079 |          57.2718 |           6.1136 |
[32m[20221214 00:12:34 @agent_ppo2.py:185][0m |          -0.0120 |          57.3835 |           6.1501 |
[32m[20221214 00:12:34 @agent_ppo2.py:185][0m |          -0.0114 |          56.3392 |           6.1293 |
[32m[20221214 00:12:34 @agent_ppo2.py:185][0m |           0.0058 |          66.5184 |           6.1782 |
[32m[20221214 00:12:34 @agent_ppo2.py:185][0m |          -0.0019 |          61.5414 |           6.1708 |
[32m[20221214 00:12:34 @agent_ppo2.py:185][0m |          -0.0125 |          55.1603 |           6.1790 |
[32m[20221214 00:12:34 @agent_ppo2.py:185][0m |          -0.0148 |          54.8611 |           6.1551 |
[32m[20221214 00:12:34 @agent_ppo2.py:185][0m |          -0.0165 |          55.0984 |           6.1777 |
[32m[20221214 00:12:34 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:12:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 518.55
[32m[20221214 00:12:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 559.48
[32m[20221214 00:12:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 367.39
[32m[20221214 00:12:35 @agent_ppo2.py:143][0m Total time:      15.07 min
[32m[20221214 00:12:35 @agent_ppo2.py:145][0m 1398784 total steps have happened
[32m[20221214 00:12:35 @agent_ppo2.py:121][0m #------------------------ Iteration 4683 --------------------------#
[32m[20221214 00:12:35 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:12:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:12:35 @agent_ppo2.py:185][0m |          -0.0001 |          26.2695 |           6.9411 |
[32m[20221214 00:12:35 @agent_ppo2.py:185][0m |          -0.0098 |          22.7007 |           6.8626 |
[32m[20221214 00:12:35 @agent_ppo2.py:185][0m |          -0.0119 |          22.1349 |           6.8582 |
[32m[20221214 00:12:35 @agent_ppo2.py:185][0m |          -0.0074 |          21.2342 |           6.8974 |
[32m[20221214 00:12:35 @agent_ppo2.py:185][0m |          -0.0125 |          20.5595 |           6.8116 |
[32m[20221214 00:12:35 @agent_ppo2.py:185][0m |          -0.0146 |          20.3151 |           6.8032 |
[32m[20221214 00:12:35 @agent_ppo2.py:185][0m |          -0.0146 |          19.8785 |           6.7992 |
[32m[20221214 00:12:36 @agent_ppo2.py:185][0m |          -0.0095 |          20.6186 |           6.6667 |
[32m[20221214 00:12:36 @agent_ppo2.py:185][0m |          -0.0203 |          19.3524 |           6.7933 |
[32m[20221214 00:12:36 @agent_ppo2.py:185][0m |          -0.0159 |          19.4126 |           6.7354 |
[32m[20221214 00:12:36 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:12:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 357.34
[32m[20221214 00:12:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 408.07
[32m[20221214 00:12:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 556.00
[32m[20221214 00:12:36 @agent_ppo2.py:143][0m Total time:      15.09 min
[32m[20221214 00:12:36 @agent_ppo2.py:145][0m 1400832 total steps have happened
[32m[20221214 00:12:36 @agent_ppo2.py:121][0m #------------------------ Iteration 4684 --------------------------#
[32m[20221214 00:12:36 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:12:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:12:36 @agent_ppo2.py:185][0m |           0.0020 |          61.9262 |           6.4533 |
[32m[20221214 00:12:36 @agent_ppo2.py:185][0m |          -0.0063 |          55.1871 |           6.3889 |
[32m[20221214 00:12:36 @agent_ppo2.py:185][0m |          -0.0082 |          53.1351 |           6.3561 |
[32m[20221214 00:12:37 @agent_ppo2.py:185][0m |           0.0010 |          53.2176 |           6.3798 |
[32m[20221214 00:12:37 @agent_ppo2.py:185][0m |          -0.0096 |          51.2513 |           6.3484 |
[32m[20221214 00:12:37 @agent_ppo2.py:185][0m |          -0.0151 |          50.5021 |           6.3262 |
[32m[20221214 00:12:37 @agent_ppo2.py:185][0m |          -0.0138 |          50.0047 |           6.2798 |
[32m[20221214 00:12:37 @agent_ppo2.py:185][0m |          -0.0076 |          51.1643 |           6.2538 |
[32m[20221214 00:12:37 @agent_ppo2.py:185][0m |          -0.0157 |          49.3596 |           6.2744 |
[32m[20221214 00:12:37 @agent_ppo2.py:185][0m |          -0.0165 |          48.7631 |           6.2953 |
[32m[20221214 00:12:37 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:12:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 462.76
[32m[20221214 00:12:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 542.63
[32m[20221214 00:12:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 517.56
[32m[20221214 00:12:37 @agent_ppo2.py:143][0m Total time:      15.12 min
[32m[20221214 00:12:37 @agent_ppo2.py:145][0m 1402880 total steps have happened
[32m[20221214 00:12:37 @agent_ppo2.py:121][0m #------------------------ Iteration 4685 --------------------------#
[32m[20221214 00:12:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:12:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:12:38 @agent_ppo2.py:185][0m |           0.0026 |          74.1371 |           5.8685 |
[32m[20221214 00:12:38 @agent_ppo2.py:185][0m |          -0.0027 |          65.3145 |           5.8700 |
[32m[20221214 00:12:38 @agent_ppo2.py:185][0m |          -0.0083 |          62.5001 |           5.9304 |
[32m[20221214 00:12:38 @agent_ppo2.py:185][0m |          -0.0062 |          60.5418 |           5.8208 |
[32m[20221214 00:12:38 @agent_ppo2.py:185][0m |          -0.0075 |          59.6535 |           5.8672 |
[32m[20221214 00:12:38 @agent_ppo2.py:185][0m |          -0.0099 |          58.6485 |           5.9261 |
[32m[20221214 00:12:38 @agent_ppo2.py:185][0m |           0.0007 |          61.6238 |           5.9037 |
[32m[20221214 00:12:38 @agent_ppo2.py:185][0m |          -0.0107 |          57.9650 |           5.9373 |
[32m[20221214 00:12:38 @agent_ppo2.py:185][0m |          -0.0087 |          57.0235 |           5.8796 |
[32m[20221214 00:12:38 @agent_ppo2.py:185][0m |          -0.0106 |          56.6977 |           5.8683 |
[32m[20221214 00:12:38 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:12:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 528.37
[32m[20221214 00:12:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 592.03
[32m[20221214 00:12:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 453.09
[32m[20221214 00:12:39 @agent_ppo2.py:143][0m Total time:      15.14 min
[32m[20221214 00:12:39 @agent_ppo2.py:145][0m 1404928 total steps have happened
[32m[20221214 00:12:39 @agent_ppo2.py:121][0m #------------------------ Iteration 4686 --------------------------#
[32m[20221214 00:12:39 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:12:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:12:39 @agent_ppo2.py:185][0m |           0.0038 |          46.7892 |           5.4816 |
[32m[20221214 00:12:39 @agent_ppo2.py:185][0m |          -0.0066 |          37.9089 |           5.5135 |
[32m[20221214 00:12:39 @agent_ppo2.py:185][0m |          -0.0090 |          37.5364 |           5.5618 |
[32m[20221214 00:12:39 @agent_ppo2.py:185][0m |          -0.0066 |          35.6284 |           5.5525 |
[32m[20221214 00:12:39 @agent_ppo2.py:185][0m |          -0.0086 |          35.1839 |           5.5064 |
[32m[20221214 00:12:39 @agent_ppo2.py:185][0m |          -0.0106 |          34.3394 |           5.5087 |
[32m[20221214 00:12:40 @agent_ppo2.py:185][0m |          -0.0110 |          34.8879 |           5.5363 |
[32m[20221214 00:12:40 @agent_ppo2.py:185][0m |          -0.0127 |          33.4555 |           5.5003 |
[32m[20221214 00:12:40 @agent_ppo2.py:185][0m |          -0.0161 |          33.4054 |           5.4280 |
[32m[20221214 00:12:40 @agent_ppo2.py:185][0m |          -0.0140 |          33.0142 |           5.3551 |
[32m[20221214 00:12:40 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:12:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 396.29
[32m[20221214 00:12:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 515.62
[32m[20221214 00:12:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 530.87
[32m[20221214 00:12:40 @agent_ppo2.py:143][0m Total time:      15.16 min
[32m[20221214 00:12:40 @agent_ppo2.py:145][0m 1406976 total steps have happened
[32m[20221214 00:12:40 @agent_ppo2.py:121][0m #------------------------ Iteration 4687 --------------------------#
[32m[20221214 00:12:40 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:12:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:12:40 @agent_ppo2.py:185][0m |           0.0010 |          55.1460 |           5.9994 |
[32m[20221214 00:12:40 @agent_ppo2.py:185][0m |          -0.0075 |          49.0594 |           5.9511 |
[32m[20221214 00:12:40 @agent_ppo2.py:185][0m |          -0.0102 |          48.0570 |           6.0034 |
[32m[20221214 00:12:41 @agent_ppo2.py:185][0m |          -0.0121 |          47.1443 |           6.0360 |
[32m[20221214 00:12:41 @agent_ppo2.py:185][0m |          -0.0157 |          46.7505 |           6.0306 |
[32m[20221214 00:12:41 @agent_ppo2.py:185][0m |          -0.0117 |          46.1623 |           6.0129 |
[32m[20221214 00:12:41 @agent_ppo2.py:185][0m |          -0.0124 |          45.7955 |           5.9584 |
[32m[20221214 00:12:41 @agent_ppo2.py:185][0m |          -0.0134 |          45.7776 |           6.0197 |
[32m[20221214 00:12:41 @agent_ppo2.py:185][0m |          -0.0184 |          45.1763 |           6.0297 |
[32m[20221214 00:12:41 @agent_ppo2.py:185][0m |          -0.0185 |          44.8401 |           6.0674 |
[32m[20221214 00:12:41 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:12:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 499.61
[32m[20221214 00:12:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 545.52
[32m[20221214 00:12:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 449.84
[32m[20221214 00:12:41 @agent_ppo2.py:143][0m Total time:      15.18 min
[32m[20221214 00:12:41 @agent_ppo2.py:145][0m 1409024 total steps have happened
[32m[20221214 00:12:41 @agent_ppo2.py:121][0m #------------------------ Iteration 4688 --------------------------#
[32m[20221214 00:12:41 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:12:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:12:42 @agent_ppo2.py:185][0m |          -0.0014 |          55.9154 |           5.5034 |
[32m[20221214 00:12:42 @agent_ppo2.py:185][0m |          -0.0019 |          50.4245 |           5.4863 |
[32m[20221214 00:12:42 @agent_ppo2.py:185][0m |          -0.0093 |          48.6762 |           5.5820 |
[32m[20221214 00:12:42 @agent_ppo2.py:185][0m |          -0.0082 |          47.7043 |           5.5661 |
[32m[20221214 00:12:42 @agent_ppo2.py:185][0m |          -0.0133 |          46.9315 |           5.5212 |
[32m[20221214 00:12:42 @agent_ppo2.py:185][0m |          -0.0069 |          46.5236 |           5.5197 |
[32m[20221214 00:12:42 @agent_ppo2.py:185][0m |          -0.0136 |          45.8954 |           5.5260 |
[32m[20221214 00:12:42 @agent_ppo2.py:185][0m |          -0.0141 |          45.5305 |           5.5389 |
[32m[20221214 00:12:42 @agent_ppo2.py:185][0m |          -0.0155 |          45.2605 |           5.5381 |
[32m[20221214 00:12:42 @agent_ppo2.py:185][0m |          -0.0141 |          44.9916 |           5.5474 |
[32m[20221214 00:12:42 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:12:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 437.62
[32m[20221214 00:12:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 542.71
[32m[20221214 00:12:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 562.44
[32m[20221214 00:12:43 @agent_ppo2.py:143][0m Total time:      15.20 min
[32m[20221214 00:12:43 @agent_ppo2.py:145][0m 1411072 total steps have happened
[32m[20221214 00:12:43 @agent_ppo2.py:121][0m #------------------------ Iteration 4689 --------------------------#
[32m[20221214 00:12:43 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:12:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:12:43 @agent_ppo2.py:185][0m |          -0.0021 |          82.1945 |           5.7735 |
[32m[20221214 00:12:43 @agent_ppo2.py:185][0m |          -0.0061 |          79.8598 |           5.7937 |
[32m[20221214 00:12:43 @agent_ppo2.py:185][0m |          -0.0097 |          78.3232 |           5.7698 |
[32m[20221214 00:12:43 @agent_ppo2.py:185][0m |          -0.0109 |          78.0508 |           5.8248 |
[32m[20221214 00:12:43 @agent_ppo2.py:185][0m |          -0.0143 |          77.5805 |           5.7935 |
[32m[20221214 00:12:43 @agent_ppo2.py:185][0m |          -0.0145 |          77.6205 |           5.8408 |
[32m[20221214 00:12:44 @agent_ppo2.py:185][0m |          -0.0155 |          77.2290 |           5.8165 |
[32m[20221214 00:12:44 @agent_ppo2.py:185][0m |          -0.0133 |          76.9849 |           5.7538 |
[32m[20221214 00:12:44 @agent_ppo2.py:185][0m |          -0.0177 |          76.8691 |           5.7231 |
[32m[20221214 00:12:44 @agent_ppo2.py:185][0m |          -0.0141 |          76.4413 |           5.6993 |
[32m[20221214 00:12:44 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:12:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 504.60
[32m[20221214 00:12:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 545.26
[32m[20221214 00:12:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 577.01
[32m[20221214 00:12:44 @agent_ppo2.py:143][0m Total time:      15.23 min
[32m[20221214 00:12:44 @agent_ppo2.py:145][0m 1413120 total steps have happened
[32m[20221214 00:12:44 @agent_ppo2.py:121][0m #------------------------ Iteration 4690 --------------------------#
[32m[20221214 00:12:44 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:12:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:12:44 @agent_ppo2.py:185][0m |           0.0023 |          30.0655 |           5.3246 |
[32m[20221214 00:12:44 @agent_ppo2.py:185][0m |          -0.0029 |          26.2860 |           5.2769 |
[32m[20221214 00:12:45 @agent_ppo2.py:185][0m |          -0.0123 |          24.4567 |           5.3483 |
[32m[20221214 00:12:45 @agent_ppo2.py:185][0m |          -0.0058 |          24.6438 |           5.2900 |
[32m[20221214 00:12:45 @agent_ppo2.py:185][0m |          -0.0168 |          23.1131 |           5.3314 |
[32m[20221214 00:12:45 @agent_ppo2.py:185][0m |          -0.0147 |          22.4002 |           5.3610 |
[32m[20221214 00:12:45 @agent_ppo2.py:185][0m |          -0.0228 |          21.9874 |           5.3399 |
[32m[20221214 00:12:45 @agent_ppo2.py:185][0m |          -0.0193 |          21.5456 |           5.3580 |
[32m[20221214 00:12:45 @agent_ppo2.py:185][0m |          -0.0216 |          21.0110 |           5.3604 |
[32m[20221214 00:12:45 @agent_ppo2.py:185][0m |          -0.0195 |          20.6481 |           5.3458 |
[32m[20221214 00:12:45 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:12:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 377.71
[32m[20221214 00:12:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 501.98
[32m[20221214 00:12:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 549.99
[32m[20221214 00:12:45 @agent_ppo2.py:143][0m Total time:      15.25 min
[32m[20221214 00:12:45 @agent_ppo2.py:145][0m 1415168 total steps have happened
[32m[20221214 00:12:45 @agent_ppo2.py:121][0m #------------------------ Iteration 4691 --------------------------#
[32m[20221214 00:12:45 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:12:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:12:46 @agent_ppo2.py:185][0m |          -0.0008 |          74.3811 |           5.0364 |
[32m[20221214 00:12:46 @agent_ppo2.py:185][0m |          -0.0061 |          69.1375 |           5.0472 |
[32m[20221214 00:12:46 @agent_ppo2.py:185][0m |          -0.0081 |          67.7465 |           5.0442 |
[32m[20221214 00:12:46 @agent_ppo2.py:185][0m |          -0.0089 |          66.2567 |           5.0089 |
[32m[20221214 00:12:46 @agent_ppo2.py:185][0m |          -0.0078 |          66.0675 |           5.0493 |
[32m[20221214 00:12:46 @agent_ppo2.py:185][0m |          -0.0119 |          65.2973 |           4.9725 |
[32m[20221214 00:12:46 @agent_ppo2.py:185][0m |          -0.0100 |          64.6502 |           4.9761 |
[32m[20221214 00:12:46 @agent_ppo2.py:185][0m |          -0.0090 |          64.9715 |           4.9630 |
[32m[20221214 00:12:46 @agent_ppo2.py:185][0m |          -0.0149 |          63.9242 |           5.0200 |
[32m[20221214 00:12:46 @agent_ppo2.py:185][0m |          -0.0145 |          63.4825 |           4.9418 |
[32m[20221214 00:12:46 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:12:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 511.44
[32m[20221214 00:12:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 537.18
[32m[20221214 00:12:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 539.35
[32m[20221214 00:12:47 @agent_ppo2.py:143][0m Total time:      15.27 min
[32m[20221214 00:12:47 @agent_ppo2.py:145][0m 1417216 total steps have happened
[32m[20221214 00:12:47 @agent_ppo2.py:121][0m #------------------------ Iteration 4692 --------------------------#
[32m[20221214 00:12:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:12:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:12:47 @agent_ppo2.py:185][0m |           0.0030 |          80.4042 |           5.0141 |
[32m[20221214 00:12:47 @agent_ppo2.py:185][0m |          -0.0058 |          76.9552 |           5.0802 |
[32m[20221214 00:12:47 @agent_ppo2.py:185][0m |           0.0011 |          77.5016 |           5.0462 |
[32m[20221214 00:12:47 @agent_ppo2.py:185][0m |          -0.0093 |          75.1084 |           5.0559 |
[32m[20221214 00:12:47 @agent_ppo2.py:185][0m |          -0.0121 |          74.2762 |           5.0046 |
[32m[20221214 00:12:47 @agent_ppo2.py:185][0m |          -0.0129 |          73.7595 |           5.0534 |
[32m[20221214 00:12:47 @agent_ppo2.py:185][0m |          -0.0107 |          73.3884 |           4.9553 |
[32m[20221214 00:12:48 @agent_ppo2.py:185][0m |          -0.0131 |          72.9933 |           4.9257 |
[32m[20221214 00:12:48 @agent_ppo2.py:185][0m |          -0.0083 |          74.1221 |           4.9577 |
[32m[20221214 00:12:48 @agent_ppo2.py:185][0m |          -0.0101 |          72.7042 |           4.9579 |
[32m[20221214 00:12:48 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:12:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 526.65
[32m[20221214 00:12:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 541.08
[32m[20221214 00:12:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 542.31
[32m[20221214 00:12:48 @agent_ppo2.py:143][0m Total time:      15.29 min
[32m[20221214 00:12:48 @agent_ppo2.py:145][0m 1419264 total steps have happened
[32m[20221214 00:12:48 @agent_ppo2.py:121][0m #------------------------ Iteration 4693 --------------------------#
[32m[20221214 00:12:48 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:12:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:12:48 @agent_ppo2.py:185][0m |           0.0187 |          78.5416 |           5.2909 |
[32m[20221214 00:12:48 @agent_ppo2.py:185][0m |          -0.0059 |          67.1017 |           5.4063 |
[32m[20221214 00:12:48 @agent_ppo2.py:185][0m |          -0.0077 |          66.5272 |           5.4356 |
[32m[20221214 00:12:49 @agent_ppo2.py:185][0m |           0.0060 |          69.1320 |           5.3970 |
[32m[20221214 00:12:49 @agent_ppo2.py:185][0m |          -0.0176 |          63.7807 |           5.3521 |
[32m[20221214 00:12:49 @agent_ppo2.py:185][0m |          -0.0178 |          62.9823 |           5.3463 |
[32m[20221214 00:12:49 @agent_ppo2.py:185][0m |          -0.0132 |          62.4705 |           5.4092 |
[32m[20221214 00:12:49 @agent_ppo2.py:185][0m |          -0.0167 |          61.9440 |           5.4098 |
[32m[20221214 00:12:49 @agent_ppo2.py:185][0m |          -0.0172 |          61.6224 |           5.3939 |
[32m[20221214 00:12:49 @agent_ppo2.py:185][0m |          -0.0189 |          61.2740 |           5.3604 |
[32m[20221214 00:12:49 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:12:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 529.86
[32m[20221214 00:12:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 553.77
[32m[20221214 00:12:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 583.92
[32m[20221214 00:12:49 @agent_ppo2.py:143][0m Total time:      15.31 min
[32m[20221214 00:12:49 @agent_ppo2.py:145][0m 1421312 total steps have happened
[32m[20221214 00:12:49 @agent_ppo2.py:121][0m #------------------------ Iteration 4694 --------------------------#
[32m[20221214 00:12:49 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:12:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:12:50 @agent_ppo2.py:185][0m |           0.0035 |          82.8577 |           4.7532 |
[32m[20221214 00:12:50 @agent_ppo2.py:185][0m |          -0.0042 |          79.3258 |           4.7071 |
[32m[20221214 00:12:50 @agent_ppo2.py:185][0m |          -0.0059 |          78.2162 |           4.7433 |
[32m[20221214 00:12:50 @agent_ppo2.py:185][0m |          -0.0069 |          77.4740 |           4.7754 |
[32m[20221214 00:12:50 @agent_ppo2.py:185][0m |          -0.0092 |          77.3640 |           4.7834 |
[32m[20221214 00:12:50 @agent_ppo2.py:185][0m |          -0.0091 |          76.7509 |           4.7880 |
[32m[20221214 00:12:50 @agent_ppo2.py:185][0m |          -0.0135 |          76.6084 |           4.8592 |
[32m[20221214 00:12:50 @agent_ppo2.py:185][0m |          -0.0130 |          76.3240 |           4.8231 |
[32m[20221214 00:12:50 @agent_ppo2.py:185][0m |          -0.0108 |          76.1217 |           4.8455 |
[32m[20221214 00:12:50 @agent_ppo2.py:185][0m |          -0.0147 |          75.7638 |           4.8894 |
[32m[20221214 00:12:50 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:12:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 528.54
[32m[20221214 00:12:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 597.89
[32m[20221214 00:12:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 513.48
[32m[20221214 00:12:51 @agent_ppo2.py:143][0m Total time:      15.34 min
[32m[20221214 00:12:51 @agent_ppo2.py:145][0m 1423360 total steps have happened
[32m[20221214 00:12:51 @agent_ppo2.py:121][0m #------------------------ Iteration 4695 --------------------------#
[32m[20221214 00:12:51 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:12:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:12:51 @agent_ppo2.py:185][0m |          -0.0005 |          67.6122 |           5.2856 |
[32m[20221214 00:12:51 @agent_ppo2.py:185][0m |          -0.0035 |          62.8725 |           5.3734 |
[32m[20221214 00:12:51 @agent_ppo2.py:185][0m |          -0.0022 |          61.2143 |           5.3298 |
[32m[20221214 00:12:51 @agent_ppo2.py:185][0m |          -0.0056 |          59.8853 |           5.3065 |
[32m[20221214 00:12:51 @agent_ppo2.py:185][0m |          -0.0008 |          69.1987 |           5.3063 |
[32m[20221214 00:12:51 @agent_ppo2.py:185][0m |          -0.0066 |          58.8045 |           5.3632 |
[32m[20221214 00:12:51 @agent_ppo2.py:185][0m |          -0.0118 |          58.2571 |           5.3843 |
[32m[20221214 00:12:51 @agent_ppo2.py:185][0m |          -0.0157 |          57.4329 |           5.3988 |
[32m[20221214 00:12:52 @agent_ppo2.py:185][0m |          -0.0184 |          57.1468 |           5.4117 |
[32m[20221214 00:12:52 @agent_ppo2.py:185][0m |          -0.0140 |          56.8566 |           5.4068 |
[32m[20221214 00:12:52 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:12:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 409.46
[32m[20221214 00:12:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 515.36
[32m[20221214 00:12:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 277.91
[32m[20221214 00:12:52 @agent_ppo2.py:143][0m Total time:      15.36 min
[32m[20221214 00:12:52 @agent_ppo2.py:145][0m 1425408 total steps have happened
[32m[20221214 00:12:52 @agent_ppo2.py:121][0m #------------------------ Iteration 4696 --------------------------#
[32m[20221214 00:12:52 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:12:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:12:52 @agent_ppo2.py:185][0m |           0.0021 |          53.9041 |           5.4902 |
[32m[20221214 00:12:52 @agent_ppo2.py:185][0m |          -0.0074 |          48.7257 |           5.4439 |
[32m[20221214 00:12:52 @agent_ppo2.py:185][0m |          -0.0118 |          47.1447 |           5.4769 |
[32m[20221214 00:12:52 @agent_ppo2.py:185][0m |          -0.0132 |          46.2711 |           5.4565 |
[32m[20221214 00:12:52 @agent_ppo2.py:185][0m |          -0.0155 |          45.9019 |           5.4567 |
[32m[20221214 00:12:53 @agent_ppo2.py:185][0m |          -0.0159 |          45.3413 |           5.4594 |
[32m[20221214 00:12:53 @agent_ppo2.py:185][0m |          -0.0185 |          45.1371 |           5.4878 |
[32m[20221214 00:12:53 @agent_ppo2.py:185][0m |          -0.0228 |          44.3370 |           5.4629 |
[32m[20221214 00:12:53 @agent_ppo2.py:185][0m |          -0.0253 |          43.8369 |           5.4971 |
[32m[20221214 00:12:53 @agent_ppo2.py:185][0m |          -0.0212 |          43.5990 |           5.4999 |
[32m[20221214 00:12:53 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:12:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 437.98
[32m[20221214 00:12:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 509.78
[32m[20221214 00:12:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 381.42
[32m[20221214 00:12:53 @agent_ppo2.py:143][0m Total time:      15.38 min
[32m[20221214 00:12:53 @agent_ppo2.py:145][0m 1427456 total steps have happened
[32m[20221214 00:12:53 @agent_ppo2.py:121][0m #------------------------ Iteration 4697 --------------------------#
[32m[20221214 00:12:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:12:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:12:53 @agent_ppo2.py:185][0m |          -0.0008 |          76.0821 |           5.6201 |
[32m[20221214 00:12:53 @agent_ppo2.py:185][0m |           0.0002 |          68.3260 |           5.6122 |
[32m[20221214 00:12:54 @agent_ppo2.py:185][0m |          -0.0105 |          61.5742 |           5.6128 |
[32m[20221214 00:12:54 @agent_ppo2.py:185][0m |          -0.0123 |          58.5634 |           5.5931 |
[32m[20221214 00:12:54 @agent_ppo2.py:185][0m |          -0.0124 |          57.0440 |           5.6338 |
[32m[20221214 00:12:54 @agent_ppo2.py:185][0m |          -0.0155 |          55.9151 |           5.6015 |
[32m[20221214 00:12:54 @agent_ppo2.py:185][0m |          -0.0167 |          54.7341 |           5.6950 |
[32m[20221214 00:12:54 @agent_ppo2.py:185][0m |          -0.0166 |          54.0324 |           5.6607 |
[32m[20221214 00:12:54 @agent_ppo2.py:185][0m |          -0.0154 |          53.1590 |           5.7589 |
[32m[20221214 00:12:54 @agent_ppo2.py:185][0m |          -0.0187 |          52.5287 |           5.7408 |
[32m[20221214 00:12:54 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:12:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 476.01
[32m[20221214 00:12:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 558.06
[32m[20221214 00:12:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 540.02
[32m[20221214 00:12:54 @agent_ppo2.py:143][0m Total time:      15.40 min
[32m[20221214 00:12:54 @agent_ppo2.py:145][0m 1429504 total steps have happened
[32m[20221214 00:12:54 @agent_ppo2.py:121][0m #------------------------ Iteration 4698 --------------------------#
[32m[20221214 00:12:54 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:12:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:12:55 @agent_ppo2.py:185][0m |           0.0067 |          77.4635 |           5.5181 |
[32m[20221214 00:12:55 @agent_ppo2.py:185][0m |           0.0009 |          66.4561 |           5.5037 |
[32m[20221214 00:12:55 @agent_ppo2.py:185][0m |          -0.0019 |          63.7808 |           5.4989 |
[32m[20221214 00:12:55 @agent_ppo2.py:185][0m |          -0.0021 |          64.6680 |           5.4764 |
[32m[20221214 00:12:55 @agent_ppo2.py:185][0m |          -0.0052 |          60.7951 |           5.4820 |
[32m[20221214 00:12:55 @agent_ppo2.py:185][0m |          -0.0059 |          59.9053 |           5.4207 |
[32m[20221214 00:12:55 @agent_ppo2.py:185][0m |          -0.0098 |          59.3635 |           5.4270 |
[32m[20221214 00:12:55 @agent_ppo2.py:185][0m |          -0.0105 |          58.9508 |           5.4461 |
[32m[20221214 00:12:55 @agent_ppo2.py:185][0m |          -0.0092 |          58.8297 |           5.3821 |
[32m[20221214 00:12:55 @agent_ppo2.py:185][0m |          -0.0063 |          58.3223 |           5.3843 |
[32m[20221214 00:12:55 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:12:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 507.60
[32m[20221214 00:12:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 545.18
[32m[20221214 00:12:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 525.62
[32m[20221214 00:12:56 @agent_ppo2.py:143][0m Total time:      15.42 min
[32m[20221214 00:12:56 @agent_ppo2.py:145][0m 1431552 total steps have happened
[32m[20221214 00:12:56 @agent_ppo2.py:121][0m #------------------------ Iteration 4699 --------------------------#
[32m[20221214 00:12:56 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:12:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:12:56 @agent_ppo2.py:185][0m |           0.0014 |          79.2482 |           5.2166 |
[32m[20221214 00:12:56 @agent_ppo2.py:185][0m |          -0.0087 |          75.5992 |           5.3264 |
[32m[20221214 00:12:56 @agent_ppo2.py:185][0m |          -0.0116 |          74.4691 |           5.2626 |
[32m[20221214 00:12:56 @agent_ppo2.py:185][0m |          -0.0120 |          73.6954 |           5.2346 |
[32m[20221214 00:12:57 @agent_ppo2.py:185][0m |          -0.0141 |          72.9770 |           5.2604 |
[32m[20221214 00:12:57 @agent_ppo2.py:185][0m |          -0.0133 |          72.6629 |           5.2544 |
[32m[20221214 00:12:57 @agent_ppo2.py:185][0m |          -0.0175 |          72.0304 |           5.2270 |
[32m[20221214 00:12:57 @agent_ppo2.py:185][0m |          -0.0168 |          71.5754 |           5.2770 |
[32m[20221214 00:12:57 @agent_ppo2.py:185][0m |          -0.0187 |          71.3626 |           5.2676 |
[32m[20221214 00:12:57 @agent_ppo2.py:185][0m |          -0.0162 |          70.8621 |           5.2485 |
[32m[20221214 00:12:57 @agent_ppo2.py:130][0m Policy update time: 1.30 s
[32m[20221214 00:12:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 499.78
[32m[20221214 00:12:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 520.49
[32m[20221214 00:12:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 534.45
[32m[20221214 00:12:57 @agent_ppo2.py:143][0m Total time:      15.45 min
[32m[20221214 00:12:57 @agent_ppo2.py:145][0m 1433600 total steps have happened
[32m[20221214 00:12:57 @agent_ppo2.py:121][0m #------------------------ Iteration 4700 --------------------------#
[32m[20221214 00:12:57 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:12:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:12:58 @agent_ppo2.py:185][0m |          -0.0004 |          84.8887 |           5.0658 |
[32m[20221214 00:12:58 @agent_ppo2.py:185][0m |          -0.0057 |          82.3735 |           5.0479 |
[32m[20221214 00:12:58 @agent_ppo2.py:185][0m |          -0.0061 |          81.6141 |           5.0082 |
[32m[20221214 00:12:58 @agent_ppo2.py:185][0m |          -0.0074 |          81.1069 |           4.9669 |
[32m[20221214 00:12:58 @agent_ppo2.py:185][0m |          -0.0093 |          80.8203 |           4.9385 |
[32m[20221214 00:12:58 @agent_ppo2.py:185][0m |           0.0043 |          88.4120 |           4.9530 |
[32m[20221214 00:12:58 @agent_ppo2.py:185][0m |          -0.0034 |          80.2961 |           4.8971 |
[32m[20221214 00:12:58 @agent_ppo2.py:185][0m |          -0.0081 |          80.1687 |           4.9386 |
[32m[20221214 00:12:58 @agent_ppo2.py:185][0m |          -0.0061 |          80.8905 |           4.9235 |
[32m[20221214 00:12:58 @agent_ppo2.py:185][0m |          -0.0094 |          79.9649 |           4.9183 |
[32m[20221214 00:12:58 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:12:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 529.50
[32m[20221214 00:12:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 551.82
[32m[20221214 00:12:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 543.61
[32m[20221214 00:12:59 @agent_ppo2.py:143][0m Total time:      15.47 min
[32m[20221214 00:12:59 @agent_ppo2.py:145][0m 1435648 total steps have happened
[32m[20221214 00:12:59 @agent_ppo2.py:121][0m #------------------------ Iteration 4701 --------------------------#
[32m[20221214 00:12:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:12:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:12:59 @agent_ppo2.py:185][0m |          -0.0026 |          71.9390 |           4.3799 |
[32m[20221214 00:12:59 @agent_ppo2.py:185][0m |          -0.0099 |          65.7654 |           4.4090 |
[32m[20221214 00:12:59 @agent_ppo2.py:185][0m |          -0.0135 |          63.8594 |           4.3729 |
[32m[20221214 00:12:59 @agent_ppo2.py:185][0m |          -0.0088 |          62.6713 |           4.4723 |
[32m[20221214 00:12:59 @agent_ppo2.py:185][0m |          -0.0098 |          61.8248 |           4.4742 |
[32m[20221214 00:12:59 @agent_ppo2.py:185][0m |          -0.0106 |          61.4953 |           4.5624 |
[32m[20221214 00:12:59 @agent_ppo2.py:185][0m |          -0.0146 |          60.9220 |           4.5601 |
[32m[20221214 00:12:59 @agent_ppo2.py:185][0m |          -0.0126 |          60.3976 |           4.5419 |
[32m[20221214 00:13:00 @agent_ppo2.py:185][0m |          -0.0192 |          60.0696 |           4.6075 |
[32m[20221214 00:13:00 @agent_ppo2.py:185][0m |          -0.0157 |          59.8506 |           4.6996 |
[32m[20221214 00:13:00 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:13:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 506.79
[32m[20221214 00:13:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 532.03
[32m[20221214 00:13:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 544.45
[32m[20221214 00:13:00 @agent_ppo2.py:143][0m Total time:      15.49 min
[32m[20221214 00:13:00 @agent_ppo2.py:145][0m 1437696 total steps have happened
[32m[20221214 00:13:00 @agent_ppo2.py:121][0m #------------------------ Iteration 4702 --------------------------#
[32m[20221214 00:13:00 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:13:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:13:00 @agent_ppo2.py:185][0m |          -0.0004 |          62.1353 |           4.4436 |
[32m[20221214 00:13:00 @agent_ppo2.py:185][0m |          -0.0081 |          56.3034 |           4.4154 |
[32m[20221214 00:13:00 @agent_ppo2.py:185][0m |          -0.0080 |          55.0823 |           4.4658 |
[32m[20221214 00:13:00 @agent_ppo2.py:185][0m |          -0.0132 |          54.2516 |           4.4213 |
[32m[20221214 00:13:01 @agent_ppo2.py:185][0m |          -0.0104 |          53.6352 |           4.4909 |
[32m[20221214 00:13:01 @agent_ppo2.py:185][0m |          -0.0163 |          53.3214 |           4.4495 |
[32m[20221214 00:13:01 @agent_ppo2.py:185][0m |          -0.0029 |          58.7181 |           4.4807 |
[32m[20221214 00:13:01 @agent_ppo2.py:185][0m |          -0.0117 |          55.1785 |           4.3758 |
[32m[20221214 00:13:01 @agent_ppo2.py:185][0m |          -0.0197 |          52.2716 |           4.3497 |
[32m[20221214 00:13:01 @agent_ppo2.py:185][0m |          -0.0163 |          52.3704 |           4.3503 |
[32m[20221214 00:13:01 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:13:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 410.86
[32m[20221214 00:13:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 510.17
[32m[20221214 00:13:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 507.04
[32m[20221214 00:13:01 @agent_ppo2.py:143][0m Total time:      15.51 min
[32m[20221214 00:13:01 @agent_ppo2.py:145][0m 1439744 total steps have happened
[32m[20221214 00:13:01 @agent_ppo2.py:121][0m #------------------------ Iteration 4703 --------------------------#
[32m[20221214 00:13:01 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:13:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:13:01 @agent_ppo2.py:185][0m |           0.0077 |          73.8800 |           4.4147 |
[32m[20221214 00:13:02 @agent_ppo2.py:185][0m |          -0.0052 |          67.7173 |           4.4592 |
[32m[20221214 00:13:02 @agent_ppo2.py:185][0m |          -0.0052 |          66.2745 |           4.4113 |
[32m[20221214 00:13:02 @agent_ppo2.py:185][0m |          -0.0073 |          64.9916 |           4.5058 |
[32m[20221214 00:13:02 @agent_ppo2.py:185][0m |          -0.0061 |          65.0262 |           4.4549 |
[32m[20221214 00:13:02 @agent_ppo2.py:185][0m |          -0.0119 |          63.4012 |           4.4676 |
[32m[20221214 00:13:02 @agent_ppo2.py:185][0m |          -0.0122 |          62.8277 |           4.4388 |
[32m[20221214 00:13:02 @agent_ppo2.py:185][0m |          -0.0131 |          62.4870 |           4.4745 |
[32m[20221214 00:13:02 @agent_ppo2.py:185][0m |          -0.0124 |          61.8840 |           4.4504 |
[32m[20221214 00:13:02 @agent_ppo2.py:185][0m |          -0.0151 |          61.6376 |           4.4991 |
[32m[20221214 00:13:02 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:13:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 447.44
[32m[20221214 00:13:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 519.52
[32m[20221214 00:13:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 521.11
[32m[20221214 00:13:03 @agent_ppo2.py:143][0m Total time:      15.54 min
[32m[20221214 00:13:03 @agent_ppo2.py:145][0m 1441792 total steps have happened
[32m[20221214 00:13:03 @agent_ppo2.py:121][0m #------------------------ Iteration 4704 --------------------------#
[32m[20221214 00:13:03 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:13:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:13:03 @agent_ppo2.py:185][0m |           0.0052 |          64.8707 |           3.8172 |
[32m[20221214 00:13:03 @agent_ppo2.py:185][0m |           0.0044 |          61.4749 |           4.0132 |
[32m[20221214 00:13:03 @agent_ppo2.py:185][0m |          -0.0069 |          57.0129 |           3.9332 |
[32m[20221214 00:13:03 @agent_ppo2.py:185][0m |          -0.0005 |          56.7750 |           4.0040 |
[32m[20221214 00:13:03 @agent_ppo2.py:185][0m |           0.0014 |          59.8707 |           4.0201 |
[32m[20221214 00:13:03 @agent_ppo2.py:185][0m |          -0.0114 |          55.4921 |           3.9598 |
[32m[20221214 00:13:03 @agent_ppo2.py:185][0m |          -0.0139 |          54.6877 |           3.9362 |
[32m[20221214 00:13:04 @agent_ppo2.py:185][0m |          -0.0139 |          54.3981 |           3.9799 |
[32m[20221214 00:13:04 @agent_ppo2.py:185][0m |          -0.0045 |          55.8834 |           3.9346 |
[32m[20221214 00:13:04 @agent_ppo2.py:185][0m |          -0.0057 |          60.5191 |           3.9890 |
[32m[20221214 00:13:04 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:13:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 494.00
[32m[20221214 00:13:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 558.88
[32m[20221214 00:13:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 480.35
[32m[20221214 00:13:04 @agent_ppo2.py:143][0m Total time:      15.56 min
[32m[20221214 00:13:04 @agent_ppo2.py:145][0m 1443840 total steps have happened
[32m[20221214 00:13:04 @agent_ppo2.py:121][0m #------------------------ Iteration 4705 --------------------------#
[32m[20221214 00:13:04 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:13:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:13:04 @agent_ppo2.py:185][0m |           0.0013 |          59.5717 |           4.6550 |
[32m[20221214 00:13:04 @agent_ppo2.py:185][0m |          -0.0034 |          55.0228 |           4.6420 |
[32m[20221214 00:13:04 @agent_ppo2.py:185][0m |          -0.0069 |          52.3180 |           4.5688 |
[32m[20221214 00:13:05 @agent_ppo2.py:185][0m |          -0.0093 |          51.3854 |           4.5958 |
[32m[20221214 00:13:05 @agent_ppo2.py:185][0m |          -0.0102 |          50.1220 |           4.6049 |
[32m[20221214 00:13:05 @agent_ppo2.py:185][0m |          -0.0128 |          49.5146 |           4.5228 |
[32m[20221214 00:13:05 @agent_ppo2.py:185][0m |          -0.0077 |          49.8599 |           4.5101 |
[32m[20221214 00:13:05 @agent_ppo2.py:185][0m |          -0.0123 |          49.4163 |           4.5240 |
[32m[20221214 00:13:05 @agent_ppo2.py:185][0m |          -0.0114 |          48.7063 |           4.5339 |
[32m[20221214 00:13:05 @agent_ppo2.py:185][0m |          -0.0163 |          49.0166 |           4.4828 |
[32m[20221214 00:13:05 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:13:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 526.92
[32m[20221214 00:13:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 541.88
[32m[20221214 00:13:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 477.74
[32m[20221214 00:13:05 @agent_ppo2.py:143][0m Total time:      15.58 min
[32m[20221214 00:13:05 @agent_ppo2.py:145][0m 1445888 total steps have happened
[32m[20221214 00:13:05 @agent_ppo2.py:121][0m #------------------------ Iteration 4706 --------------------------#
[32m[20221214 00:13:05 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:13:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:13:06 @agent_ppo2.py:185][0m |           0.0049 |          82.9341 |           4.5964 |
[32m[20221214 00:13:06 @agent_ppo2.py:185][0m |          -0.0015 |          82.5475 |           4.5713 |
[32m[20221214 00:13:06 @agent_ppo2.py:185][0m |          -0.0062 |          77.5692 |           4.6156 |
[32m[20221214 00:13:06 @agent_ppo2.py:185][0m |          -0.0132 |          76.8112 |           4.5711 |
[32m[20221214 00:13:06 @agent_ppo2.py:185][0m |          -0.0135 |          75.8992 |           4.6039 |
[32m[20221214 00:13:06 @agent_ppo2.py:185][0m |          -0.0155 |          75.6875 |           4.5475 |
[32m[20221214 00:13:06 @agent_ppo2.py:185][0m |          -0.0155 |          75.2490 |           4.5006 |
[32m[20221214 00:13:06 @agent_ppo2.py:185][0m |          -0.0143 |          75.1605 |           4.5039 |
[32m[20221214 00:13:06 @agent_ppo2.py:185][0m |          -0.0058 |          83.3550 |           4.5091 |
[32m[20221214 00:13:06 @agent_ppo2.py:185][0m |          -0.0099 |          78.7788 |           4.4905 |
[32m[20221214 00:13:06 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:13:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 499.84
[32m[20221214 00:13:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 539.04
[32m[20221214 00:13:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 573.14
[32m[20221214 00:13:07 @agent_ppo2.py:143][0m Total time:      15.60 min
[32m[20221214 00:13:07 @agent_ppo2.py:145][0m 1447936 total steps have happened
[32m[20221214 00:13:07 @agent_ppo2.py:121][0m #------------------------ Iteration 4707 --------------------------#
[32m[20221214 00:13:07 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:13:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:13:07 @agent_ppo2.py:185][0m |          -0.0004 |          73.0487 |           4.0780 |
[32m[20221214 00:13:07 @agent_ppo2.py:185][0m |          -0.0040 |          64.8552 |           3.9848 |
[32m[20221214 00:13:07 @agent_ppo2.py:185][0m |          -0.0053 |          61.7570 |           3.9864 |
[32m[20221214 00:13:07 @agent_ppo2.py:185][0m |          -0.0066 |          59.9620 |           3.9905 |
[32m[20221214 00:13:07 @agent_ppo2.py:185][0m |          -0.0084 |          58.8637 |           4.1735 |
[32m[20221214 00:13:07 @agent_ppo2.py:185][0m |          -0.0115 |          57.7294 |           4.0681 |
[32m[20221214 00:13:07 @agent_ppo2.py:185][0m |          -0.0111 |          57.0926 |           4.0793 |
[32m[20221214 00:13:08 @agent_ppo2.py:185][0m |          -0.0114 |          56.5106 |           4.0056 |
[32m[20221214 00:13:08 @agent_ppo2.py:185][0m |          -0.0143 |          55.3653 |           3.9977 |
[32m[20221214 00:13:08 @agent_ppo2.py:185][0m |          -0.0120 |          54.5987 |           4.0186 |
[32m[20221214 00:13:08 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:13:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 506.23
[32m[20221214 00:13:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 528.12
[32m[20221214 00:13:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 538.60
[32m[20221214 00:13:08 @agent_ppo2.py:143][0m Total time:      15.62 min
[32m[20221214 00:13:08 @agent_ppo2.py:145][0m 1449984 total steps have happened
[32m[20221214 00:13:08 @agent_ppo2.py:121][0m #------------------------ Iteration 4708 --------------------------#
[32m[20221214 00:13:08 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:13:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:13:08 @agent_ppo2.py:185][0m |           0.0114 |          88.5248 |           3.6817 |
[32m[20221214 00:13:08 @agent_ppo2.py:185][0m |           0.0039 |          79.5711 |           3.6264 |
[32m[20221214 00:13:08 @agent_ppo2.py:185][0m |          -0.0071 |          75.4855 |           3.6500 |
[32m[20221214 00:13:08 @agent_ppo2.py:185][0m |          -0.0103 |          73.5462 |           3.6298 |
[32m[20221214 00:13:09 @agent_ppo2.py:185][0m |          -0.0168 |          72.8578 |           3.5983 |
[32m[20221214 00:13:09 @agent_ppo2.py:185][0m |          -0.0139 |          72.2705 |           3.6331 |
[32m[20221214 00:13:09 @agent_ppo2.py:185][0m |          -0.0063 |          76.5246 |           3.6166 |
[32m[20221214 00:13:09 @agent_ppo2.py:185][0m |          -0.0146 |          71.3016 |           3.6020 |
[32m[20221214 00:13:09 @agent_ppo2.py:185][0m |          -0.0045 |          77.7702 |           3.5973 |
[32m[20221214 00:13:09 @agent_ppo2.py:185][0m |          -0.0174 |          71.4710 |           3.5024 |
[32m[20221214 00:13:09 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:13:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 520.48
[32m[20221214 00:13:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 563.51
[32m[20221214 00:13:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 567.18
[32m[20221214 00:13:09 @agent_ppo2.py:143][0m Total time:      15.65 min
[32m[20221214 00:13:09 @agent_ppo2.py:145][0m 1452032 total steps have happened
[32m[20221214 00:13:09 @agent_ppo2.py:121][0m #------------------------ Iteration 4709 --------------------------#
[32m[20221214 00:13:09 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:13:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:13:10 @agent_ppo2.py:185][0m |           0.0007 |          87.5850 |           2.9275 |
[32m[20221214 00:13:10 @agent_ppo2.py:185][0m |          -0.0072 |          85.0961 |           2.9368 |
[32m[20221214 00:13:10 @agent_ppo2.py:185][0m |          -0.0028 |          85.2292 |           2.9240 |
[32m[20221214 00:13:10 @agent_ppo2.py:185][0m |          -0.0081 |          83.0237 |           3.0253 |
[32m[20221214 00:13:10 @agent_ppo2.py:185][0m |          -0.0108 |          82.4251 |           2.9617 |
[32m[20221214 00:13:10 @agent_ppo2.py:185][0m |          -0.0108 |          82.2486 |           3.0568 |
[32m[20221214 00:13:10 @agent_ppo2.py:185][0m |          -0.0129 |          81.7887 |           2.9943 |
[32m[20221214 00:13:10 @agent_ppo2.py:185][0m |          -0.0065 |          88.4819 |           2.9527 |
[32m[20221214 00:13:10 @agent_ppo2.py:185][0m |          -0.0147 |          81.2966 |           2.9824 |
[32m[20221214 00:13:10 @agent_ppo2.py:185][0m |          -0.0165 |          81.3827 |           2.9096 |
[32m[20221214 00:13:10 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:13:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 512.35
[32m[20221214 00:13:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 554.00
[32m[20221214 00:13:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 463.48
[32m[20221214 00:13:11 @agent_ppo2.py:143][0m Total time:      15.67 min
[32m[20221214 00:13:11 @agent_ppo2.py:145][0m 1454080 total steps have happened
[32m[20221214 00:13:11 @agent_ppo2.py:121][0m #------------------------ Iteration 4710 --------------------------#
[32m[20221214 00:13:11 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:13:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:13:11 @agent_ppo2.py:185][0m |           0.0011 |          33.7172 |           3.8517 |
[32m[20221214 00:13:11 @agent_ppo2.py:185][0m |          -0.0060 |          22.9304 |           3.7105 |
[32m[20221214 00:13:11 @agent_ppo2.py:185][0m |          -0.0085 |          20.8679 |           3.7012 |
[32m[20221214 00:13:11 @agent_ppo2.py:185][0m |          -0.0112 |          19.4793 |           3.7278 |
[32m[20221214 00:13:11 @agent_ppo2.py:185][0m |          -0.0045 |          18.4900 |           3.6319 |
[32m[20221214 00:13:11 @agent_ppo2.py:185][0m |          -0.0126 |          17.7714 |           3.6288 |
[32m[20221214 00:13:11 @agent_ppo2.py:185][0m |          -0.0148 |          16.9925 |           3.6537 |
[32m[20221214 00:13:12 @agent_ppo2.py:185][0m |          -0.0233 |          16.6105 |           3.5820 |
[32m[20221214 00:13:12 @agent_ppo2.py:185][0m |          -0.0198 |          16.0288 |           3.5847 |
[32m[20221214 00:13:12 @agent_ppo2.py:185][0m |          -0.0122 |          16.3548 |           3.6442 |
[32m[20221214 00:13:12 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:13:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 352.71
[32m[20221214 00:13:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 432.16
[32m[20221214 00:13:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 323.97
[32m[20221214 00:13:12 @agent_ppo2.py:143][0m Total time:      15.69 min
[32m[20221214 00:13:12 @agent_ppo2.py:145][0m 1456128 total steps have happened
[32m[20221214 00:13:12 @agent_ppo2.py:121][0m #------------------------ Iteration 4711 --------------------------#
[32m[20221214 00:13:12 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:13:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:13:12 @agent_ppo2.py:185][0m |          -0.0025 |          59.1681 |           3.4244 |
[32m[20221214 00:13:12 @agent_ppo2.py:185][0m |           0.0022 |          53.2720 |           3.4883 |
[32m[20221214 00:13:12 @agent_ppo2.py:185][0m |          -0.0065 |          48.9360 |           3.4199 |
[32m[20221214 00:13:12 @agent_ppo2.py:185][0m |          -0.0066 |          47.6135 |           3.3994 |
[32m[20221214 00:13:13 @agent_ppo2.py:185][0m |          -0.0059 |          47.0510 |           3.4004 |
[32m[20221214 00:13:13 @agent_ppo2.py:185][0m |           0.0033 |          50.2848 |           3.3540 |
[32m[20221214 00:13:13 @agent_ppo2.py:185][0m |          -0.0090 |          46.1011 |           3.3902 |
[32m[20221214 00:13:13 @agent_ppo2.py:185][0m |          -0.0123 |          45.5698 |           3.3288 |
[32m[20221214 00:13:13 @agent_ppo2.py:185][0m |          -0.0162 |          45.3105 |           3.3148 |
[32m[20221214 00:13:13 @agent_ppo2.py:185][0m |          -0.0133 |          45.3888 |           3.2544 |
[32m[20221214 00:13:13 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:13:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 482.39
[32m[20221214 00:13:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 542.48
[32m[20221214 00:13:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 537.31
[32m[20221214 00:13:13 @agent_ppo2.py:143][0m Total time:      15.71 min
[32m[20221214 00:13:13 @agent_ppo2.py:145][0m 1458176 total steps have happened
[32m[20221214 00:13:13 @agent_ppo2.py:121][0m #------------------------ Iteration 4712 --------------------------#
[32m[20221214 00:13:13 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:13:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:13:13 @agent_ppo2.py:185][0m |           0.0027 |          68.3774 |           3.0709 |
[32m[20221214 00:13:14 @agent_ppo2.py:185][0m |          -0.0034 |          64.2510 |           3.1543 |
[32m[20221214 00:13:14 @agent_ppo2.py:185][0m |          -0.0133 |          61.9511 |           3.0698 |
[32m[20221214 00:13:14 @agent_ppo2.py:185][0m |          -0.0120 |          61.2858 |           3.0959 |
[32m[20221214 00:13:14 @agent_ppo2.py:185][0m |          -0.0095 |          60.4691 |           3.0803 |
[32m[20221214 00:13:14 @agent_ppo2.py:185][0m |          -0.0173 |          58.9503 |           3.0972 |
[32m[20221214 00:13:14 @agent_ppo2.py:185][0m |          -0.0167 |          58.3295 |           2.9981 |
[32m[20221214 00:13:14 @agent_ppo2.py:185][0m |          -0.0175 |          57.7300 |           3.1386 |
[32m[20221214 00:13:14 @agent_ppo2.py:185][0m |          -0.0100 |          58.9701 |           3.1162 |
[32m[20221214 00:13:14 @agent_ppo2.py:185][0m |          -0.0141 |          57.0509 |           3.1478 |
[32m[20221214 00:13:14 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:13:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 506.33
[32m[20221214 00:13:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 563.08
[32m[20221214 00:13:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 480.98
[32m[20221214 00:13:14 @agent_ppo2.py:143][0m Total time:      15.73 min
[32m[20221214 00:13:14 @agent_ppo2.py:145][0m 1460224 total steps have happened
[32m[20221214 00:13:14 @agent_ppo2.py:121][0m #------------------------ Iteration 4713 --------------------------#
[32m[20221214 00:13:15 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:13:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:13:15 @agent_ppo2.py:185][0m |           0.0031 |         100.4982 |           2.5476 |
[32m[20221214 00:13:15 @agent_ppo2.py:185][0m |           0.0047 |         104.0606 |           2.6544 |
[32m[20221214 00:13:15 @agent_ppo2.py:185][0m |          -0.0072 |          96.7894 |           2.6717 |
[32m[20221214 00:13:15 @agent_ppo2.py:185][0m |          -0.0121 |          96.7453 |           2.6087 |
[32m[20221214 00:13:15 @agent_ppo2.py:185][0m |          -0.0106 |          95.9617 |           2.6647 |
[32m[20221214 00:13:15 @agent_ppo2.py:185][0m |          -0.0104 |          95.9478 |           2.6327 |
[32m[20221214 00:13:15 @agent_ppo2.py:185][0m |          -0.0131 |          95.6064 |           2.6726 |
[32m[20221214 00:13:15 @agent_ppo2.py:185][0m |          -0.0122 |          95.5227 |           2.6672 |
[32m[20221214 00:13:15 @agent_ppo2.py:185][0m |          -0.0138 |          95.1158 |           2.6198 |
[32m[20221214 00:13:16 @agent_ppo2.py:185][0m |          -0.0103 |          94.9539 |           2.5625 |
[32m[20221214 00:13:16 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:13:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 535.77
[32m[20221214 00:13:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 542.41
[32m[20221214 00:13:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 570.02
[32m[20221214 00:13:16 @agent_ppo2.py:143][0m Total time:      15.76 min
[32m[20221214 00:13:16 @agent_ppo2.py:145][0m 1462272 total steps have happened
[32m[20221214 00:13:16 @agent_ppo2.py:121][0m #------------------------ Iteration 4714 --------------------------#
[32m[20221214 00:13:16 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:13:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:13:16 @agent_ppo2.py:185][0m |           0.0013 |          67.3415 |           2.7998 |
[32m[20221214 00:13:16 @agent_ppo2.py:185][0m |           0.0011 |          63.7247 |           2.7265 |
[32m[20221214 00:13:16 @agent_ppo2.py:185][0m |          -0.0071 |          60.2337 |           2.7105 |
[32m[20221214 00:13:16 @agent_ppo2.py:185][0m |          -0.0113 |          58.7065 |           2.7671 |
[32m[20221214 00:13:16 @agent_ppo2.py:185][0m |          -0.0165 |          57.6792 |           2.6034 |
[32m[20221214 00:13:17 @agent_ppo2.py:185][0m |          -0.0109 |          57.3321 |           2.6278 |
[32m[20221214 00:13:17 @agent_ppo2.py:185][0m |          -0.0142 |          56.5717 |           2.5965 |
[32m[20221214 00:13:17 @agent_ppo2.py:185][0m |          -0.0199 |          56.1159 |           2.5366 |
[32m[20221214 00:13:17 @agent_ppo2.py:185][0m |          -0.0159 |          55.6515 |           2.4526 |
[32m[20221214 00:13:17 @agent_ppo2.py:185][0m |          -0.0175 |          55.2255 |           2.4868 |
[32m[20221214 00:13:17 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:13:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 459.14
[32m[20221214 00:13:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 546.76
[32m[20221214 00:13:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 482.35
[32m[20221214 00:13:17 @agent_ppo2.py:143][0m Total time:      15.78 min
[32m[20221214 00:13:17 @agent_ppo2.py:145][0m 1464320 total steps have happened
[32m[20221214 00:13:17 @agent_ppo2.py:121][0m #------------------------ Iteration 4715 --------------------------#
[32m[20221214 00:13:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:13:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:13:17 @agent_ppo2.py:185][0m |          -0.0010 |          85.5415 |           2.6636 |
[32m[20221214 00:13:17 @agent_ppo2.py:185][0m |          -0.0042 |          79.3410 |           2.8157 |
[32m[20221214 00:13:18 @agent_ppo2.py:185][0m |          -0.0060 |          77.2866 |           2.8486 |
[32m[20221214 00:13:18 @agent_ppo2.py:185][0m |          -0.0045 |          79.1287 |           2.8869 |
[32m[20221214 00:13:18 @agent_ppo2.py:185][0m |          -0.0084 |          75.3490 |           2.9372 |
[32m[20221214 00:13:18 @agent_ppo2.py:185][0m |          -0.0110 |          74.0429 |           2.9764 |
[32m[20221214 00:13:18 @agent_ppo2.py:185][0m |          -0.0139 |          73.1811 |           2.9405 |
[32m[20221214 00:13:18 @agent_ppo2.py:185][0m |          -0.0134 |          72.9583 |           2.9997 |
[32m[20221214 00:13:18 @agent_ppo2.py:185][0m |          -0.0148 |          72.6756 |           3.0699 |
[32m[20221214 00:13:18 @agent_ppo2.py:185][0m |          -0.0128 |          71.6737 |           3.0928 |
[32m[20221214 00:13:18 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:13:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 529.87
[32m[20221214 00:13:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 561.05
[32m[20221214 00:13:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 315.79
[32m[20221214 00:13:18 @agent_ppo2.py:143][0m Total time:      15.80 min
[32m[20221214 00:13:18 @agent_ppo2.py:145][0m 1466368 total steps have happened
[32m[20221214 00:13:18 @agent_ppo2.py:121][0m #------------------------ Iteration 4716 --------------------------#
[32m[20221214 00:13:18 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:13:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:13:19 @agent_ppo2.py:185][0m |          -0.0017 |          83.9951 |           2.5462 |
[32m[20221214 00:13:19 @agent_ppo2.py:185][0m |          -0.0029 |          82.2374 |           2.7084 |
[32m[20221214 00:13:19 @agent_ppo2.py:185][0m |          -0.0074 |          81.0718 |           2.7683 |
[32m[20221214 00:13:19 @agent_ppo2.py:185][0m |          -0.0083 |          80.1821 |           2.7969 |
[32m[20221214 00:13:19 @agent_ppo2.py:185][0m |          -0.0111 |          79.9016 |           2.8270 |
[32m[20221214 00:13:19 @agent_ppo2.py:185][0m |           0.0021 |          90.6292 |           2.8800 |
[32m[20221214 00:13:19 @agent_ppo2.py:185][0m |          -0.0130 |          79.3939 |           2.8915 |
[32m[20221214 00:13:19 @agent_ppo2.py:185][0m |          -0.0090 |          80.9206 |           2.9666 |
[32m[20221214 00:13:19 @agent_ppo2.py:185][0m |          -0.0137 |          78.7414 |           3.0479 |
[32m[20221214 00:13:19 @agent_ppo2.py:185][0m |          -0.0125 |          78.8039 |           3.0547 |
[32m[20221214 00:13:19 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:13:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 536.87
[32m[20221214 00:13:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 563.47
[32m[20221214 00:13:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 547.09
[32m[20221214 00:13:20 @agent_ppo2.py:143][0m Total time:      15.82 min
[32m[20221214 00:13:20 @agent_ppo2.py:145][0m 1468416 total steps have happened
[32m[20221214 00:13:20 @agent_ppo2.py:121][0m #------------------------ Iteration 4717 --------------------------#
[32m[20221214 00:13:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:13:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:13:20 @agent_ppo2.py:185][0m |           0.0020 |          92.0246 |           3.3370 |
[32m[20221214 00:13:20 @agent_ppo2.py:185][0m |          -0.0043 |          87.6148 |           3.2530 |
[32m[20221214 00:13:20 @agent_ppo2.py:185][0m |          -0.0074 |          84.8473 |           3.2564 |
[32m[20221214 00:13:20 @agent_ppo2.py:185][0m |          -0.0082 |          84.1740 |           3.3011 |
[32m[20221214 00:13:20 @agent_ppo2.py:185][0m |          -0.0110 |          82.8488 |           3.2286 |
[32m[20221214 00:13:20 @agent_ppo2.py:185][0m |          -0.0114 |          82.2469 |           3.2569 |
[32m[20221214 00:13:20 @agent_ppo2.py:185][0m |          -0.0085 |          83.5526 |           3.2861 |
[32m[20221214 00:13:21 @agent_ppo2.py:185][0m |          -0.0120 |          80.7586 |           3.3110 |
[32m[20221214 00:13:21 @agent_ppo2.py:185][0m |          -0.0155 |          79.5896 |           3.3282 |
[32m[20221214 00:13:21 @agent_ppo2.py:185][0m |          -0.0166 |          79.0655 |           3.3256 |
[32m[20221214 00:13:21 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:13:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 520.24
[32m[20221214 00:13:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 535.81
[32m[20221214 00:13:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 548.90
[32m[20221214 00:13:21 @agent_ppo2.py:143][0m Total time:      15.84 min
[32m[20221214 00:13:21 @agent_ppo2.py:145][0m 1470464 total steps have happened
[32m[20221214 00:13:21 @agent_ppo2.py:121][0m #------------------------ Iteration 4718 --------------------------#
[32m[20221214 00:13:21 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:13:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:13:21 @agent_ppo2.py:185][0m |          -0.0029 |          78.7577 |           3.6909 |
[32m[20221214 00:13:21 @agent_ppo2.py:185][0m |          -0.0043 |          73.8565 |           3.7877 |
[32m[20221214 00:13:21 @agent_ppo2.py:185][0m |          -0.0004 |          74.9696 |           3.8204 |
[32m[20221214 00:13:21 @agent_ppo2.py:185][0m |          -0.0059 |          70.8555 |           3.8537 |
[32m[20221214 00:13:22 @agent_ppo2.py:185][0m |          -0.0122 |          69.8665 |           4.0052 |
[32m[20221214 00:13:22 @agent_ppo2.py:185][0m |          -0.0111 |          69.0363 |           4.0672 |
[32m[20221214 00:13:22 @agent_ppo2.py:185][0m |          -0.0123 |          68.5383 |           4.0284 |
[32m[20221214 00:13:22 @agent_ppo2.py:185][0m |          -0.0137 |          67.8768 |           4.1519 |
[32m[20221214 00:13:22 @agent_ppo2.py:185][0m |          -0.0115 |          67.5587 |           4.2068 |
[32m[20221214 00:13:22 @agent_ppo2.py:185][0m |          -0.0152 |          66.9821 |           4.2285 |
[32m[20221214 00:13:22 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:13:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 523.11
[32m[20221214 00:13:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 567.35
[32m[20221214 00:13:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 556.20
[32m[20221214 00:13:22 @agent_ppo2.py:143][0m Total time:      15.86 min
[32m[20221214 00:13:22 @agent_ppo2.py:145][0m 1472512 total steps have happened
[32m[20221214 00:13:22 @agent_ppo2.py:121][0m #------------------------ Iteration 4719 --------------------------#
[32m[20221214 00:13:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:13:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:13:22 @agent_ppo2.py:185][0m |           0.0051 |          65.3440 |           4.1832 |
[32m[20221214 00:13:23 @agent_ppo2.py:185][0m |          -0.0038 |          52.0553 |           4.1696 |
[32m[20221214 00:13:23 @agent_ppo2.py:185][0m |          -0.0104 |          48.8597 |           4.1917 |
[32m[20221214 00:13:23 @agent_ppo2.py:185][0m |          -0.0121 |          47.6175 |           4.2186 |
[32m[20221214 00:13:23 @agent_ppo2.py:185][0m |          -0.0109 |          46.0115 |           4.2621 |
[32m[20221214 00:13:23 @agent_ppo2.py:185][0m |          -0.0159 |          45.4354 |           4.2793 |
[32m[20221214 00:13:23 @agent_ppo2.py:185][0m |          -0.0156 |          44.7932 |           4.2348 |
[32m[20221214 00:13:23 @agent_ppo2.py:185][0m |          -0.0132 |          44.2212 |           4.1888 |
[32m[20221214 00:13:23 @agent_ppo2.py:185][0m |          -0.0174 |          44.8281 |           4.2737 |
[32m[20221214 00:13:23 @agent_ppo2.py:185][0m |          -0.0136 |          43.2948 |           4.2811 |
[32m[20221214 00:13:23 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:13:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 323.11
[32m[20221214 00:13:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 439.49
[32m[20221214 00:13:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 564.12
[32m[20221214 00:13:23 @agent_ppo2.py:143][0m Total time:      15.88 min
[32m[20221214 00:13:23 @agent_ppo2.py:145][0m 1474560 total steps have happened
[32m[20221214 00:13:23 @agent_ppo2.py:121][0m #------------------------ Iteration 4720 --------------------------#
[32m[20221214 00:13:24 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:13:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:13:24 @agent_ppo2.py:185][0m |          -0.0017 |          31.7634 |           3.5219 |
[32m[20221214 00:13:24 @agent_ppo2.py:185][0m |          -0.0081 |          23.9777 |           3.6192 |
[32m[20221214 00:13:24 @agent_ppo2.py:185][0m |          -0.0138 |          21.6513 |           3.6154 |
[32m[20221214 00:13:24 @agent_ppo2.py:185][0m |          -0.0077 |          20.5075 |           3.5654 |
[32m[20221214 00:13:24 @agent_ppo2.py:185][0m |          -0.0176 |          19.5443 |           3.5296 |
[32m[20221214 00:13:24 @agent_ppo2.py:185][0m |          -0.0218 |          18.8289 |           3.6182 |
[32m[20221214 00:13:24 @agent_ppo2.py:185][0m |          -0.0211 |          18.3312 |           3.5902 |
[32m[20221214 00:13:24 @agent_ppo2.py:185][0m |          -0.0175 |          17.8256 |           3.5963 |
[32m[20221214 00:13:24 @agent_ppo2.py:185][0m |          -0.0149 |          17.3052 |           3.6021 |
[32m[20221214 00:13:24 @agent_ppo2.py:185][0m |          -0.0223 |          16.9896 |           3.6355 |
[32m[20221214 00:13:24 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:13:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 268.50
[32m[20221214 00:13:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 483.66
[32m[20221214 00:13:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 533.89
[32m[20221214 00:13:25 @agent_ppo2.py:143][0m Total time:      15.90 min
[32m[20221214 00:13:25 @agent_ppo2.py:145][0m 1476608 total steps have happened
[32m[20221214 00:13:25 @agent_ppo2.py:121][0m #------------------------ Iteration 4721 --------------------------#
[32m[20221214 00:13:25 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:13:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:13:25 @agent_ppo2.py:185][0m |           0.0055 |          40.4959 |           3.6282 |
[32m[20221214 00:13:25 @agent_ppo2.py:185][0m |           0.0012 |          33.3592 |           3.6965 |
[32m[20221214 00:13:25 @agent_ppo2.py:185][0m |          -0.0051 |          31.7370 |           3.6867 |
[32m[20221214 00:13:25 @agent_ppo2.py:185][0m |          -0.0069 |          30.6293 |           3.7076 |
[32m[20221214 00:13:25 @agent_ppo2.py:185][0m |          -0.0093 |          29.9965 |           3.7065 |
[32m[20221214 00:13:25 @agent_ppo2.py:185][0m |          -0.0196 |          29.6060 |           3.7625 |
[32m[20221214 00:13:26 @agent_ppo2.py:185][0m |          -0.0195 |          29.0227 |           3.7758 |
[32m[20221214 00:13:26 @agent_ppo2.py:185][0m |          -0.0146 |          28.7465 |           3.7569 |
[32m[20221214 00:13:26 @agent_ppo2.py:185][0m |          -0.0182 |          28.3136 |           3.8185 |
[32m[20221214 00:13:26 @agent_ppo2.py:185][0m |          -0.0179 |          28.3102 |           3.8587 |
[32m[20221214 00:13:26 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:13:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 445.42
[32m[20221214 00:13:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 529.48
[32m[20221214 00:13:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 521.76
[32m[20221214 00:13:26 @agent_ppo2.py:143][0m Total time:      15.93 min
[32m[20221214 00:13:26 @agent_ppo2.py:145][0m 1478656 total steps have happened
[32m[20221214 00:13:26 @agent_ppo2.py:121][0m #------------------------ Iteration 4722 --------------------------#
[32m[20221214 00:13:26 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:13:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:13:26 @agent_ppo2.py:185][0m |           0.0029 |          93.8206 |           4.3751 |
[32m[20221214 00:13:26 @agent_ppo2.py:185][0m |          -0.0109 |          89.1125 |           4.6324 |
[32m[20221214 00:13:26 @agent_ppo2.py:185][0m |          -0.0102 |          91.7313 |           4.5817 |
[32m[20221214 00:13:27 @agent_ppo2.py:185][0m |          -0.0144 |          87.8274 |           4.6052 |
[32m[20221214 00:13:27 @agent_ppo2.py:185][0m |          -0.0167 |          86.8298 |           4.6916 |
[32m[20221214 00:13:27 @agent_ppo2.py:185][0m |          -0.0147 |          86.5762 |           4.6326 |
[32m[20221214 00:13:27 @agent_ppo2.py:185][0m |          -0.0158 |          86.3127 |           4.6438 |
[32m[20221214 00:13:27 @agent_ppo2.py:185][0m |          -0.0182 |          86.1831 |           4.6654 |
[32m[20221214 00:13:27 @agent_ppo2.py:185][0m |          -0.0189 |          86.1311 |           4.6945 |
[32m[20221214 00:13:27 @agent_ppo2.py:185][0m |          -0.0175 |          86.0678 |           4.7315 |
[32m[20221214 00:13:27 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:13:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 535.49
[32m[20221214 00:13:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 565.28
[32m[20221214 00:13:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 570.82
[32m[20221214 00:13:27 @agent_ppo2.py:143][0m Total time:      15.95 min
[32m[20221214 00:13:27 @agent_ppo2.py:145][0m 1480704 total steps have happened
[32m[20221214 00:13:27 @agent_ppo2.py:121][0m #------------------------ Iteration 4723 --------------------------#
[32m[20221214 00:13:27 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:13:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:13:28 @agent_ppo2.py:185][0m |           0.0033 |          50.9632 |           3.2919 |
[32m[20221214 00:13:28 @agent_ppo2.py:185][0m |          -0.0086 |          44.7192 |           3.3224 |
[32m[20221214 00:13:28 @agent_ppo2.py:185][0m |          -0.0063 |          43.0608 |           3.3149 |
[32m[20221214 00:13:28 @agent_ppo2.py:185][0m |          -0.0144 |          41.5813 |           3.3188 |
[32m[20221214 00:13:28 @agent_ppo2.py:185][0m |          -0.0259 |          40.9079 |           3.3319 |
[32m[20221214 00:13:28 @agent_ppo2.py:185][0m |          -0.0185 |          40.1691 |           3.3125 |
[32m[20221214 00:13:28 @agent_ppo2.py:185][0m |          -0.0190 |          39.8101 |           3.2901 |
[32m[20221214 00:13:28 @agent_ppo2.py:185][0m |          -0.0219 |          39.0673 |           3.3123 |
[32m[20221214 00:13:28 @agent_ppo2.py:185][0m |          -0.0258 |          38.3912 |           3.1962 |
[32m[20221214 00:13:28 @agent_ppo2.py:185][0m |          -0.0175 |          38.3680 |           3.2698 |
[32m[20221214 00:13:28 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:13:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 395.98
[32m[20221214 00:13:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 489.46
[32m[20221214 00:13:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 496.44
[32m[20221214 00:13:29 @agent_ppo2.py:143][0m Total time:      15.97 min
[32m[20221214 00:13:29 @agent_ppo2.py:145][0m 1482752 total steps have happened
[32m[20221214 00:13:29 @agent_ppo2.py:121][0m #------------------------ Iteration 4724 --------------------------#
[32m[20221214 00:13:29 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:13:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:13:29 @agent_ppo2.py:185][0m |           0.0011 |          51.8152 |           4.7925 |
[32m[20221214 00:13:29 @agent_ppo2.py:185][0m |          -0.0094 |          46.8701 |           4.8069 |
[32m[20221214 00:13:29 @agent_ppo2.py:185][0m |          -0.0067 |          44.8133 |           4.7224 |
[32m[20221214 00:13:29 @agent_ppo2.py:185][0m |          -0.0087 |          43.7273 |           4.7478 |
[32m[20221214 00:13:29 @agent_ppo2.py:185][0m |          -0.0078 |          42.8642 |           4.7108 |
[32m[20221214 00:13:29 @agent_ppo2.py:185][0m |          -0.0168 |          42.0667 |           4.6910 |
[32m[20221214 00:13:29 @agent_ppo2.py:185][0m |          -0.0144 |          42.1907 |           4.6620 |
[32m[20221214 00:13:30 @agent_ppo2.py:185][0m |          -0.0143 |          41.1025 |           4.6242 |
[32m[20221214 00:13:30 @agent_ppo2.py:185][0m |          -0.0189 |          40.6069 |           4.6182 |
[32m[20221214 00:13:30 @agent_ppo2.py:185][0m |          -0.0190 |          40.2276 |           4.6342 |
[32m[20221214 00:13:30 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:13:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 504.53
[32m[20221214 00:13:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 553.00
[32m[20221214 00:13:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 550.94
[32m[20221214 00:13:30 @agent_ppo2.py:143][0m Total time:      15.99 min
[32m[20221214 00:13:30 @agent_ppo2.py:145][0m 1484800 total steps have happened
[32m[20221214 00:13:30 @agent_ppo2.py:121][0m #------------------------ Iteration 4725 --------------------------#
[32m[20221214 00:13:30 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:13:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:13:30 @agent_ppo2.py:185][0m |           0.0086 |          79.8168 |           2.4717 |
[32m[20221214 00:13:30 @agent_ppo2.py:185][0m |          -0.0096 |          66.2899 |           2.6445 |
[32m[20221214 00:13:30 @agent_ppo2.py:185][0m |          -0.0108 |          64.5812 |           2.6697 |
[32m[20221214 00:13:30 @agent_ppo2.py:185][0m |          -0.0083 |          63.8393 |           2.6912 |
[32m[20221214 00:13:31 @agent_ppo2.py:185][0m |          -0.0164 |          62.2914 |           2.8554 |
[32m[20221214 00:13:31 @agent_ppo2.py:185][0m |          -0.0133 |          61.5852 |           2.8035 |
[32m[20221214 00:13:31 @agent_ppo2.py:185][0m |          -0.0148 |          60.9904 |           2.8709 |
[32m[20221214 00:13:31 @agent_ppo2.py:185][0m |          -0.0161 |          60.6267 |           2.9119 |
[32m[20221214 00:13:31 @agent_ppo2.py:185][0m |          -0.0163 |          60.1915 |           2.9331 |
[32m[20221214 00:13:31 @agent_ppo2.py:185][0m |          -0.0216 |          59.9186 |           2.9058 |
[32m[20221214 00:13:31 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:13:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 522.83
[32m[20221214 00:13:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 542.32
[32m[20221214 00:13:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 494.81
[32m[20221214 00:13:31 @agent_ppo2.py:143][0m Total time:      16.01 min
[32m[20221214 00:13:31 @agent_ppo2.py:145][0m 1486848 total steps have happened
[32m[20221214 00:13:31 @agent_ppo2.py:121][0m #------------------------ Iteration 4726 --------------------------#
[32m[20221214 00:13:31 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:13:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:13:31 @agent_ppo2.py:185][0m |           0.0003 |          70.7837 |           4.2812 |
[32m[20221214 00:13:32 @agent_ppo2.py:185][0m |          -0.0073 |          66.6721 |           4.2750 |
[32m[20221214 00:13:32 @agent_ppo2.py:185][0m |          -0.0098 |          65.1449 |           4.3080 |
[32m[20221214 00:13:32 @agent_ppo2.py:185][0m |          -0.0108 |          64.5371 |           4.2227 |
[32m[20221214 00:13:32 @agent_ppo2.py:185][0m |          -0.0116 |          63.9558 |           4.2227 |
[32m[20221214 00:13:32 @agent_ppo2.py:185][0m |          -0.0129 |          63.5211 |           4.2638 |
[32m[20221214 00:13:32 @agent_ppo2.py:185][0m |          -0.0183 |          63.1572 |           4.1977 |
[32m[20221214 00:13:32 @agent_ppo2.py:185][0m |          -0.0166 |          62.6865 |           4.1552 |
[32m[20221214 00:13:32 @agent_ppo2.py:185][0m |          -0.0125 |          63.6667 |           4.1501 |
[32m[20221214 00:13:32 @agent_ppo2.py:185][0m |          -0.0157 |          62.2173 |           4.1819 |
[32m[20221214 00:13:32 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:13:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 533.45
[32m[20221214 00:13:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 549.80
[32m[20221214 00:13:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 560.02
[32m[20221214 00:13:32 @agent_ppo2.py:143][0m Total time:      16.03 min
[32m[20221214 00:13:32 @agent_ppo2.py:145][0m 1488896 total steps have happened
[32m[20221214 00:13:32 @agent_ppo2.py:121][0m #------------------------ Iteration 4727 --------------------------#
[32m[20221214 00:13:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:13:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:13:33 @agent_ppo2.py:185][0m |           0.0020 |          77.7753 |           3.9750 |
[32m[20221214 00:13:33 @agent_ppo2.py:185][0m |          -0.0083 |          73.1680 |           3.8997 |
[32m[20221214 00:13:33 @agent_ppo2.py:185][0m |          -0.0118 |          71.1035 |           3.8530 |
[32m[20221214 00:13:33 @agent_ppo2.py:185][0m |          -0.0120 |          69.2221 |           3.8599 |
[32m[20221214 00:13:33 @agent_ppo2.py:185][0m |          -0.0108 |          68.3427 |           3.8187 |
[32m[20221214 00:13:33 @agent_ppo2.py:185][0m |          -0.0127 |          66.8906 |           3.7576 |
[32m[20221214 00:13:33 @agent_ppo2.py:185][0m |          -0.0143 |          66.1704 |           3.7480 |
[32m[20221214 00:13:33 @agent_ppo2.py:185][0m |          -0.0114 |          65.6180 |           3.7488 |
[32m[20221214 00:13:34 @agent_ppo2.py:185][0m |          -0.0147 |          66.1017 |           3.6916 |
[32m[20221214 00:13:34 @agent_ppo2.py:185][0m |          -0.0143 |          64.3495 |           3.6264 |
[32m[20221214 00:13:34 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:13:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 532.72
[32m[20221214 00:13:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 570.54
[32m[20221214 00:13:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 550.85
[32m[20221214 00:13:34 @agent_ppo2.py:143][0m Total time:      16.06 min
[32m[20221214 00:13:34 @agent_ppo2.py:145][0m 1490944 total steps have happened
[32m[20221214 00:13:34 @agent_ppo2.py:121][0m #------------------------ Iteration 4728 --------------------------#
[32m[20221214 00:13:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:13:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:13:34 @agent_ppo2.py:185][0m |          -0.0036 |          76.8604 |           3.3763 |
[32m[20221214 00:13:34 @agent_ppo2.py:185][0m |          -0.0059 |          57.7641 |           3.3365 |
[32m[20221214 00:13:34 @agent_ppo2.py:185][0m |          -0.0128 |          55.0489 |           3.3272 |
[32m[20221214 00:13:34 @agent_ppo2.py:185][0m |          -0.0115 |          53.7392 |           3.2698 |
[32m[20221214 00:13:34 @agent_ppo2.py:185][0m |          -0.0150 |          52.4998 |           3.3052 |
[32m[20221214 00:13:35 @agent_ppo2.py:185][0m |          -0.0109 |          52.2065 |           3.2612 |
[32m[20221214 00:13:35 @agent_ppo2.py:185][0m |          -0.0161 |          52.2712 |           3.2196 |
[32m[20221214 00:13:35 @agent_ppo2.py:185][0m |          -0.0142 |          50.8829 |           3.1632 |
[32m[20221214 00:13:35 @agent_ppo2.py:185][0m |          -0.0197 |          50.4568 |           3.1010 |
[32m[20221214 00:13:35 @agent_ppo2.py:185][0m |          -0.0189 |          50.1456 |           3.0828 |
[32m[20221214 00:13:35 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:13:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 404.02
[32m[20221214 00:13:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 462.84
[32m[20221214 00:13:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 574.04
[32m[20221214 00:13:35 @agent_ppo2.py:143][0m Total time:      16.08 min
[32m[20221214 00:13:35 @agent_ppo2.py:145][0m 1492992 total steps have happened
[32m[20221214 00:13:35 @agent_ppo2.py:121][0m #------------------------ Iteration 4729 --------------------------#
[32m[20221214 00:13:35 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:13:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:13:35 @agent_ppo2.py:185][0m |          -0.0009 |          72.6503 |           2.9523 |
[32m[20221214 00:13:35 @agent_ppo2.py:185][0m |          -0.0117 |          63.1241 |           3.0457 |
[32m[20221214 00:13:36 @agent_ppo2.py:185][0m |          -0.0081 |          61.1737 |           3.0708 |
[32m[20221214 00:13:36 @agent_ppo2.py:185][0m |          -0.0133 |          60.2167 |           3.0902 |
[32m[20221214 00:13:36 @agent_ppo2.py:185][0m |          -0.0141 |          59.3454 |           3.1224 |
[32m[20221214 00:13:36 @agent_ppo2.py:185][0m |          -0.0124 |          58.7922 |           3.1291 |
[32m[20221214 00:13:36 @agent_ppo2.py:185][0m |          -0.0139 |          58.3244 |           3.1433 |
[32m[20221214 00:13:36 @agent_ppo2.py:185][0m |          -0.0216 |          58.0380 |           3.0988 |
[32m[20221214 00:13:36 @agent_ppo2.py:185][0m |          -0.0185 |          57.6067 |           3.1759 |
[32m[20221214 00:13:36 @agent_ppo2.py:185][0m |          -0.0200 |          57.4301 |           3.1348 |
[32m[20221214 00:13:36 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:13:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 420.42
[32m[20221214 00:13:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 491.16
[32m[20221214 00:13:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 411.08
[32m[20221214 00:13:36 @agent_ppo2.py:143][0m Total time:      16.10 min
[32m[20221214 00:13:36 @agent_ppo2.py:145][0m 1495040 total steps have happened
[32m[20221214 00:13:36 @agent_ppo2.py:121][0m #------------------------ Iteration 4730 --------------------------#
[32m[20221214 00:13:36 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:13:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:13:37 @agent_ppo2.py:185][0m |           0.0065 |          78.8345 |           2.6118 |
[32m[20221214 00:13:37 @agent_ppo2.py:185][0m |          -0.0034 |          70.5075 |           2.6468 |
[32m[20221214 00:13:37 @agent_ppo2.py:185][0m |          -0.0074 |          68.6803 |           2.6279 |
[32m[20221214 00:13:37 @agent_ppo2.py:185][0m |          -0.0114 |          67.5371 |           2.6067 |
[32m[20221214 00:13:37 @agent_ppo2.py:185][0m |          -0.0056 |          69.3962 |           2.5730 |
[32m[20221214 00:13:37 @agent_ppo2.py:185][0m |          -0.0136 |          66.4350 |           2.6064 |
[32m[20221214 00:13:37 @agent_ppo2.py:185][0m |          -0.0032 |          71.6158 |           2.5325 |
[32m[20221214 00:13:37 @agent_ppo2.py:185][0m |          -0.0158 |          65.4268 |           2.5254 |
[32m[20221214 00:13:37 @agent_ppo2.py:185][0m |          -0.0189 |          65.2672 |           2.5230 |
[32m[20221214 00:13:37 @agent_ppo2.py:185][0m |          -0.0193 |          64.7339 |           2.4750 |
[32m[20221214 00:13:37 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:13:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 417.66
[32m[20221214 00:13:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 521.25
[32m[20221214 00:13:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 488.10
[32m[20221214 00:13:38 @agent_ppo2.py:143][0m Total time:      16.12 min
[32m[20221214 00:13:38 @agent_ppo2.py:145][0m 1497088 total steps have happened
[32m[20221214 00:13:38 @agent_ppo2.py:121][0m #------------------------ Iteration 4731 --------------------------#
[32m[20221214 00:13:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:13:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:13:38 @agent_ppo2.py:185][0m |           0.0002 |          71.7449 |           2.0752 |
[32m[20221214 00:13:38 @agent_ppo2.py:185][0m |          -0.0053 |          69.9548 |           2.1145 |
[32m[20221214 00:13:38 @agent_ppo2.py:185][0m |          -0.0068 |          69.0831 |           2.0775 |
[32m[20221214 00:13:38 @agent_ppo2.py:185][0m |          -0.0049 |          68.6754 |           2.1568 |
[32m[20221214 00:13:38 @agent_ppo2.py:185][0m |          -0.0064 |          69.1331 |           2.1692 |
[32m[20221214 00:13:38 @agent_ppo2.py:185][0m |          -0.0107 |          68.0813 |           2.1749 |
[32m[20221214 00:13:38 @agent_ppo2.py:185][0m |          -0.0132 |          67.9438 |           2.1515 |
[32m[20221214 00:13:39 @agent_ppo2.py:185][0m |          -0.0094 |          68.5898 |           2.0862 |
[32m[20221214 00:13:39 @agent_ppo2.py:185][0m |          -0.0154 |          67.5578 |           2.1465 |
[32m[20221214 00:13:39 @agent_ppo2.py:185][0m |          -0.0127 |          67.3148 |           2.2180 |
[32m[20221214 00:13:39 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:13:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 531.86
[32m[20221214 00:13:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 573.01
[32m[20221214 00:13:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 465.78
[32m[20221214 00:13:39 @agent_ppo2.py:143][0m Total time:      16.14 min
[32m[20221214 00:13:39 @agent_ppo2.py:145][0m 1499136 total steps have happened
[32m[20221214 00:13:39 @agent_ppo2.py:121][0m #------------------------ Iteration 4732 --------------------------#
[32m[20221214 00:13:39 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:13:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:13:39 @agent_ppo2.py:185][0m |          -0.0026 |          78.8647 |           2.4312 |
[32m[20221214 00:13:39 @agent_ppo2.py:185][0m |          -0.0052 |          77.0679 |           2.4605 |
[32m[20221214 00:13:39 @agent_ppo2.py:185][0m |          -0.0083 |          76.5959 |           2.5046 |
[32m[20221214 00:13:39 @agent_ppo2.py:185][0m |          -0.0092 |          76.2344 |           2.5856 |
[32m[20221214 00:13:40 @agent_ppo2.py:185][0m |          -0.0093 |          75.9012 |           2.5367 |
[32m[20221214 00:13:40 @agent_ppo2.py:185][0m |          -0.0103 |          75.8529 |           2.4842 |
[32m[20221214 00:13:40 @agent_ppo2.py:185][0m |          -0.0092 |          75.9631 |           2.4694 |
[32m[20221214 00:13:40 @agent_ppo2.py:185][0m |          -0.0110 |          75.4210 |           2.5156 |
[32m[20221214 00:13:40 @agent_ppo2.py:185][0m |          -0.0124 |          75.3207 |           2.4697 |
[32m[20221214 00:13:40 @agent_ppo2.py:185][0m |          -0.0109 |          75.2306 |           2.4711 |
[32m[20221214 00:13:40 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:13:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 515.55
[32m[20221214 00:13:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 529.55
[32m[20221214 00:13:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 514.12
[32m[20221214 00:13:40 @agent_ppo2.py:143][0m Total time:      16.16 min
[32m[20221214 00:13:40 @agent_ppo2.py:145][0m 1501184 total steps have happened
[32m[20221214 00:13:40 @agent_ppo2.py:121][0m #------------------------ Iteration 4733 --------------------------#
[32m[20221214 00:13:40 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:13:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:13:40 @agent_ppo2.py:185][0m |           0.0088 |          69.1372 |           2.0449 |
[32m[20221214 00:13:41 @agent_ppo2.py:185][0m |          -0.0065 |          61.9845 |           2.0781 |
[32m[20221214 00:13:41 @agent_ppo2.py:185][0m |          -0.0094 |          60.8370 |           2.2287 |
[32m[20221214 00:13:41 @agent_ppo2.py:185][0m |          -0.0075 |          61.7241 |           2.1868 |
[32m[20221214 00:13:41 @agent_ppo2.py:185][0m |          -0.0135 |          59.1613 |           2.2702 |
[32m[20221214 00:13:41 @agent_ppo2.py:185][0m |          -0.0130 |          58.5089 |           2.2102 |
[32m[20221214 00:13:41 @agent_ppo2.py:185][0m |          -0.0055 |          59.6796 |           2.2329 |
[32m[20221214 00:13:41 @agent_ppo2.py:185][0m |          -0.0162 |          57.7954 |           2.2007 |
[32m[20221214 00:13:41 @agent_ppo2.py:185][0m |          -0.0153 |          57.3757 |           2.2762 |
[32m[20221214 00:13:41 @agent_ppo2.py:185][0m |          -0.0171 |          56.9939 |           2.2275 |
[32m[20221214 00:13:41 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:13:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 478.94
[32m[20221214 00:13:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 525.37
[32m[20221214 00:13:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 456.64
[32m[20221214 00:13:41 @agent_ppo2.py:143][0m Total time:      16.18 min
[32m[20221214 00:13:41 @agent_ppo2.py:145][0m 1503232 total steps have happened
[32m[20221214 00:13:41 @agent_ppo2.py:121][0m #------------------------ Iteration 4734 --------------------------#
[32m[20221214 00:13:42 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:13:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:13:42 @agent_ppo2.py:185][0m |          -0.0014 |          63.7242 |           2.3406 |
[32m[20221214 00:13:42 @agent_ppo2.py:185][0m |          -0.0097 |          59.4145 |           2.2881 |
[32m[20221214 00:13:42 @agent_ppo2.py:185][0m |          -0.0169 |          58.2481 |           2.2006 |
[32m[20221214 00:13:42 @agent_ppo2.py:185][0m |          -0.0152 |          56.8427 |           2.2275 |
[32m[20221214 00:13:42 @agent_ppo2.py:185][0m |          -0.0147 |          57.7263 |           2.2095 |
[32m[20221214 00:13:42 @agent_ppo2.py:185][0m |          -0.0254 |          55.8366 |           2.1441 |
[32m[20221214 00:13:42 @agent_ppo2.py:185][0m |          -0.0170 |          55.3015 |           2.1652 |
[32m[20221214 00:13:42 @agent_ppo2.py:185][0m |          -0.0195 |          54.5399 |           2.1387 |
[32m[20221214 00:13:42 @agent_ppo2.py:185][0m |          -0.0197 |          54.1734 |           2.2060 |
[32m[20221214 00:13:43 @agent_ppo2.py:185][0m |          -0.0191 |          54.9204 |           2.0775 |
[32m[20221214 00:13:43 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:13:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 312.63
[32m[20221214 00:13:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 504.31
[32m[20221214 00:13:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 560.81
[32m[20221214 00:13:43 @agent_ppo2.py:143][0m Total time:      16.21 min
[32m[20221214 00:13:43 @agent_ppo2.py:145][0m 1505280 total steps have happened
[32m[20221214 00:13:43 @agent_ppo2.py:121][0m #------------------------ Iteration 4735 --------------------------#
[32m[20221214 00:13:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:13:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:13:43 @agent_ppo2.py:185][0m |           0.0066 |          68.2133 |           1.8367 |
[32m[20221214 00:13:43 @agent_ppo2.py:185][0m |           0.0009 |          66.0334 |           1.8286 |
[32m[20221214 00:13:43 @agent_ppo2.py:185][0m |          -0.0107 |          61.4363 |           1.7383 |
[32m[20221214 00:13:43 @agent_ppo2.py:185][0m |          -0.0142 |          60.3675 |           1.7196 |
[32m[20221214 00:13:43 @agent_ppo2.py:185][0m |          -0.0174 |          59.4233 |           1.7359 |
[32m[20221214 00:13:44 @agent_ppo2.py:185][0m |          -0.0164 |          59.1794 |           1.7422 |
[32m[20221214 00:13:44 @agent_ppo2.py:185][0m |          -0.0178 |          58.8735 |           1.6373 |
[32m[20221214 00:13:44 @agent_ppo2.py:185][0m |          -0.0179 |          58.1365 |           1.5744 |
[32m[20221214 00:13:44 @agent_ppo2.py:185][0m |          -0.0143 |          61.3711 |           1.5383 |
[32m[20221214 00:13:44 @agent_ppo2.py:185][0m |          -0.0215 |          57.7871 |           1.5428 |
[32m[20221214 00:13:44 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:13:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 448.23
[32m[20221214 00:13:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 506.96
[32m[20221214 00:13:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 513.45
[32m[20221214 00:13:44 @agent_ppo2.py:143][0m Total time:      16.23 min
[32m[20221214 00:13:44 @agent_ppo2.py:145][0m 1507328 total steps have happened
[32m[20221214 00:13:44 @agent_ppo2.py:121][0m #------------------------ Iteration 4736 --------------------------#
[32m[20221214 00:13:44 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:13:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:13:44 @agent_ppo2.py:185][0m |          -0.0007 |          61.9328 |           1.0813 |
[32m[20221214 00:13:44 @agent_ppo2.py:185][0m |          -0.0015 |          56.2996 |           1.2276 |
[32m[20221214 00:13:45 @agent_ppo2.py:185][0m |           0.0015 |          54.9944 |           1.2574 |
[32m[20221214 00:13:45 @agent_ppo2.py:185][0m |          -0.0119 |          52.7348 |           1.1480 |
[32m[20221214 00:13:45 @agent_ppo2.py:185][0m |          -0.0139 |          50.5513 |           1.1537 |
[32m[20221214 00:13:45 @agent_ppo2.py:185][0m |          -0.0108 |          49.4665 |           1.1583 |
[32m[20221214 00:13:45 @agent_ppo2.py:185][0m |          -0.0078 |          51.2029 |           1.0891 |
[32m[20221214 00:13:45 @agent_ppo2.py:185][0m |          -0.0168 |          48.4708 |           1.0931 |
[32m[20221214 00:13:45 @agent_ppo2.py:185][0m |          -0.0135 |          47.6546 |           1.1213 |
[32m[20221214 00:13:45 @agent_ppo2.py:185][0m |          -0.0191 |          47.0626 |           1.1055 |
[32m[20221214 00:13:45 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:13:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 398.66
[32m[20221214 00:13:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 473.83
[32m[20221214 00:13:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 562.77
[32m[20221214 00:13:45 @agent_ppo2.py:143][0m Total time:      16.25 min
[32m[20221214 00:13:45 @agent_ppo2.py:145][0m 1509376 total steps have happened
[32m[20221214 00:13:45 @agent_ppo2.py:121][0m #------------------------ Iteration 4737 --------------------------#
[32m[20221214 00:13:46 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:13:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:13:46 @agent_ppo2.py:185][0m |          -0.0020 |          85.3245 |           2.0540 |
[32m[20221214 00:13:46 @agent_ppo2.py:185][0m |          -0.0070 |          82.9597 |           2.1000 |
[32m[20221214 00:13:46 @agent_ppo2.py:185][0m |          -0.0100 |          82.2174 |           2.1073 |
[32m[20221214 00:13:46 @agent_ppo2.py:185][0m |          -0.0122 |          81.7855 |           2.1718 |
[32m[20221214 00:13:46 @agent_ppo2.py:185][0m |          -0.0115 |          81.5236 |           2.1494 |
[32m[20221214 00:13:46 @agent_ppo2.py:185][0m |          -0.0155 |          81.1388 |           2.2302 |
[32m[20221214 00:13:46 @agent_ppo2.py:185][0m |          -0.0119 |          81.1389 |           2.2056 |
[32m[20221214 00:13:46 @agent_ppo2.py:185][0m |          -0.0139 |          80.7003 |           2.1643 |
[32m[20221214 00:13:46 @agent_ppo2.py:185][0m |          -0.0179 |          80.4264 |           2.2316 |
[32m[20221214 00:13:47 @agent_ppo2.py:185][0m |          -0.0119 |          81.0060 |           2.1988 |
[32m[20221214 00:13:47 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:13:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 499.84
[32m[20221214 00:13:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 522.84
[32m[20221214 00:13:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 423.74
[32m[20221214 00:13:47 @agent_ppo2.py:143][0m Total time:      16.27 min
[32m[20221214 00:13:47 @agent_ppo2.py:145][0m 1511424 total steps have happened
[32m[20221214 00:13:47 @agent_ppo2.py:121][0m #------------------------ Iteration 4738 --------------------------#
[32m[20221214 00:13:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:13:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:13:47 @agent_ppo2.py:185][0m |           0.0021 |          65.9996 |           1.7601 |
[32m[20221214 00:13:47 @agent_ppo2.py:185][0m |          -0.0071 |          59.6810 |           1.8487 |
[32m[20221214 00:13:47 @agent_ppo2.py:185][0m |          -0.0098 |          57.5385 |           1.7819 |
[32m[20221214 00:13:47 @agent_ppo2.py:185][0m |          -0.0104 |          56.6107 |           1.8407 |
[32m[20221214 00:13:47 @agent_ppo2.py:185][0m |          -0.0185 |          56.3451 |           1.7857 |
[32m[20221214 00:13:48 @agent_ppo2.py:185][0m |          -0.0154 |          55.3325 |           1.7288 |
[32m[20221214 00:13:48 @agent_ppo2.py:185][0m |          -0.0176 |          54.8079 |           1.6969 |
[32m[20221214 00:13:48 @agent_ppo2.py:185][0m |          -0.0164 |          54.0348 |           1.6844 |
[32m[20221214 00:13:48 @agent_ppo2.py:185][0m |          -0.0159 |          54.2311 |           1.7115 |
[32m[20221214 00:13:48 @agent_ppo2.py:185][0m |           0.0059 |          67.9699 |           1.7020 |
[32m[20221214 00:13:48 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:13:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 380.03
[32m[20221214 00:13:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 483.72
[32m[20221214 00:13:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 558.03
[32m[20221214 00:13:48 @agent_ppo2.py:143][0m Total time:      16.30 min
[32m[20221214 00:13:48 @agent_ppo2.py:145][0m 1513472 total steps have happened
[32m[20221214 00:13:48 @agent_ppo2.py:121][0m #------------------------ Iteration 4739 --------------------------#
[32m[20221214 00:13:48 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:13:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:13:48 @agent_ppo2.py:185][0m |          -0.0010 |          71.7537 |           0.0208 |
[32m[20221214 00:13:49 @agent_ppo2.py:185][0m |          -0.0064 |          68.7239 |           0.0775 |
[32m[20221214 00:13:49 @agent_ppo2.py:185][0m |          -0.0089 |          67.5112 |           0.1095 |
[32m[20221214 00:13:49 @agent_ppo2.py:185][0m |          -0.0115 |          66.4457 |           0.0232 |
[32m[20221214 00:13:49 @agent_ppo2.py:185][0m |          -0.0083 |          66.1885 |           0.0436 |
[32m[20221214 00:13:49 @agent_ppo2.py:185][0m |          -0.0099 |          65.3103 |           0.0610 |
[32m[20221214 00:13:49 @agent_ppo2.py:185][0m |          -0.0121 |          64.8126 |           0.0193 |
[32m[20221214 00:13:49 @agent_ppo2.py:185][0m |          -0.0148 |          64.3089 |          -0.0329 |
[32m[20221214 00:13:49 @agent_ppo2.py:185][0m |          -0.0135 |          63.8786 |          -0.0339 |
[32m[20221214 00:13:49 @agent_ppo2.py:185][0m |          -0.0127 |          63.6290 |           0.0052 |
[32m[20221214 00:13:49 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:13:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 544.55
[32m[20221214 00:13:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 567.02
[32m[20221214 00:13:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 539.39
[32m[20221214 00:13:49 @agent_ppo2.py:143][0m Total time:      16.32 min
[32m[20221214 00:13:49 @agent_ppo2.py:145][0m 1515520 total steps have happened
[32m[20221214 00:13:49 @agent_ppo2.py:121][0m #------------------------ Iteration 4740 --------------------------#
[32m[20221214 00:13:50 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:13:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:13:50 @agent_ppo2.py:185][0m |          -0.0003 |          66.7390 |           1.5465 |
[32m[20221214 00:13:50 @agent_ppo2.py:185][0m |          -0.0081 |          61.9701 |           1.5121 |
[32m[20221214 00:13:50 @agent_ppo2.py:185][0m |          -0.0105 |          60.4723 |           1.5627 |
[32m[20221214 00:13:50 @agent_ppo2.py:185][0m |          -0.0102 |          59.0685 |           1.5877 |
[32m[20221214 00:13:50 @agent_ppo2.py:185][0m |          -0.0168 |          58.2477 |           1.5846 |
[32m[20221214 00:13:50 @agent_ppo2.py:185][0m |          -0.0144 |          57.5201 |           1.6062 |
[32m[20221214 00:13:50 @agent_ppo2.py:185][0m |          -0.0158 |          57.1134 |           1.5807 |
[32m[20221214 00:13:50 @agent_ppo2.py:185][0m |          -0.0161 |          57.1900 |           1.6048 |
[32m[20221214 00:13:51 @agent_ppo2.py:185][0m |          -0.0188 |          56.3761 |           1.6188 |
[32m[20221214 00:13:51 @agent_ppo2.py:185][0m |          -0.0194 |          56.2559 |           1.4932 |
[32m[20221214 00:13:51 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:13:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 425.11
[32m[20221214 00:13:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 524.82
[32m[20221214 00:13:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 561.78
[32m[20221214 00:13:51 @agent_ppo2.py:143][0m Total time:      16.34 min
[32m[20221214 00:13:51 @agent_ppo2.py:145][0m 1517568 total steps have happened
[32m[20221214 00:13:51 @agent_ppo2.py:121][0m #------------------------ Iteration 4741 --------------------------#
[32m[20221214 00:13:51 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:13:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:13:51 @agent_ppo2.py:185][0m |          -0.0036 |          66.4017 |           0.2186 |
[32m[20221214 00:13:51 @agent_ppo2.py:185][0m |          -0.0099 |          64.5166 |           0.3009 |
[32m[20221214 00:13:51 @agent_ppo2.py:185][0m |           0.0061 |          70.1037 |           0.3742 |
[32m[20221214 00:13:51 @agent_ppo2.py:185][0m |           0.0040 |          72.2240 |           0.3828 |
[32m[20221214 00:13:52 @agent_ppo2.py:185][0m |          -0.0112 |          63.7415 |           0.5575 |
[32m[20221214 00:13:52 @agent_ppo2.py:185][0m |          -0.0144 |          63.3913 |           0.3666 |
[32m[20221214 00:13:52 @agent_ppo2.py:185][0m |          -0.0129 |          63.1741 |           0.3726 |
[32m[20221214 00:13:52 @agent_ppo2.py:185][0m |          -0.0117 |          63.0629 |           0.4800 |
[32m[20221214 00:13:52 @agent_ppo2.py:185][0m |          -0.0106 |          63.1238 |           0.4810 |
[32m[20221214 00:13:52 @agent_ppo2.py:185][0m |          -0.0149 |          62.8856 |           0.4762 |
[32m[20221214 00:13:52 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:13:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 518.04
[32m[20221214 00:13:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 559.50
[32m[20221214 00:13:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 538.31
[32m[20221214 00:13:52 @agent_ppo2.py:143][0m Total time:      16.36 min
[32m[20221214 00:13:52 @agent_ppo2.py:145][0m 1519616 total steps have happened
[32m[20221214 00:13:52 @agent_ppo2.py:121][0m #------------------------ Iteration 4742 --------------------------#
[32m[20221214 00:13:52 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:13:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:13:52 @agent_ppo2.py:185][0m |          -0.0006 |          74.0325 |           0.3915 |
[32m[20221214 00:13:53 @agent_ppo2.py:185][0m |          -0.0068 |          71.8317 |           0.5691 |
[32m[20221214 00:13:53 @agent_ppo2.py:185][0m |          -0.0008 |          73.7265 |           0.4885 |
[32m[20221214 00:13:53 @agent_ppo2.py:185][0m |          -0.0114 |          70.5586 |           0.5547 |
[32m[20221214 00:13:53 @agent_ppo2.py:185][0m |          -0.0117 |          70.1435 |           0.6092 |
[32m[20221214 00:13:53 @agent_ppo2.py:185][0m |          -0.0108 |          69.9717 |           0.6496 |
[32m[20221214 00:13:53 @agent_ppo2.py:185][0m |          -0.0074 |          71.2095 |           0.6242 |
[32m[20221214 00:13:53 @agent_ppo2.py:185][0m |          -0.0122 |          69.7843 |           0.6085 |
[32m[20221214 00:13:53 @agent_ppo2.py:185][0m |          -0.0134 |          69.5182 |           0.6021 |
[32m[20221214 00:13:53 @agent_ppo2.py:185][0m |          -0.0057 |          72.5812 |           0.6693 |
[32m[20221214 00:13:53 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:13:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 536.63
[32m[20221214 00:13:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 559.78
[32m[20221214 00:13:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 279.08
[32m[20221214 00:13:53 @agent_ppo2.py:143][0m Total time:      16.38 min
[32m[20221214 00:13:53 @agent_ppo2.py:145][0m 1521664 total steps have happened
[32m[20221214 00:13:53 @agent_ppo2.py:121][0m #------------------------ Iteration 4743 --------------------------#
[32m[20221214 00:13:54 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:13:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:13:54 @agent_ppo2.py:185][0m |          -0.0009 |          76.2410 |           0.1910 |
[32m[20221214 00:13:54 @agent_ppo2.py:185][0m |           0.0000 |          73.2877 |           0.3022 |
[32m[20221214 00:13:54 @agent_ppo2.py:185][0m |          -0.0101 |          69.3701 |           0.3021 |
[32m[20221214 00:13:54 @agent_ppo2.py:185][0m |          -0.0107 |          68.4339 |           0.2955 |
[32m[20221214 00:13:54 @agent_ppo2.py:185][0m |          -0.0125 |          67.6039 |           0.2657 |
[32m[20221214 00:13:54 @agent_ppo2.py:185][0m |          -0.0123 |          67.0060 |           0.2603 |
[32m[20221214 00:13:54 @agent_ppo2.py:185][0m |          -0.0060 |          70.4383 |           0.2864 |
[32m[20221214 00:13:54 @agent_ppo2.py:185][0m |          -0.0150 |          66.0817 |           0.3440 |
[32m[20221214 00:13:54 @agent_ppo2.py:185][0m |          -0.0161 |          65.5815 |           0.3167 |
[32m[20221214 00:13:55 @agent_ppo2.py:185][0m |          -0.0185 |          65.3717 |           0.3412 |
[32m[20221214 00:13:55 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:13:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 511.39
[32m[20221214 00:13:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 542.68
[32m[20221214 00:13:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 301.77
[32m[20221214 00:13:55 @agent_ppo2.py:143][0m Total time:      16.41 min
[32m[20221214 00:13:55 @agent_ppo2.py:145][0m 1523712 total steps have happened
[32m[20221214 00:13:55 @agent_ppo2.py:121][0m #------------------------ Iteration 4744 --------------------------#
[32m[20221214 00:13:55 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:13:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:13:55 @agent_ppo2.py:185][0m |           0.0025 |          56.5539 |           0.6637 |
[32m[20221214 00:13:55 @agent_ppo2.py:185][0m |          -0.0072 |          51.1309 |           0.6105 |
[32m[20221214 00:13:55 @agent_ppo2.py:185][0m |          -0.0123 |          49.9919 |           0.5751 |
[32m[20221214 00:13:55 @agent_ppo2.py:185][0m |          -0.0123 |          49.0382 |           0.5506 |
[32m[20221214 00:13:55 @agent_ppo2.py:185][0m |          -0.0126 |          48.3470 |           0.4630 |
[32m[20221214 00:13:56 @agent_ppo2.py:185][0m |          -0.0165 |          47.9854 |           0.4999 |
[32m[20221214 00:13:56 @agent_ppo2.py:185][0m |          -0.0158 |          47.4475 |           0.3927 |
[32m[20221214 00:13:56 @agent_ppo2.py:185][0m |          -0.0177 |          47.1268 |           0.3811 |
[32m[20221214 00:13:56 @agent_ppo2.py:185][0m |          -0.0198 |          46.8687 |           0.3801 |
[32m[20221214 00:13:56 @agent_ppo2.py:185][0m |          -0.0140 |          48.1175 |           0.2823 |
[32m[20221214 00:13:56 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:13:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 550.51
[32m[20221214 00:13:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 604.13
[32m[20221214 00:13:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 419.08
[32m[20221214 00:13:56 @agent_ppo2.py:143][0m Total time:      16.43 min
[32m[20221214 00:13:56 @agent_ppo2.py:145][0m 1525760 total steps have happened
[32m[20221214 00:13:56 @agent_ppo2.py:121][0m #------------------------ Iteration 4745 --------------------------#
[32m[20221214 00:13:56 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:13:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:13:56 @agent_ppo2.py:185][0m |           0.0016 |          79.9794 |           0.3398 |
[32m[20221214 00:13:56 @agent_ppo2.py:185][0m |          -0.0049 |          76.0923 |           0.3595 |
[32m[20221214 00:13:57 @agent_ppo2.py:185][0m |          -0.0086 |          75.0941 |           0.3722 |
[32m[20221214 00:13:57 @agent_ppo2.py:185][0m |          -0.0100 |          74.4358 |           0.4728 |
[32m[20221214 00:13:57 @agent_ppo2.py:185][0m |          -0.0097 |          73.6659 |           0.3903 |
[32m[20221214 00:13:57 @agent_ppo2.py:185][0m |          -0.0103 |          73.3196 |           0.3282 |
[32m[20221214 00:13:57 @agent_ppo2.py:185][0m |          -0.0087 |          73.9379 |           0.3680 |
[32m[20221214 00:13:57 @agent_ppo2.py:185][0m |          -0.0100 |          72.9249 |           0.3698 |
[32m[20221214 00:13:57 @agent_ppo2.py:185][0m |          -0.0157 |          71.9664 |           0.3157 |
[32m[20221214 00:13:57 @agent_ppo2.py:185][0m |          -0.0130 |          71.0364 |           0.2971 |
[32m[20221214 00:13:57 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:13:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 546.04
[32m[20221214 00:13:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 571.86
[32m[20221214 00:13:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 296.07
[32m[20221214 00:13:57 @agent_ppo2.py:143][0m Total time:      16.45 min
[32m[20221214 00:13:57 @agent_ppo2.py:145][0m 1527808 total steps have happened
[32m[20221214 00:13:57 @agent_ppo2.py:121][0m #------------------------ Iteration 4746 --------------------------#
[32m[20221214 00:13:58 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:13:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:13:58 @agent_ppo2.py:185][0m |          -0.0052 |          78.0585 |           0.6515 |
[32m[20221214 00:13:58 @agent_ppo2.py:185][0m |          -0.0081 |          70.8886 |           0.7656 |
[32m[20221214 00:13:58 @agent_ppo2.py:185][0m |          -0.0056 |          68.8208 |           0.8094 |
[32m[20221214 00:13:58 @agent_ppo2.py:185][0m |          -0.0070 |          67.0000 |           1.0466 |
[32m[20221214 00:13:58 @agent_ppo2.py:185][0m |          -0.0106 |          66.4176 |           0.9619 |
[32m[20221214 00:13:58 @agent_ppo2.py:185][0m |          -0.0120 |          65.0491 |           1.0186 |
[32m[20221214 00:13:58 @agent_ppo2.py:185][0m |          -0.0124 |          64.1716 |           1.0580 |
[32m[20221214 00:13:58 @agent_ppo2.py:185][0m |          -0.0095 |          64.8809 |           1.1719 |
[32m[20221214 00:13:58 @agent_ppo2.py:185][0m |          -0.0015 |          70.8055 |           1.1562 |
[32m[20221214 00:13:59 @agent_ppo2.py:185][0m |          -0.0108 |          62.9195 |           1.3641 |
[32m[20221214 00:13:59 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:13:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 546.52
[32m[20221214 00:13:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 568.71
[32m[20221214 00:13:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 287.87
[32m[20221214 00:13:59 @agent_ppo2.py:143][0m Total time:      16.47 min
[32m[20221214 00:13:59 @agent_ppo2.py:145][0m 1529856 total steps have happened
[32m[20221214 00:13:59 @agent_ppo2.py:121][0m #------------------------ Iteration 4747 --------------------------#
[32m[20221214 00:13:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:13:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:13:59 @agent_ppo2.py:185][0m |           0.0009 |          59.8320 |           0.6590 |
[32m[20221214 00:13:59 @agent_ppo2.py:185][0m |          -0.0033 |          52.2534 |           0.5378 |
[32m[20221214 00:13:59 @agent_ppo2.py:185][0m |          -0.0148 |          50.2188 |           0.5461 |
[32m[20221214 00:13:59 @agent_ppo2.py:185][0m |          -0.0056 |          48.2108 |           0.5842 |
[32m[20221214 00:13:59 @agent_ppo2.py:185][0m |          -0.0110 |          47.6463 |           0.5727 |
[32m[20221214 00:13:59 @agent_ppo2.py:185][0m |          -0.0137 |          46.1175 |           0.5933 |
[32m[20221214 00:14:00 @agent_ppo2.py:185][0m |          -0.0125 |          46.1301 |           0.5680 |
[32m[20221214 00:14:00 @agent_ppo2.py:185][0m |          -0.0159 |          44.9872 |           0.4733 |
[32m[20221214 00:14:00 @agent_ppo2.py:185][0m |          -0.0116 |          44.4130 |           0.5112 |
[32m[20221214 00:14:00 @agent_ppo2.py:185][0m |          -0.0172 |          43.9698 |           0.5222 |
[32m[20221214 00:14:00 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:14:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 491.97
[32m[20221214 00:14:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 552.50
[32m[20221214 00:14:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 407.69
[32m[20221214 00:14:00 @agent_ppo2.py:143][0m Total time:      16.49 min
[32m[20221214 00:14:00 @agent_ppo2.py:145][0m 1531904 total steps have happened
[32m[20221214 00:14:00 @agent_ppo2.py:121][0m #------------------------ Iteration 4748 --------------------------#
[32m[20221214 00:14:00 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:14:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:14:00 @agent_ppo2.py:185][0m |           0.0023 |          44.3671 |           0.5306 |
[32m[20221214 00:14:00 @agent_ppo2.py:185][0m |          -0.0027 |          39.2584 |           0.6374 |
[32m[20221214 00:14:00 @agent_ppo2.py:185][0m |          -0.0106 |          37.2257 |           0.5728 |
[32m[20221214 00:14:01 @agent_ppo2.py:185][0m |          -0.0143 |          36.0351 |           0.6449 |
[32m[20221214 00:14:01 @agent_ppo2.py:185][0m |           0.0039 |          41.5986 |           0.6014 |
[32m[20221214 00:14:01 @agent_ppo2.py:185][0m |          -0.0132 |          34.7307 |           0.6349 |
[32m[20221214 00:14:01 @agent_ppo2.py:185][0m |          -0.0143 |          34.0036 |           0.6296 |
[32m[20221214 00:14:01 @agent_ppo2.py:185][0m |          -0.0179 |          33.4047 |           0.6258 |
[32m[20221214 00:14:01 @agent_ppo2.py:185][0m |          -0.0150 |          32.9491 |           0.6513 |
[32m[20221214 00:14:01 @agent_ppo2.py:185][0m |          -0.0155 |          32.7091 |           0.7187 |
[32m[20221214 00:14:01 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:14:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 383.05
[32m[20221214 00:14:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 495.57
[32m[20221214 00:14:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 452.02
[32m[20221214 00:14:01 @agent_ppo2.py:143][0m Total time:      16.51 min
[32m[20221214 00:14:01 @agent_ppo2.py:145][0m 1533952 total steps have happened
[32m[20221214 00:14:01 @agent_ppo2.py:121][0m #------------------------ Iteration 4749 --------------------------#
[32m[20221214 00:14:01 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:14:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:14:02 @agent_ppo2.py:185][0m |           0.0082 |          40.9806 |           0.3968 |
[32m[20221214 00:14:02 @agent_ppo2.py:185][0m |          -0.0046 |          32.9155 |           0.4236 |
[32m[20221214 00:14:02 @agent_ppo2.py:185][0m |          -0.0133 |          30.6505 |           0.3533 |
[32m[20221214 00:14:02 @agent_ppo2.py:185][0m |          -0.0118 |          29.9122 |           0.4271 |
[32m[20221214 00:14:02 @agent_ppo2.py:185][0m |          -0.0145 |          28.9913 |           0.4359 |
[32m[20221214 00:14:02 @agent_ppo2.py:185][0m |          -0.0210 |          28.4114 |           0.3866 |
[32m[20221214 00:14:02 @agent_ppo2.py:185][0m |          -0.0184 |          28.1076 |           0.3977 |
[32m[20221214 00:14:02 @agent_ppo2.py:185][0m |          -0.0213 |          27.6301 |           0.3874 |
[32m[20221214 00:14:02 @agent_ppo2.py:185][0m |          -0.0233 |          27.2824 |           0.3867 |
[32m[20221214 00:14:02 @agent_ppo2.py:185][0m |          -0.0190 |          27.0837 |           0.3828 |
[32m[20221214 00:14:02 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:14:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 350.38
[32m[20221214 00:14:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 423.34
[32m[20221214 00:14:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 421.72
[32m[20221214 00:14:03 @agent_ppo2.py:143][0m Total time:      16.54 min
[32m[20221214 00:14:03 @agent_ppo2.py:145][0m 1536000 total steps have happened
[32m[20221214 00:14:03 @agent_ppo2.py:121][0m #------------------------ Iteration 4750 --------------------------#
[32m[20221214 00:14:03 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:14:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:14:03 @agent_ppo2.py:185][0m |           0.0025 |          56.1675 |           1.0549 |
[32m[20221214 00:14:03 @agent_ppo2.py:185][0m |          -0.0071 |          52.9052 |           1.1406 |
[32m[20221214 00:14:03 @agent_ppo2.py:185][0m |          -0.0100 |          51.6102 |           1.1277 |
[32m[20221214 00:14:03 @agent_ppo2.py:185][0m |          -0.0103 |          50.6300 |           1.0785 |
[32m[20221214 00:14:03 @agent_ppo2.py:185][0m |          -0.0140 |          50.1933 |           1.0745 |
[32m[20221214 00:14:03 @agent_ppo2.py:185][0m |          -0.0154 |          49.7789 |           1.0415 |
[32m[20221214 00:14:04 @agent_ppo2.py:185][0m |          -0.0168 |          49.2637 |           1.0181 |
[32m[20221214 00:14:04 @agent_ppo2.py:185][0m |          -0.0168 |          49.0132 |           0.9949 |
[32m[20221214 00:14:04 @agent_ppo2.py:185][0m |          -0.0174 |          48.5482 |           1.0032 |
[32m[20221214 00:14:04 @agent_ppo2.py:185][0m |          -0.0187 |          48.4682 |           1.0752 |
[32m[20221214 00:14:04 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:14:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 451.98
[32m[20221214 00:14:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 535.08
[32m[20221214 00:14:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 569.07
[32m[20221214 00:14:04 @agent_ppo2.py:143][0m Total time:      16.56 min
[32m[20221214 00:14:04 @agent_ppo2.py:145][0m 1538048 total steps have happened
[32m[20221214 00:14:04 @agent_ppo2.py:121][0m #------------------------ Iteration 4751 --------------------------#
[32m[20221214 00:14:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:14:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:14:04 @agent_ppo2.py:185][0m |           0.0140 |          81.7976 |           0.7257 |
[32m[20221214 00:14:04 @agent_ppo2.py:185][0m |          -0.0018 |          73.9279 |           0.6989 |
[32m[20221214 00:14:04 @agent_ppo2.py:185][0m |          -0.0106 |          72.2015 |           0.7010 |
[32m[20221214 00:14:05 @agent_ppo2.py:185][0m |          -0.0073 |          71.3197 |           0.6218 |
[32m[20221214 00:14:05 @agent_ppo2.py:185][0m |          -0.0166 |          70.4594 |           0.6292 |
[32m[20221214 00:14:05 @agent_ppo2.py:185][0m |          -0.0160 |          70.2081 |           0.6158 |
[32m[20221214 00:14:05 @agent_ppo2.py:185][0m |          -0.0117 |          69.4419 |           0.5742 |
[32m[20221214 00:14:05 @agent_ppo2.py:185][0m |          -0.0150 |          69.2496 |           0.5923 |
[32m[20221214 00:14:05 @agent_ppo2.py:185][0m |          -0.0103 |          69.2763 |           0.6528 |
[32m[20221214 00:14:05 @agent_ppo2.py:185][0m |          -0.0149 |          68.4639 |           0.7410 |
[32m[20221214 00:14:05 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:14:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 551.14
[32m[20221214 00:14:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 564.18
[32m[20221214 00:14:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 389.14
[32m[20221214 00:14:05 @agent_ppo2.py:143][0m Total time:      16.58 min
[32m[20221214 00:14:05 @agent_ppo2.py:145][0m 1540096 total steps have happened
[32m[20221214 00:14:05 @agent_ppo2.py:121][0m #------------------------ Iteration 4752 --------------------------#
[32m[20221214 00:14:05 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:14:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:14:06 @agent_ppo2.py:185][0m |          -0.0008 |          41.3851 |           0.8048 |
[32m[20221214 00:14:06 @agent_ppo2.py:185][0m |          -0.0072 |          36.8133 |           0.7755 |
[32m[20221214 00:14:06 @agent_ppo2.py:185][0m |          -0.0085 |          34.5845 |           0.8022 |
[32m[20221214 00:14:06 @agent_ppo2.py:185][0m |          -0.0150 |          33.3390 |           0.8407 |
[32m[20221214 00:14:06 @agent_ppo2.py:185][0m |          -0.0224 |          33.1034 |           0.8301 |
[32m[20221214 00:14:06 @agent_ppo2.py:185][0m |          -0.0183 |          31.9382 |           0.8067 |
[32m[20221214 00:14:06 @agent_ppo2.py:185][0m |          -0.0185 |          31.1193 |           0.8375 |
[32m[20221214 00:14:06 @agent_ppo2.py:185][0m |          -0.0191 |          30.4372 |           0.8389 |
[32m[20221214 00:14:06 @agent_ppo2.py:185][0m |          -0.0189 |          29.8487 |           0.8994 |
[32m[20221214 00:14:06 @agent_ppo2.py:185][0m |          -0.0159 |          29.3556 |           0.8505 |
[32m[20221214 00:14:06 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:14:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 390.30
[32m[20221214 00:14:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 483.75
[32m[20221214 00:14:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 516.29
[32m[20221214 00:14:07 @agent_ppo2.py:143][0m Total time:      16.60 min
[32m[20221214 00:14:07 @agent_ppo2.py:145][0m 1542144 total steps have happened
[32m[20221214 00:14:07 @agent_ppo2.py:121][0m #------------------------ Iteration 4753 --------------------------#
[32m[20221214 00:14:07 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:14:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:14:07 @agent_ppo2.py:185][0m |           0.0119 |          56.0215 |           1.0105 |
[32m[20221214 00:14:07 @agent_ppo2.py:185][0m |          -0.0061 |          47.7584 |           1.0013 |
[32m[20221214 00:14:07 @agent_ppo2.py:185][0m |          -0.0109 |          45.4208 |           0.9836 |
[32m[20221214 00:14:07 @agent_ppo2.py:185][0m |          -0.0050 |          43.8430 |           0.9224 |
[32m[20221214 00:14:07 @agent_ppo2.py:185][0m |          -0.0153 |          42.8677 |           0.9594 |
[32m[20221214 00:14:07 @agent_ppo2.py:185][0m |          -0.0082 |          42.2540 |           0.8808 |
[32m[20221214 00:14:07 @agent_ppo2.py:185][0m |          -0.0043 |          41.8104 |           0.9758 |
[32m[20221214 00:14:07 @agent_ppo2.py:185][0m |          -0.0184 |          40.1362 |           0.9013 |
[32m[20221214 00:14:08 @agent_ppo2.py:185][0m |          -0.0154 |          39.2183 |           0.8402 |
[32m[20221214 00:14:08 @agent_ppo2.py:185][0m |          -0.0210 |          38.3238 |           0.7919 |
[32m[20221214 00:14:08 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:14:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 432.49
[32m[20221214 00:14:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 578.31
[32m[20221214 00:14:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 493.97
[32m[20221214 00:14:08 @agent_ppo2.py:143][0m Total time:      16.62 min
[32m[20221214 00:14:08 @agent_ppo2.py:145][0m 1544192 total steps have happened
[32m[20221214 00:14:08 @agent_ppo2.py:121][0m #------------------------ Iteration 4754 --------------------------#
[32m[20221214 00:14:08 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:14:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:14:08 @agent_ppo2.py:185][0m |           0.0076 |          59.9704 |          -0.8346 |
[32m[20221214 00:14:08 @agent_ppo2.py:185][0m |          -0.0052 |          50.3571 |          -0.6929 |
[32m[20221214 00:14:08 @agent_ppo2.py:185][0m |          -0.0099 |          47.5742 |          -0.5884 |
[32m[20221214 00:14:08 @agent_ppo2.py:185][0m |          -0.0173 |          46.1560 |          -0.5942 |
[32m[20221214 00:14:09 @agent_ppo2.py:185][0m |          -0.0163 |          45.5866 |          -0.5611 |
[32m[20221214 00:14:09 @agent_ppo2.py:185][0m |          -0.0152 |          44.7329 |          -0.5193 |
[32m[20221214 00:14:09 @agent_ppo2.py:185][0m |          -0.0189 |          44.1864 |          -0.4391 |
[32m[20221214 00:14:09 @agent_ppo2.py:185][0m |          -0.0203 |          43.8536 |          -0.4162 |
[32m[20221214 00:14:09 @agent_ppo2.py:185][0m |          -0.0181 |          43.6157 |          -0.3729 |
[32m[20221214 00:14:09 @agent_ppo2.py:185][0m |          -0.0202 |          43.3207 |          -0.3816 |
[32m[20221214 00:14:09 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:14:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 443.34
[32m[20221214 00:14:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 545.03
[32m[20221214 00:14:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 529.00
[32m[20221214 00:14:09 @agent_ppo2.py:143][0m Total time:      16.65 min
[32m[20221214 00:14:09 @agent_ppo2.py:145][0m 1546240 total steps have happened
[32m[20221214 00:14:09 @agent_ppo2.py:121][0m #------------------------ Iteration 4755 --------------------------#
[32m[20221214 00:14:09 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:14:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:14:09 @agent_ppo2.py:185][0m |          -0.0022 |          77.7368 |           1.2976 |
[32m[20221214 00:14:10 @agent_ppo2.py:185][0m |          -0.0061 |          73.5304 |           1.3908 |
[32m[20221214 00:14:10 @agent_ppo2.py:185][0m |          -0.0118 |          71.7880 |           1.3996 |
[32m[20221214 00:14:10 @agent_ppo2.py:185][0m |          -0.0139 |          71.0187 |           1.3765 |
[32m[20221214 00:14:10 @agent_ppo2.py:185][0m |          -0.0177 |          70.3937 |           1.5214 |
[32m[20221214 00:14:10 @agent_ppo2.py:185][0m |          -0.0196 |          69.6945 |           1.4576 |
[32m[20221214 00:14:10 @agent_ppo2.py:185][0m |          -0.0192 |          69.1646 |           1.5564 |
[32m[20221214 00:14:10 @agent_ppo2.py:185][0m |          -0.0189 |          68.6295 |           1.5725 |
[32m[20221214 00:14:10 @agent_ppo2.py:185][0m |          -0.0187 |          68.5087 |           1.5335 |
[32m[20221214 00:14:10 @agent_ppo2.py:185][0m |          -0.0206 |          68.0108 |           1.5424 |
[32m[20221214 00:14:10 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:14:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 458.81
[32m[20221214 00:14:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 496.05
[32m[20221214 00:14:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 432.58
[32m[20221214 00:14:10 @agent_ppo2.py:143][0m Total time:      16.67 min
[32m[20221214 00:14:10 @agent_ppo2.py:145][0m 1548288 total steps have happened
[32m[20221214 00:14:10 @agent_ppo2.py:121][0m #------------------------ Iteration 4756 --------------------------#
[32m[20221214 00:14:11 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:14:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:14:11 @agent_ppo2.py:185][0m |           0.0005 |          42.5308 |           0.1092 |
[32m[20221214 00:14:11 @agent_ppo2.py:185][0m |          -0.0066 |          37.0916 |           0.2698 |
[32m[20221214 00:14:11 @agent_ppo2.py:185][0m |          -0.0083 |          35.4673 |           0.3270 |
[32m[20221214 00:14:11 @agent_ppo2.py:185][0m |          -0.0136 |          34.3649 |           0.3486 |
[32m[20221214 00:14:11 @agent_ppo2.py:185][0m |          -0.0097 |          33.5173 |           0.3940 |
[32m[20221214 00:14:11 @agent_ppo2.py:185][0m |          -0.0134 |          32.8825 |           0.3795 |
[32m[20221214 00:14:11 @agent_ppo2.py:185][0m |          -0.0112 |          32.2624 |           0.4701 |
[32m[20221214 00:14:11 @agent_ppo2.py:185][0m |          -0.0121 |          32.0494 |           0.4683 |
[32m[20221214 00:14:11 @agent_ppo2.py:185][0m |          -0.0130 |          31.6533 |           0.4570 |
[32m[20221214 00:14:12 @agent_ppo2.py:185][0m |          -0.0191 |          31.3272 |           0.4365 |
[32m[20221214 00:14:12 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:14:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 446.72
[32m[20221214 00:14:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 555.14
[32m[20221214 00:14:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 622.10
[32m[20221214 00:14:12 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 622.10
[32m[20221214 00:14:12 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 622.10
[32m[20221214 00:14:12 @agent_ppo2.py:143][0m Total time:      16.69 min
[32m[20221214 00:14:12 @agent_ppo2.py:145][0m 1550336 total steps have happened
[32m[20221214 00:14:12 @agent_ppo2.py:121][0m #------------------------ Iteration 4757 --------------------------#
[32m[20221214 00:14:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:14:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:14:12 @agent_ppo2.py:185][0m |           0.0062 |          84.4880 |           1.2602 |
[32m[20221214 00:14:12 @agent_ppo2.py:185][0m |          -0.0019 |          76.0228 |           1.2960 |
[32m[20221214 00:14:12 @agent_ppo2.py:185][0m |          -0.0050 |          72.7908 |           1.4068 |
[32m[20221214 00:14:12 @agent_ppo2.py:185][0m |          -0.0062 |          71.1892 |           1.3589 |
[32m[20221214 00:14:12 @agent_ppo2.py:185][0m |          -0.0116 |          69.0114 |           1.4946 |
[32m[20221214 00:14:12 @agent_ppo2.py:185][0m |          -0.0105 |          67.8636 |           1.4965 |
[32m[20221214 00:14:13 @agent_ppo2.py:185][0m |          -0.0141 |          66.1079 |           1.5352 |
[32m[20221214 00:14:13 @agent_ppo2.py:185][0m |          -0.0163 |          65.4575 |           1.5953 |
[32m[20221214 00:14:13 @agent_ppo2.py:185][0m |          -0.0148 |          64.8293 |           1.6192 |
[32m[20221214 00:14:13 @agent_ppo2.py:185][0m |          -0.0155 |          64.7124 |           1.5921 |
[32m[20221214 00:14:13 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:14:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 473.16
[32m[20221214 00:14:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 622.59
[32m[20221214 00:14:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 558.61
[32m[20221214 00:14:13 @agent_ppo2.py:143][0m Total time:      16.71 min
[32m[20221214 00:14:13 @agent_ppo2.py:145][0m 1552384 total steps have happened
[32m[20221214 00:14:13 @agent_ppo2.py:121][0m #------------------------ Iteration 4758 --------------------------#
[32m[20221214 00:14:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:14:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:14:13 @agent_ppo2.py:185][0m |          -0.0034 |          76.0314 |           2.3630 |
[32m[20221214 00:14:13 @agent_ppo2.py:185][0m |          -0.0029 |          65.9659 |           2.2932 |
[32m[20221214 00:14:13 @agent_ppo2.py:185][0m |          -0.0050 |          62.5298 |           2.4050 |
[32m[20221214 00:14:14 @agent_ppo2.py:185][0m |          -0.0056 |          61.2836 |           2.2339 |
[32m[20221214 00:14:14 @agent_ppo2.py:185][0m |          -0.0132 |          59.7860 |           2.1734 |
[32m[20221214 00:14:14 @agent_ppo2.py:185][0m |          -0.0074 |          68.8615 |           2.2332 |
[32m[20221214 00:14:14 @agent_ppo2.py:185][0m |          -0.0135 |          59.5603 |           2.2566 |
[32m[20221214 00:14:14 @agent_ppo2.py:185][0m |          -0.0204 |          57.9447 |           2.2334 |
[32m[20221214 00:14:14 @agent_ppo2.py:185][0m |          -0.0156 |          57.3292 |           2.2274 |
[32m[20221214 00:14:14 @agent_ppo2.py:185][0m |          -0.0205 |          56.9276 |           2.1758 |
[32m[20221214 00:14:14 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:14:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 472.69
[32m[20221214 00:14:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 519.69
[32m[20221214 00:14:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 378.33
[32m[20221214 00:14:14 @agent_ppo2.py:143][0m Total time:      16.73 min
[32m[20221214 00:14:14 @agent_ppo2.py:145][0m 1554432 total steps have happened
[32m[20221214 00:14:14 @agent_ppo2.py:121][0m #------------------------ Iteration 4759 --------------------------#
[32m[20221214 00:14:14 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:14:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:14:15 @agent_ppo2.py:185][0m |           0.0003 |          71.1190 |           1.3117 |
[32m[20221214 00:14:15 @agent_ppo2.py:185][0m |          -0.0068 |          62.5361 |           1.2372 |
[32m[20221214 00:14:15 @agent_ppo2.py:185][0m |          -0.0073 |          60.3893 |           1.2026 |
[32m[20221214 00:14:15 @agent_ppo2.py:185][0m |          -0.0132 |          58.8762 |           1.2260 |
[32m[20221214 00:14:15 @agent_ppo2.py:185][0m |          -0.0137 |          57.7432 |           1.1535 |
[32m[20221214 00:14:15 @agent_ppo2.py:185][0m |          -0.0093 |          56.9622 |           1.1020 |
[32m[20221214 00:14:15 @agent_ppo2.py:185][0m |          -0.0107 |          56.7712 |           1.1347 |
[32m[20221214 00:14:15 @agent_ppo2.py:185][0m |          -0.0104 |          55.6719 |           1.1402 |
[32m[20221214 00:14:15 @agent_ppo2.py:185][0m |          -0.0167 |          55.0562 |           1.0924 |
[32m[20221214 00:14:15 @agent_ppo2.py:185][0m |          -0.0180 |          54.3651 |           1.0556 |
[32m[20221214 00:14:15 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:14:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 446.01
[32m[20221214 00:14:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 558.44
[32m[20221214 00:14:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 487.13
[32m[20221214 00:14:15 @agent_ppo2.py:143][0m Total time:      16.75 min
[32m[20221214 00:14:15 @agent_ppo2.py:145][0m 1556480 total steps have happened
[32m[20221214 00:14:15 @agent_ppo2.py:121][0m #------------------------ Iteration 4760 --------------------------#
[32m[20221214 00:14:16 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:14:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:14:16 @agent_ppo2.py:185][0m |           0.0005 |          88.1766 |           1.0316 |
[32m[20221214 00:14:16 @agent_ppo2.py:185][0m |          -0.0045 |          83.3715 |           1.0706 |
[32m[20221214 00:14:16 @agent_ppo2.py:185][0m |          -0.0075 |          81.8676 |           1.0868 |
[32m[20221214 00:14:16 @agent_ppo2.py:185][0m |          -0.0009 |          88.2462 |           1.0803 |
[32m[20221214 00:14:16 @agent_ppo2.py:185][0m |          -0.0100 |          80.2494 |           1.1944 |
[32m[20221214 00:14:16 @agent_ppo2.py:185][0m |          -0.0095 |          79.7658 |           1.0747 |
[32m[20221214 00:14:16 @agent_ppo2.py:185][0m |          -0.0126 |          78.8073 |           1.1256 |
[32m[20221214 00:14:16 @agent_ppo2.py:185][0m |          -0.0151 |          78.1761 |           1.1043 |
[32m[20221214 00:14:17 @agent_ppo2.py:185][0m |          -0.0096 |          78.5904 |           1.1178 |
[32m[20221214 00:14:17 @agent_ppo2.py:185][0m |          -0.0180 |          77.3277 |           1.1423 |
[32m[20221214 00:14:17 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:14:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 554.42
[32m[20221214 00:14:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 579.27
[32m[20221214 00:14:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 393.78
[32m[20221214 00:14:17 @agent_ppo2.py:143][0m Total time:      16.77 min
[32m[20221214 00:14:17 @agent_ppo2.py:145][0m 1558528 total steps have happened
[32m[20221214 00:14:17 @agent_ppo2.py:121][0m #------------------------ Iteration 4761 --------------------------#
[32m[20221214 00:14:17 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:14:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:14:17 @agent_ppo2.py:185][0m |           0.0045 |          47.9718 |           0.5950 |
[32m[20221214 00:14:17 @agent_ppo2.py:185][0m |          -0.0021 |          40.7206 |           0.5015 |
[32m[20221214 00:14:17 @agent_ppo2.py:185][0m |           0.0011 |          41.4579 |           0.5449 |
[32m[20221214 00:14:17 @agent_ppo2.py:185][0m |          -0.0141 |          37.9900 |           0.4537 |
[32m[20221214 00:14:17 @agent_ppo2.py:185][0m |          -0.0133 |          37.0493 |           0.4497 |
[32m[20221214 00:14:18 @agent_ppo2.py:185][0m |          -0.0138 |          36.4485 |           0.4133 |
[32m[20221214 00:14:18 @agent_ppo2.py:185][0m |          -0.0132 |          36.1154 |           0.3301 |
[32m[20221214 00:14:18 @agent_ppo2.py:185][0m |          -0.0082 |          36.9060 |           0.3095 |
[32m[20221214 00:14:18 @agent_ppo2.py:185][0m |          -0.0132 |          35.5664 |           0.2369 |
[32m[20221214 00:14:18 @agent_ppo2.py:185][0m |          -0.0192 |          35.1522 |           0.2485 |
[32m[20221214 00:14:18 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:14:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 473.74
[32m[20221214 00:14:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 594.25
[32m[20221214 00:14:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 604.36
[32m[20221214 00:14:18 @agent_ppo2.py:143][0m Total time:      16.79 min
[32m[20221214 00:14:18 @agent_ppo2.py:145][0m 1560576 total steps have happened
[32m[20221214 00:14:18 @agent_ppo2.py:121][0m #------------------------ Iteration 4762 --------------------------#
[32m[20221214 00:14:18 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:14:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:14:18 @agent_ppo2.py:185][0m |           0.0017 |          63.3137 |           1.0801 |
[32m[20221214 00:14:18 @agent_ppo2.py:185][0m |          -0.0037 |          54.6828 |           1.0888 |
[32m[20221214 00:14:19 @agent_ppo2.py:185][0m |          -0.0031 |          50.5106 |           1.1321 |
[32m[20221214 00:14:19 @agent_ppo2.py:185][0m |           0.0080 |          55.3667 |           1.1141 |
[32m[20221214 00:14:19 @agent_ppo2.py:185][0m |          -0.0088 |          46.8907 |           1.2460 |
[32m[20221214 00:14:19 @agent_ppo2.py:185][0m |          -0.0075 |          45.8975 |           1.1672 |
[32m[20221214 00:14:19 @agent_ppo2.py:185][0m |          -0.0113 |          44.6694 |           1.1685 |
[32m[20221214 00:14:19 @agent_ppo2.py:185][0m |          -0.0124 |          43.8111 |           1.1457 |
[32m[20221214 00:14:19 @agent_ppo2.py:185][0m |          -0.0128 |          43.4399 |           1.1344 |
[32m[20221214 00:14:19 @agent_ppo2.py:185][0m |          -0.0123 |          42.9551 |           1.0788 |
[32m[20221214 00:14:19 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:14:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 536.86
[32m[20221214 00:14:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 586.41
[32m[20221214 00:14:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 467.43
[32m[20221214 00:14:19 @agent_ppo2.py:143][0m Total time:      16.81 min
[32m[20221214 00:14:19 @agent_ppo2.py:145][0m 1562624 total steps have happened
[32m[20221214 00:14:19 @agent_ppo2.py:121][0m #------------------------ Iteration 4763 --------------------------#
[32m[20221214 00:14:19 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:14:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:14:20 @agent_ppo2.py:185][0m |           0.0095 |          87.3847 |           0.7149 |
[32m[20221214 00:14:20 @agent_ppo2.py:185][0m |          -0.0031 |          77.6718 |           0.9386 |
[32m[20221214 00:14:20 @agent_ppo2.py:185][0m |           0.0024 |          76.9494 |           0.9206 |
[32m[20221214 00:14:20 @agent_ppo2.py:185][0m |          -0.0054 |          74.2084 |           0.8981 |
[32m[20221214 00:14:20 @agent_ppo2.py:185][0m |          -0.0063 |          72.7235 |           0.8719 |
[32m[20221214 00:14:20 @agent_ppo2.py:185][0m |          -0.0087 |          72.0287 |           0.8542 |
[32m[20221214 00:14:20 @agent_ppo2.py:185][0m |           0.0005 |          74.2762 |           0.8559 |
[32m[20221214 00:14:20 @agent_ppo2.py:185][0m |          -0.0124 |          70.8372 |           0.8499 |
[32m[20221214 00:14:20 @agent_ppo2.py:185][0m |          -0.0140 |          70.1785 |           0.8389 |
[32m[20221214 00:14:20 @agent_ppo2.py:185][0m |          -0.0037 |          74.0683 |           0.9127 |
[32m[20221214 00:14:20 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:14:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 506.97
[32m[20221214 00:14:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 568.33
[32m[20221214 00:14:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 349.42
[32m[20221214 00:14:21 @agent_ppo2.py:143][0m Total time:      16.84 min
[32m[20221214 00:14:21 @agent_ppo2.py:145][0m 1564672 total steps have happened
[32m[20221214 00:14:21 @agent_ppo2.py:121][0m #------------------------ Iteration 4764 --------------------------#
[32m[20221214 00:14:21 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:14:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:14:21 @agent_ppo2.py:185][0m |           0.0033 |          86.3137 |           0.6992 |
[32m[20221214 00:14:21 @agent_ppo2.py:185][0m |          -0.0055 |          74.8494 |           0.8240 |
[32m[20221214 00:14:21 @agent_ppo2.py:185][0m |          -0.0015 |          78.2999 |           0.8966 |
[32m[20221214 00:14:21 @agent_ppo2.py:185][0m |          -0.0121 |          71.9195 |           0.8983 |
[32m[20221214 00:14:21 @agent_ppo2.py:185][0m |          -0.0133 |          70.7416 |           0.9953 |
[32m[20221214 00:14:21 @agent_ppo2.py:185][0m |          -0.0140 |          70.0918 |           1.0352 |
[32m[20221214 00:14:21 @agent_ppo2.py:185][0m |          -0.0125 |          69.5759 |           1.0744 |
[32m[20221214 00:14:22 @agent_ppo2.py:185][0m |          -0.0176 |          68.9349 |           1.0319 |
[32m[20221214 00:14:22 @agent_ppo2.py:185][0m |          -0.0150 |          68.7104 |           1.1304 |
[32m[20221214 00:14:22 @agent_ppo2.py:185][0m |          -0.0178 |          68.1480 |           1.0610 |
[32m[20221214 00:14:22 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:14:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 457.35
[32m[20221214 00:14:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 490.78
[32m[20221214 00:14:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 572.08
[32m[20221214 00:14:22 @agent_ppo2.py:143][0m Total time:      16.86 min
[32m[20221214 00:14:22 @agent_ppo2.py:145][0m 1566720 total steps have happened
[32m[20221214 00:14:22 @agent_ppo2.py:121][0m #------------------------ Iteration 4765 --------------------------#
[32m[20221214 00:14:22 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 00:14:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:14:22 @agent_ppo2.py:185][0m |           0.0020 |          65.6259 |           0.4785 |
[32m[20221214 00:14:22 @agent_ppo2.py:185][0m |          -0.0060 |          57.4098 |           0.5475 |
[32m[20221214 00:14:22 @agent_ppo2.py:185][0m |          -0.0119 |          55.7261 |           0.6633 |
[32m[20221214 00:14:23 @agent_ppo2.py:185][0m |          -0.0133 |          54.3004 |           0.7367 |
[32m[20221214 00:14:23 @agent_ppo2.py:185][0m |          -0.0115 |          53.3049 |           0.7455 |
[32m[20221214 00:14:23 @agent_ppo2.py:185][0m |          -0.0099 |          52.8142 |           0.7067 |
[32m[20221214 00:14:23 @agent_ppo2.py:185][0m |          -0.0107 |          52.7981 |           0.8239 |
[32m[20221214 00:14:23 @agent_ppo2.py:185][0m |          -0.0195 |          51.7195 |           0.8672 |
[32m[20221214 00:14:23 @agent_ppo2.py:185][0m |          -0.0143 |          55.2053 |           0.8136 |
[32m[20221214 00:14:23 @agent_ppo2.py:185][0m |          -0.0158 |          51.3723 |           0.7862 |
[32m[20221214 00:14:23 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:14:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 486.48
[32m[20221214 00:14:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 520.13
[32m[20221214 00:14:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 281.45
[32m[20221214 00:14:23 @agent_ppo2.py:143][0m Total time:      16.88 min
[32m[20221214 00:14:23 @agent_ppo2.py:145][0m 1568768 total steps have happened
[32m[20221214 00:14:23 @agent_ppo2.py:121][0m #------------------------ Iteration 4766 --------------------------#
[32m[20221214 00:14:23 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:14:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:14:24 @agent_ppo2.py:185][0m |          -0.0015 |          79.6485 |           1.3493 |
[32m[20221214 00:14:24 @agent_ppo2.py:185][0m |          -0.0069 |          75.2090 |           1.4259 |
[32m[20221214 00:14:24 @agent_ppo2.py:185][0m |          -0.0110 |          73.2113 |           1.5014 |
[32m[20221214 00:14:24 @agent_ppo2.py:185][0m |          -0.0110 |          72.7059 |           1.4675 |
[32m[20221214 00:14:24 @agent_ppo2.py:185][0m |          -0.0132 |          72.0389 |           1.5378 |
[32m[20221214 00:14:24 @agent_ppo2.py:185][0m |          -0.0152 |          71.2336 |           1.5888 |
[32m[20221214 00:14:24 @agent_ppo2.py:185][0m |          -0.0158 |          71.5006 |           1.5772 |
[32m[20221214 00:14:24 @agent_ppo2.py:185][0m |          -0.0194 |          70.8547 |           1.6248 |
[32m[20221214 00:14:24 @agent_ppo2.py:185][0m |          -0.0173 |          70.3647 |           1.6440 |
[32m[20221214 00:14:24 @agent_ppo2.py:185][0m |          -0.0189 |          69.7217 |           1.6989 |
[32m[20221214 00:14:24 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:14:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 489.96
[32m[20221214 00:14:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 557.74
[32m[20221214 00:14:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 564.36
[32m[20221214 00:14:25 @agent_ppo2.py:143][0m Total time:      16.90 min
[32m[20221214 00:14:25 @agent_ppo2.py:145][0m 1570816 total steps have happened
[32m[20221214 00:14:25 @agent_ppo2.py:121][0m #------------------------ Iteration 4767 --------------------------#
[32m[20221214 00:14:25 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:14:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:14:25 @agent_ppo2.py:185][0m |          -0.0006 |          46.1384 |           1.6632 |
[32m[20221214 00:14:25 @agent_ppo2.py:185][0m |          -0.0055 |          38.6956 |           1.6698 |
[32m[20221214 00:14:25 @agent_ppo2.py:185][0m |          -0.0076 |          36.1541 |           1.6676 |
[32m[20221214 00:14:25 @agent_ppo2.py:185][0m |          -0.0127 |          34.6927 |           1.6777 |
[32m[20221214 00:14:25 @agent_ppo2.py:185][0m |          -0.0165 |          33.9036 |           1.5214 |
[32m[20221214 00:14:25 @agent_ppo2.py:185][0m |          -0.0158 |          32.9509 |           1.5766 |
[32m[20221214 00:14:25 @agent_ppo2.py:185][0m |          -0.0171 |          32.0965 |           1.4278 |
[32m[20221214 00:14:26 @agent_ppo2.py:185][0m |          -0.0220 |          31.2859 |           1.5100 |
[32m[20221214 00:14:26 @agent_ppo2.py:185][0m |          -0.0142 |          30.9310 |           1.5905 |
[32m[20221214 00:14:26 @agent_ppo2.py:185][0m |          -0.0174 |          30.3181 |           1.5627 |
[32m[20221214 00:14:26 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:14:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 380.16
[32m[20221214 00:14:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 514.17
[32m[20221214 00:14:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 584.91
[32m[20221214 00:14:26 @agent_ppo2.py:143][0m Total time:      16.92 min
[32m[20221214 00:14:26 @agent_ppo2.py:145][0m 1572864 total steps have happened
[32m[20221214 00:14:26 @agent_ppo2.py:121][0m #------------------------ Iteration 4768 --------------------------#
[32m[20221214 00:14:26 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:14:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:14:26 @agent_ppo2.py:185][0m |           0.0026 |          94.3706 |           1.7453 |
[32m[20221214 00:14:26 @agent_ppo2.py:185][0m |          -0.0059 |          84.0665 |           1.7670 |
[32m[20221214 00:14:26 @agent_ppo2.py:185][0m |          -0.0120 |          79.8257 |           1.7248 |
[32m[20221214 00:14:26 @agent_ppo2.py:185][0m |          -0.0115 |          76.9032 |           1.6455 |
[32m[20221214 00:14:27 @agent_ppo2.py:185][0m |          -0.0036 |          77.5049 |           1.7532 |
[32m[20221214 00:14:27 @agent_ppo2.py:185][0m |          -0.0078 |          74.0526 |           1.7725 |
[32m[20221214 00:14:27 @agent_ppo2.py:185][0m |          -0.0097 |          72.7033 |           1.7609 |
[32m[20221214 00:14:27 @agent_ppo2.py:185][0m |          -0.0128 |          71.7285 |           1.7713 |
[32m[20221214 00:14:27 @agent_ppo2.py:185][0m |          -0.0126 |          71.0257 |           1.8176 |
[32m[20221214 00:14:27 @agent_ppo2.py:185][0m |          -0.0142 |          70.3978 |           1.8083 |
[32m[20221214 00:14:27 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:14:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 533.69
[32m[20221214 00:14:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 546.28
[32m[20221214 00:14:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 515.25
[32m[20221214 00:14:27 @agent_ppo2.py:143][0m Total time:      16.95 min
[32m[20221214 00:14:27 @agent_ppo2.py:145][0m 1574912 total steps have happened
[32m[20221214 00:14:27 @agent_ppo2.py:121][0m #------------------------ Iteration 4769 --------------------------#
[32m[20221214 00:14:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:14:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:14:27 @agent_ppo2.py:185][0m |          -0.0027 |          50.6980 |           1.5326 |
[32m[20221214 00:14:28 @agent_ppo2.py:185][0m |          -0.0046 |          33.9879 |           1.5170 |
[32m[20221214 00:14:28 @agent_ppo2.py:185][0m |          -0.0016 |          33.1254 |           1.4030 |
[32m[20221214 00:14:28 @agent_ppo2.py:185][0m |          -0.0052 |          31.0434 |           1.3341 |
[32m[20221214 00:14:28 @agent_ppo2.py:185][0m |          -0.0143 |          29.0737 |           1.3691 |
[32m[20221214 00:14:28 @agent_ppo2.py:185][0m |          -0.0141 |          28.3043 |           1.3057 |
[32m[20221214 00:14:28 @agent_ppo2.py:185][0m |          -0.0174 |          27.7219 |           1.2553 |
[32m[20221214 00:14:28 @agent_ppo2.py:185][0m |          -0.0184 |          27.2938 |           1.2248 |
[32m[20221214 00:14:28 @agent_ppo2.py:185][0m |          -0.0198 |          27.0185 |           1.1817 |
[32m[20221214 00:14:28 @agent_ppo2.py:185][0m |          -0.0112 |          28.2623 |           1.1208 |
[32m[20221214 00:14:28 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:14:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 431.11
[32m[20221214 00:14:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 492.04
[32m[20221214 00:14:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 536.97
[32m[20221214 00:14:28 @agent_ppo2.py:143][0m Total time:      16.97 min
[32m[20221214 00:14:28 @agent_ppo2.py:145][0m 1576960 total steps have happened
[32m[20221214 00:14:28 @agent_ppo2.py:121][0m #------------------------ Iteration 4770 --------------------------#
[32m[20221214 00:14:29 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:14:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:14:29 @agent_ppo2.py:185][0m |           0.0025 |          83.5964 |           1.0339 |
[32m[20221214 00:14:29 @agent_ppo2.py:185][0m |          -0.0068 |          75.9556 |           1.0115 |
[32m[20221214 00:14:29 @agent_ppo2.py:185][0m |          -0.0099 |          73.3347 |           0.9856 |
[32m[20221214 00:14:29 @agent_ppo2.py:185][0m |          -0.0205 |          71.7073 |           0.9688 |
[32m[20221214 00:14:29 @agent_ppo2.py:185][0m |          -0.0148 |          70.0713 |           1.0222 |
[32m[20221214 00:14:29 @agent_ppo2.py:185][0m |          -0.0142 |          69.0052 |           1.0863 |
[32m[20221214 00:14:29 @agent_ppo2.py:185][0m |          -0.0136 |          68.7029 |           1.0077 |
[32m[20221214 00:14:29 @agent_ppo2.py:185][0m |          -0.0186 |          66.9558 |           1.0407 |
[32m[20221214 00:14:29 @agent_ppo2.py:185][0m |          -0.0171 |          65.9234 |           1.1252 |
[32m[20221214 00:14:30 @agent_ppo2.py:185][0m |          -0.0204 |          65.2012 |           1.1489 |
[32m[20221214 00:14:30 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:14:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 449.75
[32m[20221214 00:14:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 499.17
[32m[20221214 00:14:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 332.95
[32m[20221214 00:14:30 @agent_ppo2.py:143][0m Total time:      16.99 min
[32m[20221214 00:14:30 @agent_ppo2.py:145][0m 1579008 total steps have happened
[32m[20221214 00:14:30 @agent_ppo2.py:121][0m #------------------------ Iteration 4771 --------------------------#
[32m[20221214 00:14:30 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:14:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:14:30 @agent_ppo2.py:185][0m |          -0.0009 |          50.1031 |           1.0540 |
[32m[20221214 00:14:30 @agent_ppo2.py:185][0m |          -0.0023 |          43.0458 |           1.1679 |
[32m[20221214 00:14:30 @agent_ppo2.py:185][0m |          -0.0041 |          41.1866 |           1.0635 |
[32m[20221214 00:14:30 @agent_ppo2.py:185][0m |          -0.0061 |          40.9057 |           1.1707 |
[32m[20221214 00:14:30 @agent_ppo2.py:185][0m |          -0.0156 |          39.8830 |           1.1116 |
[32m[20221214 00:14:30 @agent_ppo2.py:185][0m |          -0.0102 |          39.6411 |           1.1566 |
[32m[20221214 00:14:31 @agent_ppo2.py:185][0m |          -0.0124 |          38.9671 |           1.0798 |
[32m[20221214 00:14:31 @agent_ppo2.py:185][0m |          -0.0159 |          38.7516 |           1.1082 |
[32m[20221214 00:14:31 @agent_ppo2.py:185][0m |          -0.0142 |          38.4120 |           1.1545 |
[32m[20221214 00:14:31 @agent_ppo2.py:185][0m |          -0.0129 |          38.2439 |           1.0636 |
[32m[20221214 00:14:31 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:14:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 470.11
[32m[20221214 00:14:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 554.93
[32m[20221214 00:14:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 283.93
[32m[20221214 00:14:31 @agent_ppo2.py:143][0m Total time:      17.01 min
[32m[20221214 00:14:31 @agent_ppo2.py:145][0m 1581056 total steps have happened
[32m[20221214 00:14:31 @agent_ppo2.py:121][0m #------------------------ Iteration 4772 --------------------------#
[32m[20221214 00:14:31 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:14:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:14:31 @agent_ppo2.py:185][0m |           0.0106 |          94.7721 |           1.5165 |
[32m[20221214 00:14:31 @agent_ppo2.py:185][0m |          -0.0010 |          85.5356 |           1.5882 |
[32m[20221214 00:14:31 @agent_ppo2.py:185][0m |          -0.0054 |          84.7403 |           1.5392 |
[32m[20221214 00:14:32 @agent_ppo2.py:185][0m |          -0.0053 |          83.6097 |           1.4396 |
[32m[20221214 00:14:32 @agent_ppo2.py:185][0m |          -0.0071 |          82.9675 |           1.5071 |
[32m[20221214 00:14:32 @agent_ppo2.py:185][0m |          -0.0068 |          82.7166 |           1.4692 |
[32m[20221214 00:14:32 @agent_ppo2.py:185][0m |          -0.0084 |          82.2580 |           1.5164 |
[32m[20221214 00:14:32 @agent_ppo2.py:185][0m |          -0.0079 |          81.8060 |           1.4391 |
[32m[20221214 00:14:32 @agent_ppo2.py:185][0m |          -0.0078 |          81.2792 |           1.4669 |
[32m[20221214 00:14:32 @agent_ppo2.py:185][0m |          -0.0094 |          80.8521 |           1.4029 |
[32m[20221214 00:14:32 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:14:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 537.08
[32m[20221214 00:14:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 545.95
[32m[20221214 00:14:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 344.86
[32m[20221214 00:14:32 @agent_ppo2.py:143][0m Total time:      17.03 min
[32m[20221214 00:14:32 @agent_ppo2.py:145][0m 1583104 total steps have happened
[32m[20221214 00:14:32 @agent_ppo2.py:121][0m #------------------------ Iteration 4773 --------------------------#
[32m[20221214 00:14:32 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:14:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:14:33 @agent_ppo2.py:185][0m |           0.0169 |          60.4942 |           0.3340 |
[32m[20221214 00:14:33 @agent_ppo2.py:185][0m |          -0.0040 |          49.3660 |           0.4081 |
[32m[20221214 00:14:33 @agent_ppo2.py:185][0m |          -0.0038 |          47.2244 |           0.5097 |
[32m[20221214 00:14:33 @agent_ppo2.py:185][0m |          -0.0076 |          46.3792 |           0.4146 |
[32m[20221214 00:14:33 @agent_ppo2.py:185][0m |          -0.0016 |          45.9765 |           0.4786 |
[32m[20221214 00:14:33 @agent_ppo2.py:185][0m |          -0.0018 |          46.9477 |           0.5414 |
[32m[20221214 00:14:33 @agent_ppo2.py:185][0m |          -0.0113 |          44.5410 |           0.5457 |
[32m[20221214 00:14:33 @agent_ppo2.py:185][0m |          -0.0128 |          44.4278 |           0.5886 |
[32m[20221214 00:14:33 @agent_ppo2.py:185][0m |          -0.0133 |          44.0721 |           0.6337 |
[32m[20221214 00:14:33 @agent_ppo2.py:185][0m |          -0.0060 |          48.1780 |           0.6157 |
[32m[20221214 00:14:33 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:14:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 453.86
[32m[20221214 00:14:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 505.29
[32m[20221214 00:14:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 556.03
[32m[20221214 00:14:33 @agent_ppo2.py:143][0m Total time:      17.05 min
[32m[20221214 00:14:33 @agent_ppo2.py:145][0m 1585152 total steps have happened
[32m[20221214 00:14:33 @agent_ppo2.py:121][0m #------------------------ Iteration 4774 --------------------------#
[32m[20221214 00:14:34 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:14:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:14:34 @agent_ppo2.py:185][0m |           0.0007 |          62.7834 |           0.2981 |
[32m[20221214 00:14:34 @agent_ppo2.py:185][0m |          -0.0062 |          58.4257 |           0.3525 |
[32m[20221214 00:14:34 @agent_ppo2.py:185][0m |          -0.0054 |          57.5189 |           0.3527 |
[32m[20221214 00:14:34 @agent_ppo2.py:185][0m |          -0.0093 |          56.6203 |           0.3688 |
[32m[20221214 00:14:34 @agent_ppo2.py:185][0m |          -0.0078 |          58.3100 |           0.3654 |
[32m[20221214 00:14:34 @agent_ppo2.py:185][0m |          -0.0154 |          55.7247 |           0.3414 |
[32m[20221214 00:14:34 @agent_ppo2.py:185][0m |          -0.0076 |          57.6865 |           0.3740 |
[32m[20221214 00:14:34 @agent_ppo2.py:185][0m |          -0.0095 |          55.4765 |           0.3498 |
[32m[20221214 00:14:34 @agent_ppo2.py:185][0m |          -0.0107 |          54.7086 |           0.3071 |
[32m[20221214 00:14:35 @agent_ppo2.py:185][0m |          -0.0156 |          54.4588 |           0.3279 |
[32m[20221214 00:14:35 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:14:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 443.73
[32m[20221214 00:14:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 460.99
[32m[20221214 00:14:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 562.03
[32m[20221214 00:14:35 @agent_ppo2.py:143][0m Total time:      17.07 min
[32m[20221214 00:14:35 @agent_ppo2.py:145][0m 1587200 total steps have happened
[32m[20221214 00:14:35 @agent_ppo2.py:121][0m #------------------------ Iteration 4775 --------------------------#
[32m[20221214 00:14:35 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:14:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:14:35 @agent_ppo2.py:185][0m |          -0.0002 |          85.6925 |           0.9592 |
[32m[20221214 00:14:35 @agent_ppo2.py:185][0m |           0.0027 |          87.4870 |           0.9123 |
[32m[20221214 00:14:35 @agent_ppo2.py:185][0m |          -0.0021 |          85.8074 |           1.0123 |
[32m[20221214 00:14:35 @agent_ppo2.py:185][0m |          -0.0018 |          86.8310 |           1.0574 |
[32m[20221214 00:14:35 @agent_ppo2.py:185][0m |          -0.0047 |          83.2178 |           1.0146 |
[32m[20221214 00:14:35 @agent_ppo2.py:185][0m |          -0.0146 |          78.0747 |           0.9772 |
[32m[20221214 00:14:36 @agent_ppo2.py:185][0m |          -0.0159 |          77.6767 |           1.0223 |
[32m[20221214 00:14:36 @agent_ppo2.py:185][0m |          -0.0168 |          77.7330 |           1.0174 |
[32m[20221214 00:14:36 @agent_ppo2.py:185][0m |          -0.0184 |          76.9648 |           1.0990 |
[32m[20221214 00:14:36 @agent_ppo2.py:185][0m |          -0.0184 |          76.7309 |           1.1123 |
[32m[20221214 00:14:36 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:14:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 511.94
[32m[20221214 00:14:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 542.40
[32m[20221214 00:14:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 506.50
[32m[20221214 00:14:36 @agent_ppo2.py:143][0m Total time:      17.09 min
[32m[20221214 00:14:36 @agent_ppo2.py:145][0m 1589248 total steps have happened
[32m[20221214 00:14:36 @agent_ppo2.py:121][0m #------------------------ Iteration 4776 --------------------------#
[32m[20221214 00:14:36 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:14:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:14:36 @agent_ppo2.py:185][0m |           0.0157 |          87.6029 |           0.5783 |
[32m[20221214 00:14:36 @agent_ppo2.py:185][0m |          -0.0043 |          76.6313 |           0.5448 |
[32m[20221214 00:14:36 @agent_ppo2.py:185][0m |          -0.0087 |          75.6812 |           0.5381 |
[32m[20221214 00:14:37 @agent_ppo2.py:185][0m |          -0.0113 |          75.2786 |           0.4604 |
[32m[20221214 00:14:37 @agent_ppo2.py:185][0m |          -0.0110 |          74.9614 |           0.5543 |
[32m[20221214 00:14:37 @agent_ppo2.py:185][0m |          -0.0029 |          80.9032 |           0.5642 |
[32m[20221214 00:14:37 @agent_ppo2.py:185][0m |           0.0068 |          83.5879 |           0.5499 |
[32m[20221214 00:14:37 @agent_ppo2.py:185][0m |          -0.0145 |          74.9580 |           0.4887 |
[32m[20221214 00:14:37 @agent_ppo2.py:185][0m |          -0.0161 |          74.1975 |           0.5821 |
[32m[20221214 00:14:37 @agent_ppo2.py:185][0m |          -0.0154 |          73.9506 |           0.5719 |
[32m[20221214 00:14:37 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:14:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 521.31
[32m[20221214 00:14:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 539.77
[32m[20221214 00:14:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 535.38
[32m[20221214 00:14:37 @agent_ppo2.py:143][0m Total time:      17.11 min
[32m[20221214 00:14:37 @agent_ppo2.py:145][0m 1591296 total steps have happened
[32m[20221214 00:14:37 @agent_ppo2.py:121][0m #------------------------ Iteration 4777 --------------------------#
[32m[20221214 00:14:37 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:14:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:14:38 @agent_ppo2.py:185][0m |           0.0010 |          77.9231 |           0.7004 |
[32m[20221214 00:14:38 @agent_ppo2.py:185][0m |          -0.0049 |          74.4362 |           0.9597 |
[32m[20221214 00:14:38 @agent_ppo2.py:185][0m |          -0.0101 |          73.4640 |           0.8536 |
[32m[20221214 00:14:38 @agent_ppo2.py:185][0m |          -0.0133 |          72.7029 |           0.9025 |
[32m[20221214 00:14:38 @agent_ppo2.py:185][0m |          -0.0132 |          72.4576 |           0.9100 |
[32m[20221214 00:14:38 @agent_ppo2.py:185][0m |          -0.0167 |          72.2396 |           0.7466 |
[32m[20221214 00:14:38 @agent_ppo2.py:185][0m |          -0.0041 |          76.8769 |           0.8480 |
[32m[20221214 00:14:38 @agent_ppo2.py:185][0m |          -0.0168 |          71.6314 |           0.8758 |
[32m[20221214 00:14:38 @agent_ppo2.py:185][0m |          -0.0194 |          70.9706 |           0.9097 |
[32m[20221214 00:14:38 @agent_ppo2.py:185][0m |          -0.0128 |          71.2643 |           0.8866 |
[32m[20221214 00:14:38 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:14:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 471.96
[32m[20221214 00:14:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 500.44
[32m[20221214 00:14:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 522.93
[32m[20221214 00:14:39 @agent_ppo2.py:143][0m Total time:      17.14 min
[32m[20221214 00:14:39 @agent_ppo2.py:145][0m 1593344 total steps have happened
[32m[20221214 00:14:39 @agent_ppo2.py:121][0m #------------------------ Iteration 4778 --------------------------#
[32m[20221214 00:14:39 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:14:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:14:39 @agent_ppo2.py:185][0m |           0.0020 |          72.3730 |           1.0271 |
[32m[20221214 00:14:39 @agent_ppo2.py:185][0m |          -0.0069 |          67.1953 |           1.0481 |
[32m[20221214 00:14:39 @agent_ppo2.py:185][0m |          -0.0103 |          65.4487 |           0.9113 |
[32m[20221214 00:14:39 @agent_ppo2.py:185][0m |          -0.0094 |          64.6100 |           0.8984 |
[32m[20221214 00:14:39 @agent_ppo2.py:185][0m |          -0.0104 |          64.9126 |           0.9326 |
[32m[20221214 00:14:39 @agent_ppo2.py:185][0m |          -0.0140 |          63.4029 |           0.9653 |
[32m[20221214 00:14:39 @agent_ppo2.py:185][0m |          -0.0135 |          63.1511 |           0.9618 |
[32m[20221214 00:14:40 @agent_ppo2.py:185][0m |          -0.0152 |          62.3378 |           0.9173 |
[32m[20221214 00:14:40 @agent_ppo2.py:185][0m |          -0.0149 |          62.0331 |           0.8669 |
[32m[20221214 00:14:40 @agent_ppo2.py:185][0m |          -0.0154 |          61.8007 |           0.8720 |
[32m[20221214 00:14:40 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:14:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 473.72
[32m[20221214 00:14:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 509.56
[32m[20221214 00:14:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 529.53
[32m[20221214 00:14:40 @agent_ppo2.py:143][0m Total time:      17.16 min
[32m[20221214 00:14:40 @agent_ppo2.py:145][0m 1595392 total steps have happened
[32m[20221214 00:14:40 @agent_ppo2.py:121][0m #------------------------ Iteration 4779 --------------------------#
[32m[20221214 00:14:40 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:14:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:14:40 @agent_ppo2.py:185][0m |           0.0025 |          66.1946 |           0.1577 |
[32m[20221214 00:14:40 @agent_ppo2.py:185][0m |          -0.0024 |          61.8444 |           0.3161 |
[32m[20221214 00:14:41 @agent_ppo2.py:185][0m |          -0.0088 |          60.8008 |           0.3750 |
[32m[20221214 00:14:41 @agent_ppo2.py:185][0m |           0.0016 |          66.0968 |           0.3792 |
[32m[20221214 00:14:41 @agent_ppo2.py:185][0m |          -0.0109 |          59.3256 |           0.4508 |
[32m[20221214 00:14:41 @agent_ppo2.py:185][0m |          -0.0121 |          58.6709 |           0.5194 |
[32m[20221214 00:14:41 @agent_ppo2.py:185][0m |          -0.0019 |          67.8590 |           0.5180 |
[32m[20221214 00:14:41 @agent_ppo2.py:185][0m |          -0.0148 |          58.2851 |           0.5516 |
[32m[20221214 00:14:41 @agent_ppo2.py:185][0m |          -0.0146 |          57.5756 |           0.5749 |
[32m[20221214 00:14:41 @agent_ppo2.py:185][0m |          -0.0168 |          57.1406 |           0.5099 |
[32m[20221214 00:14:41 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 00:14:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 468.93
[32m[20221214 00:14:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 498.94
[32m[20221214 00:14:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 512.40
[32m[20221214 00:14:41 @agent_ppo2.py:143][0m Total time:      17.19 min
[32m[20221214 00:14:41 @agent_ppo2.py:145][0m 1597440 total steps have happened
[32m[20221214 00:14:41 @agent_ppo2.py:121][0m #------------------------ Iteration 4780 --------------------------#
[32m[20221214 00:14:42 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:14:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:14:42 @agent_ppo2.py:185][0m |           0.0119 |          82.0306 |           0.2460 |
[32m[20221214 00:14:42 @agent_ppo2.py:185][0m |          -0.0061 |          70.2977 |           0.3596 |
[32m[20221214 00:14:42 @agent_ppo2.py:185][0m |          -0.0113 |          68.3805 |           0.4423 |
[32m[20221214 00:14:42 @agent_ppo2.py:185][0m |          -0.0166 |          67.1376 |           0.3914 |
[32m[20221214 00:14:42 @agent_ppo2.py:185][0m |          -0.0157 |          66.2950 |           0.3209 |
[32m[20221214 00:14:42 @agent_ppo2.py:185][0m |          -0.0123 |          66.7370 |           0.3218 |
[32m[20221214 00:14:42 @agent_ppo2.py:185][0m |          -0.0159 |          65.0238 |           0.3616 |
[32m[20221214 00:14:42 @agent_ppo2.py:185][0m |          -0.0206 |          64.3941 |           0.4125 |
[32m[20221214 00:14:43 @agent_ppo2.py:185][0m |          -0.0180 |          63.9701 |           0.4846 |
[32m[20221214 00:14:43 @agent_ppo2.py:185][0m |          -0.0198 |          63.6465 |           0.4385 |
[32m[20221214 00:14:43 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:14:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 517.02
[32m[20221214 00:14:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 578.29
[32m[20221214 00:14:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 494.73
[32m[20221214 00:14:43 @agent_ppo2.py:143][0m Total time:      17.21 min
[32m[20221214 00:14:43 @agent_ppo2.py:145][0m 1599488 total steps have happened
[32m[20221214 00:14:43 @agent_ppo2.py:121][0m #------------------------ Iteration 4781 --------------------------#
[32m[20221214 00:14:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:14:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:14:43 @agent_ppo2.py:185][0m |           0.0047 |          44.2297 |           1.7632 |
[32m[20221214 00:14:43 @agent_ppo2.py:185][0m |          -0.0053 |          32.8680 |           1.7332 |
[32m[20221214 00:14:43 @agent_ppo2.py:185][0m |          -0.0096 |          29.4995 |           1.7619 |
[32m[20221214 00:14:43 @agent_ppo2.py:185][0m |          -0.0131 |          27.3962 |           1.7539 |
[32m[20221214 00:14:44 @agent_ppo2.py:185][0m |          -0.0165 |          25.7971 |           1.6330 |
[32m[20221214 00:14:44 @agent_ppo2.py:185][0m |          -0.0100 |          24.4160 |           1.5881 |
[32m[20221214 00:14:44 @agent_ppo2.py:185][0m |          -0.0189 |          23.5517 |           1.6149 |
[32m[20221214 00:14:44 @agent_ppo2.py:185][0m |          -0.0207 |          22.8179 |           1.4930 |
[32m[20221214 00:14:44 @agent_ppo2.py:185][0m |          -0.0098 |          25.7285 |           1.4891 |
[32m[20221214 00:14:44 @agent_ppo2.py:185][0m |          -0.0270 |          22.0841 |           1.4936 |
[32m[20221214 00:14:44 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221214 00:14:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 382.07
[32m[20221214 00:14:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 483.46
[32m[20221214 00:14:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 573.36
[32m[20221214 00:14:44 @agent_ppo2.py:143][0m Total time:      17.23 min
[32m[20221214 00:14:44 @agent_ppo2.py:145][0m 1601536 total steps have happened
[32m[20221214 00:14:44 @agent_ppo2.py:121][0m #------------------------ Iteration 4782 --------------------------#
[32m[20221214 00:14:44 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:14:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:14:45 @agent_ppo2.py:185][0m |           0.0045 |          66.0718 |           0.5717 |
[32m[20221214 00:14:45 @agent_ppo2.py:185][0m |          -0.0082 |          60.5378 |           0.4385 |
[32m[20221214 00:14:45 @agent_ppo2.py:185][0m |          -0.0073 |          58.3969 |           0.5277 |
[32m[20221214 00:14:45 @agent_ppo2.py:185][0m |          -0.0148 |          56.8996 |           0.4446 |
[32m[20221214 00:14:45 @agent_ppo2.py:185][0m |          -0.0154 |          56.0080 |           0.4192 |
[32m[20221214 00:14:45 @agent_ppo2.py:185][0m |          -0.0201 |          55.3694 |           0.4330 |
[32m[20221214 00:14:45 @agent_ppo2.py:185][0m |          -0.0153 |          56.9516 |           0.3284 |
[32m[20221214 00:14:45 @agent_ppo2.py:185][0m |          -0.0194 |          54.4151 |           0.4003 |
[32m[20221214 00:14:45 @agent_ppo2.py:185][0m |          -0.0177 |          53.8912 |           0.3338 |
[32m[20221214 00:14:45 @agent_ppo2.py:185][0m |          -0.0190 |          53.4000 |           0.3456 |
[32m[20221214 00:14:45 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:14:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 437.61
[32m[20221214 00:14:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 506.36
[32m[20221214 00:14:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 553.51
[32m[20221214 00:14:46 @agent_ppo2.py:143][0m Total time:      17.25 min
[32m[20221214 00:14:46 @agent_ppo2.py:145][0m 1603584 total steps have happened
[32m[20221214 00:14:46 @agent_ppo2.py:121][0m #------------------------ Iteration 4783 --------------------------#
[32m[20221214 00:14:46 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:14:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:14:46 @agent_ppo2.py:185][0m |           0.0035 |          72.3283 |          -0.1039 |
[32m[20221214 00:14:46 @agent_ppo2.py:185][0m |          -0.0005 |          63.1430 |           0.0292 |
[32m[20221214 00:14:46 @agent_ppo2.py:185][0m |           0.0093 |          68.0568 |           0.0579 |
[32m[20221214 00:14:46 @agent_ppo2.py:185][0m |          -0.0075 |          57.8864 |           0.0553 |
[32m[20221214 00:14:46 @agent_ppo2.py:185][0m |          -0.0105 |          56.2420 |           0.1409 |
[32m[20221214 00:14:46 @agent_ppo2.py:185][0m |          -0.0101 |          55.0645 |           0.1521 |
[32m[20221214 00:14:46 @agent_ppo2.py:185][0m |          -0.0132 |          54.3676 |           0.2335 |
[32m[20221214 00:14:47 @agent_ppo2.py:185][0m |          -0.0154 |          54.0742 |           0.2465 |
[32m[20221214 00:14:47 @agent_ppo2.py:185][0m |          -0.0184 |          53.5931 |           0.2383 |
[32m[20221214 00:14:47 @agent_ppo2.py:185][0m |          -0.0181 |          53.2260 |           0.2255 |
[32m[20221214 00:14:47 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:14:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 432.26
[32m[20221214 00:14:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 494.71
[32m[20221214 00:14:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 592.19
[32m[20221214 00:14:47 @agent_ppo2.py:143][0m Total time:      17.28 min
[32m[20221214 00:14:47 @agent_ppo2.py:145][0m 1605632 total steps have happened
[32m[20221214 00:14:47 @agent_ppo2.py:121][0m #------------------------ Iteration 4784 --------------------------#
[32m[20221214 00:14:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:14:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:14:47 @agent_ppo2.py:185][0m |           0.0013 |          74.3056 |          -1.3120 |
[32m[20221214 00:14:47 @agent_ppo2.py:185][0m |           0.0020 |          74.7709 |          -1.2444 |
[32m[20221214 00:14:48 @agent_ppo2.py:185][0m |          -0.0008 |          69.6523 |          -1.2615 |
[32m[20221214 00:14:48 @agent_ppo2.py:185][0m |          -0.0101 |          67.0868 |          -1.3144 |
[32m[20221214 00:14:48 @agent_ppo2.py:185][0m |           0.0006 |          69.4534 |          -1.3620 |
[32m[20221214 00:14:48 @agent_ppo2.py:185][0m |          -0.0145 |          65.9220 |          -1.3406 |
[32m[20221214 00:14:48 @agent_ppo2.py:185][0m |          -0.0156 |          65.4100 |          -1.3849 |
[32m[20221214 00:14:48 @agent_ppo2.py:185][0m |           0.0002 |          68.4367 |          -1.2407 |
[32m[20221214 00:14:48 @agent_ppo2.py:185][0m |          -0.0103 |          65.6941 |          -1.4188 |
[32m[20221214 00:14:48 @agent_ppo2.py:185][0m |          -0.0165 |          63.9030 |          -1.3242 |
[32m[20221214 00:14:48 @agent_ppo2.py:130][0m Policy update time: 1.18 s
[32m[20221214 00:14:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 448.60
[32m[20221214 00:14:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 519.22
[32m[20221214 00:14:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 455.52
[32m[20221214 00:14:48 @agent_ppo2.py:143][0m Total time:      17.30 min
[32m[20221214 00:14:48 @agent_ppo2.py:145][0m 1607680 total steps have happened
[32m[20221214 00:14:48 @agent_ppo2.py:121][0m #------------------------ Iteration 4785 --------------------------#
[32m[20221214 00:14:49 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:14:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:14:49 @agent_ppo2.py:185][0m |          -0.0007 |          69.8752 |          -0.3888 |
[32m[20221214 00:14:49 @agent_ppo2.py:185][0m |          -0.0054 |          65.7749 |          -0.3475 |
[32m[20221214 00:14:49 @agent_ppo2.py:185][0m |          -0.0124 |          64.5099 |          -0.3983 |
[32m[20221214 00:14:49 @agent_ppo2.py:185][0m |          -0.0093 |          63.4277 |          -0.4609 |
[32m[20221214 00:14:49 @agent_ppo2.py:185][0m |          -0.0027 |          65.2946 |          -0.4052 |
[32m[20221214 00:14:49 @agent_ppo2.py:185][0m |          -0.0122 |          61.8293 |          -0.4250 |
[32m[20221214 00:14:49 @agent_ppo2.py:185][0m |          -0.0154 |          61.3170 |          -0.3928 |
[32m[20221214 00:14:49 @agent_ppo2.py:185][0m |          -0.0158 |          60.9498 |          -0.4239 |
[32m[20221214 00:14:50 @agent_ppo2.py:185][0m |          -0.0113 |          61.9158 |          -0.4713 |
[32m[20221214 00:14:50 @agent_ppo2.py:185][0m |          -0.0160 |          60.3945 |          -0.4088 |
[32m[20221214 00:14:50 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:14:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 448.16
[32m[20221214 00:14:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 546.61
[32m[20221214 00:14:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 571.94
[32m[20221214 00:14:50 @agent_ppo2.py:143][0m Total time:      17.32 min
[32m[20221214 00:14:50 @agent_ppo2.py:145][0m 1609728 total steps have happened
[32m[20221214 00:14:50 @agent_ppo2.py:121][0m #------------------------ Iteration 4786 --------------------------#
[32m[20221214 00:14:50 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:14:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:14:50 @agent_ppo2.py:185][0m |          -0.0038 |          64.8800 |          -0.9486 |
[32m[20221214 00:14:50 @agent_ppo2.py:185][0m |          -0.0052 |          61.4925 |          -0.9491 |
[32m[20221214 00:14:50 @agent_ppo2.py:185][0m |          -0.0005 |          61.5919 |          -0.9600 |
[32m[20221214 00:14:50 @agent_ppo2.py:185][0m |          -0.0086 |          59.4001 |          -1.0373 |
[32m[20221214 00:14:51 @agent_ppo2.py:185][0m |          -0.0136 |          58.4242 |          -1.0348 |
[32m[20221214 00:14:51 @agent_ppo2.py:185][0m |          -0.0048 |          59.1591 |          -1.0965 |
[32m[20221214 00:14:51 @agent_ppo2.py:185][0m |          -0.0130 |          57.5184 |          -1.0136 |
[32m[20221214 00:14:51 @agent_ppo2.py:185][0m |          -0.0177 |          56.9640 |          -1.0779 |
[32m[20221214 00:14:51 @agent_ppo2.py:185][0m |          -0.0169 |          56.5099 |          -1.0902 |
[32m[20221214 00:14:51 @agent_ppo2.py:185][0m |          -0.0145 |          56.8307 |          -1.0448 |
[32m[20221214 00:14:51 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:14:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 466.04
[32m[20221214 00:14:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 571.91
[32m[20221214 00:14:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 600.62
[32m[20221214 00:14:51 @agent_ppo2.py:143][0m Total time:      17.35 min
[32m[20221214 00:14:51 @agent_ppo2.py:145][0m 1611776 total steps have happened
[32m[20221214 00:14:51 @agent_ppo2.py:121][0m #------------------------ Iteration 4787 --------------------------#
[32m[20221214 00:14:51 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:14:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:14:51 @agent_ppo2.py:185][0m |           0.0033 |          81.4437 |          -0.6001 |
[32m[20221214 00:14:52 @agent_ppo2.py:185][0m |          -0.0021 |          76.2131 |          -0.6952 |
[32m[20221214 00:14:52 @agent_ppo2.py:185][0m |          -0.0081 |          73.5753 |          -0.6397 |
[32m[20221214 00:14:52 @agent_ppo2.py:185][0m |          -0.0097 |          71.8829 |          -0.6348 |
[32m[20221214 00:14:52 @agent_ppo2.py:185][0m |          -0.0067 |          71.7617 |          -0.5782 |
[32m[20221214 00:14:52 @agent_ppo2.py:185][0m |          -0.0138 |          69.7296 |          -0.6770 |
[32m[20221214 00:14:52 @agent_ppo2.py:185][0m |          -0.0115 |          69.3371 |          -0.6614 |
[32m[20221214 00:14:52 @agent_ppo2.py:185][0m |          -0.0126 |          68.4615 |          -0.7255 |
[32m[20221214 00:14:52 @agent_ppo2.py:185][0m |          -0.0100 |          68.1715 |          -0.6500 |
[32m[20221214 00:14:52 @agent_ppo2.py:185][0m |          -0.0164 |          67.2621 |          -0.7340 |
[32m[20221214 00:14:52 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:14:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 442.90
[32m[20221214 00:14:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 521.08
[32m[20221214 00:14:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 334.86
[32m[20221214 00:14:52 @agent_ppo2.py:143][0m Total time:      17.37 min
[32m[20221214 00:14:52 @agent_ppo2.py:145][0m 1613824 total steps have happened
[32m[20221214 00:14:52 @agent_ppo2.py:121][0m #------------------------ Iteration 4788 --------------------------#
[32m[20221214 00:14:53 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:14:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:14:53 @agent_ppo2.py:185][0m |           0.0055 |          13.3068 |           0.1816 |
[32m[20221214 00:14:53 @agent_ppo2.py:185][0m |          -0.0011 |          11.9128 |           0.2068 |
[32m[20221214 00:14:53 @agent_ppo2.py:185][0m |          -0.0004 |          11.7525 |           0.0306 |
[32m[20221214 00:14:53 @agent_ppo2.py:185][0m |          -0.0032 |          11.6782 |           0.1816 |
[32m[20221214 00:14:53 @agent_ppo2.py:185][0m |          -0.0021 |          11.6327 |           0.1218 |
[32m[20221214 00:14:53 @agent_ppo2.py:185][0m |           0.0028 |          11.5856 |           0.0621 |
[32m[20221214 00:14:53 @agent_ppo2.py:185][0m |          -0.0039 |          11.5754 |           0.0422 |
[32m[20221214 00:14:53 @agent_ppo2.py:185][0m |           0.0080 |          12.1195 |           0.1120 |
[32m[20221214 00:14:53 @agent_ppo2.py:185][0m |          -0.0038 |          11.5448 |          -0.0568 |
[32m[20221214 00:14:53 @agent_ppo2.py:185][0m |          -0.0015 |          11.5612 |          -0.1232 |
[32m[20221214 00:14:53 @agent_ppo2.py:130][0m Policy update time: 0.93 s
[32m[20221214 00:14:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221214 00:14:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221214 00:14:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 245.57
[32m[20221214 00:14:54 @agent_ppo2.py:143][0m Total time:      17.39 min
[32m[20221214 00:14:54 @agent_ppo2.py:145][0m 1615872 total steps have happened
[32m[20221214 00:14:54 @agent_ppo2.py:121][0m #------------------------ Iteration 4789 --------------------------#
[32m[20221214 00:14:54 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:14:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:14:54 @agent_ppo2.py:185][0m |          -0.0016 |          74.2968 |          -0.7294 |
[32m[20221214 00:14:54 @agent_ppo2.py:185][0m |          -0.0081 |          69.4850 |          -0.6726 |
[32m[20221214 00:14:54 @agent_ppo2.py:185][0m |          -0.0082 |          68.0850 |          -0.6425 |
[32m[20221214 00:14:54 @agent_ppo2.py:185][0m |          -0.0075 |          67.3039 |          -0.6267 |
[32m[20221214 00:14:54 @agent_ppo2.py:185][0m |          -0.0148 |          66.9399 |          -0.6411 |
[32m[20221214 00:14:54 @agent_ppo2.py:185][0m |          -0.0115 |          66.7642 |          -0.6266 |
[32m[20221214 00:14:54 @agent_ppo2.py:185][0m |          -0.0176 |          66.3052 |          -0.6429 |
[32m[20221214 00:14:55 @agent_ppo2.py:185][0m |          -0.0159 |          65.8964 |          -0.5989 |
[32m[20221214 00:14:55 @agent_ppo2.py:185][0m |          -0.0167 |          65.9955 |          -0.6105 |
[32m[20221214 00:14:55 @agent_ppo2.py:185][0m |          -0.0137 |          65.9213 |          -0.5599 |
[32m[20221214 00:14:55 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:14:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 494.59
[32m[20221214 00:14:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 511.91
[32m[20221214 00:14:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 334.45
[32m[20221214 00:14:55 @agent_ppo2.py:143][0m Total time:      17.41 min
[32m[20221214 00:14:55 @agent_ppo2.py:145][0m 1617920 total steps have happened
[32m[20221214 00:14:55 @agent_ppo2.py:121][0m #------------------------ Iteration 4790 --------------------------#
[32m[20221214 00:14:55 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:14:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:14:55 @agent_ppo2.py:185][0m |           0.0054 |          70.7447 |          -0.3196 |
[32m[20221214 00:14:55 @agent_ppo2.py:185][0m |           0.0048 |          65.7241 |          -0.2012 |
[32m[20221214 00:14:55 @agent_ppo2.py:185][0m |          -0.0095 |          60.3755 |          -0.1842 |
[32m[20221214 00:14:56 @agent_ppo2.py:185][0m |          -0.0121 |          58.6684 |          -0.1786 |
[32m[20221214 00:14:56 @agent_ppo2.py:185][0m |          -0.0168 |          57.6625 |          -0.3346 |
[32m[20221214 00:14:56 @agent_ppo2.py:185][0m |          -0.0072 |          57.0745 |          -0.2564 |
[32m[20221214 00:14:56 @agent_ppo2.py:185][0m |          -0.0209 |          56.4179 |          -0.2347 |
[32m[20221214 00:14:56 @agent_ppo2.py:185][0m |          -0.0178 |          56.1354 |          -0.2001 |
[32m[20221214 00:14:56 @agent_ppo2.py:185][0m |          -0.0123 |          55.3228 |          -0.3502 |
[32m[20221214 00:14:56 @agent_ppo2.py:185][0m |          -0.0142 |          54.9040 |          -0.2767 |
[32m[20221214 00:14:56 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:14:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 435.99
[32m[20221214 00:14:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 519.99
[32m[20221214 00:14:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 423.83
[32m[20221214 00:14:56 @agent_ppo2.py:143][0m Total time:      17.43 min
[32m[20221214 00:14:56 @agent_ppo2.py:145][0m 1619968 total steps have happened
[32m[20221214 00:14:56 @agent_ppo2.py:121][0m #------------------------ Iteration 4791 --------------------------#
[32m[20221214 00:14:56 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:14:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:14:57 @agent_ppo2.py:185][0m |           0.0014 |          58.0893 |          -1.5125 |
[32m[20221214 00:14:57 @agent_ppo2.py:185][0m |          -0.0062 |          50.7525 |          -1.5942 |
[32m[20221214 00:14:57 @agent_ppo2.py:185][0m |          -0.0139 |          48.6223 |          -1.6867 |
[32m[20221214 00:14:57 @agent_ppo2.py:185][0m |          -0.0126 |          47.3652 |          -1.6619 |
[32m[20221214 00:14:57 @agent_ppo2.py:185][0m |          -0.0163 |          45.8876 |          -1.6704 |
[32m[20221214 00:14:57 @agent_ppo2.py:185][0m |          -0.0116 |          45.1377 |          -1.6899 |
[32m[20221214 00:14:57 @agent_ppo2.py:185][0m |          -0.0180 |          44.5896 |          -1.7943 |
[32m[20221214 00:14:57 @agent_ppo2.py:185][0m |          -0.0174 |          43.7273 |          -1.7734 |
[32m[20221214 00:14:57 @agent_ppo2.py:185][0m |          -0.0192 |          43.3906 |          -1.7775 |
[32m[20221214 00:14:57 @agent_ppo2.py:185][0m |          -0.0226 |          42.9647 |          -1.7602 |
[32m[20221214 00:14:57 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:14:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 313.24
[32m[20221214 00:14:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 428.85
[32m[20221214 00:14:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 642.39
[32m[20221214 00:14:57 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 642.39
[32m[20221214 00:14:57 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 642.39
[32m[20221214 00:14:57 @agent_ppo2.py:143][0m Total time:      17.45 min
[32m[20221214 00:14:57 @agent_ppo2.py:145][0m 1622016 total steps have happened
[32m[20221214 00:14:57 @agent_ppo2.py:121][0m #------------------------ Iteration 4792 --------------------------#
[32m[20221214 00:14:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:14:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:14:58 @agent_ppo2.py:185][0m |           0.0018 |          65.3187 |          -1.2945 |
[32m[20221214 00:14:58 @agent_ppo2.py:185][0m |          -0.0049 |          60.2747 |          -1.3237 |
[32m[20221214 00:14:58 @agent_ppo2.py:185][0m |          -0.0008 |          59.1739 |          -1.2834 |
[32m[20221214 00:14:58 @agent_ppo2.py:185][0m |          -0.0105 |          55.3369 |          -1.2648 |
[32m[20221214 00:14:58 @agent_ppo2.py:185][0m |          -0.0112 |          53.9232 |          -1.2711 |
[32m[20221214 00:14:58 @agent_ppo2.py:185][0m |          -0.0057 |          53.4180 |          -1.2655 |
[32m[20221214 00:14:58 @agent_ppo2.py:185][0m |          -0.0022 |          56.8614 |          -1.2726 |
[32m[20221214 00:14:58 @agent_ppo2.py:185][0m |          -0.0156 |          51.9239 |          -1.1467 |
[32m[20221214 00:14:58 @agent_ppo2.py:185][0m |          -0.0144 |          51.4866 |          -1.2088 |
[32m[20221214 00:14:59 @agent_ppo2.py:185][0m |          -0.0171 |          50.8222 |          -1.2138 |
[32m[20221214 00:14:59 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:14:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 487.44
[32m[20221214 00:14:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 584.82
[32m[20221214 00:14:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 387.39
[32m[20221214 00:14:59 @agent_ppo2.py:143][0m Total time:      17.47 min
[32m[20221214 00:14:59 @agent_ppo2.py:145][0m 1624064 total steps have happened
[32m[20221214 00:14:59 @agent_ppo2.py:121][0m #------------------------ Iteration 4793 --------------------------#
[32m[20221214 00:14:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:14:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:14:59 @agent_ppo2.py:185][0m |           0.0023 |          86.1178 |          -0.8214 |
[32m[20221214 00:14:59 @agent_ppo2.py:185][0m |          -0.0053 |          78.9983 |          -0.7171 |
[32m[20221214 00:14:59 @agent_ppo2.py:185][0m |           0.0019 |          81.1227 |          -0.6693 |
[32m[20221214 00:14:59 @agent_ppo2.py:185][0m |          -0.0071 |          76.9907 |          -0.4936 |
[32m[20221214 00:14:59 @agent_ppo2.py:185][0m |          -0.0136 |          75.2787 |          -0.5705 |
[32m[20221214 00:15:00 @agent_ppo2.py:185][0m |          -0.0119 |          74.4974 |          -0.4961 |
[32m[20221214 00:15:00 @agent_ppo2.py:185][0m |          -0.0151 |          74.0980 |          -0.4075 |
[32m[20221214 00:15:00 @agent_ppo2.py:185][0m |          -0.0098 |          74.3357 |          -0.3381 |
[32m[20221214 00:15:00 @agent_ppo2.py:185][0m |          -0.0144 |          73.0902 |          -0.2994 |
[32m[20221214 00:15:00 @agent_ppo2.py:185][0m |          -0.0157 |          72.9908 |          -0.2903 |
[32m[20221214 00:15:00 @agent_ppo2.py:130][0m Policy update time: 0.96 s
[32m[20221214 00:15:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 510.03
[32m[20221214 00:15:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 550.46
[32m[20221214 00:15:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 451.74
[32m[20221214 00:15:00 @agent_ppo2.py:143][0m Total time:      17.49 min
[32m[20221214 00:15:00 @agent_ppo2.py:145][0m 1626112 total steps have happened
[32m[20221214 00:15:00 @agent_ppo2.py:121][0m #------------------------ Iteration 4794 --------------------------#
[32m[20221214 00:15:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:15:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:15:00 @agent_ppo2.py:185][0m |           0.0021 |          59.3043 |          -0.5502 |
[32m[20221214 00:15:00 @agent_ppo2.py:185][0m |           0.0054 |          60.2204 |          -0.4823 |
[32m[20221214 00:15:01 @agent_ppo2.py:185][0m |          -0.0079 |          52.7935 |          -0.5798 |
[32m[20221214 00:15:01 @agent_ppo2.py:185][0m |          -0.0118 |          51.2950 |          -0.5419 |
[32m[20221214 00:15:01 @agent_ppo2.py:185][0m |          -0.0106 |          50.2443 |          -0.5265 |
[32m[20221214 00:15:01 @agent_ppo2.py:185][0m |          -0.0130 |          49.5584 |          -0.5691 |
[32m[20221214 00:15:01 @agent_ppo2.py:185][0m |          -0.0170 |          48.8999 |          -0.6853 |
[32m[20221214 00:15:01 @agent_ppo2.py:185][0m |          -0.0130 |          48.2199 |          -0.7092 |
[32m[20221214 00:15:01 @agent_ppo2.py:185][0m |          -0.0075 |          48.9327 |          -0.6695 |
[32m[20221214 00:15:01 @agent_ppo2.py:185][0m |          -0.0152 |          47.6958 |          -0.6844 |
[32m[20221214 00:15:01 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:15:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 442.33
[32m[20221214 00:15:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 562.29
[32m[20221214 00:15:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 540.29
[32m[20221214 00:15:01 @agent_ppo2.py:143][0m Total time:      17.52 min
[32m[20221214 00:15:01 @agent_ppo2.py:145][0m 1628160 total steps have happened
[32m[20221214 00:15:01 @agent_ppo2.py:121][0m #------------------------ Iteration 4795 --------------------------#
[32m[20221214 00:15:01 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:15:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:15:02 @agent_ppo2.py:185][0m |           0.0005 |          80.9160 |          -1.3470 |
[32m[20221214 00:15:02 @agent_ppo2.py:185][0m |          -0.0074 |          74.5626 |          -1.3165 |
[32m[20221214 00:15:02 @agent_ppo2.py:185][0m |          -0.0038 |          72.0454 |          -1.2423 |
[32m[20221214 00:15:02 @agent_ppo2.py:185][0m |          -0.0112 |          69.3693 |          -1.2973 |
[32m[20221214 00:15:02 @agent_ppo2.py:185][0m |          -0.0128 |          67.8889 |          -1.2075 |
[32m[20221214 00:15:02 @agent_ppo2.py:185][0m |          -0.0115 |          66.7877 |          -1.3180 |
[32m[20221214 00:15:02 @agent_ppo2.py:185][0m |          -0.0166 |          65.2305 |          -1.2416 |
[32m[20221214 00:15:02 @agent_ppo2.py:185][0m |          -0.0147 |          64.6879 |          -1.1733 |
[32m[20221214 00:15:02 @agent_ppo2.py:185][0m |          -0.0167 |          63.7243 |          -1.1901 |
[32m[20221214 00:15:02 @agent_ppo2.py:185][0m |          -0.0188 |          63.1032 |          -1.2529 |
[32m[20221214 00:15:02 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:15:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 550.67
[32m[20221214 00:15:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 625.32
[32m[20221214 00:15:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 600.99
[32m[20221214 00:15:03 @agent_ppo2.py:143][0m Total time:      17.54 min
[32m[20221214 00:15:03 @agent_ppo2.py:145][0m 1630208 total steps have happened
[32m[20221214 00:15:03 @agent_ppo2.py:121][0m #------------------------ Iteration 4796 --------------------------#
[32m[20221214 00:15:03 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:15:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:15:03 @agent_ppo2.py:185][0m |          -0.0041 |          34.5725 |          -1.4044 |
[32m[20221214 00:15:03 @agent_ppo2.py:185][0m |          -0.0079 |          28.2441 |          -1.3384 |
[32m[20221214 00:15:03 @agent_ppo2.py:185][0m |          -0.0071 |          26.6329 |          -1.3951 |
[32m[20221214 00:15:03 @agent_ppo2.py:185][0m |          -0.0061 |          25.4601 |          -1.2981 |
[32m[20221214 00:15:03 @agent_ppo2.py:185][0m |          -0.0054 |          24.7515 |          -1.3733 |
[32m[20221214 00:15:03 @agent_ppo2.py:185][0m |          -0.0119 |          24.1970 |          -1.2634 |
[32m[20221214 00:15:03 @agent_ppo2.py:185][0m |          -0.0212 |          23.8635 |          -1.3134 |
[32m[20221214 00:15:03 @agent_ppo2.py:185][0m |          -0.0212 |          23.5115 |          -1.2909 |
[32m[20221214 00:15:04 @agent_ppo2.py:185][0m |          -0.0188 |          23.1862 |          -1.2888 |
[32m[20221214 00:15:04 @agent_ppo2.py:185][0m |          -0.0126 |          22.7493 |          -1.3464 |
[32m[20221214 00:15:04 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:15:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 362.29
[32m[20221214 00:15:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 456.13
[32m[20221214 00:15:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 582.02
[32m[20221214 00:15:04 @agent_ppo2.py:143][0m Total time:      17.56 min
[32m[20221214 00:15:04 @agent_ppo2.py:145][0m 1632256 total steps have happened
[32m[20221214 00:15:04 @agent_ppo2.py:121][0m #------------------------ Iteration 4797 --------------------------#
[32m[20221214 00:15:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:15:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:15:04 @agent_ppo2.py:185][0m |           0.0009 |          53.4976 |          -1.3353 |
[32m[20221214 00:15:04 @agent_ppo2.py:185][0m |          -0.0041 |          47.7555 |          -1.2771 |
[32m[20221214 00:15:04 @agent_ppo2.py:185][0m |          -0.0013 |          45.5066 |          -1.2883 |
[32m[20221214 00:15:04 @agent_ppo2.py:185][0m |          -0.0124 |          44.2078 |          -1.2696 |
[32m[20221214 00:15:05 @agent_ppo2.py:185][0m |          -0.0134 |          43.4018 |          -1.2255 |
[32m[20221214 00:15:05 @agent_ppo2.py:185][0m |          -0.0105 |          42.3559 |          -1.2130 |
[32m[20221214 00:15:05 @agent_ppo2.py:185][0m |          -0.0162 |          41.6132 |          -1.2229 |
[32m[20221214 00:15:05 @agent_ppo2.py:185][0m |          -0.0221 |          41.0193 |          -1.1417 |
[32m[20221214 00:15:05 @agent_ppo2.py:185][0m |          -0.0149 |          40.0931 |          -1.1343 |
[32m[20221214 00:15:05 @agent_ppo2.py:185][0m |          -0.0168 |          39.4413 |          -1.1173 |
[32m[20221214 00:15:05 @agent_ppo2.py:130][0m Policy update time: 0.95 s
[32m[20221214 00:15:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 338.84
[32m[20221214 00:15:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 467.41
[32m[20221214 00:15:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 230.11
[32m[20221214 00:15:05 @agent_ppo2.py:143][0m Total time:      17.58 min
[32m[20221214 00:15:05 @agent_ppo2.py:145][0m 1634304 total steps have happened
[32m[20221214 00:15:05 @agent_ppo2.py:121][0m #------------------------ Iteration 4798 --------------------------#
[32m[20221214 00:15:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:15:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:15:05 @agent_ppo2.py:185][0m |          -0.0026 |          30.8506 |          -0.8693 |
[32m[20221214 00:15:05 @agent_ppo2.py:185][0m |          -0.0030 |          25.4571 |          -0.7528 |
[32m[20221214 00:15:06 @agent_ppo2.py:185][0m |          -0.0073 |          23.1345 |          -0.8287 |
[32m[20221214 00:15:06 @agent_ppo2.py:185][0m |          -0.0143 |          22.6145 |          -0.7415 |
[32m[20221214 00:15:06 @agent_ppo2.py:185][0m |          -0.0098 |          21.8250 |          -0.6412 |
[32m[20221214 00:15:06 @agent_ppo2.py:185][0m |          -0.0136 |          21.4562 |          -0.6893 |
[32m[20221214 00:15:06 @agent_ppo2.py:185][0m |          -0.0182 |          20.3961 |          -0.6022 |
[32m[20221214 00:15:06 @agent_ppo2.py:185][0m |          -0.0155 |          20.4180 |          -0.5323 |
[32m[20221214 00:15:06 @agent_ppo2.py:185][0m |          -0.0209 |          19.6709 |          -0.4353 |
[32m[20221214 00:15:06 @agent_ppo2.py:185][0m |          -0.0220 |          19.3670 |          -0.4642 |
[32m[20221214 00:15:06 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:15:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 405.97
[32m[20221214 00:15:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 560.69
[32m[20221214 00:15:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 546.88
[32m[20221214 00:15:06 @agent_ppo2.py:143][0m Total time:      17.60 min
[32m[20221214 00:15:06 @agent_ppo2.py:145][0m 1636352 total steps have happened
[32m[20221214 00:15:06 @agent_ppo2.py:121][0m #------------------------ Iteration 4799 --------------------------#
[32m[20221214 00:15:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:15:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:15:07 @agent_ppo2.py:185][0m |          -0.0029 |          50.4812 |          -1.3194 |
[32m[20221214 00:15:07 @agent_ppo2.py:185][0m |          -0.0108 |          44.5008 |          -1.2280 |
[32m[20221214 00:15:07 @agent_ppo2.py:185][0m |          -0.0110 |          42.3303 |          -1.1341 |
[32m[20221214 00:15:07 @agent_ppo2.py:185][0m |          -0.0139 |          41.6141 |          -1.0444 |
[32m[20221214 00:15:07 @agent_ppo2.py:185][0m |          -0.0156 |          40.0608 |          -1.0061 |
[32m[20221214 00:15:07 @agent_ppo2.py:185][0m |          -0.0162 |          39.4023 |          -1.0238 |
[32m[20221214 00:15:07 @agent_ppo2.py:185][0m |          -0.0164 |          38.7235 |          -0.9250 |
[32m[20221214 00:15:07 @agent_ppo2.py:185][0m |          -0.0182 |          38.0033 |          -0.9050 |
[32m[20221214 00:15:07 @agent_ppo2.py:185][0m |          -0.0229 |          37.8701 |          -0.8406 |
[32m[20221214 00:15:07 @agent_ppo2.py:185][0m |          -0.0232 |          37.3682 |          -0.8855 |
[32m[20221214 00:15:07 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:15:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 370.90
[32m[20221214 00:15:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 495.92
[32m[20221214 00:15:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 344.50
[32m[20221214 00:15:08 @agent_ppo2.py:143][0m Total time:      17.62 min
[32m[20221214 00:15:08 @agent_ppo2.py:145][0m 1638400 total steps have happened
[32m[20221214 00:15:08 @agent_ppo2.py:121][0m #------------------------ Iteration 4800 --------------------------#
[32m[20221214 00:15:08 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:15:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:15:08 @agent_ppo2.py:185][0m |           0.0055 |          63.3405 |          -0.3138 |
[32m[20221214 00:15:08 @agent_ppo2.py:185][0m |          -0.0064 |          59.2894 |          -0.3044 |
[32m[20221214 00:15:08 @agent_ppo2.py:185][0m |          -0.0091 |          56.4803 |          -0.3692 |
[32m[20221214 00:15:08 @agent_ppo2.py:185][0m |          -0.0149 |          54.8083 |          -0.4289 |
[32m[20221214 00:15:08 @agent_ppo2.py:185][0m |          -0.0148 |          54.8178 |          -0.2542 |
[32m[20221214 00:15:08 @agent_ppo2.py:185][0m |          -0.0168 |          53.2257 |          -0.3873 |
[32m[20221214 00:15:08 @agent_ppo2.py:185][0m |          -0.0178 |          52.5902 |          -0.3414 |
[32m[20221214 00:15:09 @agent_ppo2.py:185][0m |          -0.0188 |          51.9284 |          -0.3124 |
[32m[20221214 00:15:09 @agent_ppo2.py:185][0m |          -0.0227 |          52.1558 |          -0.3404 |
[32m[20221214 00:15:09 @agent_ppo2.py:185][0m |          -0.0221 |          51.9644 |          -0.3409 |
[32m[20221214 00:15:09 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:15:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 421.27
[32m[20221214 00:15:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 498.71
[32m[20221214 00:15:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 552.35
[32m[20221214 00:15:09 @agent_ppo2.py:143][0m Total time:      17.64 min
[32m[20221214 00:15:09 @agent_ppo2.py:145][0m 1640448 total steps have happened
[32m[20221214 00:15:09 @agent_ppo2.py:121][0m #------------------------ Iteration 4801 --------------------------#
[32m[20221214 00:15:09 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:15:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:15:09 @agent_ppo2.py:185][0m |           0.0011 |          46.9964 |           0.5966 |
[32m[20221214 00:15:09 @agent_ppo2.py:185][0m |          -0.0050 |          41.2256 |           0.5561 |
[32m[20221214 00:15:09 @agent_ppo2.py:185][0m |          -0.0083 |          38.9646 |           0.5048 |
[32m[20221214 00:15:09 @agent_ppo2.py:185][0m |          -0.0071 |          37.8976 |           0.4584 |
[32m[20221214 00:15:10 @agent_ppo2.py:185][0m |          -0.0085 |          37.1889 |           0.4335 |
[32m[20221214 00:15:10 @agent_ppo2.py:185][0m |          -0.0112 |          36.3555 |           0.4479 |
[32m[20221214 00:15:10 @agent_ppo2.py:185][0m |          -0.0193 |          35.6221 |           0.4300 |
[32m[20221214 00:15:10 @agent_ppo2.py:185][0m |          -0.0170 |          35.2604 |           0.3871 |
[32m[20221214 00:15:10 @agent_ppo2.py:185][0m |          -0.0136 |          34.9730 |           0.4620 |
[32m[20221214 00:15:10 @agent_ppo2.py:185][0m |          -0.0187 |          34.3715 |           0.3551 |
[32m[20221214 00:15:10 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:15:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 419.93
[32m[20221214 00:15:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 558.11
[32m[20221214 00:15:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 542.44
[32m[20221214 00:15:10 @agent_ppo2.py:143][0m Total time:      17.66 min
[32m[20221214 00:15:10 @agent_ppo2.py:145][0m 1642496 total steps have happened
[32m[20221214 00:15:10 @agent_ppo2.py:121][0m #------------------------ Iteration 4802 --------------------------#
[32m[20221214 00:15:10 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:15:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:15:10 @agent_ppo2.py:185][0m |           0.0007 |          58.6043 |           0.1384 |
[32m[20221214 00:15:11 @agent_ppo2.py:185][0m |          -0.0028 |          53.6457 |           0.1347 |
[32m[20221214 00:15:11 @agent_ppo2.py:185][0m |          -0.0087 |          51.7967 |           0.2486 |
[32m[20221214 00:15:11 @agent_ppo2.py:185][0m |          -0.0127 |          50.0845 |           0.3562 |
[32m[20221214 00:15:11 @agent_ppo2.py:185][0m |          -0.0101 |          49.0955 |           0.3114 |
[32m[20221214 00:15:11 @agent_ppo2.py:185][0m |          -0.0147 |          48.5357 |           0.3886 |
[32m[20221214 00:15:11 @agent_ppo2.py:185][0m |          -0.0114 |          48.0838 |           0.4213 |
[32m[20221214 00:15:11 @agent_ppo2.py:185][0m |          -0.0133 |          47.3672 |           0.4850 |
[32m[20221214 00:15:11 @agent_ppo2.py:185][0m |          -0.0147 |          46.8669 |           0.5871 |
[32m[20221214 00:15:11 @agent_ppo2.py:185][0m |          -0.0207 |          46.6638 |           0.6086 |
[32m[20221214 00:15:11 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:15:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 386.02
[32m[20221214 00:15:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 516.36
[32m[20221214 00:15:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 390.74
[32m[20221214 00:15:11 @agent_ppo2.py:143][0m Total time:      17.68 min
[32m[20221214 00:15:11 @agent_ppo2.py:145][0m 1644544 total steps have happened
[32m[20221214 00:15:11 @agent_ppo2.py:121][0m #------------------------ Iteration 4803 --------------------------#
[32m[20221214 00:15:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:15:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:15:12 @agent_ppo2.py:185][0m |           0.0141 |          91.6991 |          -0.8560 |
[32m[20221214 00:15:12 @agent_ppo2.py:185][0m |          -0.0011 |          81.8782 |          -0.9257 |
[32m[20221214 00:15:12 @agent_ppo2.py:185][0m |          -0.0083 |          79.4748 |          -0.8407 |
[32m[20221214 00:15:12 @agent_ppo2.py:185][0m |          -0.0078 |          78.6509 |          -0.9060 |
[32m[20221214 00:15:12 @agent_ppo2.py:185][0m |          -0.0091 |          78.0984 |          -0.9461 |
[32m[20221214 00:15:12 @agent_ppo2.py:185][0m |          -0.0128 |          77.1779 |          -0.9981 |
[32m[20221214 00:15:12 @agent_ppo2.py:185][0m |          -0.0100 |          76.9906 |          -0.9448 |
[32m[20221214 00:15:12 @agent_ppo2.py:185][0m |          -0.0147 |          76.5637 |          -1.0105 |
[32m[20221214 00:15:12 @agent_ppo2.py:185][0m |          -0.0039 |          86.0218 |          -1.0772 |
[32m[20221214 00:15:12 @agent_ppo2.py:185][0m |          -0.0133 |          76.2543 |          -1.0848 |
[32m[20221214 00:15:12 @agent_ppo2.py:130][0m Policy update time: 0.94 s
[32m[20221214 00:15:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 462.67
[32m[20221214 00:15:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 496.50
[32m[20221214 00:15:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 566.12
[32m[20221214 00:15:13 @agent_ppo2.py:143][0m Total time:      17.70 min
[32m[20221214 00:15:13 @agent_ppo2.py:145][0m 1646592 total steps have happened
[32m[20221214 00:15:13 @agent_ppo2.py:121][0m #------------------------ Iteration 4804 --------------------------#
[32m[20221214 00:15:13 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:15:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:15:13 @agent_ppo2.py:185][0m |           0.0038 |          80.5728 |          -0.9835 |
[32m[20221214 00:15:13 @agent_ppo2.py:185][0m |          -0.0023 |          75.6204 |          -0.8903 |
[32m[20221214 00:15:13 @agent_ppo2.py:185][0m |          -0.0073 |          74.0725 |          -0.8407 |
[32m[20221214 00:15:13 @agent_ppo2.py:185][0m |          -0.0031 |          73.1098 |          -0.8026 |
[32m[20221214 00:15:13 @agent_ppo2.py:185][0m |          -0.0113 |          72.4935 |          -0.7704 |
[32m[20221214 00:15:13 @agent_ppo2.py:185][0m |           0.0064 |          77.5582 |          -0.7735 |
[32m[20221214 00:15:13 @agent_ppo2.py:185][0m |          -0.0088 |          71.3661 |          -0.6444 |
[32m[20221214 00:15:14 @agent_ppo2.py:185][0m |          -0.0112 |          70.6846 |          -0.6818 |
[32m[20221214 00:15:14 @agent_ppo2.py:185][0m |          -0.0106 |          70.0517 |          -0.6299 |
[32m[20221214 00:15:14 @agent_ppo2.py:185][0m |          -0.0128 |          69.7853 |          -0.5468 |
[32m[20221214 00:15:14 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:15:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 505.14
[32m[20221214 00:15:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 540.27
[32m[20221214 00:15:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 410.26
[32m[20221214 00:15:14 @agent_ppo2.py:143][0m Total time:      17.73 min
[32m[20221214 00:15:14 @agent_ppo2.py:145][0m 1648640 total steps have happened
[32m[20221214 00:15:14 @agent_ppo2.py:121][0m #------------------------ Iteration 4805 --------------------------#
[32m[20221214 00:15:14 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:15:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:15:14 @agent_ppo2.py:185][0m |           0.0036 |          86.0200 |           0.1987 |
[32m[20221214 00:15:14 @agent_ppo2.py:185][0m |          -0.0100 |          81.6993 |           0.2198 |
[32m[20221214 00:15:14 @agent_ppo2.py:185][0m |          -0.0079 |          80.3197 |           0.2588 |
[32m[20221214 00:15:15 @agent_ppo2.py:185][0m |           0.0017 |          91.5951 |           0.3228 |
[32m[20221214 00:15:15 @agent_ppo2.py:185][0m |          -0.0170 |          80.1944 |           0.3423 |
[32m[20221214 00:15:15 @agent_ppo2.py:185][0m |          -0.0180 |          78.8959 |           0.4312 |
[32m[20221214 00:15:15 @agent_ppo2.py:185][0m |          -0.0088 |          84.5880 |           0.3864 |
[32m[20221214 00:15:15 @agent_ppo2.py:185][0m |          -0.0155 |          78.4899 |           0.4783 |
[32m[20221214 00:15:15 @agent_ppo2.py:185][0m |          -0.0180 |          78.1044 |           0.5347 |
[32m[20221214 00:15:15 @agent_ppo2.py:185][0m |          -0.0055 |          87.0943 |           0.6124 |
[32m[20221214 00:15:15 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:15:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 536.09
[32m[20221214 00:15:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 550.12
[32m[20221214 00:15:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 524.04
[32m[20221214 00:15:15 @agent_ppo2.py:143][0m Total time:      17.75 min
[32m[20221214 00:15:15 @agent_ppo2.py:145][0m 1650688 total steps have happened
[32m[20221214 00:15:15 @agent_ppo2.py:121][0m #------------------------ Iteration 4806 --------------------------#
[32m[20221214 00:15:15 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:15:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:15:16 @agent_ppo2.py:185][0m |           0.0017 |          73.3854 |           1.6523 |
[32m[20221214 00:15:16 @agent_ppo2.py:185][0m |          -0.0114 |          66.4622 |           1.8016 |
[32m[20221214 00:15:16 @agent_ppo2.py:185][0m |          -0.0053 |          63.9148 |           1.7984 |
[32m[20221214 00:15:16 @agent_ppo2.py:185][0m |          -0.0126 |          62.0456 |           1.8869 |
[32m[20221214 00:15:16 @agent_ppo2.py:185][0m |          -0.0128 |          62.0652 |           1.9508 |
[32m[20221214 00:15:16 @agent_ppo2.py:185][0m |          -0.0148 |          60.7686 |           1.9557 |
[32m[20221214 00:15:16 @agent_ppo2.py:185][0m |          -0.0150 |          59.9076 |           2.0388 |
[32m[20221214 00:15:16 @agent_ppo2.py:185][0m |          -0.0174 |          59.5880 |           2.0682 |
[32m[20221214 00:15:16 @agent_ppo2.py:185][0m |          -0.0184 |          58.8810 |           2.1262 |
[32m[20221214 00:15:16 @agent_ppo2.py:185][0m |          -0.0185 |          58.9588 |           2.1934 |
[32m[20221214 00:15:16 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:15:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 462.78
[32m[20221214 00:15:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 518.45
[32m[20221214 00:15:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 606.67
[32m[20221214 00:15:17 @agent_ppo2.py:143][0m Total time:      17.77 min
[32m[20221214 00:15:17 @agent_ppo2.py:145][0m 1652736 total steps have happened
[32m[20221214 00:15:17 @agent_ppo2.py:121][0m #------------------------ Iteration 4807 --------------------------#
[32m[20221214 00:15:17 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:15:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:15:17 @agent_ppo2.py:185][0m |           0.0042 |          41.3753 |           0.9220 |
[32m[20221214 00:15:17 @agent_ppo2.py:185][0m |          -0.0084 |          36.8448 |           1.0636 |
[32m[20221214 00:15:17 @agent_ppo2.py:185][0m |          -0.0058 |          35.1520 |           1.0740 |
[32m[20221214 00:15:17 @agent_ppo2.py:185][0m |          -0.0133 |          33.9638 |           1.0705 |
[32m[20221214 00:15:17 @agent_ppo2.py:185][0m |          -0.0181 |          33.2213 |           1.0163 |
[32m[20221214 00:15:17 @agent_ppo2.py:185][0m |          -0.0156 |          32.5119 |           1.0714 |
[32m[20221214 00:15:17 @agent_ppo2.py:185][0m |          -0.0250 |          32.0250 |           1.0765 |
[32m[20221214 00:15:18 @agent_ppo2.py:185][0m |          -0.0229 |          31.5271 |           1.0773 |
[32m[20221214 00:15:18 @agent_ppo2.py:185][0m |          -0.0271 |          31.4627 |           0.9950 |
[32m[20221214 00:15:18 @agent_ppo2.py:185][0m |          -0.0265 |          30.8233 |           1.0449 |
[32m[20221214 00:15:18 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:15:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 297.84
[32m[20221214 00:15:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 492.41
[32m[20221214 00:15:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 534.86
[32m[20221214 00:15:18 @agent_ppo2.py:143][0m Total time:      17.79 min
[32m[20221214 00:15:18 @agent_ppo2.py:145][0m 1654784 total steps have happened
[32m[20221214 00:15:18 @agent_ppo2.py:121][0m #------------------------ Iteration 4808 --------------------------#
[32m[20221214 00:15:18 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:15:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:15:18 @agent_ppo2.py:185][0m |          -0.0024 |          49.3856 |           2.1515 |
[32m[20221214 00:15:18 @agent_ppo2.py:185][0m |          -0.0090 |          44.1301 |           2.2567 |
[32m[20221214 00:15:18 @agent_ppo2.py:185][0m |          -0.0098 |          42.1680 |           2.2257 |
[32m[20221214 00:15:18 @agent_ppo2.py:185][0m |          -0.0109 |          40.9877 |           2.2583 |
[32m[20221214 00:15:19 @agent_ppo2.py:185][0m |          -0.0145 |          40.1910 |           2.2718 |
[32m[20221214 00:15:19 @agent_ppo2.py:185][0m |          -0.0168 |          39.6956 |           2.3156 |
[32m[20221214 00:15:19 @agent_ppo2.py:185][0m |          -0.0160 |          39.0729 |           2.3188 |
[32m[20221214 00:15:19 @agent_ppo2.py:185][0m |          -0.0099 |          38.5920 |           2.3144 |
[32m[20221214 00:15:19 @agent_ppo2.py:185][0m |          -0.0183 |          38.2469 |           2.2964 |
[32m[20221214 00:15:19 @agent_ppo2.py:185][0m |          -0.0108 |          38.5081 |           2.3237 |
[32m[20221214 00:15:19 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:15:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 419.02
[32m[20221214 00:15:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 543.11
[32m[20221214 00:15:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 573.05
[32m[20221214 00:15:19 @agent_ppo2.py:143][0m Total time:      17.81 min
[32m[20221214 00:15:19 @agent_ppo2.py:145][0m 1656832 total steps have happened
[32m[20221214 00:15:19 @agent_ppo2.py:121][0m #------------------------ Iteration 4809 --------------------------#
[32m[20221214 00:15:19 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:15:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:15:20 @agent_ppo2.py:185][0m |           0.0006 |          51.7119 |           1.2514 |
[32m[20221214 00:15:20 @agent_ppo2.py:185][0m |          -0.0077 |          45.9915 |           1.2789 |
[32m[20221214 00:15:20 @agent_ppo2.py:185][0m |          -0.0091 |          44.7721 |           1.2390 |
[32m[20221214 00:15:20 @agent_ppo2.py:185][0m |          -0.0122 |          43.2933 |           1.2737 |
[32m[20221214 00:15:20 @agent_ppo2.py:185][0m |          -0.0118 |          44.5444 |           1.2855 |
[32m[20221214 00:15:20 @agent_ppo2.py:185][0m |          -0.0161 |          42.3608 |           1.3422 |
[32m[20221214 00:15:20 @agent_ppo2.py:185][0m |          -0.0182 |          41.3531 |           1.3324 |
[32m[20221214 00:15:20 @agent_ppo2.py:185][0m |          -0.0129 |          41.5438 |           1.3184 |
[32m[20221214 00:15:20 @agent_ppo2.py:185][0m |          -0.0177 |          40.8486 |           1.2360 |
[32m[20221214 00:15:20 @agent_ppo2.py:185][0m |          -0.0200 |          39.8616 |           1.3290 |
[32m[20221214 00:15:20 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:15:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 512.01
[32m[20221214 00:15:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 570.89
[32m[20221214 00:15:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 552.15
[32m[20221214 00:15:20 @agent_ppo2.py:143][0m Total time:      17.84 min
[32m[20221214 00:15:20 @agent_ppo2.py:145][0m 1658880 total steps have happened
[32m[20221214 00:15:20 @agent_ppo2.py:121][0m #------------------------ Iteration 4810 --------------------------#
[32m[20221214 00:15:21 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:15:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:15:21 @agent_ppo2.py:185][0m |           0.0005 |          81.7301 |           1.8496 |
[32m[20221214 00:15:21 @agent_ppo2.py:185][0m |          -0.0087 |          78.2313 |           1.9724 |
[32m[20221214 00:15:21 @agent_ppo2.py:185][0m |          -0.0033 |          77.5016 |           1.8459 |
[32m[20221214 00:15:21 @agent_ppo2.py:185][0m |          -0.0024 |          80.2530 |           1.9384 |
[32m[20221214 00:15:21 @agent_ppo2.py:185][0m |          -0.0082 |          76.4272 |           1.8905 |
[32m[20221214 00:15:21 @agent_ppo2.py:185][0m |          -0.0128 |          75.5251 |           1.9306 |
[32m[20221214 00:15:21 @agent_ppo2.py:185][0m |          -0.0123 |          75.0945 |           1.9288 |
[32m[20221214 00:15:21 @agent_ppo2.py:185][0m |          -0.0159 |          74.7454 |           1.9566 |
[32m[20221214 00:15:22 @agent_ppo2.py:185][0m |          -0.0128 |          74.2999 |           1.9017 |
[32m[20221214 00:15:22 @agent_ppo2.py:185][0m |          -0.0164 |          74.2632 |           1.9594 |
[32m[20221214 00:15:22 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:15:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 543.01
[32m[20221214 00:15:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 568.01
[32m[20221214 00:15:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 181.68
[32m[20221214 00:15:22 @agent_ppo2.py:143][0m Total time:      17.86 min
[32m[20221214 00:15:22 @agent_ppo2.py:145][0m 1660928 total steps have happened
[32m[20221214 00:15:22 @agent_ppo2.py:121][0m #------------------------ Iteration 4811 --------------------------#
[32m[20221214 00:15:22 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:15:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:15:22 @agent_ppo2.py:185][0m |           0.0006 |          80.8176 |           0.9284 |
[32m[20221214 00:15:22 @agent_ppo2.py:185][0m |          -0.0041 |          77.3380 |           0.9281 |
[32m[20221214 00:15:22 @agent_ppo2.py:185][0m |          -0.0061 |          76.0376 |           1.0470 |
[32m[20221214 00:15:22 @agent_ppo2.py:185][0m |           0.0046 |          84.6820 |           1.0539 |
[32m[20221214 00:15:23 @agent_ppo2.py:185][0m |          -0.0106 |          75.1086 |           1.1705 |
[32m[20221214 00:15:23 @agent_ppo2.py:185][0m |          -0.0132 |          74.2884 |           1.1520 |
[32m[20221214 00:15:23 @agent_ppo2.py:185][0m |          -0.0026 |          81.6771 |           1.1944 |
[32m[20221214 00:15:23 @agent_ppo2.py:185][0m |          -0.0121 |          73.8961 |           1.3799 |
[32m[20221214 00:15:23 @agent_ppo2.py:185][0m |          -0.0134 |          73.2414 |           1.2661 |
[32m[20221214 00:15:23 @agent_ppo2.py:185][0m |          -0.0156 |          73.1510 |           1.2351 |
[32m[20221214 00:15:23 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:15:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 545.05
[32m[20221214 00:15:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 584.90
[32m[20221214 00:15:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 578.16
[32m[20221214 00:15:23 @agent_ppo2.py:143][0m Total time:      17.88 min
[32m[20221214 00:15:23 @agent_ppo2.py:145][0m 1662976 total steps have happened
[32m[20221214 00:15:23 @agent_ppo2.py:121][0m #------------------------ Iteration 4812 --------------------------#
[32m[20221214 00:15:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:15:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:15:23 @agent_ppo2.py:185][0m |          -0.0018 |          81.9259 |           1.4880 |
[32m[20221214 00:15:24 @agent_ppo2.py:185][0m |          -0.0064 |          75.3509 |           1.5332 |
[32m[20221214 00:15:24 @agent_ppo2.py:185][0m |          -0.0099 |          73.8344 |           1.5034 |
[32m[20221214 00:15:24 @agent_ppo2.py:185][0m |          -0.0090 |          72.5107 |           1.4845 |
[32m[20221214 00:15:24 @agent_ppo2.py:185][0m |          -0.0125 |          71.8680 |           1.5537 |
[32m[20221214 00:15:24 @agent_ppo2.py:185][0m |          -0.0138 |          71.4528 |           1.5767 |
[32m[20221214 00:15:24 @agent_ppo2.py:185][0m |          -0.0015 |          75.0115 |           1.6344 |
[32m[20221214 00:15:24 @agent_ppo2.py:185][0m |          -0.0064 |          71.9197 |           1.5798 |
[32m[20221214 00:15:24 @agent_ppo2.py:185][0m |          -0.0106 |          71.3496 |           1.5593 |
[32m[20221214 00:15:24 @agent_ppo2.py:185][0m |          -0.0144 |          69.5126 |           1.6119 |
[32m[20221214 00:15:24 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:15:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 534.79
[32m[20221214 00:15:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 574.68
[32m[20221214 00:15:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 507.13
[32m[20221214 00:15:24 @agent_ppo2.py:143][0m Total time:      17.90 min
[32m[20221214 00:15:24 @agent_ppo2.py:145][0m 1665024 total steps have happened
[32m[20221214 00:15:24 @agent_ppo2.py:121][0m #------------------------ Iteration 4813 --------------------------#
[32m[20221214 00:15:25 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:15:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:15:25 @agent_ppo2.py:185][0m |          -0.0013 |          89.0288 |           2.7763 |
[32m[20221214 00:15:25 @agent_ppo2.py:185][0m |          -0.0019 |          86.4861 |           2.9045 |
[32m[20221214 00:15:25 @agent_ppo2.py:185][0m |          -0.0004 |          88.0866 |           2.9324 |
[32m[20221214 00:15:25 @agent_ppo2.py:185][0m |           0.0002 |          86.0203 |           3.0414 |
[32m[20221214 00:15:25 @agent_ppo2.py:185][0m |          -0.0081 |          83.0259 |           3.0466 |
[32m[20221214 00:15:25 @agent_ppo2.py:185][0m |          -0.0114 |          82.4640 |           3.0428 |
[32m[20221214 00:15:25 @agent_ppo2.py:185][0m |          -0.0100 |          81.8121 |           3.0867 |
[32m[20221214 00:15:25 @agent_ppo2.py:185][0m |          -0.0107 |          81.6892 |           3.0851 |
[32m[20221214 00:15:26 @agent_ppo2.py:185][0m |          -0.0132 |          81.2846 |           3.0924 |
[32m[20221214 00:15:26 @agent_ppo2.py:185][0m |          -0.0114 |          80.9202 |           3.1894 |
[32m[20221214 00:15:26 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:15:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 537.40
[32m[20221214 00:15:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 566.31
[32m[20221214 00:15:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 275.74
[32m[20221214 00:15:26 @agent_ppo2.py:143][0m Total time:      17.92 min
[32m[20221214 00:15:26 @agent_ppo2.py:145][0m 1667072 total steps have happened
[32m[20221214 00:15:26 @agent_ppo2.py:121][0m #------------------------ Iteration 4814 --------------------------#
[32m[20221214 00:15:26 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:15:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:15:26 @agent_ppo2.py:185][0m |           0.0025 |          69.8389 |           2.6132 |
[32m[20221214 00:15:26 @agent_ppo2.py:185][0m |          -0.0098 |          56.8296 |           2.6519 |
[32m[20221214 00:15:26 @agent_ppo2.py:185][0m |          -0.0020 |          53.0016 |           2.6792 |
[32m[20221214 00:15:26 @agent_ppo2.py:185][0m |          -0.0140 |          49.8456 |           2.7474 |
[32m[20221214 00:15:26 @agent_ppo2.py:185][0m |          -0.0140 |          48.1077 |           2.7964 |
[32m[20221214 00:15:27 @agent_ppo2.py:185][0m |          -0.0154 |          46.8753 |           2.6908 |
[32m[20221214 00:15:27 @agent_ppo2.py:185][0m |          -0.0186 |          45.8239 |           2.7588 |
[32m[20221214 00:15:27 @agent_ppo2.py:185][0m |          -0.0242 |          45.0720 |           2.7620 |
[32m[20221214 00:15:27 @agent_ppo2.py:185][0m |          -0.0210 |          44.2025 |           2.7403 |
[32m[20221214 00:15:27 @agent_ppo2.py:185][0m |          -0.0221 |          43.7511 |           2.7004 |
[32m[20221214 00:15:27 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:15:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 502.77
[32m[20221214 00:15:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 548.71
[32m[20221214 00:15:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 583.43
[32m[20221214 00:15:27 @agent_ppo2.py:143][0m Total time:      17.94 min
[32m[20221214 00:15:27 @agent_ppo2.py:145][0m 1669120 total steps have happened
[32m[20221214 00:15:27 @agent_ppo2.py:121][0m #------------------------ Iteration 4815 --------------------------#
[32m[20221214 00:15:27 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:15:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:15:27 @agent_ppo2.py:185][0m |          -0.0033 |          84.6948 |           2.7536 |
[32m[20221214 00:15:27 @agent_ppo2.py:185][0m |          -0.0055 |          72.6238 |           2.8000 |
[32m[20221214 00:15:28 @agent_ppo2.py:185][0m |          -0.0029 |          68.7531 |           2.8462 |
[32m[20221214 00:15:28 @agent_ppo2.py:185][0m |          -0.0053 |          66.6944 |           2.7826 |
[32m[20221214 00:15:28 @agent_ppo2.py:185][0m |          -0.0056 |          65.7317 |           2.8100 |
[32m[20221214 00:15:28 @agent_ppo2.py:185][0m |          -0.0085 |          65.1506 |           2.8254 |
[32m[20221214 00:15:28 @agent_ppo2.py:185][0m |          -0.0071 |          63.8656 |           2.8139 |
[32m[20221214 00:15:28 @agent_ppo2.py:185][0m |          -0.0143 |          63.5689 |           2.8156 |
[32m[20221214 00:15:28 @agent_ppo2.py:185][0m |          -0.0133 |          62.5787 |           2.8828 |
[32m[20221214 00:15:28 @agent_ppo2.py:185][0m |          -0.0160 |          62.4716 |           2.8661 |
[32m[20221214 00:15:28 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:15:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 513.05
[32m[20221214 00:15:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 570.67
[32m[20221214 00:15:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 576.88
[32m[20221214 00:15:28 @agent_ppo2.py:143][0m Total time:      17.97 min
[32m[20221214 00:15:28 @agent_ppo2.py:145][0m 1671168 total steps have happened
[32m[20221214 00:15:28 @agent_ppo2.py:121][0m #------------------------ Iteration 4816 --------------------------#
[32m[20221214 00:15:29 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:15:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:15:29 @agent_ppo2.py:185][0m |           0.0023 |          65.0384 |           2.4459 |
[32m[20221214 00:15:29 @agent_ppo2.py:185][0m |          -0.0063 |          57.2013 |           2.4637 |
[32m[20221214 00:15:29 @agent_ppo2.py:185][0m |          -0.0117 |          55.6131 |           2.4959 |
[32m[20221214 00:15:29 @agent_ppo2.py:185][0m |          -0.0095 |          54.3826 |           2.4061 |
[32m[20221214 00:15:29 @agent_ppo2.py:185][0m |          -0.0149 |          53.9010 |           2.3902 |
[32m[20221214 00:15:29 @agent_ppo2.py:185][0m |          -0.0190 |          53.0669 |           2.3523 |
[32m[20221214 00:15:29 @agent_ppo2.py:185][0m |          -0.0133 |          52.5088 |           2.3561 |
[32m[20221214 00:15:29 @agent_ppo2.py:185][0m |          -0.0187 |          51.9981 |           2.4030 |
[32m[20221214 00:15:29 @agent_ppo2.py:185][0m |          -0.0145 |          51.9666 |           2.3703 |
[32m[20221214 00:15:30 @agent_ppo2.py:185][0m |          -0.0205 |          51.2215 |           2.3599 |
[32m[20221214 00:15:30 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:15:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 447.73
[32m[20221214 00:15:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 530.73
[32m[20221214 00:15:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 581.71
[32m[20221214 00:15:30 @agent_ppo2.py:143][0m Total time:      17.99 min
[32m[20221214 00:15:30 @agent_ppo2.py:145][0m 1673216 total steps have happened
[32m[20221214 00:15:30 @agent_ppo2.py:121][0m #------------------------ Iteration 4817 --------------------------#
[32m[20221214 00:15:30 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:15:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:15:30 @agent_ppo2.py:185][0m |          -0.0005 |          89.5184 |           2.3296 |
[32m[20221214 00:15:30 @agent_ppo2.py:185][0m |           0.0089 |          87.5659 |           2.2421 |
[32m[20221214 00:15:30 @agent_ppo2.py:185][0m |          -0.0081 |          80.6557 |           2.2718 |
[32m[20221214 00:15:30 @agent_ppo2.py:185][0m |          -0.0089 |          79.5241 |           2.1898 |
[32m[20221214 00:15:30 @agent_ppo2.py:185][0m |          -0.0122 |          78.2651 |           2.1192 |
[32m[20221214 00:15:30 @agent_ppo2.py:185][0m |          -0.0183 |          77.7814 |           2.1536 |
[32m[20221214 00:15:31 @agent_ppo2.py:185][0m |          -0.0114 |          77.8531 |           2.1118 |
[32m[20221214 00:15:31 @agent_ppo2.py:185][0m |          -0.0104 |          77.7000 |           2.1219 |
[32m[20221214 00:15:31 @agent_ppo2.py:185][0m |          -0.0189 |          76.4505 |           2.1200 |
[32m[20221214 00:15:31 @agent_ppo2.py:185][0m |          -0.0197 |          75.6413 |           2.0576 |
[32m[20221214 00:15:31 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:15:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 416.54
[32m[20221214 00:15:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 472.13
[32m[20221214 00:15:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 508.86
[32m[20221214 00:15:31 @agent_ppo2.py:143][0m Total time:      18.01 min
[32m[20221214 00:15:31 @agent_ppo2.py:145][0m 1675264 total steps have happened
[32m[20221214 00:15:31 @agent_ppo2.py:121][0m #------------------------ Iteration 4818 --------------------------#
[32m[20221214 00:15:31 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:15:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:15:31 @agent_ppo2.py:185][0m |           0.0022 |          80.3507 |           2.1822 |
[32m[20221214 00:15:31 @agent_ppo2.py:185][0m |          -0.0038 |          73.0244 |           2.0243 |
[32m[20221214 00:15:32 @agent_ppo2.py:185][0m |          -0.0095 |          69.8883 |           1.9891 |
[32m[20221214 00:15:32 @agent_ppo2.py:185][0m |          -0.0078 |          68.2270 |           1.9293 |
[32m[20221214 00:15:32 @agent_ppo2.py:185][0m |          -0.0134 |          65.5727 |           2.0138 |
[32m[20221214 00:15:32 @agent_ppo2.py:185][0m |          -0.0163 |          63.8311 |           1.9146 |
[32m[20221214 00:15:32 @agent_ppo2.py:185][0m |          -0.0174 |          64.1617 |           1.8690 |
[32m[20221214 00:15:32 @agent_ppo2.py:185][0m |          -0.0155 |          62.1592 |           1.8520 |
[32m[20221214 00:15:32 @agent_ppo2.py:185][0m |          -0.0114 |          64.6537 |           1.8335 |
[32m[20221214 00:15:32 @agent_ppo2.py:185][0m |          -0.0194 |          59.7590 |           1.8367 |
[32m[20221214 00:15:32 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:15:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 483.77
[32m[20221214 00:15:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 542.92
[32m[20221214 00:15:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 550.74
[32m[20221214 00:15:32 @agent_ppo2.py:143][0m Total time:      18.03 min
[32m[20221214 00:15:32 @agent_ppo2.py:145][0m 1677312 total steps have happened
[32m[20221214 00:15:32 @agent_ppo2.py:121][0m #------------------------ Iteration 4819 --------------------------#
[32m[20221214 00:15:32 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:15:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:15:33 @agent_ppo2.py:185][0m |           0.0015 |          77.9691 |           1.8982 |
[32m[20221214 00:15:33 @agent_ppo2.py:185][0m |          -0.0058 |          66.1595 |           2.1539 |
[32m[20221214 00:15:33 @agent_ppo2.py:185][0m |          -0.0126 |          63.0345 |           2.1507 |
[32m[20221214 00:15:33 @agent_ppo2.py:185][0m |          -0.0124 |          61.3455 |           2.2418 |
[32m[20221214 00:15:33 @agent_ppo2.py:185][0m |          -0.0161 |          60.4013 |           2.2180 |
[32m[20221214 00:15:33 @agent_ppo2.py:185][0m |          -0.0122 |          59.6218 |           2.2553 |
[32m[20221214 00:15:33 @agent_ppo2.py:185][0m |          -0.0145 |          58.9575 |           2.2776 |
[32m[20221214 00:15:33 @agent_ppo2.py:185][0m |          -0.0192 |          58.1215 |           2.2655 |
[32m[20221214 00:15:33 @agent_ppo2.py:185][0m |          -0.0194 |          57.7764 |           2.3354 |
[32m[20221214 00:15:34 @agent_ppo2.py:185][0m |          -0.0136 |          57.3272 |           2.3665 |
[32m[20221214 00:15:34 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:15:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 456.34
[32m[20221214 00:15:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 534.81
[32m[20221214 00:15:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 497.68
[32m[20221214 00:15:34 @agent_ppo2.py:143][0m Total time:      18.06 min
[32m[20221214 00:15:34 @agent_ppo2.py:145][0m 1679360 total steps have happened
[32m[20221214 00:15:34 @agent_ppo2.py:121][0m #------------------------ Iteration 4820 --------------------------#
[32m[20221214 00:15:34 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221214 00:15:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:15:34 @agent_ppo2.py:185][0m |          -0.0029 |          66.4808 |           1.2529 |
[32m[20221214 00:15:34 @agent_ppo2.py:185][0m |          -0.0007 |          62.8577 |           1.2690 |
[32m[20221214 00:15:34 @agent_ppo2.py:185][0m |          -0.0072 |          58.5811 |           1.4232 |
[32m[20221214 00:15:34 @agent_ppo2.py:185][0m |          -0.0079 |          57.5958 |           1.2770 |
[32m[20221214 00:15:35 @agent_ppo2.py:185][0m |          -0.0122 |          56.4929 |           1.2933 |
[32m[20221214 00:15:35 @agent_ppo2.py:185][0m |          -0.0129 |          55.9622 |           1.2195 |
[32m[20221214 00:15:35 @agent_ppo2.py:185][0m |          -0.0149 |          55.6073 |           1.2622 |
[32m[20221214 00:15:35 @agent_ppo2.py:185][0m |          -0.0194 |          55.2090 |           1.2959 |
[32m[20221214 00:15:35 @agent_ppo2.py:185][0m |          -0.0141 |          54.8564 |           1.2663 |
[32m[20221214 00:15:35 @agent_ppo2.py:185][0m |          -0.0090 |          58.9945 |           1.3013 |
[32m[20221214 00:15:35 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:15:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 483.18
[32m[20221214 00:15:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 577.50
[32m[20221214 00:15:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 626.35
[32m[20221214 00:15:35 @agent_ppo2.py:143][0m Total time:      18.08 min
[32m[20221214 00:15:35 @agent_ppo2.py:145][0m 1681408 total steps have happened
[32m[20221214 00:15:35 @agent_ppo2.py:121][0m #------------------------ Iteration 4821 --------------------------#
[32m[20221214 00:15:35 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:15:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:15:35 @agent_ppo2.py:185][0m |          -0.0042 |          44.4368 |           2.0215 |
[32m[20221214 00:15:36 @agent_ppo2.py:185][0m |          -0.0008 |          38.2116 |           1.9679 |
[32m[20221214 00:15:36 @agent_ppo2.py:185][0m |          -0.0142 |          36.6270 |           2.0070 |
[32m[20221214 00:15:36 @agent_ppo2.py:185][0m |          -0.0090 |          35.7334 |           1.9527 |
[32m[20221214 00:15:36 @agent_ppo2.py:185][0m |          -0.0120 |          35.0176 |           1.9034 |
[32m[20221214 00:15:36 @agent_ppo2.py:185][0m |          -0.0131 |          34.5709 |           1.9373 |
[32m[20221214 00:15:36 @agent_ppo2.py:185][0m |          -0.0162 |          34.2610 |           1.8578 |
[32m[20221214 00:15:36 @agent_ppo2.py:185][0m |          -0.0155 |          33.8372 |           1.8333 |
[32m[20221214 00:15:36 @agent_ppo2.py:185][0m |          -0.0162 |          33.7146 |           1.7027 |
[32m[20221214 00:15:36 @agent_ppo2.py:185][0m |          -0.0253 |          33.3925 |           1.7188 |
[32m[20221214 00:15:36 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:15:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 452.24
[32m[20221214 00:15:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 527.39
[32m[20221214 00:15:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 520.53
[32m[20221214 00:15:36 @agent_ppo2.py:143][0m Total time:      18.10 min
[32m[20221214 00:15:36 @agent_ppo2.py:145][0m 1683456 total steps have happened
[32m[20221214 00:15:36 @agent_ppo2.py:121][0m #------------------------ Iteration 4822 --------------------------#
[32m[20221214 00:15:37 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:15:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:15:37 @agent_ppo2.py:185][0m |           0.0044 |          74.1148 |           1.4467 |
[32m[20221214 00:15:37 @agent_ppo2.py:185][0m |          -0.0094 |          62.9932 |           1.5764 |
[32m[20221214 00:15:37 @agent_ppo2.py:185][0m |          -0.0134 |          60.6853 |           1.5010 |
[32m[20221214 00:15:37 @agent_ppo2.py:185][0m |           0.0070 |          64.0446 |           1.6122 |
[32m[20221214 00:15:37 @agent_ppo2.py:185][0m |          -0.0152 |          57.8220 |           1.6710 |
[32m[20221214 00:15:37 @agent_ppo2.py:185][0m |          -0.0190 |          56.6376 |           1.6797 |
[32m[20221214 00:15:37 @agent_ppo2.py:185][0m |          -0.0111 |          58.3291 |           1.7469 |
[32m[20221214 00:15:37 @agent_ppo2.py:185][0m |          -0.0169 |          55.0989 |           1.8432 |
[32m[20221214 00:15:38 @agent_ppo2.py:185][0m |          -0.0138 |          55.2571 |           1.8686 |
[32m[20221214 00:15:38 @agent_ppo2.py:185][0m |          -0.0203 |          53.8787 |           1.9326 |
[32m[20221214 00:15:38 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:15:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 534.20
[32m[20221214 00:15:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 603.10
[32m[20221214 00:15:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 374.32
[32m[20221214 00:15:38 @agent_ppo2.py:143][0m Total time:      18.12 min
[32m[20221214 00:15:38 @agent_ppo2.py:145][0m 1685504 total steps have happened
[32m[20221214 00:15:38 @agent_ppo2.py:121][0m #------------------------ Iteration 4823 --------------------------#
[32m[20221214 00:15:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:15:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:15:38 @agent_ppo2.py:185][0m |           0.0002 |          83.7425 |           1.7977 |
[32m[20221214 00:15:38 @agent_ppo2.py:185][0m |          -0.0085 |          75.3203 |           1.8365 |
[32m[20221214 00:15:38 @agent_ppo2.py:185][0m |           0.0081 |          82.0658 |           1.8256 |
[32m[20221214 00:15:38 @agent_ppo2.py:185][0m |          -0.0074 |          71.4880 |           1.9405 |
[32m[20221214 00:15:38 @agent_ppo2.py:185][0m |          -0.0132 |          70.5779 |           1.8037 |
[32m[20221214 00:15:39 @agent_ppo2.py:185][0m |          -0.0044 |          79.4512 |           1.6905 |
[32m[20221214 00:15:39 @agent_ppo2.py:185][0m |          -0.0156 |          69.5700 |           1.6698 |
[32m[20221214 00:15:39 @agent_ppo2.py:185][0m |          -0.0210 |          68.9964 |           1.6419 |
[32m[20221214 00:15:39 @agent_ppo2.py:185][0m |          -0.0205 |          68.8538 |           1.6181 |
[32m[20221214 00:15:39 @agent_ppo2.py:185][0m |          -0.0205 |          68.2022 |           1.6275 |
[32m[20221214 00:15:39 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:15:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 427.33
[32m[20221214 00:15:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 542.60
[32m[20221214 00:15:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 511.83
[32m[20221214 00:15:39 @agent_ppo2.py:143][0m Total time:      18.15 min
[32m[20221214 00:15:39 @agent_ppo2.py:145][0m 1687552 total steps have happened
[32m[20221214 00:15:39 @agent_ppo2.py:121][0m #------------------------ Iteration 4824 --------------------------#
[32m[20221214 00:15:39 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:15:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:15:39 @agent_ppo2.py:185][0m |           0.0118 |          88.8675 |           0.9074 |
[32m[20221214 00:15:39 @agent_ppo2.py:185][0m |          -0.0021 |          77.8384 |           1.0178 |
[32m[20221214 00:15:40 @agent_ppo2.py:185][0m |          -0.0063 |          75.1465 |           1.0668 |
[32m[20221214 00:15:40 @agent_ppo2.py:185][0m |          -0.0091 |          73.8321 |           1.2555 |
[32m[20221214 00:15:40 @agent_ppo2.py:185][0m |          -0.0093 |          72.9942 |           1.1073 |
[32m[20221214 00:15:40 @agent_ppo2.py:185][0m |          -0.0020 |          78.1231 |           1.1957 |
[32m[20221214 00:15:40 @agent_ppo2.py:185][0m |          -0.0134 |          71.2644 |           1.2567 |
[32m[20221214 00:15:40 @agent_ppo2.py:185][0m |          -0.0150 |          70.9039 |           1.2942 |
[32m[20221214 00:15:40 @agent_ppo2.py:185][0m |          -0.0132 |          70.1033 |           1.3112 |
[32m[20221214 00:15:40 @agent_ppo2.py:185][0m |          -0.0121 |          69.9507 |           1.4118 |
[32m[20221214 00:15:40 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:15:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 484.94
[32m[20221214 00:15:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 582.53
[32m[20221214 00:15:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 519.81
[32m[20221214 00:15:40 @agent_ppo2.py:143][0m Total time:      18.17 min
[32m[20221214 00:15:40 @agent_ppo2.py:145][0m 1689600 total steps have happened
[32m[20221214 00:15:40 @agent_ppo2.py:121][0m #------------------------ Iteration 4825 --------------------------#
[32m[20221214 00:15:41 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:15:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:15:41 @agent_ppo2.py:185][0m |          -0.0017 |          69.8907 |           0.6431 |
[32m[20221214 00:15:41 @agent_ppo2.py:185][0m |          -0.0060 |          56.4390 |           0.7273 |
[32m[20221214 00:15:41 @agent_ppo2.py:185][0m |          -0.0045 |          52.4345 |           0.6603 |
[32m[20221214 00:15:41 @agent_ppo2.py:185][0m |          -0.0149 |          50.0000 |           0.7211 |
[32m[20221214 00:15:41 @agent_ppo2.py:185][0m |          -0.0138 |          48.6177 |           0.7181 |
[32m[20221214 00:15:41 @agent_ppo2.py:185][0m |          -0.0127 |          46.4285 |           0.7984 |
[32m[20221214 00:15:41 @agent_ppo2.py:185][0m |          -0.0165 |          45.0481 |           0.7346 |
[32m[20221214 00:15:41 @agent_ppo2.py:185][0m |          -0.0125 |          44.6402 |           0.7123 |
[32m[20221214 00:15:41 @agent_ppo2.py:185][0m |          -0.0160 |          44.0427 |           0.7491 |
[32m[20221214 00:15:42 @agent_ppo2.py:185][0m |          -0.0177 |          43.1168 |           0.6936 |
[32m[20221214 00:15:42 @agent_ppo2.py:130][0m Policy update time: 0.97 s
[32m[20221214 00:15:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 520.04
[32m[20221214 00:15:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 587.70
[32m[20221214 00:15:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 544.84
[32m[20221214 00:15:42 @agent_ppo2.py:143][0m Total time:      18.19 min
[32m[20221214 00:15:42 @agent_ppo2.py:145][0m 1691648 total steps have happened
[32m[20221214 00:15:42 @agent_ppo2.py:121][0m #------------------------ Iteration 4826 --------------------------#
[32m[20221214 00:15:42 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:15:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:15:42 @agent_ppo2.py:185][0m |           0.0056 |          83.3052 |           1.5725 |
[32m[20221214 00:15:42 @agent_ppo2.py:185][0m |          -0.0107 |          62.6415 |           1.6690 |
[32m[20221214 00:15:42 @agent_ppo2.py:185][0m |          -0.0009 |          59.8529 |           1.6762 |
[32m[20221214 00:15:42 @agent_ppo2.py:185][0m |          -0.0013 |          58.1568 |           1.7015 |
[32m[20221214 00:15:42 @agent_ppo2.py:185][0m |          -0.0065 |          57.0067 |           1.6438 |
[32m[20221214 00:15:42 @agent_ppo2.py:185][0m |          -0.0082 |          56.4547 |           1.7972 |
[32m[20221214 00:15:43 @agent_ppo2.py:185][0m |          -0.0108 |          56.5861 |           1.7549 |
[32m[20221214 00:15:43 @agent_ppo2.py:185][0m |          -0.0090 |          55.3990 |           1.8010 |
[32m[20221214 00:15:43 @agent_ppo2.py:185][0m |          -0.0047 |          55.1643 |           1.8067 |
[32m[20221214 00:15:43 @agent_ppo2.py:185][0m |          -0.0052 |          56.4498 |           1.8595 |
[32m[20221214 00:15:43 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:15:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 535.50
[32m[20221214 00:15:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 580.75
[32m[20221214 00:15:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 608.66
[32m[20221214 00:15:43 @agent_ppo2.py:143][0m Total time:      18.21 min
[32m[20221214 00:15:43 @agent_ppo2.py:145][0m 1693696 total steps have happened
[32m[20221214 00:15:43 @agent_ppo2.py:121][0m #------------------------ Iteration 4827 --------------------------#
[32m[20221214 00:15:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:15:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:15:43 @agent_ppo2.py:185][0m |          -0.0008 |          51.7009 |           2.4423 |
[32m[20221214 00:15:43 @agent_ppo2.py:185][0m |          -0.0044 |          41.1640 |           2.3516 |
[32m[20221214 00:15:43 @agent_ppo2.py:185][0m |          -0.0064 |          38.0337 |           2.4255 |
[32m[20221214 00:15:44 @agent_ppo2.py:185][0m |          -0.0061 |          37.1897 |           2.3506 |
[32m[20221214 00:15:44 @agent_ppo2.py:185][0m |          -0.0199 |          35.7805 |           2.3778 |
[32m[20221214 00:15:44 @agent_ppo2.py:185][0m |          -0.0082 |          34.5691 |           2.3684 |
[32m[20221214 00:15:44 @agent_ppo2.py:185][0m |          -0.0073 |          34.4634 |           2.2909 |
[32m[20221214 00:15:44 @agent_ppo2.py:185][0m |          -0.0116 |          33.5706 |           2.3878 |
[32m[20221214 00:15:44 @agent_ppo2.py:185][0m |          -0.0183 |          32.8200 |           2.2829 |
[32m[20221214 00:15:44 @agent_ppo2.py:185][0m |          -0.0221 |          32.3992 |           2.2837 |
[32m[20221214 00:15:44 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:15:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 335.13
[32m[20221214 00:15:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 479.43
[32m[20221214 00:15:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 549.88
[32m[20221214 00:15:44 @agent_ppo2.py:143][0m Total time:      18.23 min
[32m[20221214 00:15:44 @agent_ppo2.py:145][0m 1695744 total steps have happened
[32m[20221214 00:15:44 @agent_ppo2.py:121][0m #------------------------ Iteration 4828 --------------------------#
[32m[20221214 00:15:44 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:15:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:15:45 @agent_ppo2.py:185][0m |           0.0019 |          63.7815 |           2.0743 |
[32m[20221214 00:15:45 @agent_ppo2.py:185][0m |          -0.0096 |          56.3312 |           2.0404 |
[32m[20221214 00:15:45 @agent_ppo2.py:185][0m |          -0.0104 |          54.4551 |           2.0637 |
[32m[20221214 00:15:45 @agent_ppo2.py:185][0m |          -0.0138 |          54.0434 |           1.9517 |
[32m[20221214 00:15:45 @agent_ppo2.py:185][0m |          -0.0082 |          52.1217 |           1.9201 |
[32m[20221214 00:15:45 @agent_ppo2.py:185][0m |          -0.0176 |          51.1302 |           1.9718 |
[32m[20221214 00:15:45 @agent_ppo2.py:185][0m |          -0.0172 |          50.2545 |           1.9464 |
[32m[20221214 00:15:45 @agent_ppo2.py:185][0m |          -0.0123 |          50.3037 |           1.9103 |
[32m[20221214 00:15:45 @agent_ppo2.py:185][0m |          -0.0172 |          48.7490 |           1.9443 |
[32m[20221214 00:15:45 @agent_ppo2.py:185][0m |          -0.0141 |          48.8550 |           1.9102 |
[32m[20221214 00:15:45 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:15:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 425.92
[32m[20221214 00:15:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 485.01
[32m[20221214 00:15:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 332.28
[32m[20221214 00:15:46 @agent_ppo2.py:143][0m Total time:      18.25 min
[32m[20221214 00:15:46 @agent_ppo2.py:145][0m 1697792 total steps have happened
[32m[20221214 00:15:46 @agent_ppo2.py:121][0m #------------------------ Iteration 4829 --------------------------#
[32m[20221214 00:15:46 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:15:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:15:46 @agent_ppo2.py:185][0m |           0.0151 |          83.3548 |           1.2112 |
[32m[20221214 00:15:46 @agent_ppo2.py:185][0m |          -0.0062 |          66.4815 |           1.3210 |
[32m[20221214 00:15:46 @agent_ppo2.py:185][0m |          -0.0077 |          62.9573 |           1.2871 |
[32m[20221214 00:15:46 @agent_ppo2.py:185][0m |          -0.0151 |          61.0180 |           1.3016 |
[32m[20221214 00:15:46 @agent_ppo2.py:185][0m |          -0.0078 |          63.2390 |           1.3758 |
[32m[20221214 00:15:46 @agent_ppo2.py:185][0m |          -0.0134 |          58.7138 |           1.3716 |
[32m[20221214 00:15:46 @agent_ppo2.py:185][0m |          -0.0172 |          57.7157 |           1.3964 |
[32m[20221214 00:15:47 @agent_ppo2.py:185][0m |          -0.0128 |          58.1253 |           1.4343 |
[32m[20221214 00:15:47 @agent_ppo2.py:185][0m |          -0.0160 |          56.5509 |           1.4450 |
[32m[20221214 00:15:47 @agent_ppo2.py:185][0m |          -0.0202 |          56.4359 |           1.4084 |
[32m[20221214 00:15:47 @agent_ppo2.py:130][0m Policy update time: 0.98 s
[32m[20221214 00:15:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 469.78
[32m[20221214 00:15:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 597.75
[32m[20221214 00:15:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 627.96
[32m[20221214 00:15:47 @agent_ppo2.py:143][0m Total time:      18.28 min
[32m[20221214 00:15:47 @agent_ppo2.py:145][0m 1699840 total steps have happened
[32m[20221214 00:15:47 @agent_ppo2.py:121][0m #------------------------ Iteration 4830 --------------------------#
[32m[20221214 00:15:47 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:15:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:15:47 @agent_ppo2.py:185][0m |           0.0082 |          59.7653 |           2.1720 |
[32m[20221214 00:15:47 @agent_ppo2.py:185][0m |          -0.0050 |          49.2001 |           2.2018 |
[32m[20221214 00:15:47 @agent_ppo2.py:185][0m |          -0.0060 |          46.4615 |           2.0977 |
[32m[20221214 00:15:48 @agent_ppo2.py:185][0m |          -0.0062 |          44.4352 |           2.1840 |
[32m[20221214 00:15:48 @agent_ppo2.py:185][0m |          -0.0126 |          43.5881 |           2.1553 |
[32m[20221214 00:15:48 @agent_ppo2.py:185][0m |          -0.0122 |          42.0568 |           2.1278 |
[32m[20221214 00:15:48 @agent_ppo2.py:185][0m |          -0.0143 |          41.3379 |           2.0802 |
[32m[20221214 00:15:48 @agent_ppo2.py:185][0m |          -0.0141 |          40.1510 |           2.1156 |
[32m[20221214 00:15:48 @agent_ppo2.py:185][0m |          -0.0134 |          39.6786 |           2.0527 |
[32m[20221214 00:15:48 @agent_ppo2.py:185][0m |          -0.0169 |          39.2012 |           2.0242 |
[32m[20221214 00:15:48 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:15:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 430.22
[32m[20221214 00:15:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 538.00
[32m[20221214 00:15:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 301.73
[32m[20221214 00:15:48 @agent_ppo2.py:143][0m Total time:      18.30 min
[32m[20221214 00:15:48 @agent_ppo2.py:145][0m 1701888 total steps have happened
[32m[20221214 00:15:48 @agent_ppo2.py:121][0m #------------------------ Iteration 4831 --------------------------#
[32m[20221214 00:15:48 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:15:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:15:49 @agent_ppo2.py:185][0m |           0.0154 |          66.0001 |           0.7520 |
[32m[20221214 00:15:49 @agent_ppo2.py:185][0m |          -0.0094 |          56.4058 |           0.7725 |
[32m[20221214 00:15:49 @agent_ppo2.py:185][0m |          -0.0058 |          55.5869 |           0.9636 |
[32m[20221214 00:15:49 @agent_ppo2.py:185][0m |          -0.0065 |          53.5915 |           0.9814 |
[32m[20221214 00:15:49 @agent_ppo2.py:185][0m |          -0.0105 |          52.8391 |           0.9725 |
[32m[20221214 00:15:49 @agent_ppo2.py:185][0m |          -0.0101 |          52.9347 |           0.9970 |
[32m[20221214 00:15:49 @agent_ppo2.py:185][0m |          -0.0129 |          51.9622 |           1.0536 |
[32m[20221214 00:15:49 @agent_ppo2.py:185][0m |          -0.0132 |          51.6176 |           1.1388 |
[32m[20221214 00:15:49 @agent_ppo2.py:185][0m |          -0.0175 |          51.6982 |           1.1192 |
[32m[20221214 00:15:50 @agent_ppo2.py:185][0m |          -0.0056 |          51.6313 |           1.1446 |
[32m[20221214 00:15:50 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:15:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 506.88
[32m[20221214 00:15:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 605.08
[32m[20221214 00:15:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 505.52
[32m[20221214 00:15:50 @agent_ppo2.py:143][0m Total time:      18.32 min
[32m[20221214 00:15:50 @agent_ppo2.py:145][0m 1703936 total steps have happened
[32m[20221214 00:15:50 @agent_ppo2.py:121][0m #------------------------ Iteration 4832 --------------------------#
[32m[20221214 00:15:50 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:15:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:15:50 @agent_ppo2.py:185][0m |          -0.0006 |          72.7814 |           0.8767 |
[32m[20221214 00:15:50 @agent_ppo2.py:185][0m |          -0.0044 |          67.4625 |           0.8665 |
[32m[20221214 00:15:50 @agent_ppo2.py:185][0m |          -0.0071 |          65.1610 |           0.9065 |
[32m[20221214 00:15:50 @agent_ppo2.py:185][0m |          -0.0121 |          63.1291 |           0.9821 |
[32m[20221214 00:15:50 @agent_ppo2.py:185][0m |          -0.0159 |          62.1968 |           0.9922 |
[32m[20221214 00:15:50 @agent_ppo2.py:185][0m |          -0.0132 |          60.8523 |           0.9744 |
[32m[20221214 00:15:51 @agent_ppo2.py:185][0m |          -0.0168 |          60.3775 |           0.9540 |
[32m[20221214 00:15:51 @agent_ppo2.py:185][0m |          -0.0185 |          59.6855 |           1.0416 |
[32m[20221214 00:15:51 @agent_ppo2.py:185][0m |          -0.0094 |          60.1162 |           0.9732 |
[32m[20221214 00:15:51 @agent_ppo2.py:185][0m |          -0.0205 |          59.1372 |           1.0377 |
[32m[20221214 00:15:51 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:15:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 504.37
[32m[20221214 00:15:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 580.97
[32m[20221214 00:15:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 613.00
[32m[20221214 00:15:51 @agent_ppo2.py:143][0m Total time:      18.34 min
[32m[20221214 00:15:51 @agent_ppo2.py:145][0m 1705984 total steps have happened
[32m[20221214 00:15:51 @agent_ppo2.py:121][0m #------------------------ Iteration 4833 --------------------------#
[32m[20221214 00:15:51 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:15:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:15:51 @agent_ppo2.py:185][0m |           0.0180 |          69.6955 |           2.0675 |
[32m[20221214 00:15:51 @agent_ppo2.py:185][0m |          -0.0085 |          58.3943 |           2.2326 |
[32m[20221214 00:15:52 @agent_ppo2.py:185][0m |          -0.0065 |          56.0974 |           2.2542 |
[32m[20221214 00:15:52 @agent_ppo2.py:185][0m |          -0.0063 |          55.0327 |           2.3116 |
[32m[20221214 00:15:52 @agent_ppo2.py:185][0m |          -0.0098 |          54.0789 |           2.2523 |
[32m[20221214 00:15:52 @agent_ppo2.py:185][0m |          -0.0143 |          52.9981 |           2.2147 |
[32m[20221214 00:15:52 @agent_ppo2.py:185][0m |          -0.0144 |          52.2734 |           2.2607 |
[32m[20221214 00:15:52 @agent_ppo2.py:185][0m |          -0.0138 |          51.7229 |           2.2352 |
[32m[20221214 00:15:52 @agent_ppo2.py:185][0m |          -0.0172 |          51.3947 |           2.3035 |
[32m[20221214 00:15:52 @agent_ppo2.py:185][0m |          -0.0187 |          51.1715 |           2.2538 |
[32m[20221214 00:15:52 @agent_ppo2.py:130][0m Policy update time: 0.99 s
[32m[20221214 00:15:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 517.37
[32m[20221214 00:15:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 567.71
[32m[20221214 00:15:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 502.42
[32m[20221214 00:15:52 @agent_ppo2.py:143][0m Total time:      18.37 min
[32m[20221214 00:15:52 @agent_ppo2.py:145][0m 1708032 total steps have happened
[32m[20221214 00:15:52 @agent_ppo2.py:121][0m #------------------------ Iteration 4834 --------------------------#
[32m[20221214 00:15:53 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:15:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:15:53 @agent_ppo2.py:185][0m |           0.0030 |          61.3932 |           1.5700 |
[32m[20221214 00:15:53 @agent_ppo2.py:185][0m |          -0.0069 |          53.9474 |           1.7357 |
[32m[20221214 00:15:53 @agent_ppo2.py:185][0m |          -0.0064 |          51.7741 |           1.8145 |
[32m[20221214 00:15:53 @agent_ppo2.py:185][0m |          -0.0108 |          50.1251 |           1.7973 |
[32m[20221214 00:15:53 @agent_ppo2.py:185][0m |          -0.0148 |          49.1845 |           1.8980 |
[32m[20221214 00:15:53 @agent_ppo2.py:185][0m |          -0.0127 |          48.8361 |           1.8793 |
[32m[20221214 00:15:53 @agent_ppo2.py:185][0m |          -0.0111 |          49.2061 |           1.9039 |
[32m[20221214 00:15:53 @agent_ppo2.py:185][0m |          -0.0181 |          46.9151 |           1.9065 |
[32m[20221214 00:15:53 @agent_ppo2.py:185][0m |          -0.0112 |          52.5666 |           1.9254 |
[32m[20221214 00:15:54 @agent_ppo2.py:185][0m |          -0.0096 |          46.6167 |           1.9577 |
[32m[20221214 00:15:54 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:15:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 429.77
[32m[20221214 00:15:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 542.81
[32m[20221214 00:15:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 566.99
[32m[20221214 00:15:54 @agent_ppo2.py:143][0m Total time:      18.39 min
[32m[20221214 00:15:54 @agent_ppo2.py:145][0m 1710080 total steps have happened
[32m[20221214 00:15:54 @agent_ppo2.py:121][0m #------------------------ Iteration 4835 --------------------------#
[32m[20221214 00:15:54 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:15:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:15:54 @agent_ppo2.py:185][0m |          -0.0005 |          67.3083 |           1.9615 |
[32m[20221214 00:15:54 @agent_ppo2.py:185][0m |          -0.0039 |          61.4561 |           2.0924 |
[32m[20221214 00:15:54 @agent_ppo2.py:185][0m |          -0.0122 |          59.7100 |           2.0663 |
[32m[20221214 00:15:54 @agent_ppo2.py:185][0m |          -0.0129 |          58.3276 |           2.0688 |
[32m[20221214 00:15:54 @agent_ppo2.py:185][0m |          -0.0092 |          57.7551 |           2.0478 |
[32m[20221214 00:15:54 @agent_ppo2.py:185][0m |          -0.0162 |          56.4054 |           1.9685 |
[32m[20221214 00:15:55 @agent_ppo2.py:185][0m |          -0.0155 |          55.7971 |           2.0974 |
[32m[20221214 00:15:55 @agent_ppo2.py:185][0m |          -0.0167 |          55.4202 |           2.0496 |
[32m[20221214 00:15:55 @agent_ppo2.py:185][0m |          -0.0072 |          58.9771 |           2.1243 |
[32m[20221214 00:15:55 @agent_ppo2.py:185][0m |          -0.0226 |          54.4554 |           2.0368 |
[32m[20221214 00:15:55 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:15:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 490.81
[32m[20221214 00:15:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 522.05
[32m[20221214 00:15:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 596.04
[32m[20221214 00:15:55 @agent_ppo2.py:143][0m Total time:      18.41 min
[32m[20221214 00:15:55 @agent_ppo2.py:145][0m 1712128 total steps have happened
[32m[20221214 00:15:55 @agent_ppo2.py:121][0m #------------------------ Iteration 4836 --------------------------#
[32m[20221214 00:15:55 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:15:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:15:55 @agent_ppo2.py:185][0m |           0.0072 |          61.0592 |           1.7893 |
[32m[20221214 00:15:55 @agent_ppo2.py:185][0m |          -0.0029 |          47.9235 |           1.7333 |
[32m[20221214 00:15:56 @agent_ppo2.py:185][0m |          -0.0011 |          44.3442 |           1.5975 |
[32m[20221214 00:15:56 @agent_ppo2.py:185][0m |          -0.0066 |          42.1199 |           1.5517 |
[32m[20221214 00:15:56 @agent_ppo2.py:185][0m |          -0.0105 |          40.5922 |           1.5380 |
[32m[20221214 00:15:56 @agent_ppo2.py:185][0m |          -0.0121 |          39.2324 |           1.5076 |
[32m[20221214 00:15:56 @agent_ppo2.py:185][0m |          -0.0134 |          38.5294 |           1.5038 |
[32m[20221214 00:15:56 @agent_ppo2.py:185][0m |          -0.0154 |          37.7042 |           1.4491 |
[32m[20221214 00:15:56 @agent_ppo2.py:185][0m |          -0.0165 |          37.0103 |           1.3723 |
[32m[20221214 00:15:56 @agent_ppo2.py:185][0m |          -0.0132 |          36.2963 |           1.3285 |
[32m[20221214 00:15:56 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:15:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 397.27
[32m[20221214 00:15:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 486.14
[32m[20221214 00:15:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 526.31
[32m[20221214 00:15:56 @agent_ppo2.py:143][0m Total time:      18.43 min
[32m[20221214 00:15:56 @agent_ppo2.py:145][0m 1714176 total steps have happened
[32m[20221214 00:15:56 @agent_ppo2.py:121][0m #------------------------ Iteration 4837 --------------------------#
[32m[20221214 00:15:57 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:15:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:15:57 @agent_ppo2.py:185][0m |           0.0056 |          51.0626 |           1.0728 |
[32m[20221214 00:15:57 @agent_ppo2.py:185][0m |          -0.0078 |          42.6096 |           1.0727 |
[32m[20221214 00:15:57 @agent_ppo2.py:185][0m |          -0.0095 |          40.4898 |           1.1135 |
[32m[20221214 00:15:57 @agent_ppo2.py:185][0m |          -0.0110 |          39.3096 |           1.0665 |
[32m[20221214 00:15:57 @agent_ppo2.py:185][0m |           0.0043 |          44.1120 |           1.1757 |
[32m[20221214 00:15:57 @agent_ppo2.py:185][0m |          -0.0127 |          37.6708 |           1.1460 |
[32m[20221214 00:15:57 @agent_ppo2.py:185][0m |          -0.0196 |          37.0196 |           1.1783 |
[32m[20221214 00:15:57 @agent_ppo2.py:185][0m |          -0.0167 |          36.4341 |           1.2058 |
[32m[20221214 00:15:57 @agent_ppo2.py:185][0m |          -0.0177 |          35.9438 |           1.2002 |
[32m[20221214 00:15:58 @agent_ppo2.py:185][0m |          -0.0111 |          35.8180 |           1.2231 |
[32m[20221214 00:15:58 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:15:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 418.87
[32m[20221214 00:15:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 530.71
[32m[20221214 00:15:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 580.20
[32m[20221214 00:15:58 @agent_ppo2.py:143][0m Total time:      18.46 min
[32m[20221214 00:15:58 @agent_ppo2.py:145][0m 1716224 total steps have happened
[32m[20221214 00:15:58 @agent_ppo2.py:121][0m #------------------------ Iteration 4838 --------------------------#
[32m[20221214 00:15:58 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:15:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:15:58 @agent_ppo2.py:185][0m |          -0.0011 |          92.1367 |           1.0022 |
[32m[20221214 00:15:58 @agent_ppo2.py:185][0m |          -0.0037 |          84.1913 |           1.0319 |
[32m[20221214 00:15:58 @agent_ppo2.py:185][0m |          -0.0068 |          81.1706 |           1.0068 |
[32m[20221214 00:15:58 @agent_ppo2.py:185][0m |          -0.0008 |          82.6652 |           1.0610 |
[32m[20221214 00:15:58 @agent_ppo2.py:185][0m |          -0.0098 |          77.1853 |           1.1296 |
[32m[20221214 00:15:59 @agent_ppo2.py:185][0m |          -0.0087 |          76.0395 |           1.1299 |
[32m[20221214 00:15:59 @agent_ppo2.py:185][0m |          -0.0008 |          75.3812 |           1.2150 |
[32m[20221214 00:15:59 @agent_ppo2.py:185][0m |          -0.0129 |          74.2617 |           1.2251 |
[32m[20221214 00:15:59 @agent_ppo2.py:185][0m |          -0.0133 |          73.2063 |           1.2856 |
[32m[20221214 00:15:59 @agent_ppo2.py:185][0m |          -0.0142 |          72.7795 |           1.2499 |
[32m[20221214 00:15:59 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:15:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 527.09
[32m[20221214 00:15:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 600.64
[32m[20221214 00:15:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 610.60
[32m[20221214 00:15:59 @agent_ppo2.py:143][0m Total time:      18.48 min
[32m[20221214 00:15:59 @agent_ppo2.py:145][0m 1718272 total steps have happened
[32m[20221214 00:15:59 @agent_ppo2.py:121][0m #------------------------ Iteration 4839 --------------------------#
[32m[20221214 00:15:59 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:15:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:15:59 @agent_ppo2.py:185][0m |           0.0016 |          83.1656 |           1.8702 |
[32m[20221214 00:15:59 @agent_ppo2.py:185][0m |           0.0014 |          78.9533 |           1.9965 |
[32m[20221214 00:16:00 @agent_ppo2.py:185][0m |          -0.0143 |          71.4279 |           2.1141 |
[32m[20221214 00:16:00 @agent_ppo2.py:185][0m |          -0.0148 |          69.7679 |           2.1549 |
[32m[20221214 00:16:00 @agent_ppo2.py:185][0m |          -0.0168 |          68.1201 |           2.1711 |
[32m[20221214 00:16:00 @agent_ppo2.py:185][0m |          -0.0165 |          67.2694 |           2.2395 |
[32m[20221214 00:16:00 @agent_ppo2.py:185][0m |          -0.0034 |          70.0246 |           2.3186 |
[32m[20221214 00:16:00 @agent_ppo2.py:185][0m |          -0.0191 |          65.9343 |           2.3625 |
[32m[20221214 00:16:00 @agent_ppo2.py:185][0m |          -0.0185 |          65.0475 |           2.4551 |
[32m[20221214 00:16:00 @agent_ppo2.py:185][0m |          -0.0222 |          64.6140 |           2.4282 |
[32m[20221214 00:16:00 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:16:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 509.31
[32m[20221214 00:16:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 570.28
[32m[20221214 00:16:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 476.44
[32m[20221214 00:16:00 @agent_ppo2.py:143][0m Total time:      18.50 min
[32m[20221214 00:16:00 @agent_ppo2.py:145][0m 1720320 total steps have happened
[32m[20221214 00:16:00 @agent_ppo2.py:121][0m #------------------------ Iteration 4840 --------------------------#
[32m[20221214 00:16:01 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:16:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:16:01 @agent_ppo2.py:185][0m |           0.0002 |          79.6364 |           1.6503 |
[32m[20221214 00:16:01 @agent_ppo2.py:185][0m |          -0.0052 |          71.9276 |           1.7843 |
[32m[20221214 00:16:01 @agent_ppo2.py:185][0m |          -0.0128 |          69.5264 |           1.6498 |
[32m[20221214 00:16:01 @agent_ppo2.py:185][0m |          -0.0149 |          66.9143 |           1.6926 |
[32m[20221214 00:16:01 @agent_ppo2.py:185][0m |          -0.0161 |          65.4846 |           1.7771 |
[32m[20221214 00:16:01 @agent_ppo2.py:185][0m |          -0.0134 |          66.8670 |           1.6989 |
[32m[20221214 00:16:01 @agent_ppo2.py:185][0m |          -0.0157 |          63.3826 |           1.6941 |
[32m[20221214 00:16:01 @agent_ppo2.py:185][0m |          -0.0151 |          62.8010 |           1.7436 |
[32m[20221214 00:16:02 @agent_ppo2.py:185][0m |          -0.0145 |          62.3095 |           1.7482 |
[32m[20221214 00:16:02 @agent_ppo2.py:185][0m |          -0.0134 |          61.6333 |           1.8453 |
[32m[20221214 00:16:02 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:16:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 501.83
[32m[20221214 00:16:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 572.18
[32m[20221214 00:16:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 588.04
[32m[20221214 00:16:02 @agent_ppo2.py:143][0m Total time:      18.52 min
[32m[20221214 00:16:02 @agent_ppo2.py:145][0m 1722368 total steps have happened
[32m[20221214 00:16:02 @agent_ppo2.py:121][0m #------------------------ Iteration 4841 --------------------------#
[32m[20221214 00:16:02 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:16:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:16:02 @agent_ppo2.py:185][0m |           0.0026 |          61.4154 |           1.8876 |
[32m[20221214 00:16:02 @agent_ppo2.py:185][0m |          -0.0064 |          54.0931 |           1.9959 |
[32m[20221214 00:16:02 @agent_ppo2.py:185][0m |          -0.0119 |          52.3073 |           2.0679 |
[32m[20221214 00:16:02 @agent_ppo2.py:185][0m |          -0.0134 |          51.1507 |           2.1713 |
[32m[20221214 00:16:03 @agent_ppo2.py:185][0m |          -0.0109 |          50.5429 |           2.1515 |
[32m[20221214 00:16:03 @agent_ppo2.py:185][0m |          -0.0158 |          49.9977 |           2.1439 |
[32m[20221214 00:16:03 @agent_ppo2.py:185][0m |          -0.0163 |          49.6404 |           2.2415 |
[32m[20221214 00:16:03 @agent_ppo2.py:185][0m |          -0.0161 |          49.2939 |           2.2708 |
[32m[20221214 00:16:03 @agent_ppo2.py:185][0m |          -0.0139 |          49.0913 |           2.2992 |
[32m[20221214 00:16:03 @agent_ppo2.py:185][0m |          -0.0167 |          48.6097 |           2.3828 |
[32m[20221214 00:16:03 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:16:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 456.61
[32m[20221214 00:16:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 525.46
[32m[20221214 00:16:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 369.65
[32m[20221214 00:16:03 @agent_ppo2.py:143][0m Total time:      18.55 min
[32m[20221214 00:16:03 @agent_ppo2.py:145][0m 1724416 total steps have happened
[32m[20221214 00:16:03 @agent_ppo2.py:121][0m #------------------------ Iteration 4842 --------------------------#
[32m[20221214 00:16:03 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:16:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:16:04 @agent_ppo2.py:185][0m |           0.0017 |          62.4493 |           1.8388 |
[32m[20221214 00:16:04 @agent_ppo2.py:185][0m |          -0.0050 |          57.4658 |           1.8214 |
[32m[20221214 00:16:04 @agent_ppo2.py:185][0m |          -0.0085 |          55.6410 |           1.7621 |
[32m[20221214 00:16:04 @agent_ppo2.py:185][0m |          -0.0112 |          54.8788 |           1.8552 |
[32m[20221214 00:16:04 @agent_ppo2.py:185][0m |          -0.0075 |          54.2370 |           1.7595 |
[32m[20221214 00:16:04 @agent_ppo2.py:185][0m |          -0.0157 |          53.7553 |           1.7462 |
[32m[20221214 00:16:04 @agent_ppo2.py:185][0m |          -0.0140 |          53.3841 |           1.7569 |
[32m[20221214 00:16:04 @agent_ppo2.py:185][0m |          -0.0179 |          52.9933 |           1.7071 |
[32m[20221214 00:16:04 @agent_ppo2.py:185][0m |          -0.0052 |          54.0437 |           1.7460 |
[32m[20221214 00:16:04 @agent_ppo2.py:185][0m |          -0.0131 |          52.5202 |           1.6831 |
[32m[20221214 00:16:04 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:16:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 503.26
[32m[20221214 00:16:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 557.33
[32m[20221214 00:16:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 481.57
[32m[20221214 00:16:05 @agent_ppo2.py:143][0m Total time:      18.57 min
[32m[20221214 00:16:05 @agent_ppo2.py:145][0m 1726464 total steps have happened
[32m[20221214 00:16:05 @agent_ppo2.py:121][0m #------------------------ Iteration 4843 --------------------------#
[32m[20221214 00:16:05 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:16:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:16:05 @agent_ppo2.py:185][0m |          -0.0016 |          59.3511 |           1.8073 |
[32m[20221214 00:16:05 @agent_ppo2.py:185][0m |          -0.0081 |          55.7657 |           1.8026 |
[32m[20221214 00:16:05 @agent_ppo2.py:185][0m |          -0.0139 |          54.1917 |           1.9148 |
[32m[20221214 00:16:05 @agent_ppo2.py:185][0m |          -0.0104 |          53.1193 |           1.8658 |
[32m[20221214 00:16:05 @agent_ppo2.py:185][0m |          -0.0125 |          52.4036 |           1.9635 |
[32m[20221214 00:16:05 @agent_ppo2.py:185][0m |          -0.0151 |          51.8196 |           1.9671 |
[32m[20221214 00:16:05 @agent_ppo2.py:185][0m |          -0.0166 |          51.3526 |           2.0170 |
[32m[20221214 00:16:06 @agent_ppo2.py:185][0m |          -0.0160 |          50.9609 |           1.9559 |
[32m[20221214 00:16:06 @agent_ppo2.py:185][0m |          -0.0183 |          50.5016 |           2.1010 |
[32m[20221214 00:16:06 @agent_ppo2.py:185][0m |          -0.0173 |          50.1647 |           2.1344 |
[32m[20221214 00:16:06 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:16:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 443.59
[32m[20221214 00:16:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 486.81
[32m[20221214 00:16:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 606.11
[32m[20221214 00:16:06 @agent_ppo2.py:143][0m Total time:      18.59 min
[32m[20221214 00:16:06 @agent_ppo2.py:145][0m 1728512 total steps have happened
[32m[20221214 00:16:06 @agent_ppo2.py:121][0m #------------------------ Iteration 4844 --------------------------#
[32m[20221214 00:16:06 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:16:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:16:06 @agent_ppo2.py:185][0m |           0.0071 |          81.5478 |           1.9514 |
[32m[20221214 00:16:06 @agent_ppo2.py:185][0m |          -0.0043 |          74.6213 |           1.9214 |
[32m[20221214 00:16:06 @agent_ppo2.py:185][0m |          -0.0098 |          70.4785 |           1.9426 |
[32m[20221214 00:16:07 @agent_ppo2.py:185][0m |          -0.0107 |          68.9361 |           2.0506 |
[32m[20221214 00:16:07 @agent_ppo2.py:185][0m |          -0.0175 |          67.9239 |           1.9855 |
[32m[20221214 00:16:07 @agent_ppo2.py:185][0m |          -0.0155 |          67.1415 |           1.9509 |
[32m[20221214 00:16:07 @agent_ppo2.py:185][0m |          -0.0159 |          66.3948 |           2.0278 |
[32m[20221214 00:16:07 @agent_ppo2.py:185][0m |          -0.0191 |          66.1897 |           2.0553 |
[32m[20221214 00:16:07 @agent_ppo2.py:185][0m |          -0.0182 |          65.4484 |           2.0494 |
[32m[20221214 00:16:07 @agent_ppo2.py:185][0m |          -0.0182 |          65.0175 |           2.0387 |
[32m[20221214 00:16:07 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:16:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 522.62
[32m[20221214 00:16:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 598.50
[32m[20221214 00:16:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 630.32
[32m[20221214 00:16:07 @agent_ppo2.py:143][0m Total time:      18.62 min
[32m[20221214 00:16:07 @agent_ppo2.py:145][0m 1730560 total steps have happened
[32m[20221214 00:16:07 @agent_ppo2.py:121][0m #------------------------ Iteration 4845 --------------------------#
[32m[20221214 00:16:07 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:16:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:16:08 @agent_ppo2.py:185][0m |           0.0175 |         104.9730 |           2.9438 |
[32m[20221214 00:16:08 @agent_ppo2.py:185][0m |          -0.0074 |          84.4855 |           3.0848 |
[32m[20221214 00:16:08 @agent_ppo2.py:185][0m |          -0.0094 |          81.4154 |           3.0921 |
[32m[20221214 00:16:08 @agent_ppo2.py:185][0m |          -0.0118 |          80.1021 |           3.1407 |
[32m[20221214 00:16:08 @agent_ppo2.py:185][0m |          -0.0099 |          78.5521 |           3.2707 |
[32m[20221214 00:16:08 @agent_ppo2.py:185][0m |          -0.0137 |          77.5453 |           3.2925 |
[32m[20221214 00:16:08 @agent_ppo2.py:185][0m |          -0.0147 |          76.7246 |           3.3455 |
[32m[20221214 00:16:08 @agent_ppo2.py:185][0m |          -0.0146 |          76.3111 |           3.4170 |
[32m[20221214 00:16:08 @agent_ppo2.py:185][0m |          -0.0144 |          75.5956 |           3.4272 |
[32m[20221214 00:16:08 @agent_ppo2.py:185][0m |          -0.0152 |          74.7948 |           3.4216 |
[32m[20221214 00:16:08 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:16:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 497.32
[32m[20221214 00:16:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 537.08
[32m[20221214 00:16:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 476.50
[32m[20221214 00:16:09 @agent_ppo2.py:143][0m Total time:      18.64 min
[32m[20221214 00:16:09 @agent_ppo2.py:145][0m 1732608 total steps have happened
[32m[20221214 00:16:09 @agent_ppo2.py:121][0m #------------------------ Iteration 4846 --------------------------#
[32m[20221214 00:16:09 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:16:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:16:09 @agent_ppo2.py:185][0m |           0.0045 |          83.4858 |           3.0917 |
[32m[20221214 00:16:09 @agent_ppo2.py:185][0m |          -0.0046 |          77.5834 |           3.0654 |
[32m[20221214 00:16:09 @agent_ppo2.py:185][0m |           0.0004 |          76.3748 |           3.1374 |
[32m[20221214 00:16:09 @agent_ppo2.py:185][0m |          -0.0026 |          73.9995 |           3.1765 |
[32m[20221214 00:16:09 @agent_ppo2.py:185][0m |          -0.0148 |          71.8728 |           3.1166 |
[32m[20221214 00:16:10 @agent_ppo2.py:185][0m |          -0.0138 |          70.8269 |           3.0914 |
[32m[20221214 00:16:10 @agent_ppo2.py:185][0m |          -0.0131 |          70.1241 |           3.1191 |
[32m[20221214 00:16:10 @agent_ppo2.py:185][0m |          -0.0154 |          69.6691 |           3.1080 |
[32m[20221214 00:16:10 @agent_ppo2.py:185][0m |          -0.0190 |          69.0766 |           3.1725 |
[32m[20221214 00:16:10 @agent_ppo2.py:185][0m |          -0.0150 |          68.7509 |           3.0707 |
[32m[20221214 00:16:10 @agent_ppo2.py:130][0m Policy update time: 1.26 s
[32m[20221214 00:16:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 502.83
[32m[20221214 00:16:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 564.96
[32m[20221214 00:16:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 482.55
[32m[20221214 00:16:10 @agent_ppo2.py:143][0m Total time:      18.66 min
[32m[20221214 00:16:10 @agent_ppo2.py:145][0m 1734656 total steps have happened
[32m[20221214 00:16:10 @agent_ppo2.py:121][0m #------------------------ Iteration 4847 --------------------------#
[32m[20221214 00:16:10 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:16:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:16:11 @agent_ppo2.py:185][0m |           0.0086 |          78.7116 |           2.6735 |
[32m[20221214 00:16:11 @agent_ppo2.py:185][0m |          -0.0059 |          67.2992 |           2.6555 |
[32m[20221214 00:16:11 @agent_ppo2.py:185][0m |          -0.0062 |          64.8421 |           2.7032 |
[32m[20221214 00:16:11 @agent_ppo2.py:185][0m |          -0.0109 |          63.5831 |           2.6595 |
[32m[20221214 00:16:11 @agent_ppo2.py:185][0m |          -0.0113 |          62.4874 |           2.7631 |
[32m[20221214 00:16:11 @agent_ppo2.py:185][0m |          -0.0174 |          61.6194 |           2.6786 |
[32m[20221214 00:16:11 @agent_ppo2.py:185][0m |          -0.0120 |          62.8291 |           2.7060 |
[32m[20221214 00:16:11 @agent_ppo2.py:185][0m |          -0.0158 |          60.5566 |           2.7296 |
[32m[20221214 00:16:11 @agent_ppo2.py:185][0m |          -0.0191 |          59.9512 |           2.8256 |
[32m[20221214 00:16:12 @agent_ppo2.py:185][0m |          -0.0187 |          60.3615 |           2.7900 |
[32m[20221214 00:16:12 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:16:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 487.09
[32m[20221214 00:16:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 558.97
[32m[20221214 00:16:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 563.55
[32m[20221214 00:16:12 @agent_ppo2.py:143][0m Total time:      18.69 min
[32m[20221214 00:16:12 @agent_ppo2.py:145][0m 1736704 total steps have happened
[32m[20221214 00:16:12 @agent_ppo2.py:121][0m #------------------------ Iteration 4848 --------------------------#
[32m[20221214 00:16:12 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:16:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:16:12 @agent_ppo2.py:185][0m |           0.0044 |          66.4916 |           3.0537 |
[32m[20221214 00:16:12 @agent_ppo2.py:185][0m |           0.0024 |          64.6772 |           2.9704 |
[32m[20221214 00:16:12 @agent_ppo2.py:185][0m |          -0.0106 |          58.2864 |           3.0653 |
[32m[20221214 00:16:12 @agent_ppo2.py:185][0m |          -0.0147 |          56.7329 |           3.0197 |
[32m[20221214 00:16:12 @agent_ppo2.py:185][0m |          -0.0145 |          55.6204 |           3.0113 |
[32m[20221214 00:16:13 @agent_ppo2.py:185][0m |          -0.0130 |          54.9827 |           2.9557 |
[32m[20221214 00:16:13 @agent_ppo2.py:185][0m |          -0.0171 |          54.3133 |           3.0028 |
[32m[20221214 00:16:13 @agent_ppo2.py:185][0m |          -0.0157 |          53.8247 |           2.9866 |
[32m[20221214 00:16:13 @agent_ppo2.py:185][0m |          -0.0206 |          53.4381 |           2.9683 |
[32m[20221214 00:16:13 @agent_ppo2.py:185][0m |          -0.0235 |          53.0946 |           3.0222 |
[32m[20221214 00:16:13 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 00:16:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 464.22
[32m[20221214 00:16:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 487.64
[32m[20221214 00:16:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 577.62
[32m[20221214 00:16:13 @agent_ppo2.py:143][0m Total time:      18.71 min
[32m[20221214 00:16:13 @agent_ppo2.py:145][0m 1738752 total steps have happened
[32m[20221214 00:16:13 @agent_ppo2.py:121][0m #------------------------ Iteration 4849 --------------------------#
[32m[20221214 00:16:13 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:16:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:16:13 @agent_ppo2.py:185][0m |          -0.0007 |          82.6412 |           3.1018 |
[32m[20221214 00:16:14 @agent_ppo2.py:185][0m |          -0.0070 |          77.3542 |           3.0584 |
[32m[20221214 00:16:14 @agent_ppo2.py:185][0m |          -0.0116 |          75.2964 |           3.1119 |
[32m[20221214 00:16:14 @agent_ppo2.py:185][0m |          -0.0118 |          73.8656 |           3.1625 |
[32m[20221214 00:16:14 @agent_ppo2.py:185][0m |          -0.0127 |          73.0031 |           3.2565 |
[32m[20221214 00:16:14 @agent_ppo2.py:185][0m |          -0.0156 |          72.1143 |           3.3463 |
[32m[20221214 00:16:14 @agent_ppo2.py:185][0m |          -0.0163 |          71.7500 |           3.3467 |
[32m[20221214 00:16:14 @agent_ppo2.py:185][0m |          -0.0110 |          71.2435 |           3.3200 |
[32m[20221214 00:16:14 @agent_ppo2.py:185][0m |          -0.0156 |          70.8230 |           3.3575 |
[32m[20221214 00:16:14 @agent_ppo2.py:185][0m |          -0.0157 |          70.3900 |           3.4723 |
[32m[20221214 00:16:14 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:16:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 540.96
[32m[20221214 00:16:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 572.77
[32m[20221214 00:16:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 600.57
[32m[20221214 00:16:15 @agent_ppo2.py:143][0m Total time:      18.74 min
[32m[20221214 00:16:15 @agent_ppo2.py:145][0m 1740800 total steps have happened
[32m[20221214 00:16:15 @agent_ppo2.py:121][0m #------------------------ Iteration 4850 --------------------------#
[32m[20221214 00:16:15 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:16:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:16:15 @agent_ppo2.py:185][0m |           0.0047 |          49.8612 |           3.1857 |
[32m[20221214 00:16:15 @agent_ppo2.py:185][0m |          -0.0110 |          44.8295 |           3.1551 |
[32m[20221214 00:16:15 @agent_ppo2.py:185][0m |          -0.0086 |          42.8289 |           3.1925 |
[32m[20221214 00:16:15 @agent_ppo2.py:185][0m |          -0.0109 |          41.5719 |           3.1431 |
[32m[20221214 00:16:15 @agent_ppo2.py:185][0m |          -0.0134 |          40.7554 |           3.2392 |
[32m[20221214 00:16:15 @agent_ppo2.py:185][0m |          -0.0126 |          39.9931 |           3.2239 |
[32m[20221214 00:16:16 @agent_ppo2.py:185][0m |          -0.0166 |          39.3344 |           3.2321 |
[32m[20221214 00:16:16 @agent_ppo2.py:185][0m |          -0.0173 |          38.9381 |           3.2634 |
[32m[20221214 00:16:16 @agent_ppo2.py:185][0m |          -0.0190 |          38.4061 |           3.2752 |
[32m[20221214 00:16:16 @agent_ppo2.py:185][0m |          -0.0184 |          38.1642 |           3.2072 |
[32m[20221214 00:16:16 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:16:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 499.25
[32m[20221214 00:16:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 546.25
[32m[20221214 00:16:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 415.14
[32m[20221214 00:16:16 @agent_ppo2.py:143][0m Total time:      18.76 min
[32m[20221214 00:16:16 @agent_ppo2.py:145][0m 1742848 total steps have happened
[32m[20221214 00:16:16 @agent_ppo2.py:121][0m #------------------------ Iteration 4851 --------------------------#
[32m[20221214 00:16:16 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:16:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:16:16 @agent_ppo2.py:185][0m |          -0.0005 |          50.6581 |           2.8356 |
[32m[20221214 00:16:16 @agent_ppo2.py:185][0m |          -0.0136 |          40.6483 |           2.9080 |
[32m[20221214 00:16:17 @agent_ppo2.py:185][0m |          -0.0101 |          39.0835 |           2.8498 |
[32m[20221214 00:16:17 @agent_ppo2.py:185][0m |          -0.0055 |          40.5164 |           2.8620 |
[32m[20221214 00:16:17 @agent_ppo2.py:185][0m |          -0.0107 |          38.5065 |           2.8601 |
[32m[20221214 00:16:17 @agent_ppo2.py:185][0m |          -0.0102 |          38.6329 |           2.8053 |
[32m[20221214 00:16:17 @agent_ppo2.py:185][0m |          -0.0155 |          36.7752 |           2.8985 |
[32m[20221214 00:16:17 @agent_ppo2.py:185][0m |          -0.0141 |          36.3290 |           2.8411 |
[32m[20221214 00:16:17 @agent_ppo2.py:185][0m |          -0.0100 |          40.7802 |           2.9287 |
[32m[20221214 00:16:17 @agent_ppo2.py:185][0m |          -0.0134 |          37.1442 |           2.8449 |
[32m[20221214 00:16:17 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:16:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 479.83
[32m[20221214 00:16:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 531.55
[32m[20221214 00:16:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 489.17
[32m[20221214 00:16:17 @agent_ppo2.py:143][0m Total time:      18.78 min
[32m[20221214 00:16:17 @agent_ppo2.py:145][0m 1744896 total steps have happened
[32m[20221214 00:16:17 @agent_ppo2.py:121][0m #------------------------ Iteration 4852 --------------------------#
[32m[20221214 00:16:18 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:16:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:16:18 @agent_ppo2.py:185][0m |          -0.0010 |          50.9426 |           2.7708 |
[32m[20221214 00:16:18 @agent_ppo2.py:185][0m |           0.0044 |          47.9180 |           2.7468 |
[32m[20221214 00:16:18 @agent_ppo2.py:185][0m |          -0.0081 |          42.9717 |           2.8204 |
[32m[20221214 00:16:18 @agent_ppo2.py:185][0m |          -0.0070 |          41.6863 |           2.7907 |
[32m[20221214 00:16:18 @agent_ppo2.py:185][0m |          -0.0038 |          42.9782 |           2.7892 |
[32m[20221214 00:16:18 @agent_ppo2.py:185][0m |          -0.0081 |          40.2595 |           2.8733 |
[32m[20221214 00:16:18 @agent_ppo2.py:185][0m |          -0.0143 |          39.1603 |           2.8159 |
[32m[20221214 00:16:18 @agent_ppo2.py:185][0m |          -0.0126 |          38.4510 |           2.8198 |
[32m[20221214 00:16:19 @agent_ppo2.py:185][0m |          -0.0110 |          38.3299 |           2.8471 |
[32m[20221214 00:16:19 @agent_ppo2.py:185][0m |          -0.0108 |          37.6673 |           2.7824 |
[32m[20221214 00:16:19 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:16:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 501.35
[32m[20221214 00:16:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 524.65
[32m[20221214 00:16:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 385.83
[32m[20221214 00:16:19 @agent_ppo2.py:143][0m Total time:      18.81 min
[32m[20221214 00:16:19 @agent_ppo2.py:145][0m 1746944 total steps have happened
[32m[20221214 00:16:19 @agent_ppo2.py:121][0m #------------------------ Iteration 4853 --------------------------#
[32m[20221214 00:16:19 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:16:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:16:19 @agent_ppo2.py:185][0m |           0.0012 |          20.1702 |           3.0058 |
[32m[20221214 00:16:19 @agent_ppo2.py:185][0m |          -0.0058 |          16.1053 |           2.9676 |
[32m[20221214 00:16:19 @agent_ppo2.py:185][0m |          -0.0005 |          15.6974 |           3.0263 |
[32m[20221214 00:16:19 @agent_ppo2.py:185][0m |          -0.0053 |          15.5421 |           2.9043 |
[32m[20221214 00:16:20 @agent_ppo2.py:185][0m |          -0.0043 |          15.5041 |           2.9527 |
[32m[20221214 00:16:20 @agent_ppo2.py:185][0m |           0.0000 |          15.5319 |           2.8663 |
[32m[20221214 00:16:20 @agent_ppo2.py:185][0m |          -0.0077 |          15.4353 |           2.8792 |
[32m[20221214 00:16:20 @agent_ppo2.py:185][0m |          -0.0040 |          15.4760 |           2.8677 |
[32m[20221214 00:16:20 @agent_ppo2.py:185][0m |          -0.0053 |          15.3383 |           2.9467 |
[32m[20221214 00:16:20 @agent_ppo2.py:185][0m |          -0.0075 |          15.3167 |           2.8027 |
[32m[20221214 00:16:20 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:16:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221214 00:16:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221214 00:16:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 409.36
[32m[20221214 00:16:20 @agent_ppo2.py:143][0m Total time:      18.83 min
[32m[20221214 00:16:20 @agent_ppo2.py:145][0m 1748992 total steps have happened
[32m[20221214 00:16:20 @agent_ppo2.py:121][0m #------------------------ Iteration 4854 --------------------------#
[32m[20221214 00:16:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:16:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:16:21 @agent_ppo2.py:185][0m |          -0.0063 |          58.6838 |           2.6487 |
[32m[20221214 00:16:21 @agent_ppo2.py:185][0m |          -0.0096 |          54.8840 |           2.7242 |
[32m[20221214 00:16:21 @agent_ppo2.py:185][0m |           0.0022 |          60.8191 |           2.7090 |
[32m[20221214 00:16:21 @agent_ppo2.py:185][0m |          -0.0126 |          54.1726 |           2.7645 |
[32m[20221214 00:16:21 @agent_ppo2.py:185][0m |          -0.0116 |          53.4074 |           2.7827 |
[32m[20221214 00:16:21 @agent_ppo2.py:185][0m |          -0.0170 |          52.5790 |           2.7512 |
[32m[20221214 00:16:21 @agent_ppo2.py:185][0m |          -0.0142 |          52.2655 |           2.7312 |
[32m[20221214 00:16:21 @agent_ppo2.py:185][0m |          -0.0209 |          51.9445 |           2.8177 |
[32m[20221214 00:16:21 @agent_ppo2.py:185][0m |          -0.0242 |          51.9841 |           2.8471 |
[32m[20221214 00:16:21 @agent_ppo2.py:185][0m |          -0.0184 |          51.6705 |           2.8683 |
[32m[20221214 00:16:21 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:16:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 446.94
[32m[20221214 00:16:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 476.02
[32m[20221214 00:16:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 580.84
[32m[20221214 00:16:22 @agent_ppo2.py:143][0m Total time:      18.85 min
[32m[20221214 00:16:22 @agent_ppo2.py:145][0m 1751040 total steps have happened
[32m[20221214 00:16:22 @agent_ppo2.py:121][0m #------------------------ Iteration 4855 --------------------------#
[32m[20221214 00:16:22 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:16:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:16:22 @agent_ppo2.py:185][0m |           0.0017 |          59.0680 |           2.3795 |
[32m[20221214 00:16:22 @agent_ppo2.py:185][0m |          -0.0054 |          53.1911 |           2.4354 |
[32m[20221214 00:16:22 @agent_ppo2.py:185][0m |          -0.0132 |          51.5328 |           2.3345 |
[32m[20221214 00:16:22 @agent_ppo2.py:185][0m |          -0.0101 |          50.4825 |           2.3244 |
[32m[20221214 00:16:22 @agent_ppo2.py:185][0m |          -0.0132 |          49.6623 |           2.3951 |
[32m[20221214 00:16:22 @agent_ppo2.py:185][0m |          -0.0190 |          49.0179 |           2.4218 |
[32m[20221214 00:16:23 @agent_ppo2.py:185][0m |          -0.0151 |          48.5393 |           2.4897 |
[32m[20221214 00:16:23 @agent_ppo2.py:185][0m |          -0.0231 |          48.2875 |           2.4796 |
[32m[20221214 00:16:23 @agent_ppo2.py:185][0m |          -0.0110 |          48.0529 |           2.5679 |
[32m[20221214 00:16:23 @agent_ppo2.py:185][0m |          -0.0198 |          47.5213 |           2.5551 |
[32m[20221214 00:16:23 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:16:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 406.19
[32m[20221214 00:16:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 463.25
[32m[20221214 00:16:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 541.77
[32m[20221214 00:16:23 @agent_ppo2.py:143][0m Total time:      18.88 min
[32m[20221214 00:16:23 @agent_ppo2.py:145][0m 1753088 total steps have happened
[32m[20221214 00:16:23 @agent_ppo2.py:121][0m #------------------------ Iteration 4856 --------------------------#
[32m[20221214 00:16:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:16:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:16:23 @agent_ppo2.py:185][0m |          -0.0016 |          50.5324 |           3.0800 |
[32m[20221214 00:16:23 @agent_ppo2.py:185][0m |          -0.0028 |          45.8642 |           3.0501 |
[32m[20221214 00:16:24 @agent_ppo2.py:185][0m |          -0.0026 |          44.3841 |           3.1434 |
[32m[20221214 00:16:24 @agent_ppo2.py:185][0m |          -0.0131 |          43.1692 |           3.0150 |
[32m[20221214 00:16:24 @agent_ppo2.py:185][0m |          -0.0164 |          41.9316 |           3.1075 |
[32m[20221214 00:16:24 @agent_ppo2.py:185][0m |          -0.0157 |          41.1153 |           3.1001 |
[32m[20221214 00:16:24 @agent_ppo2.py:185][0m |          -0.0174 |          40.4270 |           2.9843 |
[32m[20221214 00:16:24 @agent_ppo2.py:185][0m |          -0.0197 |          39.8413 |           3.0775 |
[32m[20221214 00:16:24 @agent_ppo2.py:185][0m |          -0.0182 |          39.4419 |           2.9792 |
[32m[20221214 00:16:24 @agent_ppo2.py:185][0m |          -0.0220 |          39.2610 |           3.0713 |
[32m[20221214 00:16:24 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:16:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 410.81
[32m[20221214 00:16:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 477.56
[32m[20221214 00:16:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 523.76
[32m[20221214 00:16:24 @agent_ppo2.py:143][0m Total time:      18.90 min
[32m[20221214 00:16:24 @agent_ppo2.py:145][0m 1755136 total steps have happened
[32m[20221214 00:16:24 @agent_ppo2.py:121][0m #------------------------ Iteration 4857 --------------------------#
[32m[20221214 00:16:25 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:16:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:16:25 @agent_ppo2.py:185][0m |          -0.0013 |          64.9282 |           2.1874 |
[32m[20221214 00:16:25 @agent_ppo2.py:185][0m |           0.0099 |          68.2000 |           2.0808 |
[32m[20221214 00:16:25 @agent_ppo2.py:185][0m |          -0.0110 |          60.2634 |           1.9938 |
[32m[20221214 00:16:25 @agent_ppo2.py:185][0m |          -0.0139 |          58.9378 |           2.0382 |
[32m[20221214 00:16:25 @agent_ppo2.py:185][0m |          -0.0134 |          58.3377 |           1.9301 |
[32m[20221214 00:16:25 @agent_ppo2.py:185][0m |          -0.0074 |          62.7060 |           1.9310 |
[32m[20221214 00:16:25 @agent_ppo2.py:185][0m |          -0.0152 |          57.4600 |           1.9152 |
[32m[20221214 00:16:26 @agent_ppo2.py:185][0m |          -0.0172 |          57.0865 |           1.9238 |
[32m[20221214 00:16:26 @agent_ppo2.py:185][0m |          -0.0171 |          56.5519 |           1.8825 |
[32m[20221214 00:16:26 @agent_ppo2.py:185][0m |          -0.0205 |          56.2637 |           1.8961 |
[32m[20221214 00:16:26 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221214 00:16:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 465.00
[32m[20221214 00:16:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 493.74
[32m[20221214 00:16:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 592.55
[32m[20221214 00:16:26 @agent_ppo2.py:143][0m Total time:      18.92 min
[32m[20221214 00:16:26 @agent_ppo2.py:145][0m 1757184 total steps have happened
[32m[20221214 00:16:26 @agent_ppo2.py:121][0m #------------------------ Iteration 4858 --------------------------#
[32m[20221214 00:16:26 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:16:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:16:26 @agent_ppo2.py:185][0m |           0.0071 |          65.2443 |           2.1413 |
[32m[20221214 00:16:26 @agent_ppo2.py:185][0m |          -0.0050 |          58.1140 |           2.1174 |
[32m[20221214 00:16:26 @agent_ppo2.py:185][0m |          -0.0143 |          56.6843 |           2.1664 |
[32m[20221214 00:16:27 @agent_ppo2.py:185][0m |          -0.0135 |          55.8443 |           2.1744 |
[32m[20221214 00:16:27 @agent_ppo2.py:185][0m |          -0.0168 |          54.9708 |           2.0787 |
[32m[20221214 00:16:27 @agent_ppo2.py:185][0m |          -0.0177 |          54.5271 |           2.1956 |
[32m[20221214 00:16:27 @agent_ppo2.py:185][0m |          -0.0173 |          54.0489 |           2.1503 |
[32m[20221214 00:16:27 @agent_ppo2.py:185][0m |          -0.0193 |          53.6326 |           2.0459 |
[32m[20221214 00:16:27 @agent_ppo2.py:185][0m |          -0.0190 |          53.3358 |           2.0972 |
[32m[20221214 00:16:27 @agent_ppo2.py:185][0m |          -0.0175 |          52.9540 |           2.1661 |
[32m[20221214 00:16:27 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:16:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 418.32
[32m[20221214 00:16:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 483.73
[32m[20221214 00:16:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 327.07
[32m[20221214 00:16:27 @agent_ppo2.py:143][0m Total time:      18.95 min
[32m[20221214 00:16:27 @agent_ppo2.py:145][0m 1759232 total steps have happened
[32m[20221214 00:16:27 @agent_ppo2.py:121][0m #------------------------ Iteration 4859 --------------------------#
[32m[20221214 00:16:27 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:16:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:16:28 @agent_ppo2.py:185][0m |          -0.0044 |          57.2657 |           1.9821 |
[32m[20221214 00:16:28 @agent_ppo2.py:185][0m |          -0.0053 |          50.9229 |           1.9965 |
[32m[20221214 00:16:28 @agent_ppo2.py:185][0m |          -0.0130 |          49.0037 |           1.9835 |
[32m[20221214 00:16:28 @agent_ppo2.py:185][0m |          -0.0127 |          47.5082 |           2.0320 |
[32m[20221214 00:16:28 @agent_ppo2.py:185][0m |          -0.0156 |          46.7483 |           2.0283 |
[32m[20221214 00:16:28 @agent_ppo2.py:185][0m |          -0.0208 |          46.0576 |           2.0482 |
[32m[20221214 00:16:28 @agent_ppo2.py:185][0m |          -0.0170 |          46.1994 |           2.0018 |
[32m[20221214 00:16:28 @agent_ppo2.py:185][0m |          -0.0168 |          44.8440 |           1.9389 |
[32m[20221214 00:16:29 @agent_ppo2.py:185][0m |          -0.0144 |          44.3983 |           1.9848 |
[32m[20221214 00:16:29 @agent_ppo2.py:185][0m |          -0.0158 |          44.0896 |           2.0304 |
[32m[20221214 00:16:29 @agent_ppo2.py:130][0m Policy update time: 1.21 s
[32m[20221214 00:16:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 391.16
[32m[20221214 00:16:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 456.15
[32m[20221214 00:16:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 480.08
[32m[20221214 00:16:29 @agent_ppo2.py:143][0m Total time:      18.98 min
[32m[20221214 00:16:29 @agent_ppo2.py:145][0m 1761280 total steps have happened
[32m[20221214 00:16:29 @agent_ppo2.py:121][0m #------------------------ Iteration 4860 --------------------------#
[32m[20221214 00:16:29 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:16:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:16:29 @agent_ppo2.py:185][0m |           0.0017 |          74.7549 |           2.1933 |
[32m[20221214 00:16:29 @agent_ppo2.py:185][0m |          -0.0091 |          71.7273 |           2.2487 |
[32m[20221214 00:16:29 @agent_ppo2.py:185][0m |          -0.0089 |          71.0348 |           2.3179 |
[32m[20221214 00:16:30 @agent_ppo2.py:185][0m |          -0.0147 |          69.8891 |           2.3001 |
[32m[20221214 00:16:30 @agent_ppo2.py:185][0m |          -0.0154 |          69.5270 |           2.3917 |
[32m[20221214 00:16:30 @agent_ppo2.py:185][0m |          -0.0171 |          69.1503 |           2.4596 |
[32m[20221214 00:16:30 @agent_ppo2.py:185][0m |          -0.0161 |          68.5877 |           2.4286 |
[32m[20221214 00:16:30 @agent_ppo2.py:185][0m |          -0.0173 |          68.2572 |           2.5857 |
[32m[20221214 00:16:30 @agent_ppo2.py:185][0m |          -0.0167 |          67.7517 |           2.5290 |
[32m[20221214 00:16:30 @agent_ppo2.py:185][0m |          -0.0191 |          67.6386 |           2.5884 |
[32m[20221214 00:16:30 @agent_ppo2.py:130][0m Policy update time: 1.18 s
[32m[20221214 00:16:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 451.75
[32m[20221214 00:16:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 507.49
[32m[20221214 00:16:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 495.86
[32m[20221214 00:16:30 @agent_ppo2.py:143][0m Total time:      19.00 min
[32m[20221214 00:16:30 @agent_ppo2.py:145][0m 1763328 total steps have happened
[32m[20221214 00:16:30 @agent_ppo2.py:121][0m #------------------------ Iteration 4861 --------------------------#
[32m[20221214 00:16:31 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 00:16:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:16:31 @agent_ppo2.py:185][0m |           0.0012 |          58.4017 |           3.3622 |
[32m[20221214 00:16:31 @agent_ppo2.py:185][0m |          -0.0001 |          53.0020 |           3.3798 |
[32m[20221214 00:16:31 @agent_ppo2.py:185][0m |          -0.0043 |          51.0377 |           3.4669 |
[32m[20221214 00:16:31 @agent_ppo2.py:185][0m |          -0.0107 |          49.7844 |           3.4903 |
[32m[20221214 00:16:31 @agent_ppo2.py:185][0m |          -0.0118 |          48.6727 |           3.4992 |
[32m[20221214 00:16:31 @agent_ppo2.py:185][0m |          -0.0037 |          48.9910 |           3.4154 |
[32m[20221214 00:16:31 @agent_ppo2.py:185][0m |          -0.0125 |          46.4082 |           3.5261 |
[32m[20221214 00:16:32 @agent_ppo2.py:185][0m |          -0.0120 |          45.8158 |           3.5137 |
[32m[20221214 00:16:32 @agent_ppo2.py:185][0m |          -0.0121 |          45.4318 |           3.5566 |
[32m[20221214 00:16:32 @agent_ppo2.py:185][0m |          -0.0052 |          45.6881 |           3.5059 |
[32m[20221214 00:16:32 @agent_ppo2.py:130][0m Policy update time: 1.24 s
[32m[20221214 00:16:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 453.67
[32m[20221214 00:16:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 476.30
[32m[20221214 00:16:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 507.86
[32m[20221214 00:16:32 @agent_ppo2.py:143][0m Total time:      19.03 min
[32m[20221214 00:16:32 @agent_ppo2.py:145][0m 1765376 total steps have happened
[32m[20221214 00:16:32 @agent_ppo2.py:121][0m #------------------------ Iteration 4862 --------------------------#
[32m[20221214 00:16:32 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:16:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:16:32 @agent_ppo2.py:185][0m |           0.0094 |          81.4734 |           2.8079 |
[32m[20221214 00:16:33 @agent_ppo2.py:185][0m |           0.0030 |          72.7887 |           2.7302 |
[32m[20221214 00:16:33 @agent_ppo2.py:185][0m |          -0.0133 |          65.8673 |           2.6738 |
[32m[20221214 00:16:33 @agent_ppo2.py:185][0m |          -0.0121 |          64.4903 |           2.7110 |
[32m[20221214 00:16:33 @agent_ppo2.py:185][0m |          -0.0137 |          63.5896 |           2.7099 |
[32m[20221214 00:16:33 @agent_ppo2.py:185][0m |          -0.0108 |          63.0176 |           2.7422 |
[32m[20221214 00:16:33 @agent_ppo2.py:185][0m |          -0.0123 |          62.7742 |           2.6534 |
[32m[20221214 00:16:33 @agent_ppo2.py:185][0m |          -0.0111 |          62.6522 |           2.6701 |
[32m[20221214 00:16:33 @agent_ppo2.py:185][0m |          -0.0182 |          61.8540 |           2.6401 |
[32m[20221214 00:16:33 @agent_ppo2.py:185][0m |          -0.0189 |          61.1591 |           2.6360 |
[32m[20221214 00:16:33 @agent_ppo2.py:130][0m Policy update time: 1.17 s
[32m[20221214 00:16:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 495.57
[32m[20221214 00:16:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 536.05
[32m[20221214 00:16:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 367.77
[32m[20221214 00:16:34 @agent_ppo2.py:143][0m Total time:      19.05 min
[32m[20221214 00:16:34 @agent_ppo2.py:145][0m 1767424 total steps have happened
[32m[20221214 00:16:34 @agent_ppo2.py:121][0m #------------------------ Iteration 4863 --------------------------#
[32m[20221214 00:16:34 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:16:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:16:34 @agent_ppo2.py:185][0m |          -0.0025 |          71.7399 |           3.1097 |
[32m[20221214 00:16:34 @agent_ppo2.py:185][0m |          -0.0070 |          64.0175 |           3.0338 |
[32m[20221214 00:16:34 @agent_ppo2.py:185][0m |           0.0080 |          66.7320 |           3.0312 |
[32m[20221214 00:16:34 @agent_ppo2.py:185][0m |          -0.0085 |          58.4730 |           3.0774 |
[32m[20221214 00:16:34 @agent_ppo2.py:185][0m |          -0.0082 |          57.2200 |           3.0181 |
[32m[20221214 00:16:34 @agent_ppo2.py:185][0m |          -0.0126 |          55.7014 |           3.0899 |
[32m[20221214 00:16:35 @agent_ppo2.py:185][0m |          -0.0162 |          54.9931 |           3.0965 |
[32m[20221214 00:16:35 @agent_ppo2.py:185][0m |          -0.0191 |          54.3294 |           3.1749 |
[32m[20221214 00:16:35 @agent_ppo2.py:185][0m |          -0.0165 |          53.7191 |           3.0759 |
[32m[20221214 00:16:35 @agent_ppo2.py:185][0m |          -0.0172 |          53.1189 |           3.0860 |
[32m[20221214 00:16:35 @agent_ppo2.py:130][0m Policy update time: 1.18 s
[32m[20221214 00:16:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 474.31
[32m[20221214 00:16:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 536.35
[32m[20221214 00:16:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 506.38
[32m[20221214 00:16:35 @agent_ppo2.py:143][0m Total time:      19.08 min
[32m[20221214 00:16:35 @agent_ppo2.py:145][0m 1769472 total steps have happened
[32m[20221214 00:16:35 @agent_ppo2.py:121][0m #------------------------ Iteration 4864 --------------------------#
[32m[20221214 00:16:35 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:16:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:16:35 @agent_ppo2.py:185][0m |           0.0006 |          66.7756 |           2.6401 |
[32m[20221214 00:16:36 @agent_ppo2.py:185][0m |          -0.0063 |          61.5210 |           2.5326 |
[32m[20221214 00:16:36 @agent_ppo2.py:185][0m |          -0.0054 |          59.6035 |           2.7383 |
[32m[20221214 00:16:36 @agent_ppo2.py:185][0m |          -0.0103 |          58.6267 |           2.5711 |
[32m[20221214 00:16:36 @agent_ppo2.py:185][0m |          -0.0074 |          58.0904 |           2.5677 |
[32m[20221214 00:16:36 @agent_ppo2.py:185][0m |          -0.0123 |          57.6567 |           2.5432 |
[32m[20221214 00:16:36 @agent_ppo2.py:185][0m |          -0.0139 |          56.9048 |           2.5136 |
[32m[20221214 00:16:36 @agent_ppo2.py:185][0m |          -0.0145 |          56.3826 |           2.5004 |
[32m[20221214 00:16:36 @agent_ppo2.py:185][0m |          -0.0159 |          55.8508 |           2.5042 |
[32m[20221214 00:16:36 @agent_ppo2.py:185][0m |          -0.0158 |          55.3257 |           2.4315 |
[32m[20221214 00:16:36 @agent_ppo2.py:130][0m Policy update time: 1.20 s
[32m[20221214 00:16:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 537.60
[32m[20221214 00:16:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 564.60
[32m[20221214 00:16:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 536.25
[32m[20221214 00:16:37 @agent_ppo2.py:143][0m Total time:      19.10 min
[32m[20221214 00:16:37 @agent_ppo2.py:145][0m 1771520 total steps have happened
[32m[20221214 00:16:37 @agent_ppo2.py:121][0m #------------------------ Iteration 4865 --------------------------#
[32m[20221214 00:16:37 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:16:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:16:37 @agent_ppo2.py:185][0m |          -0.0008 |          61.1824 |           2.1334 |
[32m[20221214 00:16:37 @agent_ppo2.py:185][0m |          -0.0069 |          56.4271 |           2.1159 |
[32m[20221214 00:16:37 @agent_ppo2.py:185][0m |          -0.0087 |          56.5860 |           2.2217 |
[32m[20221214 00:16:37 @agent_ppo2.py:185][0m |          -0.0111 |          53.0166 |           2.1400 |
[32m[20221214 00:16:37 @agent_ppo2.py:185][0m |          -0.0173 |          51.8146 |           2.2736 |
[32m[20221214 00:16:38 @agent_ppo2.py:185][0m |          -0.0204 |          51.1883 |           2.2990 |
[32m[20221214 00:16:38 @agent_ppo2.py:185][0m |          -0.0180 |          50.6754 |           2.3408 |
[32m[20221214 00:16:38 @agent_ppo2.py:185][0m |          -0.0108 |          54.5661 |           2.3146 |
[32m[20221214 00:16:38 @agent_ppo2.py:185][0m |          -0.0253 |          49.8788 |           2.3786 |
[32m[20221214 00:16:38 @agent_ppo2.py:185][0m |          -0.0248 |          49.3167 |           2.4758 |
[32m[20221214 00:16:38 @agent_ppo2.py:130][0m Policy update time: 1.21 s
[32m[20221214 00:16:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 482.38
[32m[20221214 00:16:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 533.28
[32m[20221214 00:16:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 563.62
[32m[20221214 00:16:38 @agent_ppo2.py:143][0m Total time:      19.13 min
[32m[20221214 00:16:38 @agent_ppo2.py:145][0m 1773568 total steps have happened
[32m[20221214 00:16:38 @agent_ppo2.py:121][0m #------------------------ Iteration 4866 --------------------------#
[32m[20221214 00:16:38 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:16:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:16:38 @agent_ppo2.py:185][0m |           0.0016 |          51.0590 |           2.4553 |
[32m[20221214 00:16:39 @agent_ppo2.py:185][0m |          -0.0081 |          45.5096 |           2.5031 |
[32m[20221214 00:16:39 @agent_ppo2.py:185][0m |          -0.0111 |          43.2577 |           2.4838 |
[32m[20221214 00:16:39 @agent_ppo2.py:185][0m |          -0.0075 |          42.1479 |           2.5027 |
[32m[20221214 00:16:39 @agent_ppo2.py:185][0m |          -0.0155 |          41.5112 |           2.4120 |
[32m[20221214 00:16:39 @agent_ppo2.py:185][0m |          -0.0148 |          40.6558 |           2.3955 |
[32m[20221214 00:16:39 @agent_ppo2.py:185][0m |          -0.0130 |          40.0164 |           2.4603 |
[32m[20221214 00:16:39 @agent_ppo2.py:185][0m |          -0.0154 |          39.6340 |           2.3615 |
[32m[20221214 00:16:39 @agent_ppo2.py:185][0m |          -0.0149 |          39.0436 |           2.4120 |
[32m[20221214 00:16:40 @agent_ppo2.py:185][0m |          -0.0063 |          40.2517 |           2.3783 |
[32m[20221214 00:16:40 @agent_ppo2.py:130][0m Policy update time: 1.20 s
[32m[20221214 00:16:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 467.16
[32m[20221214 00:16:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 519.36
[32m[20221214 00:16:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 564.14
[32m[20221214 00:16:40 @agent_ppo2.py:143][0m Total time:      19.16 min
[32m[20221214 00:16:40 @agent_ppo2.py:145][0m 1775616 total steps have happened
[32m[20221214 00:16:40 @agent_ppo2.py:121][0m #------------------------ Iteration 4867 --------------------------#
[32m[20221214 00:16:40 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:16:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:16:40 @agent_ppo2.py:185][0m |          -0.0010 |          73.2805 |           1.8371 |
[32m[20221214 00:16:40 @agent_ppo2.py:185][0m |          -0.0058 |          65.2460 |           1.8913 |
[32m[20221214 00:16:40 @agent_ppo2.py:185][0m |          -0.0118 |          61.9521 |           1.8565 |
[32m[20221214 00:16:40 @agent_ppo2.py:185][0m |          -0.0096 |          59.8128 |           1.7692 |
[32m[20221214 00:16:40 @agent_ppo2.py:185][0m |          -0.0172 |          58.1963 |           1.7686 |
[32m[20221214 00:16:41 @agent_ppo2.py:185][0m |          -0.0131 |          56.8562 |           1.7953 |
[32m[20221214 00:16:41 @agent_ppo2.py:185][0m |          -0.0200 |          55.9868 |           1.7574 |
[32m[20221214 00:16:41 @agent_ppo2.py:185][0m |          -0.0162 |          54.8740 |           1.7108 |
[32m[20221214 00:16:41 @agent_ppo2.py:185][0m |          -0.0132 |          55.9951 |           1.7337 |
[32m[20221214 00:16:41 @agent_ppo2.py:185][0m |          -0.0192 |          53.6581 |           1.7201 |
[32m[20221214 00:16:41 @agent_ppo2.py:130][0m Policy update time: 1.20 s
[32m[20221214 00:16:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 498.69
[32m[20221214 00:16:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 603.07
[32m[20221214 00:16:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 567.72
[32m[20221214 00:16:41 @agent_ppo2.py:143][0m Total time:      19.18 min
[32m[20221214 00:16:41 @agent_ppo2.py:145][0m 1777664 total steps have happened
[32m[20221214 00:16:41 @agent_ppo2.py:121][0m #------------------------ Iteration 4868 --------------------------#
[32m[20221214 00:16:41 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:16:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:16:42 @agent_ppo2.py:185][0m |          -0.0017 |          61.1498 |           1.2644 |
[32m[20221214 00:16:42 @agent_ppo2.py:185][0m |          -0.0067 |          49.9179 |           1.2942 |
[32m[20221214 00:16:42 @agent_ppo2.py:185][0m |          -0.0142 |          47.4925 |           1.2749 |
[32m[20221214 00:16:42 @agent_ppo2.py:185][0m |          -0.0018 |          50.5742 |           1.1969 |
[32m[20221214 00:16:42 @agent_ppo2.py:185][0m |          -0.0179 |          46.0866 |           1.1695 |
[32m[20221214 00:16:42 @agent_ppo2.py:185][0m |          -0.0202 |          45.0425 |           1.2397 |
[32m[20221214 00:16:42 @agent_ppo2.py:185][0m |          -0.0170 |          45.0813 |           1.2263 |
[32m[20221214 00:16:42 @agent_ppo2.py:185][0m |          -0.0200 |          44.6459 |           1.2026 |
[32m[20221214 00:16:42 @agent_ppo2.py:185][0m |          -0.0212 |          44.1795 |           1.1764 |
[32m[20221214 00:16:43 @agent_ppo2.py:185][0m |          -0.0261 |          43.7585 |           1.1222 |
[32m[20221214 00:16:43 @agent_ppo2.py:130][0m Policy update time: 1.23 s
[32m[20221214 00:16:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 368.61
[32m[20221214 00:16:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 458.92
[32m[20221214 00:16:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 555.55
[32m[20221214 00:16:43 @agent_ppo2.py:143][0m Total time:      19.21 min
[32m[20221214 00:16:43 @agent_ppo2.py:145][0m 1779712 total steps have happened
[32m[20221214 00:16:43 @agent_ppo2.py:121][0m #------------------------ Iteration 4869 --------------------------#
[32m[20221214 00:16:43 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:16:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:16:43 @agent_ppo2.py:185][0m |           0.0010 |          66.3710 |           1.1729 |
[32m[20221214 00:16:43 @agent_ppo2.py:185][0m |          -0.0053 |          61.7338 |           1.2485 |
[32m[20221214 00:16:43 @agent_ppo2.py:185][0m |          -0.0074 |          59.4321 |           1.2780 |
[32m[20221214 00:16:43 @agent_ppo2.py:185][0m |          -0.0100 |          57.6412 |           1.2251 |
[32m[20221214 00:16:44 @agent_ppo2.py:185][0m |          -0.0140 |          56.6539 |           1.3209 |
[32m[20221214 00:16:44 @agent_ppo2.py:185][0m |          -0.0114 |          55.8619 |           1.2833 |
[32m[20221214 00:16:44 @agent_ppo2.py:185][0m |          -0.0057 |          56.9160 |           1.2603 |
[32m[20221214 00:16:44 @agent_ppo2.py:185][0m |          -0.0171 |          54.6868 |           1.3217 |
[32m[20221214 00:16:44 @agent_ppo2.py:185][0m |          -0.0153 |          53.9798 |           1.2973 |
[32m[20221214 00:16:44 @agent_ppo2.py:185][0m |          -0.0161 |          53.5692 |           1.3277 |
[32m[20221214 00:16:44 @agent_ppo2.py:130][0m Policy update time: 1.20 s
[32m[20221214 00:16:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 453.93
[32m[20221214 00:16:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 492.11
[32m[20221214 00:16:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 530.26
[32m[20221214 00:16:44 @agent_ppo2.py:143][0m Total time:      19.23 min
[32m[20221214 00:16:44 @agent_ppo2.py:145][0m 1781760 total steps have happened
[32m[20221214 00:16:44 @agent_ppo2.py:121][0m #------------------------ Iteration 4870 --------------------------#
[32m[20221214 00:16:44 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 00:16:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:16:45 @agent_ppo2.py:185][0m |           0.0032 |          56.8128 |           1.3303 |
[32m[20221214 00:16:45 @agent_ppo2.py:185][0m |          -0.0123 |          49.2043 |           1.2809 |
[32m[20221214 00:16:45 @agent_ppo2.py:185][0m |          -0.0135 |          45.7321 |           1.3689 |
[32m[20221214 00:16:45 @agent_ppo2.py:185][0m |          -0.0163 |          43.4069 |           1.3664 |
[32m[20221214 00:16:45 @agent_ppo2.py:185][0m |          -0.0148 |          42.3031 |           1.3477 |
[32m[20221214 00:16:45 @agent_ppo2.py:185][0m |          -0.0120 |          42.5324 |           1.3797 |
[32m[20221214 00:16:45 @agent_ppo2.py:185][0m |          -0.0177 |          39.4345 |           1.4350 |
[32m[20221214 00:16:45 @agent_ppo2.py:185][0m |          -0.0221 |          38.7681 |           1.4575 |
[32m[20221214 00:16:46 @agent_ppo2.py:185][0m |          -0.0185 |          38.2509 |           1.4791 |
[32m[20221214 00:16:46 @agent_ppo2.py:185][0m |          -0.0227 |          37.4808 |           1.3550 |
[32m[20221214 00:16:46 @agent_ppo2.py:130][0m Policy update time: 1.19 s
[32m[20221214 00:16:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 445.37
[32m[20221214 00:16:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 562.48
[32m[20221214 00:16:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 471.84
[32m[20221214 00:16:46 @agent_ppo2.py:143][0m Total time:      19.26 min
[32m[20221214 00:16:46 @agent_ppo2.py:145][0m 1783808 total steps have happened
[32m[20221214 00:16:46 @agent_ppo2.py:121][0m #------------------------ Iteration 4871 --------------------------#
[32m[20221214 00:16:46 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:16:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:16:46 @agent_ppo2.py:185][0m |           0.0003 |          72.8214 |           1.7905 |
[32m[20221214 00:16:46 @agent_ppo2.py:185][0m |          -0.0094 |          66.3154 |           1.8730 |
[32m[20221214 00:16:46 @agent_ppo2.py:185][0m |          -0.0093 |          63.2658 |           1.8481 |
[32m[20221214 00:16:47 @agent_ppo2.py:185][0m |          -0.0123 |          61.1372 |           1.9038 |
[32m[20221214 00:16:47 @agent_ppo2.py:185][0m |          -0.0171 |          59.9340 |           1.9528 |
[32m[20221214 00:16:47 @agent_ppo2.py:185][0m |          -0.0179 |          58.9165 |           1.8524 |
[32m[20221214 00:16:47 @agent_ppo2.py:185][0m |          -0.0172 |          58.0874 |           1.9570 |
[32m[20221214 00:16:47 @agent_ppo2.py:185][0m |          -0.0186 |          57.4052 |           1.9617 |
[32m[20221214 00:16:47 @agent_ppo2.py:185][0m |          -0.0214 |          56.9681 |           1.9146 |
[32m[20221214 00:16:47 @agent_ppo2.py:185][0m |          -0.0148 |          57.3224 |           1.8641 |
[32m[20221214 00:16:47 @agent_ppo2.py:130][0m Policy update time: 1.18 s
[32m[20221214 00:16:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 484.23
[32m[20221214 00:16:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 568.66
[32m[20221214 00:16:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 515.07
[32m[20221214 00:16:47 @agent_ppo2.py:143][0m Total time:      19.28 min
[32m[20221214 00:16:47 @agent_ppo2.py:145][0m 1785856 total steps have happened
[32m[20221214 00:16:47 @agent_ppo2.py:121][0m #------------------------ Iteration 4872 --------------------------#
[32m[20221214 00:16:48 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:16:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:16:48 @agent_ppo2.py:185][0m |           0.0122 |          69.8742 |           1.6583 |
[32m[20221214 00:16:48 @agent_ppo2.py:185][0m |          -0.0110 |          57.6234 |           1.6116 |
[32m[20221214 00:16:48 @agent_ppo2.py:185][0m |          -0.0139 |          53.8074 |           1.6510 |
[32m[20221214 00:16:48 @agent_ppo2.py:185][0m |          -0.0102 |          51.3056 |           1.6714 |
[32m[20221214 00:16:48 @agent_ppo2.py:185][0m |          -0.0168 |          49.0895 |           1.7108 |
[32m[20221214 00:16:48 @agent_ppo2.py:185][0m |          -0.0171 |          47.9806 |           1.8028 |
[32m[20221214 00:16:48 @agent_ppo2.py:185][0m |          -0.0172 |          46.3795 |           1.7786 |
[32m[20221214 00:16:49 @agent_ppo2.py:185][0m |          -0.0179 |          45.2498 |           1.8392 |
[32m[20221214 00:16:49 @agent_ppo2.py:185][0m |          -0.0183 |          44.6031 |           1.8781 |
[32m[20221214 00:16:49 @agent_ppo2.py:185][0m |          -0.0131 |          50.2070 |           1.9033 |
[32m[20221214 00:16:49 @agent_ppo2.py:130][0m Policy update time: 1.22 s
[32m[20221214 00:16:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 568.29
[32m[20221214 00:16:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 597.34
[32m[20221214 00:16:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 533.50
[32m[20221214 00:16:49 @agent_ppo2.py:143][0m Total time:      19.31 min
[32m[20221214 00:16:49 @agent_ppo2.py:145][0m 1787904 total steps have happened
[32m[20221214 00:16:49 @agent_ppo2.py:121][0m #------------------------ Iteration 4873 --------------------------#
[32m[20221214 00:16:49 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:16:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:16:49 @agent_ppo2.py:185][0m |           0.0007 |          72.2813 |           2.2475 |
[32m[20221214 00:16:49 @agent_ppo2.py:185][0m |           0.0037 |          53.2349 |           2.1918 |
[32m[20221214 00:16:49 @agent_ppo2.py:185][0m |          -0.0029 |          48.5869 |           2.1761 |
[32m[20221214 00:16:50 @agent_ppo2.py:185][0m |          -0.0082 |          47.2365 |           2.1014 |
[32m[20221214 00:16:50 @agent_ppo2.py:185][0m |          -0.0092 |          46.3927 |           2.1068 |
[32m[20221214 00:16:50 @agent_ppo2.py:185][0m |          -0.0091 |          45.8060 |           1.9959 |
[32m[20221214 00:16:50 @agent_ppo2.py:185][0m |          -0.0200 |          45.4070 |           2.0400 |
[32m[20221214 00:16:50 @agent_ppo2.py:185][0m |          -0.0149 |          45.0206 |           1.9736 |
[32m[20221214 00:16:50 @agent_ppo2.py:185][0m |          -0.0181 |          44.5015 |           1.9997 |
[32m[20221214 00:16:50 @agent_ppo2.py:185][0m |          -0.0130 |          44.3314 |           1.9741 |
[32m[20221214 00:16:50 @agent_ppo2.py:130][0m Policy update time: 1.19 s
[32m[20221214 00:16:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 450.72
[32m[20221214 00:16:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 534.82
[32m[20221214 00:16:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 558.78
[32m[20221214 00:16:50 @agent_ppo2.py:143][0m Total time:      19.33 min
[32m[20221214 00:16:50 @agent_ppo2.py:145][0m 1789952 total steps have happened
[32m[20221214 00:16:50 @agent_ppo2.py:121][0m #------------------------ Iteration 4874 --------------------------#
[32m[20221214 00:16:51 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:16:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:16:51 @agent_ppo2.py:185][0m |           0.0025 |          69.6173 |           1.8248 |
[32m[20221214 00:16:51 @agent_ppo2.py:185][0m |          -0.0035 |          57.4370 |           1.6284 |
[32m[20221214 00:16:51 @agent_ppo2.py:185][0m |          -0.0082 |          53.8879 |           1.6543 |
[32m[20221214 00:16:51 @agent_ppo2.py:185][0m |          -0.0129 |          51.9098 |           1.5617 |
[32m[20221214 00:16:51 @agent_ppo2.py:185][0m |          -0.0092 |          50.0154 |           1.5435 |
[32m[20221214 00:16:51 @agent_ppo2.py:185][0m |          -0.0041 |          50.5055 |           1.5360 |
[32m[20221214 00:16:52 @agent_ppo2.py:185][0m |          -0.0109 |          49.4302 |           1.4461 |
[32m[20221214 00:16:52 @agent_ppo2.py:185][0m |          -0.0136 |          47.5534 |           1.3491 |
[32m[20221214 00:16:52 @agent_ppo2.py:185][0m |          -0.0130 |          46.8053 |           1.3573 |
[32m[20221214 00:16:52 @agent_ppo2.py:185][0m |          -0.0148 |          46.0318 |           1.4194 |
[32m[20221214 00:16:52 @agent_ppo2.py:130][0m Policy update time: 1.22 s
[32m[20221214 00:16:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 535.26
[32m[20221214 00:16:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 578.48
[32m[20221214 00:16:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 624.00
[32m[20221214 00:16:52 @agent_ppo2.py:143][0m Total time:      19.36 min
[32m[20221214 00:16:52 @agent_ppo2.py:145][0m 1792000 total steps have happened
[32m[20221214 00:16:52 @agent_ppo2.py:121][0m #------------------------ Iteration 4875 --------------------------#
[32m[20221214 00:16:52 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:16:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:16:52 @agent_ppo2.py:185][0m |           0.0023 |          54.1630 |           0.9695 |
[32m[20221214 00:16:52 @agent_ppo2.py:185][0m |          -0.0016 |          44.8464 |           0.9165 |
[32m[20221214 00:16:53 @agent_ppo2.py:185][0m |          -0.0108 |          42.7620 |           0.9017 |
[32m[20221214 00:16:53 @agent_ppo2.py:185][0m |          -0.0117 |          41.9214 |           0.8750 |
[32m[20221214 00:16:53 @agent_ppo2.py:185][0m |          -0.0055 |          41.8289 |           0.8293 |
[32m[20221214 00:16:53 @agent_ppo2.py:185][0m |          -0.0105 |          40.7697 |           0.9356 |
[32m[20221214 00:16:53 @agent_ppo2.py:185][0m |          -0.0117 |          40.3484 |           0.9789 |
[32m[20221214 00:16:53 @agent_ppo2.py:185][0m |          -0.0145 |          39.9997 |           0.9503 |
[32m[20221214 00:16:53 @agent_ppo2.py:185][0m |          -0.0131 |          39.6909 |           0.9173 |
[32m[20221214 00:16:53 @agent_ppo2.py:185][0m |          -0.0181 |          39.5837 |           0.9303 |
[32m[20221214 00:16:53 @agent_ppo2.py:130][0m Policy update time: 1.20 s
[32m[20221214 00:16:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 492.04
[32m[20221214 00:16:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 523.42
[32m[20221214 00:16:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 626.55
[32m[20221214 00:16:54 @agent_ppo2.py:143][0m Total time:      19.39 min
[32m[20221214 00:16:54 @agent_ppo2.py:145][0m 1794048 total steps have happened
[32m[20221214 00:16:54 @agent_ppo2.py:121][0m #------------------------ Iteration 4876 --------------------------#
[32m[20221214 00:16:54 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:16:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:16:54 @agent_ppo2.py:185][0m |           0.0028 |          66.2856 |           0.3550 |
[32m[20221214 00:16:54 @agent_ppo2.py:185][0m |          -0.0078 |          61.0489 |           0.3633 |
[32m[20221214 00:16:54 @agent_ppo2.py:185][0m |          -0.0003 |          63.3653 |           0.4058 |
[32m[20221214 00:16:54 @agent_ppo2.py:185][0m |          -0.0107 |          59.0989 |           0.3685 |
[32m[20221214 00:16:54 @agent_ppo2.py:185][0m |          -0.0171 |          58.6934 |           0.3496 |
[32m[20221214 00:16:54 @agent_ppo2.py:185][0m |          -0.0151 |          59.2074 |           0.3858 |
[32m[20221214 00:16:55 @agent_ppo2.py:185][0m |          -0.0180 |          58.2912 |           0.4104 |
[32m[20221214 00:16:55 @agent_ppo2.py:185][0m |          -0.0058 |          61.0949 |           0.3160 |
[32m[20221214 00:16:55 @agent_ppo2.py:185][0m |          -0.0155 |          58.0243 |           0.3735 |
[32m[20221214 00:16:55 @agent_ppo2.py:185][0m |          -0.0237 |          57.5935 |           0.2851 |
[32m[20221214 00:16:55 @agent_ppo2.py:130][0m Policy update time: 1.22 s
[32m[20221214 00:16:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 431.46
[32m[20221214 00:16:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 476.01
[32m[20221214 00:16:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 526.36
[32m[20221214 00:16:55 @agent_ppo2.py:143][0m Total time:      19.41 min
[32m[20221214 00:16:55 @agent_ppo2.py:145][0m 1796096 total steps have happened
[32m[20221214 00:16:55 @agent_ppo2.py:121][0m #------------------------ Iteration 4877 --------------------------#
[32m[20221214 00:16:55 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:16:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:16:55 @agent_ppo2.py:185][0m |          -0.0003 |          50.6436 |           0.9445 |
[32m[20221214 00:16:56 @agent_ppo2.py:185][0m |          -0.0093 |          46.9160 |           0.9530 |
[32m[20221214 00:16:56 @agent_ppo2.py:185][0m |          -0.0125 |          45.7475 |           0.9003 |
[32m[20221214 00:16:56 @agent_ppo2.py:185][0m |          -0.0097 |          44.6982 |           0.9299 |
[32m[20221214 00:16:56 @agent_ppo2.py:185][0m |          -0.0158 |          43.9192 |           0.8710 |
[32m[20221214 00:16:56 @agent_ppo2.py:185][0m |          -0.0096 |          43.3683 |           0.8610 |
[32m[20221214 00:16:56 @agent_ppo2.py:185][0m |          -0.0163 |          42.9006 |           0.8842 |
[32m[20221214 00:16:56 @agent_ppo2.py:185][0m |          -0.0130 |          42.7348 |           0.8465 |
[32m[20221214 00:16:56 @agent_ppo2.py:185][0m |          -0.0189 |          42.1096 |           0.8664 |
[32m[20221214 00:16:56 @agent_ppo2.py:185][0m |          -0.0196 |          41.8586 |           0.8054 |
[32m[20221214 00:16:56 @agent_ppo2.py:130][0m Policy update time: 1.20 s
[32m[20221214 00:16:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 431.44
[32m[20221214 00:16:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 468.01
[32m[20221214 00:16:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 478.14
[32m[20221214 00:16:57 @agent_ppo2.py:143][0m Total time:      19.44 min
[32m[20221214 00:16:57 @agent_ppo2.py:145][0m 1798144 total steps have happened
[32m[20221214 00:16:57 @agent_ppo2.py:121][0m #------------------------ Iteration 4878 --------------------------#
[32m[20221214 00:16:57 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:16:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:16:57 @agent_ppo2.py:185][0m |          -0.0018 |          54.5512 |           0.3494 |
[32m[20221214 00:16:57 @agent_ppo2.py:185][0m |          -0.0050 |          50.9277 |           0.5193 |
[32m[20221214 00:16:57 @agent_ppo2.py:185][0m |          -0.0095 |          49.4953 |           0.4981 |
[32m[20221214 00:16:57 @agent_ppo2.py:185][0m |          -0.0093 |          52.2382 |           0.4838 |
[32m[20221214 00:16:57 @agent_ppo2.py:185][0m |          -0.0054 |          51.5308 |           0.6123 |
[32m[20221214 00:16:58 @agent_ppo2.py:185][0m |          -0.0133 |          48.0031 |           0.6627 |
[32m[20221214 00:16:58 @agent_ppo2.py:185][0m |          -0.0163 |          47.4169 |           0.5800 |
[32m[20221214 00:16:58 @agent_ppo2.py:185][0m |          -0.0162 |          47.0467 |           0.6221 |
[32m[20221214 00:16:58 @agent_ppo2.py:185][0m |          -0.0151 |          47.2856 |           0.6941 |
[32m[20221214 00:16:58 @agent_ppo2.py:185][0m |          -0.0180 |          46.8014 |           0.7245 |
[32m[20221214 00:16:58 @agent_ppo2.py:130][0m Policy update time: 1.22 s
[32m[20221214 00:16:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 446.91
[32m[20221214 00:16:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 545.37
[32m[20221214 00:16:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 570.88
[32m[20221214 00:16:58 @agent_ppo2.py:143][0m Total time:      19.46 min
[32m[20221214 00:16:58 @agent_ppo2.py:145][0m 1800192 total steps have happened
[32m[20221214 00:16:58 @agent_ppo2.py:121][0m #------------------------ Iteration 4879 --------------------------#
[32m[20221214 00:16:58 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:16:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:16:59 @agent_ppo2.py:185][0m |          -0.0010 |          57.0297 |           0.3514 |
[32m[20221214 00:16:59 @agent_ppo2.py:185][0m |          -0.0055 |          51.9248 |           0.4727 |
[32m[20221214 00:16:59 @agent_ppo2.py:185][0m |          -0.0092 |          49.7100 |           0.5303 |
[32m[20221214 00:16:59 @agent_ppo2.py:185][0m |          -0.0068 |          50.9313 |           0.5249 |
[32m[20221214 00:16:59 @agent_ppo2.py:185][0m |          -0.0116 |          47.3763 |           0.5980 |
[32m[20221214 00:16:59 @agent_ppo2.py:185][0m |           0.0033 |          50.0462 |           0.6512 |
[32m[20221214 00:16:59 @agent_ppo2.py:185][0m |          -0.0130 |          46.2495 |           0.6848 |
[32m[20221214 00:16:59 @agent_ppo2.py:185][0m |          -0.0081 |          48.1791 |           0.6560 |
[32m[20221214 00:16:59 @agent_ppo2.py:185][0m |          -0.0126 |          45.4007 |           0.6565 |
[32m[20221214 00:17:00 @agent_ppo2.py:185][0m |          -0.0180 |          44.8460 |           0.6515 |
[32m[20221214 00:17:00 @agent_ppo2.py:130][0m Policy update time: 1.22 s
[32m[20221214 00:17:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 509.63
[32m[20221214 00:17:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 548.46
[32m[20221214 00:17:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 556.43
[32m[20221214 00:17:00 @agent_ppo2.py:143][0m Total time:      19.49 min
[32m[20221214 00:17:00 @agent_ppo2.py:145][0m 1802240 total steps have happened
[32m[20221214 00:17:00 @agent_ppo2.py:121][0m #------------------------ Iteration 4880 --------------------------#
[32m[20221214 00:17:00 @agent_ppo2.py:127][0m Sampling time: 0.23 s by 5 slaves
[32m[20221214 00:17:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:17:00 @agent_ppo2.py:185][0m |          -0.0034 |          84.0663 |           1.4043 |
[32m[20221214 00:17:00 @agent_ppo2.py:185][0m |          -0.0039 |          77.4306 |           1.6291 |
[32m[20221214 00:17:00 @agent_ppo2.py:185][0m |          -0.0104 |          74.9887 |           1.5517 |
[32m[20221214 00:17:01 @agent_ppo2.py:185][0m |          -0.0123 |          73.5858 |           1.6296 |
[32m[20221214 00:17:01 @agent_ppo2.py:185][0m |          -0.0109 |          72.6992 |           1.5837 |
[32m[20221214 00:17:01 @agent_ppo2.py:185][0m |          -0.0066 |          77.3673 |           1.6203 |
[32m[20221214 00:17:01 @agent_ppo2.py:185][0m |          -0.0106 |          71.6567 |           1.6308 |
[32m[20221214 00:17:01 @agent_ppo2.py:185][0m |          -0.0144 |          70.0670 |           1.6453 |
[32m[20221214 00:17:01 @agent_ppo2.py:185][0m |          -0.0188 |          69.4874 |           1.5841 |
[32m[20221214 00:17:01 @agent_ppo2.py:185][0m |          -0.0167 |          69.3077 |           1.5621 |
[32m[20221214 00:17:01 @agent_ppo2.py:130][0m Policy update time: 1.19 s
[32m[20221214 00:17:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 527.84
[32m[20221214 00:17:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 599.37
[32m[20221214 00:17:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 545.95
[32m[20221214 00:17:01 @agent_ppo2.py:143][0m Total time:      19.52 min
[32m[20221214 00:17:01 @agent_ppo2.py:145][0m 1804288 total steps have happened
[32m[20221214 00:17:01 @agent_ppo2.py:121][0m #------------------------ Iteration 4881 --------------------------#
[32m[20221214 00:17:01 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:17:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:17:02 @agent_ppo2.py:185][0m |           0.0017 |          66.2131 |           0.6213 |
[32m[20221214 00:17:02 @agent_ppo2.py:185][0m |          -0.0067 |          59.5862 |           0.6165 |
[32m[20221214 00:17:02 @agent_ppo2.py:185][0m |          -0.0059 |          57.0110 |           0.5271 |
[32m[20221214 00:17:02 @agent_ppo2.py:185][0m |          -0.0081 |          55.1761 |           0.5347 |
[32m[20221214 00:17:02 @agent_ppo2.py:185][0m |          -0.0114 |          54.1183 |           0.4775 |
[32m[20221214 00:17:02 @agent_ppo2.py:185][0m |          -0.0074 |          52.9492 |           0.5347 |
[32m[20221214 00:17:02 @agent_ppo2.py:185][0m |          -0.0139 |          51.9157 |           0.5756 |
[32m[20221214 00:17:02 @agent_ppo2.py:185][0m |          -0.0091 |          51.1890 |           0.5419 |
[32m[20221214 00:17:02 @agent_ppo2.py:185][0m |          -0.0043 |          53.4420 |           0.4579 |
[32m[20221214 00:17:03 @agent_ppo2.py:185][0m |          -0.0134 |          49.9306 |           0.4557 |
[32m[20221214 00:17:03 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:17:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 560.80
[32m[20221214 00:17:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 605.05
[32m[20221214 00:17:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 567.52
[32m[20221214 00:17:03 @agent_ppo2.py:143][0m Total time:      19.54 min
[32m[20221214 00:17:03 @agent_ppo2.py:145][0m 1806336 total steps have happened
[32m[20221214 00:17:03 @agent_ppo2.py:121][0m #------------------------ Iteration 4882 --------------------------#
[32m[20221214 00:17:03 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:17:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:17:03 @agent_ppo2.py:185][0m |           0.0034 |          73.8909 |           0.9089 |
[32m[20221214 00:17:03 @agent_ppo2.py:185][0m |          -0.0010 |          68.5621 |           1.1309 |
[32m[20221214 00:17:03 @agent_ppo2.py:185][0m |          -0.0093 |          66.0310 |           1.2018 |
[32m[20221214 00:17:03 @agent_ppo2.py:185][0m |          -0.0016 |          65.6441 |           1.1850 |
[32m[20221214 00:17:03 @agent_ppo2.py:185][0m |          -0.0105 |          63.9586 |           1.2924 |
[32m[20221214 00:17:04 @agent_ppo2.py:185][0m |          -0.0114 |          62.9885 |           1.4066 |
[32m[20221214 00:17:04 @agent_ppo2.py:185][0m |          -0.0122 |          62.5514 |           1.4631 |
[32m[20221214 00:17:04 @agent_ppo2.py:185][0m |          -0.0022 |          70.5769 |           1.5354 |
[32m[20221214 00:17:04 @agent_ppo2.py:185][0m |          -0.0128 |          61.9546 |           1.6900 |
[32m[20221214 00:17:04 @agent_ppo2.py:185][0m |          -0.0088 |          61.5189 |           1.6356 |
[32m[20221214 00:17:04 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 00:17:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 464.06
[32m[20221214 00:17:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 516.16
[32m[20221214 00:17:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 567.71
[32m[20221214 00:17:04 @agent_ppo2.py:143][0m Total time:      19.56 min
[32m[20221214 00:17:04 @agent_ppo2.py:145][0m 1808384 total steps have happened
[32m[20221214 00:17:04 @agent_ppo2.py:121][0m #------------------------ Iteration 4883 --------------------------#
[32m[20221214 00:17:04 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:17:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:17:04 @agent_ppo2.py:185][0m |          -0.0030 |          55.9718 |           0.9135 |
[32m[20221214 00:17:05 @agent_ppo2.py:185][0m |          -0.0019 |          51.5150 |           1.0487 |
[32m[20221214 00:17:05 @agent_ppo2.py:185][0m |          -0.0023 |          50.5094 |           1.0696 |
[32m[20221214 00:17:05 @agent_ppo2.py:185][0m |          -0.0138 |          49.3897 |           1.1158 |
[32m[20221214 00:17:05 @agent_ppo2.py:185][0m |          -0.0105 |          48.8597 |           1.2952 |
[32m[20221214 00:17:05 @agent_ppo2.py:185][0m |          -0.0123 |          48.4429 |           1.1709 |
[32m[20221214 00:17:05 @agent_ppo2.py:185][0m |          -0.0112 |          48.4253 |           1.1896 |
[32m[20221214 00:17:05 @agent_ppo2.py:185][0m |          -0.0117 |          50.0128 |           1.2374 |
[32m[20221214 00:17:05 @agent_ppo2.py:185][0m |          -0.0128 |          47.3109 |           1.3218 |
[32m[20221214 00:17:05 @agent_ppo2.py:185][0m |          -0.0136 |          47.0635 |           1.3590 |
[32m[20221214 00:17:05 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:17:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 423.25
[32m[20221214 00:17:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 484.05
[32m[20221214 00:17:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 514.60
[32m[20221214 00:17:06 @agent_ppo2.py:143][0m Total time:      19.59 min
[32m[20221214 00:17:06 @agent_ppo2.py:145][0m 1810432 total steps have happened
[32m[20221214 00:17:06 @agent_ppo2.py:121][0m #------------------------ Iteration 4884 --------------------------#
[32m[20221214 00:17:06 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:17:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:17:06 @agent_ppo2.py:185][0m |          -0.0010 |          66.9590 |           1.9666 |
[32m[20221214 00:17:06 @agent_ppo2.py:185][0m |           0.0037 |          70.7318 |           2.0247 |
[32m[20221214 00:17:06 @agent_ppo2.py:185][0m |          -0.0022 |          66.3244 |           1.9393 |
[32m[20221214 00:17:06 @agent_ppo2.py:185][0m |          -0.0129 |          60.5644 |           1.8480 |
[32m[20221214 00:17:06 @agent_ppo2.py:185][0m |          -0.0159 |          59.7040 |           1.9416 |
[32m[20221214 00:17:06 @agent_ppo2.py:185][0m |          -0.0151 |          59.2799 |           1.8727 |
[32m[20221214 00:17:07 @agent_ppo2.py:185][0m |          -0.0147 |          58.5681 |           1.9230 |
[32m[20221214 00:17:07 @agent_ppo2.py:185][0m |          -0.0182 |          58.1389 |           1.7912 |
[32m[20221214 00:17:07 @agent_ppo2.py:185][0m |          -0.0195 |          57.7670 |           1.8707 |
[32m[20221214 00:17:07 @agent_ppo2.py:185][0m |          -0.0096 |          61.7752 |           1.8060 |
[32m[20221214 00:17:07 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221214 00:17:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 461.17
[32m[20221214 00:17:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 510.25
[32m[20221214 00:17:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 486.65
[32m[20221214 00:17:07 @agent_ppo2.py:143][0m Total time:      19.61 min
[32m[20221214 00:17:07 @agent_ppo2.py:145][0m 1812480 total steps have happened
[32m[20221214 00:17:07 @agent_ppo2.py:121][0m #------------------------ Iteration 4885 --------------------------#
[32m[20221214 00:17:07 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:17:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:17:07 @agent_ppo2.py:185][0m |           0.0034 |          82.0469 |           1.7464 |
[32m[20221214 00:17:07 @agent_ppo2.py:185][0m |          -0.0033 |          77.4370 |           1.6513 |
[32m[20221214 00:17:08 @agent_ppo2.py:185][0m |          -0.0059 |          75.6453 |           1.6625 |
[32m[20221214 00:17:08 @agent_ppo2.py:185][0m |          -0.0070 |          74.2102 |           1.6534 |
[32m[20221214 00:17:08 @agent_ppo2.py:185][0m |          -0.0058 |          73.5896 |           1.6909 |
[32m[20221214 00:17:08 @agent_ppo2.py:185][0m |          -0.0116 |          72.6335 |           1.6880 |
[32m[20221214 00:17:08 @agent_ppo2.py:185][0m |          -0.0098 |          72.4639 |           1.7072 |
[32m[20221214 00:17:08 @agent_ppo2.py:185][0m |          -0.0119 |          71.8162 |           1.7399 |
[32m[20221214 00:17:08 @agent_ppo2.py:185][0m |          -0.0149 |          71.3823 |           1.7600 |
[32m[20221214 00:17:08 @agent_ppo2.py:185][0m |          -0.0141 |          70.7768 |           1.6700 |
[32m[20221214 00:17:08 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:17:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 504.04
[32m[20221214 00:17:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 561.28
[32m[20221214 00:17:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 582.04
[32m[20221214 00:17:08 @agent_ppo2.py:143][0m Total time:      19.63 min
[32m[20221214 00:17:08 @agent_ppo2.py:145][0m 1814528 total steps have happened
[32m[20221214 00:17:08 @agent_ppo2.py:121][0m #------------------------ Iteration 4886 --------------------------#
[32m[20221214 00:17:09 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:17:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:17:09 @agent_ppo2.py:185][0m |          -0.0003 |          87.0087 |           2.5084 |
[32m[20221214 00:17:09 @agent_ppo2.py:185][0m |          -0.0051 |          82.9908 |           2.5088 |
[32m[20221214 00:17:09 @agent_ppo2.py:185][0m |          -0.0087 |          80.2017 |           2.5636 |
[32m[20221214 00:17:09 @agent_ppo2.py:185][0m |          -0.0065 |          80.6295 |           2.5803 |
[32m[20221214 00:17:09 @agent_ppo2.py:185][0m |          -0.0130 |          77.9514 |           2.5663 |
[32m[20221214 00:17:09 @agent_ppo2.py:185][0m |          -0.0120 |          77.1676 |           2.5488 |
[32m[20221214 00:17:09 @agent_ppo2.py:185][0m |          -0.0150 |          76.7992 |           2.5861 |
[32m[20221214 00:17:10 @agent_ppo2.py:185][0m |          -0.0130 |          76.3056 |           2.5395 |
[32m[20221214 00:17:10 @agent_ppo2.py:185][0m |          -0.0143 |          75.9368 |           2.5622 |
[32m[20221214 00:17:10 @agent_ppo2.py:185][0m |          -0.0128 |          75.6480 |           2.5077 |
[32m[20221214 00:17:10 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 00:17:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 498.78
[32m[20221214 00:17:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 555.43
[32m[20221214 00:17:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 554.13
[32m[20221214 00:17:10 @agent_ppo2.py:143][0m Total time:      19.66 min
[32m[20221214 00:17:10 @agent_ppo2.py:145][0m 1816576 total steps have happened
[32m[20221214 00:17:10 @agent_ppo2.py:121][0m #------------------------ Iteration 4887 --------------------------#
[32m[20221214 00:17:10 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:17:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:17:10 @agent_ppo2.py:185][0m |           0.0133 |          82.9526 |           2.4734 |
[32m[20221214 00:17:10 @agent_ppo2.py:185][0m |          -0.0056 |          74.1716 |           2.4347 |
[32m[20221214 00:17:10 @agent_ppo2.py:185][0m |          -0.0111 |          72.0644 |           2.3635 |
[32m[20221214 00:17:11 @agent_ppo2.py:185][0m |          -0.0089 |          69.9863 |           2.4119 |
[32m[20221214 00:17:11 @agent_ppo2.py:185][0m |          -0.0085 |          69.1587 |           2.2638 |
[32m[20221214 00:17:11 @agent_ppo2.py:185][0m |          -0.0126 |          68.1985 |           2.2927 |
[32m[20221214 00:17:11 @agent_ppo2.py:185][0m |          -0.0130 |          67.5332 |           2.2408 |
[32m[20221214 00:17:11 @agent_ppo2.py:185][0m |          -0.0176 |          67.3501 |           2.1742 |
[32m[20221214 00:17:11 @agent_ppo2.py:185][0m |          -0.0158 |          66.7564 |           2.1054 |
[32m[20221214 00:17:11 @agent_ppo2.py:185][0m |          -0.0192 |          66.1001 |           2.1652 |
[32m[20221214 00:17:11 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:17:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 472.78
[32m[20221214 00:17:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 540.61
[32m[20221214 00:17:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 470.13
[32m[20221214 00:17:11 @agent_ppo2.py:143][0m Total time:      19.68 min
[32m[20221214 00:17:11 @agent_ppo2.py:145][0m 1818624 total steps have happened
[32m[20221214 00:17:11 @agent_ppo2.py:121][0m #------------------------ Iteration 4888 --------------------------#
[32m[20221214 00:17:11 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:17:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:17:12 @agent_ppo2.py:185][0m |           0.0022 |          76.6422 |           0.8695 |
[32m[20221214 00:17:12 @agent_ppo2.py:185][0m |          -0.0015 |          68.5909 |           1.0273 |
[32m[20221214 00:17:12 @agent_ppo2.py:185][0m |          -0.0028 |          68.1154 |           0.9592 |
[32m[20221214 00:17:12 @agent_ppo2.py:185][0m |          -0.0065 |          65.6800 |           1.0189 |
[32m[20221214 00:17:12 @agent_ppo2.py:185][0m |          -0.0066 |          64.9097 |           0.9534 |
[32m[20221214 00:17:12 @agent_ppo2.py:185][0m |          -0.0110 |          62.9150 |           1.0623 |
[32m[20221214 00:17:12 @agent_ppo2.py:185][0m |          -0.0138 |          61.9474 |           1.0368 |
[32m[20221214 00:17:12 @agent_ppo2.py:185][0m |          -0.0115 |          61.6867 |           1.0456 |
[32m[20221214 00:17:13 @agent_ppo2.py:185][0m |          -0.0103 |          60.9352 |           1.0989 |
[32m[20221214 00:17:13 @agent_ppo2.py:185][0m |          -0.0149 |          60.5647 |           1.1140 |
[32m[20221214 00:17:13 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221214 00:17:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 545.76
[32m[20221214 00:17:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 588.06
[32m[20221214 00:17:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 608.82
[32m[20221214 00:17:13 @agent_ppo2.py:143][0m Total time:      19.71 min
[32m[20221214 00:17:13 @agent_ppo2.py:145][0m 1820672 total steps have happened
[32m[20221214 00:17:13 @agent_ppo2.py:121][0m #------------------------ Iteration 4889 --------------------------#
[32m[20221214 00:17:13 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:17:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:17:13 @agent_ppo2.py:185][0m |           0.0002 |          90.9051 |           2.0657 |
[32m[20221214 00:17:13 @agent_ppo2.py:185][0m |          -0.0055 |          85.9721 |           2.1254 |
[32m[20221214 00:17:13 @agent_ppo2.py:185][0m |          -0.0100 |          84.1637 |           2.1285 |
[32m[20221214 00:17:13 @agent_ppo2.py:185][0m |          -0.0104 |          82.9228 |           2.1688 |
[32m[20221214 00:17:14 @agent_ppo2.py:185][0m |          -0.0130 |          82.1259 |           2.1617 |
[32m[20221214 00:17:14 @agent_ppo2.py:185][0m |          -0.0127 |          81.3119 |           2.2607 |
[32m[20221214 00:17:14 @agent_ppo2.py:185][0m |          -0.0140 |          80.6099 |           2.2686 |
[32m[20221214 00:17:14 @agent_ppo2.py:185][0m |          -0.0121 |          80.8133 |           2.3171 |
[32m[20221214 00:17:14 @agent_ppo2.py:185][0m |          -0.0120 |          80.0963 |           2.3683 |
[32m[20221214 00:17:14 @agent_ppo2.py:185][0m |          -0.0173 |          79.4647 |           2.4053 |
[32m[20221214 00:17:14 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:17:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 555.69
[32m[20221214 00:17:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 625.11
[32m[20221214 00:17:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 486.10
[32m[20221214 00:17:14 @agent_ppo2.py:143][0m Total time:      19.73 min
[32m[20221214 00:17:14 @agent_ppo2.py:145][0m 1822720 total steps have happened
[32m[20221214 00:17:14 @agent_ppo2.py:121][0m #------------------------ Iteration 4890 --------------------------#
[32m[20221214 00:17:14 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:17:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:17:15 @agent_ppo2.py:185][0m |          -0.0004 |          75.0515 |           2.5531 |
[32m[20221214 00:17:15 @agent_ppo2.py:185][0m |           0.0004 |          76.9307 |           2.4344 |
[32m[20221214 00:17:15 @agent_ppo2.py:185][0m |          -0.0088 |          68.9114 |           2.4810 |
[32m[20221214 00:17:15 @agent_ppo2.py:185][0m |          -0.0189 |          68.1560 |           2.3806 |
[32m[20221214 00:17:15 @agent_ppo2.py:185][0m |          -0.0127 |          67.3870 |           2.4239 |
[32m[20221214 00:17:15 @agent_ppo2.py:185][0m |          -0.0133 |          67.1139 |           2.3912 |
[32m[20221214 00:17:15 @agent_ppo2.py:185][0m |          -0.0138 |          66.6826 |           2.4025 |
[32m[20221214 00:17:15 @agent_ppo2.py:185][0m |          -0.0157 |          66.7119 |           2.3202 |
[32m[20221214 00:17:15 @agent_ppo2.py:185][0m |          -0.0177 |          65.5396 |           2.3152 |
[32m[20221214 00:17:16 @agent_ppo2.py:185][0m |          -0.0166 |          65.4261 |           2.3455 |
[32m[20221214 00:17:16 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 00:17:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 463.63
[32m[20221214 00:17:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 533.51
[32m[20221214 00:17:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 462.66
[32m[20221214 00:17:16 @agent_ppo2.py:143][0m Total time:      19.75 min
[32m[20221214 00:17:16 @agent_ppo2.py:145][0m 1824768 total steps have happened
[32m[20221214 00:17:16 @agent_ppo2.py:121][0m #------------------------ Iteration 4891 --------------------------#
[32m[20221214 00:17:16 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:17:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:17:16 @agent_ppo2.py:185][0m |           0.0018 |          83.1648 |           1.2529 |
[32m[20221214 00:17:16 @agent_ppo2.py:185][0m |          -0.0059 |          78.8256 |           1.1717 |
[32m[20221214 00:17:16 @agent_ppo2.py:185][0m |          -0.0047 |          76.7267 |           1.1976 |
[32m[20221214 00:17:16 @agent_ppo2.py:185][0m |          -0.0103 |          75.4151 |           1.1004 |
[32m[20221214 00:17:16 @agent_ppo2.py:185][0m |          -0.0084 |          73.9130 |           1.1487 |
[32m[20221214 00:17:16 @agent_ppo2.py:185][0m |          -0.0080 |          72.6978 |           1.1411 |
[32m[20221214 00:17:17 @agent_ppo2.py:185][0m |          -0.0079 |          71.7402 |           1.1512 |
[32m[20221214 00:17:17 @agent_ppo2.py:185][0m |          -0.0146 |          71.0324 |           1.1329 |
[32m[20221214 00:17:17 @agent_ppo2.py:185][0m |          -0.0150 |          70.1446 |           1.0907 |
[32m[20221214 00:17:17 @agent_ppo2.py:185][0m |          -0.0153 |          69.9073 |           1.1634 |
[32m[20221214 00:17:17 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:17:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 525.16
[32m[20221214 00:17:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 549.87
[32m[20221214 00:17:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 499.38
[32m[20221214 00:17:17 @agent_ppo2.py:143][0m Total time:      19.78 min
[32m[20221214 00:17:17 @agent_ppo2.py:145][0m 1826816 total steps have happened
[32m[20221214 00:17:17 @agent_ppo2.py:121][0m #------------------------ Iteration 4892 --------------------------#
[32m[20221214 00:17:17 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:17:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:17:17 @agent_ppo2.py:185][0m |           0.0017 |          80.7945 |           0.6846 |
[32m[20221214 00:17:18 @agent_ppo2.py:185][0m |          -0.0049 |          77.0668 |           0.7495 |
[32m[20221214 00:17:18 @agent_ppo2.py:185][0m |          -0.0060 |          74.6526 |           0.7071 |
[32m[20221214 00:17:18 @agent_ppo2.py:185][0m |          -0.0105 |          73.3760 |           0.7041 |
[32m[20221214 00:17:18 @agent_ppo2.py:185][0m |          -0.0056 |          72.5139 |           0.6500 |
[32m[20221214 00:17:18 @agent_ppo2.py:185][0m |          -0.0102 |          71.8274 |           0.7233 |
[32m[20221214 00:17:18 @agent_ppo2.py:185][0m |          -0.0115 |          71.3759 |           0.7136 |
[32m[20221214 00:17:18 @agent_ppo2.py:185][0m |          -0.0131 |          70.7286 |           0.7349 |
[32m[20221214 00:17:18 @agent_ppo2.py:185][0m |          -0.0139 |          70.2798 |           0.7292 |
[32m[20221214 00:17:18 @agent_ppo2.py:185][0m |          -0.0143 |          69.8103 |           0.6774 |
[32m[20221214 00:17:18 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221214 00:17:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 536.58
[32m[20221214 00:17:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 593.17
[32m[20221214 00:17:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 633.36
[32m[20221214 00:17:19 @agent_ppo2.py:143][0m Total time:      19.80 min
[32m[20221214 00:17:19 @agent_ppo2.py:145][0m 1828864 total steps have happened
[32m[20221214 00:17:19 @agent_ppo2.py:121][0m #------------------------ Iteration 4893 --------------------------#
[32m[20221214 00:17:19 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:17:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:17:19 @agent_ppo2.py:185][0m |           0.0001 |          69.7381 |           0.9485 |
[32m[20221214 00:17:19 @agent_ppo2.py:185][0m |          -0.0075 |          63.2009 |           0.9951 |
[32m[20221214 00:17:19 @agent_ppo2.py:185][0m |          -0.0079 |          60.3583 |           1.0695 |
[32m[20221214 00:17:19 @agent_ppo2.py:185][0m |          -0.0101 |          58.8018 |           1.0660 |
[32m[20221214 00:17:19 @agent_ppo2.py:185][0m |          -0.0126 |          57.5580 |           1.1099 |
[32m[20221214 00:17:19 @agent_ppo2.py:185][0m |          -0.0167 |          56.7533 |           1.1918 |
[32m[20221214 00:17:19 @agent_ppo2.py:185][0m |          -0.0170 |          56.0036 |           1.1608 |
[32m[20221214 00:17:20 @agent_ppo2.py:185][0m |          -0.0125 |          56.0024 |           1.1665 |
[32m[20221214 00:17:20 @agent_ppo2.py:185][0m |          -0.0162 |          54.5506 |           1.2103 |
[32m[20221214 00:17:20 @agent_ppo2.py:185][0m |          -0.0207 |          53.8960 |           1.1857 |
[32m[20221214 00:17:20 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:17:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 494.86
[32m[20221214 00:17:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 620.52
[32m[20221214 00:17:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 563.22
[32m[20221214 00:17:20 @agent_ppo2.py:143][0m Total time:      19.83 min
[32m[20221214 00:17:20 @agent_ppo2.py:145][0m 1830912 total steps have happened
[32m[20221214 00:17:20 @agent_ppo2.py:121][0m #------------------------ Iteration 4894 --------------------------#
[32m[20221214 00:17:20 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:17:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:17:20 @agent_ppo2.py:185][0m |           0.0042 |          63.8457 |           1.9318 |
[32m[20221214 00:17:20 @agent_ppo2.py:185][0m |          -0.0035 |          56.0768 |           1.7957 |
[32m[20221214 00:17:20 @agent_ppo2.py:185][0m |          -0.0026 |          53.3113 |           1.8365 |
[32m[20221214 00:17:21 @agent_ppo2.py:185][0m |          -0.0083 |          51.5511 |           1.7021 |
[32m[20221214 00:17:21 @agent_ppo2.py:185][0m |          -0.0094 |          49.9572 |           1.6522 |
[32m[20221214 00:17:21 @agent_ppo2.py:185][0m |          -0.0109 |          49.1305 |           1.6945 |
[32m[20221214 00:17:21 @agent_ppo2.py:185][0m |          -0.0125 |          48.3120 |           1.6430 |
[32m[20221214 00:17:21 @agent_ppo2.py:185][0m |          -0.0024 |          47.6423 |           1.5222 |
[32m[20221214 00:17:21 @agent_ppo2.py:185][0m |          -0.0096 |          47.4973 |           1.5117 |
[32m[20221214 00:17:21 @agent_ppo2.py:185][0m |          -0.0136 |          46.9662 |           1.5216 |
[32m[20221214 00:17:21 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221214 00:17:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 524.47
[32m[20221214 00:17:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 550.42
[32m[20221214 00:17:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 621.21
[32m[20221214 00:17:21 @agent_ppo2.py:143][0m Total time:      19.85 min
[32m[20221214 00:17:21 @agent_ppo2.py:145][0m 1832960 total steps have happened
[32m[20221214 00:17:21 @agent_ppo2.py:121][0m #------------------------ Iteration 4895 --------------------------#
[32m[20221214 00:17:22 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:17:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:17:22 @agent_ppo2.py:185][0m |          -0.0024 |          87.8362 |           0.8172 |
[32m[20221214 00:17:22 @agent_ppo2.py:185][0m |          -0.0091 |          81.1012 |           0.9375 |
[32m[20221214 00:17:22 @agent_ppo2.py:185][0m |          -0.0073 |          79.2316 |           0.9518 |
[32m[20221214 00:17:22 @agent_ppo2.py:185][0m |          -0.0100 |          78.0776 |           0.9968 |
[32m[20221214 00:17:22 @agent_ppo2.py:185][0m |          -0.0126 |          76.0578 |           1.0157 |
[32m[20221214 00:17:22 @agent_ppo2.py:185][0m |          -0.0140 |          74.8983 |           1.0893 |
[32m[20221214 00:17:22 @agent_ppo2.py:185][0m |          -0.0168 |          74.7384 |           0.9424 |
[32m[20221214 00:17:22 @agent_ppo2.py:185][0m |          -0.0157 |          73.2618 |           1.0291 |
[32m[20221214 00:17:23 @agent_ppo2.py:185][0m |          -0.0110 |          74.7055 |           1.0638 |
[32m[20221214 00:17:23 @agent_ppo2.py:185][0m |          -0.0165 |          72.1337 |           0.9894 |
[32m[20221214 00:17:23 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:17:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 537.69
[32m[20221214 00:17:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 579.54
[32m[20221214 00:17:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 449.84
[32m[20221214 00:17:23 @agent_ppo2.py:143][0m Total time:      19.87 min
[32m[20221214 00:17:23 @agent_ppo2.py:145][0m 1835008 total steps have happened
[32m[20221214 00:17:23 @agent_ppo2.py:121][0m #------------------------ Iteration 4896 --------------------------#
[32m[20221214 00:17:23 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:17:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:17:23 @agent_ppo2.py:185][0m |           0.0061 |          81.0320 |           0.9298 |
[32m[20221214 00:17:23 @agent_ppo2.py:185][0m |           0.0034 |          67.0654 |           1.0795 |
[32m[20221214 00:17:23 @agent_ppo2.py:185][0m |          -0.0118 |          58.4588 |           0.8902 |
[32m[20221214 00:17:23 @agent_ppo2.py:185][0m |          -0.0137 |          56.4957 |           0.8823 |
[32m[20221214 00:17:24 @agent_ppo2.py:185][0m |          -0.0016 |          65.4921 |           0.8757 |
[32m[20221214 00:17:24 @agent_ppo2.py:185][0m |          -0.0116 |          54.9550 |           0.8667 |
[32m[20221214 00:17:24 @agent_ppo2.py:185][0m |          -0.0186 |          53.9332 |           0.8200 |
[32m[20221214 00:17:24 @agent_ppo2.py:185][0m |          -0.0092 |          53.2860 |           0.7856 |
[32m[20221214 00:17:24 @agent_ppo2.py:185][0m |          -0.0162 |          52.8411 |           0.7407 |
[32m[20221214 00:17:24 @agent_ppo2.py:185][0m |          -0.0130 |          52.4523 |           0.7577 |
[32m[20221214 00:17:24 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221214 00:17:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 477.00
[32m[20221214 00:17:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 492.92
[32m[20221214 00:17:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 504.55
[32m[20221214 00:17:24 @agent_ppo2.py:143][0m Total time:      19.90 min
[32m[20221214 00:17:24 @agent_ppo2.py:145][0m 1837056 total steps have happened
[32m[20221214 00:17:24 @agent_ppo2.py:121][0m #------------------------ Iteration 4897 --------------------------#
[32m[20221214 00:17:24 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:17:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:17:25 @agent_ppo2.py:185][0m |          -0.0016 |          61.5715 |          -0.1887 |
[32m[20221214 00:17:25 @agent_ppo2.py:185][0m |          -0.0005 |          55.2664 |          -0.1474 |
[32m[20221214 00:17:25 @agent_ppo2.py:185][0m |          -0.0109 |          53.1588 |          -0.1542 |
[32m[20221214 00:17:25 @agent_ppo2.py:185][0m |          -0.0030 |          56.3422 |          -0.1802 |
[32m[20221214 00:17:25 @agent_ppo2.py:185][0m |          -0.0053 |          50.9788 |          -0.0734 |
[32m[20221214 00:17:25 @agent_ppo2.py:185][0m |          -0.0070 |          50.3657 |          -0.1664 |
[32m[20221214 00:17:25 @agent_ppo2.py:185][0m |          -0.0148 |          49.5429 |          -0.1272 |
[32m[20221214 00:17:25 @agent_ppo2.py:185][0m |          -0.0114 |          49.0739 |          -0.2464 |
[32m[20221214 00:17:25 @agent_ppo2.py:185][0m |          -0.0123 |          48.4520 |          -0.2339 |
[32m[20221214 00:17:26 @agent_ppo2.py:185][0m |          -0.0173 |          47.8892 |          -0.2494 |
[32m[20221214 00:17:26 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:17:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 495.58
[32m[20221214 00:17:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 546.27
[32m[20221214 00:17:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 666.89
[32m[20221214 00:17:26 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 666.89
[32m[20221214 00:17:26 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 666.89
[32m[20221214 00:17:26 @agent_ppo2.py:143][0m Total time:      19.92 min
[32m[20221214 00:17:26 @agent_ppo2.py:145][0m 1839104 total steps have happened
[32m[20221214 00:17:26 @agent_ppo2.py:121][0m #------------------------ Iteration 4898 --------------------------#
[32m[20221214 00:17:26 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:17:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:17:26 @agent_ppo2.py:185][0m |          -0.0026 |          64.8068 |           0.2474 |
[32m[20221214 00:17:26 @agent_ppo2.py:185][0m |          -0.0045 |          59.0570 |           0.2787 |
[32m[20221214 00:17:26 @agent_ppo2.py:185][0m |          -0.0133 |          57.4175 |           0.2453 |
[32m[20221214 00:17:26 @agent_ppo2.py:185][0m |          -0.0135 |          56.1199 |           0.2738 |
[32m[20221214 00:17:26 @agent_ppo2.py:185][0m |          -0.0169 |          55.5234 |           0.3246 |
[32m[20221214 00:17:27 @agent_ppo2.py:185][0m |          -0.0121 |          54.7739 |           0.2929 |
[32m[20221214 00:17:27 @agent_ppo2.py:185][0m |          -0.0128 |          54.2765 |           0.2596 |
[32m[20221214 00:17:27 @agent_ppo2.py:185][0m |          -0.0126 |          54.9066 |           0.2424 |
[32m[20221214 00:17:27 @agent_ppo2.py:185][0m |          -0.0195 |          53.4457 |           0.2957 |
[32m[20221214 00:17:27 @agent_ppo2.py:185][0m |          -0.0143 |          53.2523 |           0.2315 |
[32m[20221214 00:17:27 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:17:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 430.36
[32m[20221214 00:17:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 486.14
[32m[20221214 00:17:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 437.27
[32m[20221214 00:17:27 @agent_ppo2.py:143][0m Total time:      19.95 min
[32m[20221214 00:17:27 @agent_ppo2.py:145][0m 1841152 total steps have happened
[32m[20221214 00:17:27 @agent_ppo2.py:121][0m #------------------------ Iteration 4899 --------------------------#
[32m[20221214 00:17:27 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:17:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:17:27 @agent_ppo2.py:185][0m |           0.0089 |          99.0741 |           0.6331 |
[32m[20221214 00:17:28 @agent_ppo2.py:185][0m |           0.0036 |          84.1706 |           0.6069 |
[32m[20221214 00:17:28 @agent_ppo2.py:185][0m |          -0.0071 |          81.5002 |           0.6910 |
[32m[20221214 00:17:28 @agent_ppo2.py:185][0m |          -0.0076 |          80.5356 |           0.6737 |
[32m[20221214 00:17:28 @agent_ppo2.py:185][0m |          -0.0145 |          78.8499 |           0.6802 |
[32m[20221214 00:17:28 @agent_ppo2.py:185][0m |          -0.0153 |          78.7514 |           0.5983 |
[32m[20221214 00:17:28 @agent_ppo2.py:185][0m |          -0.0167 |          77.3765 |           0.6688 |
[32m[20221214 00:17:28 @agent_ppo2.py:185][0m |          -0.0133 |          77.0190 |           0.6433 |
[32m[20221214 00:17:28 @agent_ppo2.py:185][0m |          -0.0148 |          76.5916 |           0.6719 |
[32m[20221214 00:17:28 @agent_ppo2.py:185][0m |          -0.0160 |          76.4583 |           0.5986 |
[32m[20221214 00:17:28 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:17:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 565.52
[32m[20221214 00:17:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 635.57
[32m[20221214 00:17:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 480.58
[32m[20221214 00:17:28 @agent_ppo2.py:143][0m Total time:      19.97 min
[32m[20221214 00:17:28 @agent_ppo2.py:145][0m 1843200 total steps have happened
[32m[20221214 00:17:28 @agent_ppo2.py:121][0m #------------------------ Iteration 4900 --------------------------#
[32m[20221214 00:17:29 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:17:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:17:29 @agent_ppo2.py:185][0m |           0.0009 |          80.6207 |           0.1553 |
[32m[20221214 00:17:29 @agent_ppo2.py:185][0m |          -0.0039 |          74.8426 |           0.1824 |
[32m[20221214 00:17:29 @agent_ppo2.py:185][0m |          -0.0091 |          71.7677 |           0.1826 |
[32m[20221214 00:17:29 @agent_ppo2.py:185][0m |          -0.0093 |          69.8196 |           0.0824 |
[32m[20221214 00:17:29 @agent_ppo2.py:185][0m |          -0.0110 |          68.6599 |           0.0648 |
[32m[20221214 00:17:29 @agent_ppo2.py:185][0m |          -0.0082 |          67.5532 |           0.0174 |
[32m[20221214 00:17:29 @agent_ppo2.py:185][0m |          -0.0121 |          66.5728 |          -0.0655 |
[32m[20221214 00:17:30 @agent_ppo2.py:185][0m |          -0.0105 |          65.8580 |          -0.0273 |
[32m[20221214 00:17:30 @agent_ppo2.py:185][0m |          -0.0133 |          65.2298 |          -0.0665 |
[32m[20221214 00:17:30 @agent_ppo2.py:185][0m |          -0.0089 |          64.2425 |          -0.1536 |
[32m[20221214 00:17:30 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:17:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 527.57
[32m[20221214 00:17:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 587.11
[32m[20221214 00:17:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 553.65
[32m[20221214 00:17:30 @agent_ppo2.py:143][0m Total time:      19.99 min
[32m[20221214 00:17:30 @agent_ppo2.py:145][0m 1845248 total steps have happened
[32m[20221214 00:17:30 @agent_ppo2.py:121][0m #------------------------ Iteration 4901 --------------------------#
[32m[20221214 00:17:30 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:17:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:17:30 @agent_ppo2.py:185][0m |          -0.0018 |          75.6378 |          -0.9995 |
[32m[20221214 00:17:30 @agent_ppo2.py:185][0m |          -0.0076 |          67.2289 |          -0.8915 |
[32m[20221214 00:17:30 @agent_ppo2.py:185][0m |          -0.0156 |          64.2703 |          -0.8706 |
[32m[20221214 00:17:31 @agent_ppo2.py:185][0m |          -0.0148 |          60.7922 |          -0.8770 |
[32m[20221214 00:17:31 @agent_ppo2.py:185][0m |          -0.0114 |          59.0656 |          -0.8667 |
[32m[20221214 00:17:31 @agent_ppo2.py:185][0m |          -0.0131 |          58.2496 |          -0.8818 |
[32m[20221214 00:17:31 @agent_ppo2.py:185][0m |          -0.0145 |          57.1825 |          -0.8932 |
[32m[20221214 00:17:31 @agent_ppo2.py:185][0m |          -0.0139 |          56.5080 |          -0.9043 |
[32m[20221214 00:17:31 @agent_ppo2.py:185][0m |          -0.0160 |          55.9226 |          -0.8158 |
[32m[20221214 00:17:31 @agent_ppo2.py:185][0m |          -0.0123 |          56.0888 |          -0.8619 |
[32m[20221214 00:17:31 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:17:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 510.70
[32m[20221214 00:17:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 599.99
[32m[20221214 00:17:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 479.04
[32m[20221214 00:17:31 @agent_ppo2.py:143][0m Total time:      20.02 min
[32m[20221214 00:17:31 @agent_ppo2.py:145][0m 1847296 total steps have happened
[32m[20221214 00:17:31 @agent_ppo2.py:121][0m #------------------------ Iteration 4902 --------------------------#
[32m[20221214 00:17:31 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:17:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:17:32 @agent_ppo2.py:185][0m |          -0.0011 |          92.3737 |           0.3319 |
[32m[20221214 00:17:32 @agent_ppo2.py:185][0m |          -0.0076 |          85.8854 |           0.2947 |
[32m[20221214 00:17:32 @agent_ppo2.py:185][0m |          -0.0084 |          84.1173 |           0.3795 |
[32m[20221214 00:17:32 @agent_ppo2.py:185][0m |          -0.0158 |          83.0847 |           0.2625 |
[32m[20221214 00:17:32 @agent_ppo2.py:185][0m |          -0.0060 |          87.9127 |           0.3243 |
[32m[20221214 00:17:32 @agent_ppo2.py:185][0m |          -0.0177 |          80.6130 |           0.2853 |
[32m[20221214 00:17:32 @agent_ppo2.py:185][0m |          -0.0111 |          83.1534 |           0.2805 |
[32m[20221214 00:17:32 @agent_ppo2.py:185][0m |          -0.0182 |          79.1080 |           0.2393 |
[32m[20221214 00:17:32 @agent_ppo2.py:185][0m |          -0.0202 |          78.7222 |           0.1716 |
[32m[20221214 00:17:33 @agent_ppo2.py:185][0m |          -0.0216 |          78.0287 |           0.2072 |
[32m[20221214 00:17:33 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:17:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 497.98
[32m[20221214 00:17:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 588.78
[32m[20221214 00:17:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 579.61
[32m[20221214 00:17:33 @agent_ppo2.py:143][0m Total time:      20.04 min
[32m[20221214 00:17:33 @agent_ppo2.py:145][0m 1849344 total steps have happened
[32m[20221214 00:17:33 @agent_ppo2.py:121][0m #------------------------ Iteration 4903 --------------------------#
[32m[20221214 00:17:33 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:17:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:17:33 @agent_ppo2.py:185][0m |           0.0036 |          74.8781 |          -0.5740 |
[32m[20221214 00:17:33 @agent_ppo2.py:185][0m |          -0.0019 |          68.9857 |          -0.6249 |
[32m[20221214 00:17:33 @agent_ppo2.py:185][0m |          -0.0068 |          65.8535 |          -0.5671 |
[32m[20221214 00:17:33 @agent_ppo2.py:185][0m |           0.0073 |          68.7451 |          -0.4558 |
[32m[20221214 00:17:33 @agent_ppo2.py:185][0m |          -0.0072 |          63.8710 |          -0.4506 |
[32m[20221214 00:17:34 @agent_ppo2.py:185][0m |          -0.0064 |          65.5960 |          -0.3681 |
[32m[20221214 00:17:34 @agent_ppo2.py:185][0m |          -0.0143 |          61.7972 |          -0.3873 |
[32m[20221214 00:17:34 @agent_ppo2.py:185][0m |          -0.0122 |          61.2578 |          -0.3021 |
[32m[20221214 00:17:34 @agent_ppo2.py:185][0m |          -0.0111 |          61.2037 |          -0.2659 |
[32m[20221214 00:17:34 @agent_ppo2.py:185][0m |          -0.0140 |          60.7671 |          -0.2273 |
[32m[20221214 00:17:34 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:17:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 495.24
[32m[20221214 00:17:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 532.71
[32m[20221214 00:17:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 473.77
[32m[20221214 00:17:34 @agent_ppo2.py:143][0m Total time:      20.06 min
[32m[20221214 00:17:34 @agent_ppo2.py:145][0m 1851392 total steps have happened
[32m[20221214 00:17:34 @agent_ppo2.py:121][0m #------------------------ Iteration 4904 --------------------------#
[32m[20221214 00:17:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:17:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:17:34 @agent_ppo2.py:185][0m |          -0.0004 |          75.3525 |           0.0223 |
[32m[20221214 00:17:35 @agent_ppo2.py:185][0m |          -0.0012 |          68.8883 |           0.1436 |
[32m[20221214 00:17:35 @agent_ppo2.py:185][0m |          -0.0084 |          67.3259 |           0.1183 |
[32m[20221214 00:17:35 @agent_ppo2.py:185][0m |          -0.0089 |          66.1235 |           0.2173 |
[32m[20221214 00:17:35 @agent_ppo2.py:185][0m |          -0.0076 |          67.4850 |           0.2149 |
[32m[20221214 00:17:35 @agent_ppo2.py:185][0m |          -0.0089 |          67.2591 |           0.3045 |
[32m[20221214 00:17:35 @agent_ppo2.py:185][0m |          -0.0165 |          64.3691 |           0.2570 |
[32m[20221214 00:17:35 @agent_ppo2.py:185][0m |          -0.0187 |          63.9516 |           0.3106 |
[32m[20221214 00:17:35 @agent_ppo2.py:185][0m |          -0.0124 |          63.8177 |           0.3419 |
[32m[20221214 00:17:35 @agent_ppo2.py:185][0m |          -0.0074 |          66.5802 |           0.4472 |
[32m[20221214 00:17:35 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:17:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 423.15
[32m[20221214 00:17:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 493.74
[32m[20221214 00:17:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 557.91
[32m[20221214 00:17:35 @agent_ppo2.py:143][0m Total time:      20.09 min
[32m[20221214 00:17:35 @agent_ppo2.py:145][0m 1853440 total steps have happened
[32m[20221214 00:17:35 @agent_ppo2.py:121][0m #------------------------ Iteration 4905 --------------------------#
[32m[20221214 00:17:36 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:17:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:17:36 @agent_ppo2.py:185][0m |           0.0036 |          65.7886 |           0.2222 |
[32m[20221214 00:17:36 @agent_ppo2.py:185][0m |          -0.0099 |          57.6448 |           0.2068 |
[32m[20221214 00:17:36 @agent_ppo2.py:185][0m |          -0.0090 |          53.1551 |           0.2612 |
[32m[20221214 00:17:36 @agent_ppo2.py:185][0m |          -0.0138 |          51.1579 |           0.2932 |
[32m[20221214 00:17:36 @agent_ppo2.py:185][0m |          -0.0145 |          49.8753 |           0.2782 |
[32m[20221214 00:17:36 @agent_ppo2.py:185][0m |          -0.0185 |          48.6669 |           0.3324 |
[32m[20221214 00:17:36 @agent_ppo2.py:185][0m |          -0.0133 |          47.9679 |           0.3342 |
[32m[20221214 00:17:37 @agent_ppo2.py:185][0m |          -0.0190 |          47.1245 |           0.3154 |
[32m[20221214 00:17:37 @agent_ppo2.py:185][0m |          -0.0174 |          46.6770 |           0.3103 |
[32m[20221214 00:17:37 @agent_ppo2.py:185][0m |          -0.0176 |          46.2119 |           0.3501 |
[32m[20221214 00:17:37 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:17:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 527.35
[32m[20221214 00:17:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 591.67
[32m[20221214 00:17:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 628.33
[32m[20221214 00:17:37 @agent_ppo2.py:143][0m Total time:      20.11 min
[32m[20221214 00:17:37 @agent_ppo2.py:145][0m 1855488 total steps have happened
[32m[20221214 00:17:37 @agent_ppo2.py:121][0m #------------------------ Iteration 4906 --------------------------#
[32m[20221214 00:17:37 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:17:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:17:37 @agent_ppo2.py:185][0m |          -0.0011 |          52.2299 |          -0.3381 |
[32m[20221214 00:17:37 @agent_ppo2.py:185][0m |          -0.0029 |          44.4036 |          -0.3802 |
[32m[20221214 00:17:37 @agent_ppo2.py:185][0m |          -0.0079 |          42.4423 |          -0.3729 |
[32m[20221214 00:17:38 @agent_ppo2.py:185][0m |          -0.0100 |          41.6524 |          -0.4196 |
[32m[20221214 00:17:38 @agent_ppo2.py:185][0m |          -0.0035 |          41.9791 |          -0.4867 |
[32m[20221214 00:17:38 @agent_ppo2.py:185][0m |          -0.0120 |          40.0704 |          -0.3618 |
[32m[20221214 00:17:38 @agent_ppo2.py:185][0m |          -0.0097 |          42.2836 |          -0.4772 |
[32m[20221214 00:17:38 @agent_ppo2.py:185][0m |          -0.0203 |          39.8106 |          -0.4295 |
[32m[20221214 00:17:38 @agent_ppo2.py:185][0m |          -0.0197 |          39.2279 |          -0.4447 |
[32m[20221214 00:17:38 @agent_ppo2.py:185][0m |          -0.0200 |          38.8672 |          -0.5245 |
[32m[20221214 00:17:38 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:17:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 489.08
[32m[20221214 00:17:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 550.75
[32m[20221214 00:17:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 567.64
[32m[20221214 00:17:38 @agent_ppo2.py:143][0m Total time:      20.13 min
[32m[20221214 00:17:38 @agent_ppo2.py:145][0m 1857536 total steps have happened
[32m[20221214 00:17:38 @agent_ppo2.py:121][0m #------------------------ Iteration 4907 --------------------------#
[32m[20221214 00:17:38 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:17:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:17:39 @agent_ppo2.py:185][0m |           0.0003 |          87.8301 |           0.2302 |
[32m[20221214 00:17:39 @agent_ppo2.py:185][0m |           0.0028 |          82.7784 |           0.2894 |
[32m[20221214 00:17:39 @agent_ppo2.py:185][0m |          -0.0058 |          80.1932 |           0.2567 |
[32m[20221214 00:17:39 @agent_ppo2.py:185][0m |          -0.0020 |          78.4874 |           0.2568 |
[32m[20221214 00:17:39 @agent_ppo2.py:185][0m |          -0.0047 |          77.1780 |           0.2679 |
[32m[20221214 00:17:39 @agent_ppo2.py:185][0m |          -0.0061 |          75.6433 |           0.1252 |
[32m[20221214 00:17:39 @agent_ppo2.py:185][0m |          -0.0065 |          74.5034 |           0.2017 |
[32m[20221214 00:17:39 @agent_ppo2.py:185][0m |          -0.0065 |          74.1021 |           0.2535 |
[32m[20221214 00:17:39 @agent_ppo2.py:185][0m |          -0.0052 |          73.7375 |           0.1818 |
[32m[20221214 00:17:40 @agent_ppo2.py:185][0m |          -0.0052 |          73.2495 |           0.1432 |
[32m[20221214 00:17:40 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:17:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 553.32
[32m[20221214 00:17:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 653.96
[32m[20221214 00:17:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 548.11
[32m[20221214 00:17:40 @agent_ppo2.py:143][0m Total time:      20.15 min
[32m[20221214 00:17:40 @agent_ppo2.py:145][0m 1859584 total steps have happened
[32m[20221214 00:17:40 @agent_ppo2.py:121][0m #------------------------ Iteration 4908 --------------------------#
[32m[20221214 00:17:40 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:17:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:17:40 @agent_ppo2.py:185][0m |          -0.0006 |          56.0231 |          -0.0930 |
[32m[20221214 00:17:40 @agent_ppo2.py:185][0m |          -0.0084 |          40.1166 |          -0.0687 |
[32m[20221214 00:17:40 @agent_ppo2.py:185][0m |          -0.0141 |          38.0088 |           0.0039 |
[32m[20221214 00:17:40 @agent_ppo2.py:185][0m |          -0.0152 |          36.6204 |          -0.0035 |
[32m[20221214 00:17:40 @agent_ppo2.py:185][0m |          -0.0132 |          35.4986 |           0.0379 |
[32m[20221214 00:17:41 @agent_ppo2.py:185][0m |          -0.0153 |          34.3864 |           0.0708 |
[32m[20221214 00:17:41 @agent_ppo2.py:185][0m |          -0.0127 |          34.1379 |           0.1013 |
[32m[20221214 00:17:41 @agent_ppo2.py:185][0m |          -0.0193 |          33.2196 |           0.1525 |
[32m[20221214 00:17:41 @agent_ppo2.py:185][0m |          -0.0262 |          32.5956 |           0.1618 |
[32m[20221214 00:17:41 @agent_ppo2.py:185][0m |          -0.0167 |          33.5693 |           0.1973 |
[32m[20221214 00:17:41 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:17:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 426.23
[32m[20221214 00:17:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 532.80
[32m[20221214 00:17:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 441.41
[32m[20221214 00:17:41 @agent_ppo2.py:143][0m Total time:      20.18 min
[32m[20221214 00:17:41 @agent_ppo2.py:145][0m 1861632 total steps have happened
[32m[20221214 00:17:41 @agent_ppo2.py:121][0m #------------------------ Iteration 4909 --------------------------#
[32m[20221214 00:17:41 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:17:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:17:41 @agent_ppo2.py:185][0m |           0.0057 |          63.0835 |          -1.4429 |
[32m[20221214 00:17:42 @agent_ppo2.py:185][0m |          -0.0014 |          50.7238 |          -1.4033 |
[32m[20221214 00:17:42 @agent_ppo2.py:185][0m |          -0.0142 |          47.6085 |          -1.3295 |
[32m[20221214 00:17:42 @agent_ppo2.py:185][0m |          -0.0132 |          46.6278 |          -1.3477 |
[32m[20221214 00:17:42 @agent_ppo2.py:185][0m |          -0.0141 |          45.7151 |          -1.3721 |
[32m[20221214 00:17:42 @agent_ppo2.py:185][0m |          -0.0188 |          45.1140 |          -1.2550 |
[32m[20221214 00:17:42 @agent_ppo2.py:185][0m |          -0.0139 |          44.5792 |          -1.3265 |
[32m[20221214 00:17:42 @agent_ppo2.py:185][0m |          -0.0216 |          44.0760 |          -1.3792 |
[32m[20221214 00:17:42 @agent_ppo2.py:185][0m |          -0.0159 |          44.1417 |          -1.2756 |
[32m[20221214 00:17:42 @agent_ppo2.py:185][0m |          -0.0171 |          43.3713 |          -1.2646 |
[32m[20221214 00:17:42 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:17:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 452.00
[32m[20221214 00:17:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 519.06
[32m[20221214 00:17:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 487.81
[32m[20221214 00:17:42 @agent_ppo2.py:143][0m Total time:      20.20 min
[32m[20221214 00:17:42 @agent_ppo2.py:145][0m 1863680 total steps have happened
[32m[20221214 00:17:42 @agent_ppo2.py:121][0m #------------------------ Iteration 4910 --------------------------#
[32m[20221214 00:17:43 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:17:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:17:43 @agent_ppo2.py:185][0m |           0.0058 |          49.1224 |          -1.0481 |
[32m[20221214 00:17:43 @agent_ppo2.py:185][0m |          -0.0044 |          41.2594 |          -1.1215 |
[32m[20221214 00:17:43 @agent_ppo2.py:185][0m |          -0.0109 |          39.9009 |          -1.0519 |
[32m[20221214 00:17:43 @agent_ppo2.py:185][0m |          -0.0113 |          39.2191 |          -1.0536 |
[32m[20221214 00:17:43 @agent_ppo2.py:185][0m |          -0.0148 |          38.8777 |          -1.0246 |
[32m[20221214 00:17:43 @agent_ppo2.py:185][0m |          -0.0203 |          38.5356 |          -0.9243 |
[32m[20221214 00:17:43 @agent_ppo2.py:185][0m |          -0.0157 |          38.2678 |          -0.8932 |
[32m[20221214 00:17:44 @agent_ppo2.py:185][0m |          -0.0139 |          38.2211 |          -0.8713 |
[32m[20221214 00:17:44 @agent_ppo2.py:185][0m |          -0.0131 |          39.3194 |          -0.8300 |
[32m[20221214 00:17:44 @agent_ppo2.py:185][0m |          -0.0230 |          37.8622 |          -0.8596 |
[32m[20221214 00:17:44 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:17:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 538.29
[32m[20221214 00:17:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 548.31
[32m[20221214 00:17:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 596.18
[32m[20221214 00:17:44 @agent_ppo2.py:143][0m Total time:      20.22 min
[32m[20221214 00:17:44 @agent_ppo2.py:145][0m 1865728 total steps have happened
[32m[20221214 00:17:44 @agent_ppo2.py:121][0m #------------------------ Iteration 4911 --------------------------#
[32m[20221214 00:17:44 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:17:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:17:44 @agent_ppo2.py:185][0m |           0.0107 |          48.0659 |           0.1514 |
[32m[20221214 00:17:44 @agent_ppo2.py:185][0m |          -0.0063 |          41.4026 |           0.2444 |
[32m[20221214 00:17:44 @agent_ppo2.py:185][0m |          -0.0044 |          41.4476 |           0.1363 |
[32m[20221214 00:17:45 @agent_ppo2.py:185][0m |          -0.0112 |          39.0062 |           0.2461 |
[32m[20221214 00:17:45 @agent_ppo2.py:185][0m |          -0.0163 |          38.3831 |           0.2005 |
[32m[20221214 00:17:45 @agent_ppo2.py:185][0m |          -0.0193 |          37.8951 |           0.1216 |
[32m[20221214 00:17:45 @agent_ppo2.py:185][0m |          -0.0243 |          37.3289 |           0.1418 |
[32m[20221214 00:17:45 @agent_ppo2.py:185][0m |          -0.0201 |          37.0047 |           0.1693 |
[32m[20221214 00:17:45 @agent_ppo2.py:185][0m |          -0.0198 |          36.6966 |           0.1691 |
[32m[20221214 00:17:45 @agent_ppo2.py:185][0m |          -0.0225 |          36.8452 |           0.2071 |
[32m[20221214 00:17:45 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:17:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 485.37
[32m[20221214 00:17:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 564.78
[32m[20221214 00:17:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 631.95
[32m[20221214 00:17:45 @agent_ppo2.py:143][0m Total time:      20.25 min
[32m[20221214 00:17:45 @agent_ppo2.py:145][0m 1867776 total steps have happened
[32m[20221214 00:17:45 @agent_ppo2.py:121][0m #------------------------ Iteration 4912 --------------------------#
[32m[20221214 00:17:45 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:17:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:17:46 @agent_ppo2.py:185][0m |           0.0037 |          64.7778 |           0.1974 |
[32m[20221214 00:17:46 @agent_ppo2.py:185][0m |          -0.0078 |          58.8718 |           0.2339 |
[32m[20221214 00:17:46 @agent_ppo2.py:185][0m |           0.0028 |          60.0818 |           0.3060 |
[32m[20221214 00:17:46 @agent_ppo2.py:185][0m |          -0.0122 |          55.8551 |           0.3091 |
[32m[20221214 00:17:46 @agent_ppo2.py:185][0m |          -0.0117 |          55.0655 |           0.3536 |
[32m[20221214 00:17:46 @agent_ppo2.py:185][0m |          -0.0135 |          54.7113 |           0.2960 |
[32m[20221214 00:17:46 @agent_ppo2.py:185][0m |          -0.0149 |          54.0082 |           0.3311 |
[32m[20221214 00:17:46 @agent_ppo2.py:185][0m |          -0.0165 |          53.3374 |           0.4233 |
[32m[20221214 00:17:46 @agent_ppo2.py:185][0m |          -0.0161 |          53.2584 |           0.3176 |
[32m[20221214 00:17:46 @agent_ppo2.py:185][0m |          -0.0179 |          52.8927 |           0.3724 |
[32m[20221214 00:17:46 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:17:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 551.62
[32m[20221214 00:17:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 595.83
[32m[20221214 00:17:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 596.88
[32m[20221214 00:17:47 @agent_ppo2.py:143][0m Total time:      20.27 min
[32m[20221214 00:17:47 @agent_ppo2.py:145][0m 1869824 total steps have happened
[32m[20221214 00:17:47 @agent_ppo2.py:121][0m #------------------------ Iteration 4913 --------------------------#
[32m[20221214 00:17:47 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:17:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:17:47 @agent_ppo2.py:185][0m |           0.0028 |          48.6999 |          -0.5359 |
[32m[20221214 00:17:47 @agent_ppo2.py:185][0m |           0.0010 |          47.5110 |          -0.4478 |
[32m[20221214 00:17:47 @agent_ppo2.py:185][0m |          -0.0125 |          42.0218 |          -0.3406 |
[32m[20221214 00:17:47 @agent_ppo2.py:185][0m |          -0.0073 |          41.0557 |          -0.3653 |
[32m[20221214 00:17:47 @agent_ppo2.py:185][0m |          -0.0149 |          40.1412 |          -0.2872 |
[32m[20221214 00:17:47 @agent_ppo2.py:185][0m |          -0.0134 |          39.8925 |          -0.3089 |
[32m[20221214 00:17:48 @agent_ppo2.py:185][0m |          -0.0198 |          39.3192 |          -0.3210 |
[32m[20221214 00:17:48 @agent_ppo2.py:185][0m |          -0.0176 |          39.0598 |          -0.2597 |
[32m[20221214 00:17:48 @agent_ppo2.py:185][0m |          -0.0189 |          38.6069 |          -0.2144 |
[32m[20221214 00:17:48 @agent_ppo2.py:185][0m |          -0.0238 |          38.6871 |          -0.1604 |
[32m[20221214 00:17:48 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:17:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 397.08
[32m[20221214 00:17:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 428.54
[32m[20221214 00:17:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 581.86
[32m[20221214 00:17:48 @agent_ppo2.py:143][0m Total time:      20.29 min
[32m[20221214 00:17:48 @agent_ppo2.py:145][0m 1871872 total steps have happened
[32m[20221214 00:17:48 @agent_ppo2.py:121][0m #------------------------ Iteration 4914 --------------------------#
[32m[20221214 00:17:48 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:17:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:17:48 @agent_ppo2.py:185][0m |          -0.0010 |          86.0715 |           0.4919 |
[32m[20221214 00:17:48 @agent_ppo2.py:185][0m |          -0.0057 |          75.8920 |           0.5000 |
[32m[20221214 00:17:49 @agent_ppo2.py:185][0m |          -0.0096 |          72.5591 |           0.4653 |
[32m[20221214 00:17:49 @agent_ppo2.py:185][0m |          -0.0099 |          70.1594 |           0.4393 |
[32m[20221214 00:17:49 @agent_ppo2.py:185][0m |          -0.0163 |          68.2624 |           0.4986 |
[32m[20221214 00:17:49 @agent_ppo2.py:185][0m |          -0.0149 |          66.7615 |           0.4035 |
[32m[20221214 00:17:49 @agent_ppo2.py:185][0m |          -0.0111 |          68.6280 |           0.3856 |
[32m[20221214 00:17:49 @agent_ppo2.py:185][0m |          -0.0039 |          73.2205 |           0.3418 |
[32m[20221214 00:17:49 @agent_ppo2.py:185][0m |          -0.0192 |          64.0886 |           0.3625 |
[32m[20221214 00:17:49 @agent_ppo2.py:185][0m |          -0.0131 |          63.8236 |           0.3961 |
[32m[20221214 00:17:49 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:17:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 396.89
[32m[20221214 00:17:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 509.08
[32m[20221214 00:17:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 373.84
[32m[20221214 00:17:49 @agent_ppo2.py:143][0m Total time:      20.32 min
[32m[20221214 00:17:49 @agent_ppo2.py:145][0m 1873920 total steps have happened
[32m[20221214 00:17:49 @agent_ppo2.py:121][0m #------------------------ Iteration 4915 --------------------------#
[32m[20221214 00:17:50 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:17:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:17:50 @agent_ppo2.py:185][0m |           0.0214 |         111.3885 |           0.1674 |
[32m[20221214 00:17:50 @agent_ppo2.py:185][0m |          -0.0063 |          91.4532 |           0.1479 |
[32m[20221214 00:17:50 @agent_ppo2.py:185][0m |          -0.0091 |          88.6236 |           0.1192 |
[32m[20221214 00:17:50 @agent_ppo2.py:185][0m |          -0.0087 |          87.0521 |           0.1898 |
[32m[20221214 00:17:50 @agent_ppo2.py:185][0m |          -0.0112 |          86.0188 |           0.2154 |
[32m[20221214 00:17:50 @agent_ppo2.py:185][0m |          -0.0140 |          85.3191 |           0.1945 |
[32m[20221214 00:17:50 @agent_ppo2.py:185][0m |          -0.0123 |          85.0533 |           0.2588 |
[32m[20221214 00:17:50 @agent_ppo2.py:185][0m |          -0.0137 |          84.3119 |           0.2913 |
[32m[20221214 00:17:51 @agent_ppo2.py:185][0m |          -0.0166 |          83.8953 |           0.2541 |
[32m[20221214 00:17:51 @agent_ppo2.py:185][0m |          -0.0135 |          83.3954 |           0.3734 |
[32m[20221214 00:17:51 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:17:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 569.53
[32m[20221214 00:17:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 590.29
[32m[20221214 00:17:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 593.70
[32m[20221214 00:17:51 @agent_ppo2.py:143][0m Total time:      20.34 min
[32m[20221214 00:17:51 @agent_ppo2.py:145][0m 1875968 total steps have happened
[32m[20221214 00:17:51 @agent_ppo2.py:121][0m #------------------------ Iteration 4916 --------------------------#
[32m[20221214 00:17:51 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:17:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:17:51 @agent_ppo2.py:185][0m |           0.0015 |          74.3582 |           0.5651 |
[32m[20221214 00:17:51 @agent_ppo2.py:185][0m |          -0.0009 |          64.3268 |           0.8430 |
[32m[20221214 00:17:51 @agent_ppo2.py:185][0m |          -0.0076 |          61.9996 |           0.7847 |
[32m[20221214 00:17:51 @agent_ppo2.py:185][0m |          -0.0074 |          60.1983 |           0.9007 |
[32m[20221214 00:17:52 @agent_ppo2.py:185][0m |          -0.0132 |          59.6844 |           0.8799 |
[32m[20221214 00:17:52 @agent_ppo2.py:185][0m |          -0.0131 |          58.7619 |           0.8610 |
[32m[20221214 00:17:52 @agent_ppo2.py:185][0m |          -0.0125 |          58.3798 |           0.8781 |
[32m[20221214 00:17:52 @agent_ppo2.py:185][0m |          -0.0152 |          57.9250 |           0.9580 |
[32m[20221214 00:17:52 @agent_ppo2.py:185][0m |          -0.0159 |          57.4994 |           0.9305 |
[32m[20221214 00:17:52 @agent_ppo2.py:185][0m |          -0.0190 |          57.0966 |           0.9235 |
[32m[20221214 00:17:52 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:17:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 477.50
[32m[20221214 00:17:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 513.25
[32m[20221214 00:17:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 283.60
[32m[20221214 00:17:52 @agent_ppo2.py:143][0m Total time:      20.36 min
[32m[20221214 00:17:52 @agent_ppo2.py:145][0m 1878016 total steps have happened
[32m[20221214 00:17:52 @agent_ppo2.py:121][0m #------------------------ Iteration 4917 --------------------------#
[32m[20221214 00:17:52 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:17:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:17:53 @agent_ppo2.py:185][0m |           0.0019 |          81.4456 |          -0.1139 |
[32m[20221214 00:17:53 @agent_ppo2.py:185][0m |          -0.0072 |          74.3343 |          -0.0142 |
[32m[20221214 00:17:53 @agent_ppo2.py:185][0m |          -0.0078 |          72.6239 |           0.1449 |
[32m[20221214 00:17:53 @agent_ppo2.py:185][0m |          -0.0008 |          72.1649 |           0.1441 |
[32m[20221214 00:17:53 @agent_ppo2.py:185][0m |          -0.0067 |          68.1929 |           0.0434 |
[32m[20221214 00:17:53 @agent_ppo2.py:185][0m |          -0.0131 |          67.2221 |           0.0244 |
[32m[20221214 00:17:53 @agent_ppo2.py:185][0m |          -0.0107 |          66.6463 |          -0.0245 |
[32m[20221214 00:17:53 @agent_ppo2.py:185][0m |          -0.0010 |          69.8963 |           0.0487 |
[32m[20221214 00:17:53 @agent_ppo2.py:185][0m |          -0.0126 |          64.6833 |           0.0945 |
[32m[20221214 00:17:53 @agent_ppo2.py:185][0m |          -0.0108 |          65.3262 |           0.1449 |
[32m[20221214 00:17:53 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:17:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 532.07
[32m[20221214 00:17:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 594.81
[32m[20221214 00:17:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 658.57
[32m[20221214 00:17:54 @agent_ppo2.py:143][0m Total time:      20.39 min
[32m[20221214 00:17:54 @agent_ppo2.py:145][0m 1880064 total steps have happened
[32m[20221214 00:17:54 @agent_ppo2.py:121][0m #------------------------ Iteration 4918 --------------------------#
[32m[20221214 00:17:54 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:17:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:17:54 @agent_ppo2.py:185][0m |           0.0009 |          79.5458 |           0.2796 |
[32m[20221214 00:17:54 @agent_ppo2.py:185][0m |          -0.0082 |          74.0929 |           0.2901 |
[32m[20221214 00:17:54 @agent_ppo2.py:185][0m |          -0.0137 |          72.6472 |           0.2765 |
[32m[20221214 00:17:54 @agent_ppo2.py:185][0m |          -0.0118 |          71.2024 |           0.4216 |
[32m[20221214 00:17:54 @agent_ppo2.py:185][0m |          -0.0146 |          70.5247 |           0.3667 |
[32m[20221214 00:17:54 @agent_ppo2.py:185][0m |          -0.0121 |          69.7590 |           0.4127 |
[32m[20221214 00:17:55 @agent_ppo2.py:185][0m |          -0.0109 |          69.5558 |           0.4748 |
[32m[20221214 00:17:55 @agent_ppo2.py:185][0m |          -0.0176 |          68.8839 |           0.4467 |
[32m[20221214 00:17:55 @agent_ppo2.py:185][0m |          -0.0178 |          68.7523 |           0.4412 |
[32m[20221214 00:17:55 @agent_ppo2.py:185][0m |          -0.0156 |          68.7246 |           0.4754 |
[32m[20221214 00:17:55 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:17:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 463.47
[32m[20221214 00:17:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 555.80
[32m[20221214 00:17:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 489.16
[32m[20221214 00:17:55 @agent_ppo2.py:143][0m Total time:      20.41 min
[32m[20221214 00:17:55 @agent_ppo2.py:145][0m 1882112 total steps have happened
[32m[20221214 00:17:55 @agent_ppo2.py:121][0m #------------------------ Iteration 4919 --------------------------#
[32m[20221214 00:17:55 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:17:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:17:55 @agent_ppo2.py:185][0m |           0.0023 |          69.6948 |           0.5081 |
[32m[20221214 00:17:55 @agent_ppo2.py:185][0m |           0.0017 |          67.0146 |           0.5876 |
[32m[20221214 00:17:56 @agent_ppo2.py:185][0m |          -0.0064 |          63.4422 |           0.6367 |
[32m[20221214 00:17:56 @agent_ppo2.py:185][0m |          -0.0069 |          62.3928 |           0.6637 |
[32m[20221214 00:17:56 @agent_ppo2.py:185][0m |          -0.0133 |          61.6328 |           0.6454 |
[32m[20221214 00:17:56 @agent_ppo2.py:185][0m |          -0.0121 |          60.9890 |           0.6710 |
[32m[20221214 00:17:56 @agent_ppo2.py:185][0m |          -0.0130 |          60.6594 |           0.5594 |
[32m[20221214 00:17:56 @agent_ppo2.py:185][0m |          -0.0099 |          61.2007 |           0.7154 |
[32m[20221214 00:17:56 @agent_ppo2.py:185][0m |          -0.0150 |          59.7760 |           0.6501 |
[32m[20221214 00:17:56 @agent_ppo2.py:185][0m |          -0.0162 |          59.7719 |           0.6974 |
[32m[20221214 00:17:56 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:17:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 439.23
[32m[20221214 00:17:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 500.06
[32m[20221214 00:17:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 546.28
[32m[20221214 00:17:56 @agent_ppo2.py:143][0m Total time:      20.43 min
[32m[20221214 00:17:56 @agent_ppo2.py:145][0m 1884160 total steps have happened
[32m[20221214 00:17:56 @agent_ppo2.py:121][0m #------------------------ Iteration 4920 --------------------------#
[32m[20221214 00:17:57 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:17:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:17:57 @agent_ppo2.py:185][0m |           0.0007 |          74.2247 |           0.9020 |
[32m[20221214 00:17:57 @agent_ppo2.py:185][0m |          -0.0069 |          69.0007 |           0.7446 |
[32m[20221214 00:17:57 @agent_ppo2.py:185][0m |          -0.0065 |          66.8685 |           0.7059 |
[32m[20221214 00:17:57 @agent_ppo2.py:185][0m |          -0.0109 |          65.6255 |           0.6838 |
[32m[20221214 00:17:57 @agent_ppo2.py:185][0m |          -0.0018 |          70.6345 |           0.6706 |
[32m[20221214 00:17:57 @agent_ppo2.py:185][0m |          -0.0097 |          64.9720 |           0.7114 |
[32m[20221214 00:17:57 @agent_ppo2.py:185][0m |          -0.0137 |          63.5226 |           0.6482 |
[32m[20221214 00:17:57 @agent_ppo2.py:185][0m |          -0.0133 |          62.8959 |           0.6025 |
[32m[20221214 00:17:58 @agent_ppo2.py:185][0m |          -0.0059 |          65.1712 |           0.5593 |
[32m[20221214 00:17:58 @agent_ppo2.py:185][0m |          -0.0058 |          64.3272 |           0.6062 |
[32m[20221214 00:17:58 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:17:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 494.05
[32m[20221214 00:17:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 509.83
[32m[20221214 00:17:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 534.56
[32m[20221214 00:17:58 @agent_ppo2.py:143][0m Total time:      20.46 min
[32m[20221214 00:17:58 @agent_ppo2.py:145][0m 1886208 total steps have happened
[32m[20221214 00:17:58 @agent_ppo2.py:121][0m #------------------------ Iteration 4921 --------------------------#
[32m[20221214 00:17:58 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:17:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:17:58 @agent_ppo2.py:185][0m |           0.0181 |          99.0622 |          -0.5755 |
[32m[20221214 00:17:58 @agent_ppo2.py:185][0m |          -0.0073 |          84.9915 |          -0.3730 |
[32m[20221214 00:17:58 @agent_ppo2.py:185][0m |           0.0004 |          87.6421 |          -0.3321 |
[32m[20221214 00:17:58 @agent_ppo2.py:185][0m |          -0.0126 |          79.9317 |          -0.2069 |
[32m[20221214 00:17:59 @agent_ppo2.py:185][0m |          -0.0120 |          78.0759 |          -0.2988 |
[32m[20221214 00:17:59 @agent_ppo2.py:185][0m |          -0.0103 |          76.2855 |          -0.2409 |
[32m[20221214 00:17:59 @agent_ppo2.py:185][0m |          -0.0172 |          74.3763 |          -0.2139 |
[32m[20221214 00:17:59 @agent_ppo2.py:185][0m |          -0.0156 |          74.0222 |          -0.1969 |
[32m[20221214 00:17:59 @agent_ppo2.py:185][0m |          -0.0164 |          72.9990 |          -0.1513 |
[32m[20221214 00:17:59 @agent_ppo2.py:185][0m |          -0.0165 |          71.7966 |          -0.1843 |
[32m[20221214 00:17:59 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:17:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 504.72
[32m[20221214 00:17:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 541.66
[32m[20221214 00:17:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 651.38
[32m[20221214 00:17:59 @agent_ppo2.py:143][0m Total time:      20.48 min
[32m[20221214 00:17:59 @agent_ppo2.py:145][0m 1888256 total steps have happened
[32m[20221214 00:17:59 @agent_ppo2.py:121][0m #------------------------ Iteration 4922 --------------------------#
[32m[20221214 00:17:59 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:17:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:18:00 @agent_ppo2.py:185][0m |          -0.0023 |          51.3561 |           0.8438 |
[32m[20221214 00:18:00 @agent_ppo2.py:185][0m |          -0.0092 |          42.4306 |           0.8954 |
[32m[20221214 00:18:00 @agent_ppo2.py:185][0m |          -0.0100 |          39.4878 |           0.9323 |
[32m[20221214 00:18:00 @agent_ppo2.py:185][0m |           0.0015 |          45.3503 |           0.8191 |
[32m[20221214 00:18:00 @agent_ppo2.py:185][0m |          -0.0107 |          37.4617 |           0.7094 |
[32m[20221214 00:18:00 @agent_ppo2.py:185][0m |          -0.0182 |          35.8210 |           0.7387 |
[32m[20221214 00:18:00 @agent_ppo2.py:185][0m |          -0.0056 |          44.1655 |           0.7183 |
[32m[20221214 00:18:00 @agent_ppo2.py:185][0m |          -0.0210 |          34.7967 |           0.6635 |
[32m[20221214 00:18:00 @agent_ppo2.py:185][0m |          -0.0235 |          34.3619 |           0.7103 |
[32m[20221214 00:18:00 @agent_ppo2.py:185][0m |          -0.0193 |          33.6541 |           0.6450 |
[32m[20221214 00:18:00 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:18:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 409.40
[32m[20221214 00:18:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 452.32
[32m[20221214 00:18:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 433.42
[32m[20221214 00:18:01 @agent_ppo2.py:143][0m Total time:      20.50 min
[32m[20221214 00:18:01 @agent_ppo2.py:145][0m 1890304 total steps have happened
[32m[20221214 00:18:01 @agent_ppo2.py:121][0m #------------------------ Iteration 4923 --------------------------#
[32m[20221214 00:18:01 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:18:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:18:01 @agent_ppo2.py:185][0m |           0.0133 |          73.6830 |           0.3775 |
[32m[20221214 00:18:01 @agent_ppo2.py:185][0m |          -0.0076 |          55.5966 |           0.2919 |
[32m[20221214 00:18:01 @agent_ppo2.py:185][0m |          -0.0037 |          52.6504 |           0.3125 |
[32m[20221214 00:18:01 @agent_ppo2.py:185][0m |          -0.0142 |          50.9922 |           0.3107 |
[32m[20221214 00:18:01 @agent_ppo2.py:185][0m |          -0.0122 |          49.2722 |           0.2869 |
[32m[20221214 00:18:01 @agent_ppo2.py:185][0m |          -0.0182 |          48.4074 |           0.2074 |
[32m[20221214 00:18:02 @agent_ppo2.py:185][0m |          -0.0203 |          46.9047 |           0.2446 |
[32m[20221214 00:18:02 @agent_ppo2.py:185][0m |          -0.0176 |          45.5628 |           0.2801 |
[32m[20221214 00:18:02 @agent_ppo2.py:185][0m |          -0.0155 |          44.8511 |           0.3092 |
[32m[20221214 00:18:02 @agent_ppo2.py:185][0m |          -0.0172 |          44.1477 |           0.2124 |
[32m[20221214 00:18:02 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:18:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 499.45
[32m[20221214 00:18:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 558.60
[32m[20221214 00:18:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 572.54
[32m[20221214 00:18:02 @agent_ppo2.py:143][0m Total time:      20.53 min
[32m[20221214 00:18:02 @agent_ppo2.py:145][0m 1892352 total steps have happened
[32m[20221214 00:18:02 @agent_ppo2.py:121][0m #------------------------ Iteration 4924 --------------------------#
[32m[20221214 00:18:02 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:18:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:18:02 @agent_ppo2.py:185][0m |          -0.0025 |          74.8320 |           0.7132 |
[32m[20221214 00:18:02 @agent_ppo2.py:185][0m |          -0.0114 |          66.2598 |           0.9746 |
[32m[20221214 00:18:03 @agent_ppo2.py:185][0m |           0.0009 |          66.7790 |           1.0319 |
[32m[20221214 00:18:03 @agent_ppo2.py:185][0m |          -0.0119 |          62.6995 |           0.9725 |
[32m[20221214 00:18:03 @agent_ppo2.py:185][0m |          -0.0099 |          61.2564 |           0.9854 |
[32m[20221214 00:18:03 @agent_ppo2.py:185][0m |          -0.0137 |          60.6123 |           1.1057 |
[32m[20221214 00:18:03 @agent_ppo2.py:185][0m |          -0.0156 |          59.7008 |           1.0764 |
[32m[20221214 00:18:03 @agent_ppo2.py:185][0m |          -0.0125 |          59.8454 |           1.2060 |
[32m[20221214 00:18:03 @agent_ppo2.py:185][0m |          -0.0150 |          59.0142 |           1.2434 |
[32m[20221214 00:18:03 @agent_ppo2.py:185][0m |          -0.0204 |          58.7897 |           1.2601 |
[32m[20221214 00:18:03 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:18:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 492.39
[32m[20221214 00:18:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 539.13
[32m[20221214 00:18:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 539.18
[32m[20221214 00:18:03 @agent_ppo2.py:143][0m Total time:      20.55 min
[32m[20221214 00:18:03 @agent_ppo2.py:145][0m 1894400 total steps have happened
[32m[20221214 00:18:03 @agent_ppo2.py:121][0m #------------------------ Iteration 4925 --------------------------#
[32m[20221214 00:18:04 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:18:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:18:04 @agent_ppo2.py:185][0m |           0.0048 |          69.2660 |           0.4526 |
[32m[20221214 00:18:04 @agent_ppo2.py:185][0m |          -0.0061 |          62.9022 |           0.4510 |
[32m[20221214 00:18:04 @agent_ppo2.py:185][0m |          -0.0061 |          60.2044 |           0.4558 |
[32m[20221214 00:18:04 @agent_ppo2.py:185][0m |          -0.0107 |          58.3787 |           0.5866 |
[32m[20221214 00:18:04 @agent_ppo2.py:185][0m |          -0.0122 |          56.9636 |           0.6578 |
[32m[20221214 00:18:04 @agent_ppo2.py:185][0m |          -0.0110 |          55.7752 |           0.6693 |
[32m[20221214 00:18:04 @agent_ppo2.py:185][0m |          -0.0165 |          54.5674 |           0.7003 |
[32m[20221214 00:18:04 @agent_ppo2.py:185][0m |          -0.0114 |          55.0412 |           0.7799 |
[32m[20221214 00:18:05 @agent_ppo2.py:185][0m |          -0.0173 |          53.1590 |           0.7182 |
[32m[20221214 00:18:05 @agent_ppo2.py:185][0m |          -0.0208 |          52.4593 |           0.8232 |
[32m[20221214 00:18:05 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:18:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 477.63
[32m[20221214 00:18:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 535.01
[32m[20221214 00:18:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 530.54
[32m[20221214 00:18:05 @agent_ppo2.py:143][0m Total time:      20.57 min
[32m[20221214 00:18:05 @agent_ppo2.py:145][0m 1896448 total steps have happened
[32m[20221214 00:18:05 @agent_ppo2.py:121][0m #------------------------ Iteration 4926 --------------------------#
[32m[20221214 00:18:05 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:18:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:18:05 @agent_ppo2.py:185][0m |          -0.0037 |          71.5632 |           1.0261 |
[32m[20221214 00:18:05 @agent_ppo2.py:185][0m |          -0.0083 |          63.6552 |           1.0057 |
[32m[20221214 00:18:05 @agent_ppo2.py:185][0m |          -0.0076 |          62.5992 |           1.0127 |
[32m[20221214 00:18:05 @agent_ppo2.py:185][0m |          -0.0133 |          59.1814 |           1.0118 |
[32m[20221214 00:18:06 @agent_ppo2.py:185][0m |          -0.0127 |          57.7587 |           1.0198 |
[32m[20221214 00:18:06 @agent_ppo2.py:185][0m |          -0.0124 |          57.1664 |           1.0661 |
[32m[20221214 00:18:06 @agent_ppo2.py:185][0m |          -0.0161 |          55.7660 |           1.1124 |
[32m[20221214 00:18:06 @agent_ppo2.py:185][0m |          -0.0165 |          55.2557 |           1.1236 |
[32m[20221214 00:18:06 @agent_ppo2.py:185][0m |          -0.0187 |          54.6624 |           1.1029 |
[32m[20221214 00:18:06 @agent_ppo2.py:185][0m |          -0.0203 |          54.1081 |           1.1485 |
[32m[20221214 00:18:06 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:18:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 398.37
[32m[20221214 00:18:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 499.84
[32m[20221214 00:18:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 647.11
[32m[20221214 00:18:06 @agent_ppo2.py:143][0m Total time:      20.60 min
[32m[20221214 00:18:06 @agent_ppo2.py:145][0m 1898496 total steps have happened
[32m[20221214 00:18:06 @agent_ppo2.py:121][0m #------------------------ Iteration 4927 --------------------------#
[32m[20221214 00:18:06 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:18:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:18:07 @agent_ppo2.py:185][0m |           0.0067 |          66.4816 |           0.5613 |
[32m[20221214 00:18:07 @agent_ppo2.py:185][0m |          -0.0063 |          58.4172 |           0.5995 |
[32m[20221214 00:18:07 @agent_ppo2.py:185][0m |           0.0039 |          55.2236 |           0.4684 |
[32m[20221214 00:18:07 @agent_ppo2.py:185][0m |          -0.0104 |          52.6749 |           0.4542 |
[32m[20221214 00:18:07 @agent_ppo2.py:185][0m |          -0.0141 |          51.5328 |           0.4609 |
[32m[20221214 00:18:07 @agent_ppo2.py:185][0m |          -0.0115 |          50.6066 |           0.4834 |
[32m[20221214 00:18:07 @agent_ppo2.py:185][0m |          -0.0186 |          49.7676 |           0.5010 |
[32m[20221214 00:18:07 @agent_ppo2.py:185][0m |          -0.0124 |          48.9538 |           0.4304 |
[32m[20221214 00:18:07 @agent_ppo2.py:185][0m |          -0.0249 |          48.6260 |           0.4282 |
[32m[20221214 00:18:07 @agent_ppo2.py:185][0m |          -0.0100 |          49.5714 |           0.4230 |
[32m[20221214 00:18:07 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:18:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 406.35
[32m[20221214 00:18:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 447.88
[32m[20221214 00:18:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 527.01
[32m[20221214 00:18:08 @agent_ppo2.py:143][0m Total time:      20.62 min
[32m[20221214 00:18:08 @agent_ppo2.py:145][0m 1900544 total steps have happened
[32m[20221214 00:18:08 @agent_ppo2.py:121][0m #------------------------ Iteration 4928 --------------------------#
[32m[20221214 00:18:08 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:18:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:18:08 @agent_ppo2.py:185][0m |           0.0013 |          82.0737 |           1.3274 |
[32m[20221214 00:18:08 @agent_ppo2.py:185][0m |          -0.0031 |          73.9297 |           1.2354 |
[32m[20221214 00:18:08 @agent_ppo2.py:185][0m |          -0.0066 |          71.5140 |           1.2475 |
[32m[20221214 00:18:08 @agent_ppo2.py:185][0m |          -0.0119 |          68.7892 |           1.1972 |
[32m[20221214 00:18:08 @agent_ppo2.py:185][0m |          -0.0130 |          67.3275 |           1.0917 |
[32m[20221214 00:18:08 @agent_ppo2.py:185][0m |          -0.0137 |          65.9496 |           1.1108 |
[32m[20221214 00:18:09 @agent_ppo2.py:185][0m |          -0.0103 |          65.7948 |           1.0416 |
[32m[20221214 00:18:09 @agent_ppo2.py:185][0m |          -0.0121 |          64.5108 |           1.0278 |
[32m[20221214 00:18:09 @agent_ppo2.py:185][0m |          -0.0061 |          65.0661 |           1.0413 |
[32m[20221214 00:18:09 @agent_ppo2.py:185][0m |          -0.0145 |          63.4383 |           0.9613 |
[32m[20221214 00:18:09 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:18:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 522.45
[32m[20221214 00:18:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 591.58
[32m[20221214 00:18:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 629.52
[32m[20221214 00:18:09 @agent_ppo2.py:143][0m Total time:      20.64 min
[32m[20221214 00:18:09 @agent_ppo2.py:145][0m 1902592 total steps have happened
[32m[20221214 00:18:09 @agent_ppo2.py:121][0m #------------------------ Iteration 4929 --------------------------#
[32m[20221214 00:18:09 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:18:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:18:09 @agent_ppo2.py:185][0m |           0.0045 |          89.1836 |           0.1025 |
[32m[20221214 00:18:09 @agent_ppo2.py:185][0m |          -0.0023 |          81.3624 |           0.1278 |
[32m[20221214 00:18:10 @agent_ppo2.py:185][0m |          -0.0053 |          78.7855 |           0.0968 |
[32m[20221214 00:18:10 @agent_ppo2.py:185][0m |          -0.0074 |          77.2557 |           0.1245 |
[32m[20221214 00:18:10 @agent_ppo2.py:185][0m |          -0.0099 |          75.7995 |           0.1757 |
[32m[20221214 00:18:10 @agent_ppo2.py:185][0m |          -0.0071 |          75.3524 |           0.1447 |
[32m[20221214 00:18:10 @agent_ppo2.py:185][0m |          -0.0120 |          74.7818 |           0.2098 |
[32m[20221214 00:18:10 @agent_ppo2.py:185][0m |          -0.0120 |          73.9563 |           0.2535 |
[32m[20221214 00:18:10 @agent_ppo2.py:185][0m |          -0.0127 |          73.2259 |           0.2282 |
[32m[20221214 00:18:10 @agent_ppo2.py:185][0m |          -0.0117 |          72.6213 |           0.1962 |
[32m[20221214 00:18:10 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:18:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 558.49
[32m[20221214 00:18:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 613.21
[32m[20221214 00:18:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 536.16
[32m[20221214 00:18:10 @agent_ppo2.py:143][0m Total time:      20.67 min
[32m[20221214 00:18:10 @agent_ppo2.py:145][0m 1904640 total steps have happened
[32m[20221214 00:18:10 @agent_ppo2.py:121][0m #------------------------ Iteration 4930 --------------------------#
[32m[20221214 00:18:11 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:18:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:18:11 @agent_ppo2.py:185][0m |           0.0063 |          81.6794 |          -0.5910 |
[32m[20221214 00:18:11 @agent_ppo2.py:185][0m |          -0.0100 |          70.1174 |          -0.5620 |
[32m[20221214 00:18:11 @agent_ppo2.py:185][0m |          -0.0136 |          68.5999 |          -0.5108 |
[32m[20221214 00:18:11 @agent_ppo2.py:185][0m |          -0.0149 |          67.6021 |          -0.4249 |
[32m[20221214 00:18:11 @agent_ppo2.py:185][0m |          -0.0102 |          66.6745 |          -0.4781 |
[32m[20221214 00:18:11 @agent_ppo2.py:185][0m |          -0.0160 |          66.1354 |          -0.4056 |
[32m[20221214 00:18:11 @agent_ppo2.py:185][0m |          -0.0179 |          65.5066 |          -0.4733 |
[32m[20221214 00:18:11 @agent_ppo2.py:185][0m |          -0.0200 |          64.8033 |          -0.4350 |
[32m[20221214 00:18:12 @agent_ppo2.py:185][0m |          -0.0179 |          64.6047 |          -0.3631 |
[32m[20221214 00:18:12 @agent_ppo2.py:185][0m |          -0.0199 |          63.9400 |          -0.4114 |
[32m[20221214 00:18:12 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:18:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 466.33
[32m[20221214 00:18:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 501.95
[32m[20221214 00:18:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 474.41
[32m[20221214 00:18:12 @agent_ppo2.py:143][0m Total time:      20.69 min
[32m[20221214 00:18:12 @agent_ppo2.py:145][0m 1906688 total steps have happened
[32m[20221214 00:18:12 @agent_ppo2.py:121][0m #------------------------ Iteration 4931 --------------------------#
[32m[20221214 00:18:12 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:18:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:18:12 @agent_ppo2.py:185][0m |           0.0039 |          56.8236 |           0.0011 |
[32m[20221214 00:18:12 @agent_ppo2.py:185][0m |          -0.0099 |          45.6839 |           0.0428 |
[32m[20221214 00:18:12 @agent_ppo2.py:185][0m |          -0.0051 |          42.9637 |           0.1808 |
[32m[20221214 00:18:12 @agent_ppo2.py:185][0m |          -0.0109 |          41.1716 |           0.2373 |
[32m[20221214 00:18:13 @agent_ppo2.py:185][0m |          -0.0179 |          39.9098 |           0.2286 |
[32m[20221214 00:18:13 @agent_ppo2.py:185][0m |          -0.0125 |          38.6818 |           0.3171 |
[32m[20221214 00:18:13 @agent_ppo2.py:185][0m |          -0.0211 |          38.0803 |           0.3828 |
[32m[20221214 00:18:13 @agent_ppo2.py:185][0m |          -0.0191 |          37.2890 |           0.3253 |
[32m[20221214 00:18:13 @agent_ppo2.py:185][0m |          -0.0152 |          38.2111 |           0.4314 |
[32m[20221214 00:18:13 @agent_ppo2.py:185][0m |          -0.0140 |          37.2283 |           0.4407 |
[32m[20221214 00:18:13 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:18:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 518.52
[32m[20221214 00:18:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 593.33
[32m[20221214 00:18:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 562.71
[32m[20221214 00:18:13 @agent_ppo2.py:143][0m Total time:      20.71 min
[32m[20221214 00:18:13 @agent_ppo2.py:145][0m 1908736 total steps have happened
[32m[20221214 00:18:13 @agent_ppo2.py:121][0m #------------------------ Iteration 4932 --------------------------#
[32m[20221214 00:18:13 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:18:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:18:14 @agent_ppo2.py:185][0m |           0.0030 |          68.8283 |           0.6670 |
[32m[20221214 00:18:14 @agent_ppo2.py:185][0m |          -0.0039 |          58.0881 |           0.7252 |
[32m[20221214 00:18:14 @agent_ppo2.py:185][0m |          -0.0042 |          53.4068 |           0.7080 |
[32m[20221214 00:18:14 @agent_ppo2.py:185][0m |          -0.0131 |          50.5600 |           0.7008 |
[32m[20221214 00:18:14 @agent_ppo2.py:185][0m |          -0.0164 |          49.5794 |           0.8018 |
[32m[20221214 00:18:14 @agent_ppo2.py:185][0m |          -0.0155 |          48.0643 |           0.7016 |
[32m[20221214 00:18:14 @agent_ppo2.py:185][0m |          -0.0218 |          47.1116 |           0.7975 |
[32m[20221214 00:18:14 @agent_ppo2.py:185][0m |          -0.0123 |          46.5801 |           0.8609 |
[32m[20221214 00:18:14 @agent_ppo2.py:185][0m |          -0.0082 |          49.3548 |           0.8796 |
[32m[20221214 00:18:14 @agent_ppo2.py:185][0m |          -0.0064 |          48.9281 |           0.7718 |
[32m[20221214 00:18:14 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:18:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 459.55
[32m[20221214 00:18:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 513.93
[32m[20221214 00:18:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 538.47
[32m[20221214 00:18:15 @agent_ppo2.py:143][0m Total time:      20.74 min
[32m[20221214 00:18:15 @agent_ppo2.py:145][0m 1910784 total steps have happened
[32m[20221214 00:18:15 @agent_ppo2.py:121][0m #------------------------ Iteration 4933 --------------------------#
[32m[20221214 00:18:15 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:18:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:18:15 @agent_ppo2.py:185][0m |           0.0028 |          50.9381 |           1.2483 |
[32m[20221214 00:18:15 @agent_ppo2.py:185][0m |          -0.0051 |          39.0764 |           1.3595 |
[32m[20221214 00:18:15 @agent_ppo2.py:185][0m |           0.0050 |          45.1198 |           1.4455 |
[32m[20221214 00:18:15 @agent_ppo2.py:185][0m |          -0.0100 |          34.9436 |           1.4656 |
[32m[20221214 00:18:15 @agent_ppo2.py:185][0m |          -0.0148 |          34.8445 |           1.4569 |
[32m[20221214 00:18:15 @agent_ppo2.py:185][0m |          -0.0158 |          33.3773 |           1.4952 |
[32m[20221214 00:18:15 @agent_ppo2.py:185][0m |          -0.0179 |          32.9465 |           1.5146 |
[32m[20221214 00:18:16 @agent_ppo2.py:185][0m |          -0.0200 |          32.4155 |           1.5046 |
[32m[20221214 00:18:16 @agent_ppo2.py:185][0m |          -0.0229 |          31.9020 |           1.5947 |
[32m[20221214 00:18:16 @agent_ppo2.py:185][0m |          -0.0226 |          31.4953 |           1.6051 |
[32m[20221214 00:18:16 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:18:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 426.09
[32m[20221214 00:18:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 546.88
[32m[20221214 00:18:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 472.40
[32m[20221214 00:18:16 @agent_ppo2.py:143][0m Total time:      20.76 min
[32m[20221214 00:18:16 @agent_ppo2.py:145][0m 1912832 total steps have happened
[32m[20221214 00:18:16 @agent_ppo2.py:121][0m #------------------------ Iteration 4934 --------------------------#
[32m[20221214 00:18:16 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:18:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:18:16 @agent_ppo2.py:185][0m |           0.0031 |          65.6704 |           0.7515 |
[32m[20221214 00:18:16 @agent_ppo2.py:185][0m |          -0.0088 |          59.8486 |           0.7223 |
[32m[20221214 00:18:16 @agent_ppo2.py:185][0m |          -0.0117 |          57.8772 |           0.6339 |
[32m[20221214 00:18:17 @agent_ppo2.py:185][0m |          -0.0141 |          57.0754 |           0.7021 |
[32m[20221214 00:18:17 @agent_ppo2.py:185][0m |          -0.0097 |          56.0973 |           0.5904 |
[32m[20221214 00:18:17 @agent_ppo2.py:185][0m |          -0.0152 |          54.8459 |           0.5839 |
[32m[20221214 00:18:17 @agent_ppo2.py:185][0m |          -0.0156 |          54.5257 |           0.5840 |
[32m[20221214 00:18:17 @agent_ppo2.py:185][0m |          -0.0138 |          53.8511 |           0.6071 |
[32m[20221214 00:18:17 @agent_ppo2.py:185][0m |          -0.0174 |          53.6029 |           0.6063 |
[32m[20221214 00:18:17 @agent_ppo2.py:185][0m |          -0.0184 |          53.3473 |           0.5450 |
[32m[20221214 00:18:17 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:18:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 444.13
[32m[20221214 00:18:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 515.40
[32m[20221214 00:18:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 625.91
[32m[20221214 00:18:17 @agent_ppo2.py:143][0m Total time:      20.78 min
[32m[20221214 00:18:17 @agent_ppo2.py:145][0m 1914880 total steps have happened
[32m[20221214 00:18:17 @agent_ppo2.py:121][0m #------------------------ Iteration 4935 --------------------------#
[32m[20221214 00:18:17 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:18:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:18:18 @agent_ppo2.py:185][0m |           0.0052 |          76.5726 |           1.2820 |
[32m[20221214 00:18:18 @agent_ppo2.py:185][0m |          -0.0015 |          72.6143 |           1.4786 |
[32m[20221214 00:18:18 @agent_ppo2.py:185][0m |          -0.0057 |          71.5749 |           1.4642 |
[32m[20221214 00:18:18 @agent_ppo2.py:185][0m |           0.0017 |          74.9349 |           1.6093 |
[32m[20221214 00:18:18 @agent_ppo2.py:185][0m |          -0.0125 |          70.2242 |           1.7262 |
[32m[20221214 00:18:18 @agent_ppo2.py:185][0m |          -0.0130 |          69.8611 |           1.7761 |
[32m[20221214 00:18:18 @agent_ppo2.py:185][0m |          -0.0142 |          69.3655 |           1.7692 |
[32m[20221214 00:18:18 @agent_ppo2.py:185][0m |          -0.0086 |          70.2122 |           1.7869 |
[32m[20221214 00:18:18 @agent_ppo2.py:185][0m |          -0.0171 |          69.2339 |           1.8140 |
[32m[20221214 00:18:19 @agent_ppo2.py:185][0m |          -0.0078 |          74.1027 |           1.8559 |
[32m[20221214 00:18:19 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:18:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 528.92
[32m[20221214 00:18:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 553.95
[32m[20221214 00:18:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 572.71
[32m[20221214 00:18:19 @agent_ppo2.py:143][0m Total time:      20.81 min
[32m[20221214 00:18:19 @agent_ppo2.py:145][0m 1916928 total steps have happened
[32m[20221214 00:18:19 @agent_ppo2.py:121][0m #------------------------ Iteration 4936 --------------------------#
[32m[20221214 00:18:19 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:18:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:18:19 @agent_ppo2.py:185][0m |           0.0048 |          60.5649 |           0.2495 |
[32m[20221214 00:18:19 @agent_ppo2.py:185][0m |          -0.0032 |          54.9420 |           0.3351 |
[32m[20221214 00:18:19 @agent_ppo2.py:185][0m |          -0.0099 |          52.0579 |           0.5877 |
[32m[20221214 00:18:19 @agent_ppo2.py:185][0m |          -0.0083 |          52.6929 |           0.5470 |
[32m[20221214 00:18:19 @agent_ppo2.py:185][0m |          -0.0117 |          50.4448 |           0.6534 |
[32m[20221214 00:18:20 @agent_ppo2.py:185][0m |          -0.0141 |          49.5379 |           0.6655 |
[32m[20221214 00:18:20 @agent_ppo2.py:185][0m |          -0.0131 |          49.5614 |           0.6845 |
[32m[20221214 00:18:20 @agent_ppo2.py:185][0m |          -0.0172 |          48.7158 |           0.7025 |
[32m[20221214 00:18:20 @agent_ppo2.py:185][0m |          -0.0170 |          48.0370 |           0.7440 |
[32m[20221214 00:18:20 @agent_ppo2.py:185][0m |          -0.0176 |          48.2506 |           0.7215 |
[32m[20221214 00:18:20 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:18:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 456.00
[32m[20221214 00:18:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 538.23
[32m[20221214 00:18:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 628.05
[32m[20221214 00:18:20 @agent_ppo2.py:143][0m Total time:      20.83 min
[32m[20221214 00:18:20 @agent_ppo2.py:145][0m 1918976 total steps have happened
[32m[20221214 00:18:20 @agent_ppo2.py:121][0m #------------------------ Iteration 4937 --------------------------#
[32m[20221214 00:18:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:18:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:18:20 @agent_ppo2.py:185][0m |           0.0065 |          57.8995 |           1.6378 |
[32m[20221214 00:18:21 @agent_ppo2.py:185][0m |          -0.0091 |          52.2513 |           1.5062 |
[32m[20221214 00:18:21 @agent_ppo2.py:185][0m |          -0.0139 |          50.0138 |           1.5401 |
[32m[20221214 00:18:21 @agent_ppo2.py:185][0m |          -0.0175 |          48.5984 |           1.5671 |
[32m[20221214 00:18:21 @agent_ppo2.py:185][0m |          -0.0050 |          50.9483 |           1.5671 |
[32m[20221214 00:18:21 @agent_ppo2.py:185][0m |          -0.0107 |          46.8817 |           1.5536 |
[32m[20221214 00:18:21 @agent_ppo2.py:185][0m |          -0.0157 |          45.8389 |           1.5884 |
[32m[20221214 00:18:21 @agent_ppo2.py:185][0m |          -0.0204 |          45.3339 |           1.6759 |
[32m[20221214 00:18:21 @agent_ppo2.py:185][0m |          -0.0186 |          44.6756 |           1.5760 |
[32m[20221214 00:18:21 @agent_ppo2.py:185][0m |          -0.0158 |          44.3408 |           1.5876 |
[32m[20221214 00:18:21 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:18:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 451.53
[32m[20221214 00:18:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 490.27
[32m[20221214 00:18:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 670.90
[32m[20221214 00:18:22 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 670.90
[32m[20221214 00:18:22 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 670.90
[32m[20221214 00:18:22 @agent_ppo2.py:143][0m Total time:      20.85 min
[32m[20221214 00:18:22 @agent_ppo2.py:145][0m 1921024 total steps have happened
[32m[20221214 00:18:22 @agent_ppo2.py:121][0m #------------------------ Iteration 4938 --------------------------#
[32m[20221214 00:18:22 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:18:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:18:22 @agent_ppo2.py:185][0m |           0.0040 |          70.4578 |           0.7927 |
[32m[20221214 00:18:22 @agent_ppo2.py:185][0m |          -0.0068 |          64.0667 |           0.8682 |
[32m[20221214 00:18:22 @agent_ppo2.py:185][0m |          -0.0103 |          62.6912 |           0.9748 |
[32m[20221214 00:18:22 @agent_ppo2.py:185][0m |          -0.0056 |          61.9583 |           0.9688 |
[32m[20221214 00:18:22 @agent_ppo2.py:185][0m |          -0.0115 |          61.0564 |           1.0017 |
[32m[20221214 00:18:22 @agent_ppo2.py:185][0m |          -0.0130 |          60.1997 |           1.0867 |
[32m[20221214 00:18:22 @agent_ppo2.py:185][0m |          -0.0137 |          59.8827 |           1.0456 |
[32m[20221214 00:18:23 @agent_ppo2.py:185][0m |          -0.0176 |          59.7265 |           1.0672 |
[32m[20221214 00:18:23 @agent_ppo2.py:185][0m |          -0.0177 |          59.3048 |           0.9922 |
[32m[20221214 00:18:23 @agent_ppo2.py:185][0m |          -0.0130 |          59.0841 |           1.0757 |
[32m[20221214 00:18:23 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:18:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 501.97
[32m[20221214 00:18:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 556.56
[32m[20221214 00:18:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 616.67
[32m[20221214 00:18:23 @agent_ppo2.py:143][0m Total time:      20.88 min
[32m[20221214 00:18:23 @agent_ppo2.py:145][0m 1923072 total steps have happened
[32m[20221214 00:18:23 @agent_ppo2.py:121][0m #------------------------ Iteration 4939 --------------------------#
[32m[20221214 00:18:23 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:18:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:18:23 @agent_ppo2.py:185][0m |          -0.0006 |          72.0572 |           1.7605 |
[32m[20221214 00:18:23 @agent_ppo2.py:185][0m |          -0.0052 |          64.5387 |           1.8292 |
[32m[20221214 00:18:23 @agent_ppo2.py:185][0m |          -0.0008 |          63.2609 |           1.8407 |
[32m[20221214 00:18:24 @agent_ppo2.py:185][0m |           0.0005 |          63.6086 |           1.8087 |
[32m[20221214 00:18:24 @agent_ppo2.py:185][0m |          -0.0089 |          59.7496 |           1.8497 |
[32m[20221214 00:18:24 @agent_ppo2.py:185][0m |          -0.0117 |          58.1843 |           1.8647 |
[32m[20221214 00:18:24 @agent_ppo2.py:185][0m |          -0.0141 |          57.5957 |           1.7693 |
[32m[20221214 00:18:24 @agent_ppo2.py:185][0m |          -0.0112 |          57.0430 |           1.7261 |
[32m[20221214 00:18:24 @agent_ppo2.py:185][0m |          -0.0115 |          56.5182 |           1.7304 |
[32m[20221214 00:18:24 @agent_ppo2.py:185][0m |          -0.0133 |          56.3474 |           1.7668 |
[32m[20221214 00:18:24 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:18:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 515.90
[32m[20221214 00:18:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 543.04
[32m[20221214 00:18:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 539.92
[32m[20221214 00:18:24 @agent_ppo2.py:143][0m Total time:      20.90 min
[32m[20221214 00:18:24 @agent_ppo2.py:145][0m 1925120 total steps have happened
[32m[20221214 00:18:24 @agent_ppo2.py:121][0m #------------------------ Iteration 4940 --------------------------#
[32m[20221214 00:18:24 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:18:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:18:25 @agent_ppo2.py:185][0m |          -0.0002 |          78.8128 |           1.4174 |
[32m[20221214 00:18:25 @agent_ppo2.py:185][0m |           0.0093 |          88.1857 |           1.4459 |
[32m[20221214 00:18:25 @agent_ppo2.py:185][0m |          -0.0089 |          72.5720 |           1.4770 |
[32m[20221214 00:18:25 @agent_ppo2.py:185][0m |          -0.0101 |          71.5068 |           1.4516 |
[32m[20221214 00:18:25 @agent_ppo2.py:185][0m |          -0.0121 |          70.2928 |           1.4780 |
[32m[20221214 00:18:25 @agent_ppo2.py:185][0m |          -0.0117 |          69.5925 |           1.5526 |
[32m[20221214 00:18:25 @agent_ppo2.py:185][0m |          -0.0156 |          69.4452 |           1.4776 |
[32m[20221214 00:18:25 @agent_ppo2.py:185][0m |          -0.0183 |          69.3640 |           1.4282 |
[32m[20221214 00:18:25 @agent_ppo2.py:185][0m |          -0.0210 |          69.0495 |           1.4360 |
[32m[20221214 00:18:26 @agent_ppo2.py:185][0m |          -0.0150 |          68.6629 |           1.4091 |
[32m[20221214 00:18:26 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:18:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 446.94
[32m[20221214 00:18:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 511.45
[32m[20221214 00:18:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 557.21
[32m[20221214 00:18:26 @agent_ppo2.py:143][0m Total time:      20.92 min
[32m[20221214 00:18:26 @agent_ppo2.py:145][0m 1927168 total steps have happened
[32m[20221214 00:18:26 @agent_ppo2.py:121][0m #------------------------ Iteration 4941 --------------------------#
[32m[20221214 00:18:26 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:18:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:18:26 @agent_ppo2.py:185][0m |           0.0085 |          67.3303 |           1.3280 |
[32m[20221214 00:18:26 @agent_ppo2.py:185][0m |          -0.0075 |          60.2919 |           1.3876 |
[32m[20221214 00:18:26 @agent_ppo2.py:185][0m |           0.0147 |          70.4766 |           1.2768 |
[32m[20221214 00:18:26 @agent_ppo2.py:185][0m |          -0.0102 |          56.7178 |           1.2678 |
[32m[20221214 00:18:26 @agent_ppo2.py:185][0m |          -0.0148 |          55.1518 |           1.3071 |
[32m[20221214 00:18:27 @agent_ppo2.py:185][0m |          -0.0164 |          54.0321 |           1.2476 |
[32m[20221214 00:18:27 @agent_ppo2.py:185][0m |          -0.0157 |          52.6018 |           1.2076 |
[32m[20221214 00:18:27 @agent_ppo2.py:185][0m |          -0.0133 |          49.7027 |           1.2126 |
[32m[20221214 00:18:27 @agent_ppo2.py:185][0m |          -0.0188 |          47.9531 |           1.3038 |
[32m[20221214 00:18:27 @agent_ppo2.py:185][0m |          -0.0185 |          47.0790 |           1.2378 |
[32m[20221214 00:18:27 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:18:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 436.40
[32m[20221214 00:18:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 510.51
[32m[20221214 00:18:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 458.58
[32m[20221214 00:18:27 @agent_ppo2.py:143][0m Total time:      20.95 min
[32m[20221214 00:18:27 @agent_ppo2.py:145][0m 1929216 total steps have happened
[32m[20221214 00:18:27 @agent_ppo2.py:121][0m #------------------------ Iteration 4942 --------------------------#
[32m[20221214 00:18:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:18:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:18:27 @agent_ppo2.py:185][0m |          -0.0008 |          76.2380 |           1.1580 |
[32m[20221214 00:18:28 @agent_ppo2.py:185][0m |          -0.0066 |          68.4981 |           1.2110 |
[32m[20221214 00:18:28 @agent_ppo2.py:185][0m |          -0.0112 |          65.7862 |           1.1912 |
[32m[20221214 00:18:28 @agent_ppo2.py:185][0m |          -0.0080 |          64.5947 |           1.2243 |
[32m[20221214 00:18:28 @agent_ppo2.py:185][0m |          -0.0159 |          62.8438 |           1.1817 |
[32m[20221214 00:18:28 @agent_ppo2.py:185][0m |          -0.0077 |          70.7050 |           1.1677 |
[32m[20221214 00:18:28 @agent_ppo2.py:185][0m |          -0.0161 |          61.6562 |           1.1637 |
[32m[20221214 00:18:28 @agent_ppo2.py:185][0m |          -0.0195 |          61.1082 |           1.1643 |
[32m[20221214 00:18:28 @agent_ppo2.py:185][0m |          -0.0137 |          60.6823 |           1.1308 |
[32m[20221214 00:18:28 @agent_ppo2.py:185][0m |          -0.0204 |          60.1960 |           1.1142 |
[32m[20221214 00:18:28 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:18:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 484.45
[32m[20221214 00:18:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 544.86
[32m[20221214 00:18:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 574.05
[32m[20221214 00:18:28 @agent_ppo2.py:143][0m Total time:      20.97 min
[32m[20221214 00:18:28 @agent_ppo2.py:145][0m 1931264 total steps have happened
[32m[20221214 00:18:28 @agent_ppo2.py:121][0m #------------------------ Iteration 4943 --------------------------#
[32m[20221214 00:18:29 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:18:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:18:29 @agent_ppo2.py:185][0m |          -0.0018 |          86.6888 |           0.7958 |
[32m[20221214 00:18:29 @agent_ppo2.py:185][0m |          -0.0061 |          81.0647 |           0.7797 |
[32m[20221214 00:18:29 @agent_ppo2.py:185][0m |          -0.0069 |          78.3752 |           0.8325 |
[32m[20221214 00:18:29 @agent_ppo2.py:185][0m |          -0.0114 |          76.8471 |           0.9495 |
[32m[20221214 00:18:29 @agent_ppo2.py:185][0m |          -0.0107 |          75.8145 |           0.8946 |
[32m[20221214 00:18:29 @agent_ppo2.py:185][0m |          -0.0123 |          74.9820 |           0.9882 |
[32m[20221214 00:18:29 @agent_ppo2.py:185][0m |          -0.0162 |          74.2168 |           1.0045 |
[32m[20221214 00:18:30 @agent_ppo2.py:185][0m |          -0.0175 |          73.3027 |           1.0880 |
[32m[20221214 00:18:30 @agent_ppo2.py:185][0m |          -0.0008 |          79.4576 |           1.0748 |
[32m[20221214 00:18:30 @agent_ppo2.py:185][0m |          -0.0180 |          72.6689 |           1.0384 |
[32m[20221214 00:18:30 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:18:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 520.55
[32m[20221214 00:18:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 556.90
[32m[20221214 00:18:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 346.68
[32m[20221214 00:18:30 @agent_ppo2.py:143][0m Total time:      20.99 min
[32m[20221214 00:18:30 @agent_ppo2.py:145][0m 1933312 total steps have happened
[32m[20221214 00:18:30 @agent_ppo2.py:121][0m #------------------------ Iteration 4944 --------------------------#
[32m[20221214 00:18:30 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:18:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:18:30 @agent_ppo2.py:185][0m |          -0.0005 |          92.9746 |           2.0793 |
[32m[20221214 00:18:30 @agent_ppo2.py:185][0m |          -0.0078 |          87.3534 |           2.0937 |
[32m[20221214 00:18:30 @agent_ppo2.py:185][0m |          -0.0050 |          86.1125 |           2.0462 |
[32m[20221214 00:18:31 @agent_ppo2.py:185][0m |          -0.0083 |          84.6564 |           2.0797 |
[32m[20221214 00:18:31 @agent_ppo2.py:185][0m |          -0.0119 |          83.3496 |           2.0782 |
[32m[20221214 00:18:31 @agent_ppo2.py:185][0m |          -0.0106 |          82.7244 |           2.0324 |
[32m[20221214 00:18:31 @agent_ppo2.py:185][0m |          -0.0139 |          82.2747 |           2.1381 |
[32m[20221214 00:18:31 @agent_ppo2.py:185][0m |          -0.0107 |          82.0039 |           2.0296 |
[32m[20221214 00:18:31 @agent_ppo2.py:185][0m |          -0.0147 |          81.5436 |           2.0903 |
[32m[20221214 00:18:31 @agent_ppo2.py:185][0m |          -0.0109 |          81.9669 |           2.0790 |
[32m[20221214 00:18:31 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:18:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 453.45
[32m[20221214 00:18:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 533.13
[32m[20221214 00:18:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 619.97
[32m[20221214 00:18:31 @agent_ppo2.py:143][0m Total time:      21.02 min
[32m[20221214 00:18:31 @agent_ppo2.py:145][0m 1935360 total steps have happened
[32m[20221214 00:18:31 @agent_ppo2.py:121][0m #------------------------ Iteration 4945 --------------------------#
[32m[20221214 00:18:31 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:18:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:18:32 @agent_ppo2.py:185][0m |           0.0006 |          70.2739 |           1.9615 |
[32m[20221214 00:18:32 @agent_ppo2.py:185][0m |          -0.0052 |          65.7274 |           2.0076 |
[32m[20221214 00:18:32 @agent_ppo2.py:185][0m |          -0.0095 |          63.2456 |           2.0020 |
[32m[20221214 00:18:32 @agent_ppo2.py:185][0m |          -0.0086 |          61.7355 |           2.0531 |
[32m[20221214 00:18:32 @agent_ppo2.py:185][0m |          -0.0134 |          60.8924 |           2.0170 |
[32m[20221214 00:18:32 @agent_ppo2.py:185][0m |          -0.0083 |          59.7834 |           2.0360 |
[32m[20221214 00:18:32 @agent_ppo2.py:185][0m |          -0.0094 |          59.7407 |           1.9692 |
[32m[20221214 00:18:32 @agent_ppo2.py:185][0m |          -0.0132 |          58.8850 |           2.1202 |
[32m[20221214 00:18:32 @agent_ppo2.py:185][0m |          -0.0153 |          58.3078 |           2.0789 |
[32m[20221214 00:18:33 @agent_ppo2.py:185][0m |          -0.0171 |          57.7342 |           2.0910 |
[32m[20221214 00:18:33 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:18:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 454.31
[32m[20221214 00:18:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 501.37
[32m[20221214 00:18:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 546.98
[32m[20221214 00:18:33 @agent_ppo2.py:143][0m Total time:      21.04 min
[32m[20221214 00:18:33 @agent_ppo2.py:145][0m 1937408 total steps have happened
[32m[20221214 00:18:33 @agent_ppo2.py:121][0m #------------------------ Iteration 4946 --------------------------#
[32m[20221214 00:18:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:18:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:18:33 @agent_ppo2.py:185][0m |           0.0026 |          79.1320 |           1.4902 |
[32m[20221214 00:18:33 @agent_ppo2.py:185][0m |           0.0014 |          74.9123 |           1.5877 |
[32m[20221214 00:18:33 @agent_ppo2.py:185][0m |          -0.0081 |          68.2329 |           1.6642 |
[32m[20221214 00:18:33 @agent_ppo2.py:185][0m |          -0.0160 |          66.4812 |           1.5144 |
[32m[20221214 00:18:33 @agent_ppo2.py:185][0m |          -0.0154 |          64.7540 |           1.5063 |
[32m[20221214 00:18:34 @agent_ppo2.py:185][0m |          -0.0173 |          63.7290 |           1.6205 |
[32m[20221214 00:18:34 @agent_ppo2.py:185][0m |          -0.0153 |          62.7524 |           1.5857 |
[32m[20221214 00:18:34 @agent_ppo2.py:185][0m |          -0.0187 |          62.0275 |           1.5549 |
[32m[20221214 00:18:34 @agent_ppo2.py:185][0m |          -0.0205 |          61.6977 |           1.6071 |
[32m[20221214 00:18:34 @agent_ppo2.py:185][0m |          -0.0162 |          60.9826 |           1.6342 |
[32m[20221214 00:18:34 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:18:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 429.92
[32m[20221214 00:18:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 511.17
[32m[20221214 00:18:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 554.16
[32m[20221214 00:18:34 @agent_ppo2.py:143][0m Total time:      21.06 min
[32m[20221214 00:18:34 @agent_ppo2.py:145][0m 1939456 total steps have happened
[32m[20221214 00:18:34 @agent_ppo2.py:121][0m #------------------------ Iteration 4947 --------------------------#
[32m[20221214 00:18:34 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:18:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:18:34 @agent_ppo2.py:185][0m |           0.0074 |          84.5926 |           1.7028 |
[32m[20221214 00:18:35 @agent_ppo2.py:185][0m |          -0.0085 |          69.7817 |           1.8422 |
[32m[20221214 00:18:35 @agent_ppo2.py:185][0m |          -0.0042 |          67.2711 |           1.6976 |
[32m[20221214 00:18:35 @agent_ppo2.py:185][0m |          -0.0096 |          66.2005 |           1.6924 |
[32m[20221214 00:18:35 @agent_ppo2.py:185][0m |          -0.0076 |          65.8126 |           1.7156 |
[32m[20221214 00:18:35 @agent_ppo2.py:185][0m |          -0.0094 |          64.8210 |           1.6108 |
[32m[20221214 00:18:35 @agent_ppo2.py:185][0m |          -0.0019 |          69.7929 |           1.7232 |
[32m[20221214 00:18:35 @agent_ppo2.py:185][0m |          -0.0122 |          64.6083 |           1.7113 |
[32m[20221214 00:18:35 @agent_ppo2.py:185][0m |          -0.0159 |          63.8099 |           1.6241 |
[32m[20221214 00:18:35 @agent_ppo2.py:185][0m |          -0.0078 |          63.9083 |           1.6815 |
[32m[20221214 00:18:35 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:18:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 513.19
[32m[20221214 00:18:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 550.63
[32m[20221214 00:18:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 614.79
[32m[20221214 00:18:35 @agent_ppo2.py:143][0m Total time:      21.08 min
[32m[20221214 00:18:35 @agent_ppo2.py:145][0m 1941504 total steps have happened
[32m[20221214 00:18:35 @agent_ppo2.py:121][0m #------------------------ Iteration 4948 --------------------------#
[32m[20221214 00:18:36 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:18:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:18:36 @agent_ppo2.py:185][0m |          -0.0001 |          88.8331 |           1.0915 |
[32m[20221214 00:18:36 @agent_ppo2.py:185][0m |          -0.0075 |          83.9479 |           1.0856 |
[32m[20221214 00:18:36 @agent_ppo2.py:185][0m |          -0.0102 |          82.0456 |           1.0763 |
[32m[20221214 00:18:36 @agent_ppo2.py:185][0m |          -0.0065 |          83.5443 |           0.9509 |
[32m[20221214 00:18:36 @agent_ppo2.py:185][0m |          -0.0098 |          80.6199 |           0.9899 |
[32m[20221214 00:18:36 @agent_ppo2.py:185][0m |          -0.0131 |          79.8754 |           1.0006 |
[32m[20221214 00:18:36 @agent_ppo2.py:185][0m |          -0.0164 |          79.4863 |           0.9558 |
[32m[20221214 00:18:36 @agent_ppo2.py:185][0m |          -0.0135 |          80.0663 |           0.9546 |
[32m[20221214 00:18:37 @agent_ppo2.py:185][0m |          -0.0180 |          78.7692 |           0.9413 |
[32m[20221214 00:18:37 @agent_ppo2.py:185][0m |          -0.0142 |          78.8285 |           0.8718 |
[32m[20221214 00:18:37 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:18:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 441.55
[32m[20221214 00:18:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 500.36
[32m[20221214 00:18:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 500.19
[32m[20221214 00:18:37 @agent_ppo2.py:143][0m Total time:      21.11 min
[32m[20221214 00:18:37 @agent_ppo2.py:145][0m 1943552 total steps have happened
[32m[20221214 00:18:37 @agent_ppo2.py:121][0m #------------------------ Iteration 4949 --------------------------#
[32m[20221214 00:18:37 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:18:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:18:37 @agent_ppo2.py:185][0m |           0.0048 |          80.6366 |           1.2136 |
[32m[20221214 00:18:37 @agent_ppo2.py:185][0m |          -0.0037 |          75.2794 |           1.3652 |
[32m[20221214 00:18:37 @agent_ppo2.py:185][0m |          -0.0082 |          73.3175 |           1.2462 |
[32m[20221214 00:18:37 @agent_ppo2.py:185][0m |          -0.0137 |          72.2461 |           1.3582 |
[32m[20221214 00:18:38 @agent_ppo2.py:185][0m |          -0.0134 |          71.2759 |           1.3313 |
[32m[20221214 00:18:38 @agent_ppo2.py:185][0m |          -0.0149 |          70.6217 |           1.3902 |
[32m[20221214 00:18:38 @agent_ppo2.py:185][0m |          -0.0087 |          70.7666 |           1.3139 |
[32m[20221214 00:18:38 @agent_ppo2.py:185][0m |          -0.0053 |          78.5710 |           1.2893 |
[32m[20221214 00:18:38 @agent_ppo2.py:185][0m |          -0.0152 |          69.1684 |           1.2838 |
[32m[20221214 00:18:38 @agent_ppo2.py:185][0m |          -0.0083 |          71.4447 |           1.2514 |
[32m[20221214 00:18:38 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:18:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 491.39
[32m[20221214 00:18:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 524.79
[32m[20221214 00:18:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 512.16
[32m[20221214 00:18:38 @agent_ppo2.py:143][0m Total time:      21.13 min
[32m[20221214 00:18:38 @agent_ppo2.py:145][0m 1945600 total steps have happened
[32m[20221214 00:18:38 @agent_ppo2.py:121][0m #------------------------ Iteration 4950 --------------------------#
[32m[20221214 00:18:38 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:18:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:18:39 @agent_ppo2.py:185][0m |           0.0031 |          55.6716 |           1.4207 |
[32m[20221214 00:18:39 @agent_ppo2.py:185][0m |          -0.0067 |          51.2295 |           1.4092 |
[32m[20221214 00:18:39 @agent_ppo2.py:185][0m |          -0.0058 |          49.6879 |           1.4871 |
[32m[20221214 00:18:39 @agent_ppo2.py:185][0m |          -0.0100 |          48.5650 |           1.4635 |
[32m[20221214 00:18:39 @agent_ppo2.py:185][0m |          -0.0064 |          48.3136 |           1.4997 |
[32m[20221214 00:18:39 @agent_ppo2.py:185][0m |          -0.0128 |          49.3421 |           1.5325 |
[32m[20221214 00:18:39 @agent_ppo2.py:185][0m |          -0.0134 |          46.7633 |           1.4955 |
[32m[20221214 00:18:39 @agent_ppo2.py:185][0m |          -0.0133 |          46.2644 |           1.5872 |
[32m[20221214 00:18:39 @agent_ppo2.py:185][0m |          -0.0140 |          46.6547 |           1.5572 |
[32m[20221214 00:18:39 @agent_ppo2.py:185][0m |          -0.0161 |          45.5127 |           1.6111 |
[32m[20221214 00:18:39 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:18:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 484.26
[32m[20221214 00:18:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 579.84
[32m[20221214 00:18:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 454.90
[32m[20221214 00:18:40 @agent_ppo2.py:143][0m Total time:      21.15 min
[32m[20221214 00:18:40 @agent_ppo2.py:145][0m 1947648 total steps have happened
[32m[20221214 00:18:40 @agent_ppo2.py:121][0m #------------------------ Iteration 4951 --------------------------#
[32m[20221214 00:18:40 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:18:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:18:40 @agent_ppo2.py:185][0m |           0.0102 |          85.7698 |           1.7284 |
[32m[20221214 00:18:40 @agent_ppo2.py:185][0m |          -0.0070 |          76.0410 |           1.7440 |
[32m[20221214 00:18:40 @agent_ppo2.py:185][0m |          -0.0098 |          74.1111 |           1.7637 |
[32m[20221214 00:18:40 @agent_ppo2.py:185][0m |          -0.0122 |          72.2403 |           1.8335 |
[32m[20221214 00:18:40 @agent_ppo2.py:185][0m |          -0.0153 |          69.6650 |           1.8435 |
[32m[20221214 00:18:40 @agent_ppo2.py:185][0m |          -0.0151 |          69.4358 |           1.8470 |
[32m[20221214 00:18:41 @agent_ppo2.py:185][0m |          -0.0173 |          67.5160 |           1.8237 |
[32m[20221214 00:18:41 @agent_ppo2.py:185][0m |          -0.0155 |          66.9814 |           1.8108 |
[32m[20221214 00:18:41 @agent_ppo2.py:185][0m |          -0.0190 |          66.5158 |           1.8786 |
[32m[20221214 00:18:41 @agent_ppo2.py:185][0m |          -0.0205 |          66.1761 |           1.8731 |
[32m[20221214 00:18:41 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:18:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 491.27
[32m[20221214 00:18:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 521.00
[32m[20221214 00:18:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 624.03
[32m[20221214 00:18:41 @agent_ppo2.py:143][0m Total time:      21.18 min
[32m[20221214 00:18:41 @agent_ppo2.py:145][0m 1949696 total steps have happened
[32m[20221214 00:18:41 @agent_ppo2.py:121][0m #------------------------ Iteration 4952 --------------------------#
[32m[20221214 00:18:41 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:18:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:18:41 @agent_ppo2.py:185][0m |          -0.0003 |          89.2753 |           1.0181 |
[32m[20221214 00:18:41 @agent_ppo2.py:185][0m |          -0.0091 |          81.1865 |           1.1826 |
[32m[20221214 00:18:42 @agent_ppo2.py:185][0m |          -0.0119 |          77.7936 |           1.1680 |
[32m[20221214 00:18:42 @agent_ppo2.py:185][0m |          -0.0095 |          75.3891 |           1.1860 |
[32m[20221214 00:18:42 @agent_ppo2.py:185][0m |          -0.0124 |          73.9376 |           1.1898 |
[32m[20221214 00:18:42 @agent_ppo2.py:185][0m |          -0.0180 |          72.8442 |           1.3114 |
[32m[20221214 00:18:42 @agent_ppo2.py:185][0m |          -0.0165 |          71.9125 |           1.2688 |
[32m[20221214 00:18:42 @agent_ppo2.py:185][0m |          -0.0176 |          71.0902 |           1.3021 |
[32m[20221214 00:18:42 @agent_ppo2.py:185][0m |          -0.0100 |          75.6400 |           1.3176 |
[32m[20221214 00:18:42 @agent_ppo2.py:185][0m |          -0.0196 |          70.2510 |           1.3066 |
[32m[20221214 00:18:42 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:18:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 392.26
[32m[20221214 00:18:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 446.08
[32m[20221214 00:18:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 521.88
[32m[20221214 00:18:42 @agent_ppo2.py:143][0m Total time:      21.20 min
[32m[20221214 00:18:42 @agent_ppo2.py:145][0m 1951744 total steps have happened
[32m[20221214 00:18:42 @agent_ppo2.py:121][0m #------------------------ Iteration 4953 --------------------------#
[32m[20221214 00:18:43 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:18:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:18:43 @agent_ppo2.py:185][0m |           0.0006 |          68.3558 |           0.7665 |
[32m[20221214 00:18:43 @agent_ppo2.py:185][0m |          -0.0069 |          63.2814 |           0.8035 |
[32m[20221214 00:18:43 @agent_ppo2.py:185][0m |           0.0039 |          65.5335 |           0.8214 |
[32m[20221214 00:18:43 @agent_ppo2.py:185][0m |          -0.0057 |          61.3150 |           0.9379 |
[32m[20221214 00:18:43 @agent_ppo2.py:185][0m |          -0.0110 |          59.3833 |           0.9616 |
[32m[20221214 00:18:43 @agent_ppo2.py:185][0m |          -0.0077 |          59.6313 |           0.8801 |
[32m[20221214 00:18:43 @agent_ppo2.py:185][0m |          -0.0193 |          58.3583 |           0.9341 |
[32m[20221214 00:18:43 @agent_ppo2.py:185][0m |          -0.0161 |          57.9801 |           0.9439 |
[32m[20221214 00:18:44 @agent_ppo2.py:185][0m |          -0.0168 |          57.6316 |           0.8753 |
[32m[20221214 00:18:44 @agent_ppo2.py:185][0m |          -0.0188 |          57.2297 |           0.9720 |
[32m[20221214 00:18:44 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:18:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 438.95
[32m[20221214 00:18:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 516.78
[32m[20221214 00:18:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 510.86
[32m[20221214 00:18:44 @agent_ppo2.py:143][0m Total time:      21.22 min
[32m[20221214 00:18:44 @agent_ppo2.py:145][0m 1953792 total steps have happened
[32m[20221214 00:18:44 @agent_ppo2.py:121][0m #------------------------ Iteration 4954 --------------------------#
[32m[20221214 00:18:44 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:18:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:18:44 @agent_ppo2.py:185][0m |           0.0015 |          66.1384 |           1.5908 |
[32m[20221214 00:18:44 @agent_ppo2.py:185][0m |          -0.0053 |          53.0131 |           1.6487 |
[32m[20221214 00:18:44 @agent_ppo2.py:185][0m |          -0.0096 |          49.3644 |           1.5531 |
[32m[20221214 00:18:44 @agent_ppo2.py:185][0m |          -0.0137 |          46.9112 |           1.5914 |
[32m[20221214 00:18:45 @agent_ppo2.py:185][0m |          -0.0141 |          45.1134 |           1.5515 |
[32m[20221214 00:18:45 @agent_ppo2.py:185][0m |          -0.0130 |          44.0283 |           1.5384 |
[32m[20221214 00:18:45 @agent_ppo2.py:185][0m |          -0.0050 |          43.5848 |           1.5934 |
[32m[20221214 00:18:45 @agent_ppo2.py:185][0m |          -0.0140 |          40.1351 |           1.4902 |
[32m[20221214 00:18:45 @agent_ppo2.py:185][0m |          -0.0143 |          39.0521 |           1.5002 |
[32m[20221214 00:18:45 @agent_ppo2.py:185][0m |          -0.0190 |          38.0356 |           1.4530 |
[32m[20221214 00:18:45 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:18:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 523.44
[32m[20221214 00:18:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 587.09
[32m[20221214 00:18:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 481.19
[32m[20221214 00:18:45 @agent_ppo2.py:143][0m Total time:      21.25 min
[32m[20221214 00:18:45 @agent_ppo2.py:145][0m 1955840 total steps have happened
[32m[20221214 00:18:45 @agent_ppo2.py:121][0m #------------------------ Iteration 4955 --------------------------#
[32m[20221214 00:18:45 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:18:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:18:46 @agent_ppo2.py:185][0m |           0.0003 |          83.3276 |           0.7910 |
[32m[20221214 00:18:46 @agent_ppo2.py:185][0m |          -0.0036 |          73.9454 |           0.6787 |
[32m[20221214 00:18:46 @agent_ppo2.py:185][0m |          -0.0142 |          69.8933 |           0.7405 |
[32m[20221214 00:18:46 @agent_ppo2.py:185][0m |          -0.0118 |          68.6692 |           0.7689 |
[32m[20221214 00:18:46 @agent_ppo2.py:185][0m |          -0.0167 |          67.0284 |           0.7240 |
[32m[20221214 00:18:46 @agent_ppo2.py:185][0m |          -0.0129 |          66.2935 |           0.7793 |
[32m[20221214 00:18:46 @agent_ppo2.py:185][0m |          -0.0168 |          65.6779 |           0.6910 |
[32m[20221214 00:18:46 @agent_ppo2.py:185][0m |          -0.0177 |          65.0851 |           0.6352 |
[32m[20221214 00:18:46 @agent_ppo2.py:185][0m |          -0.0192 |          64.4167 |           0.6489 |
[32m[20221214 00:18:46 @agent_ppo2.py:185][0m |          -0.0215 |          63.9884 |           0.6859 |
[32m[20221214 00:18:46 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:18:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 459.22
[32m[20221214 00:18:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 522.54
[32m[20221214 00:18:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 481.35
[32m[20221214 00:18:47 @agent_ppo2.py:143][0m Total time:      21.27 min
[32m[20221214 00:18:47 @agent_ppo2.py:145][0m 1957888 total steps have happened
[32m[20221214 00:18:47 @agent_ppo2.py:121][0m #------------------------ Iteration 4956 --------------------------#
[32m[20221214 00:18:47 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:18:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:18:47 @agent_ppo2.py:185][0m |          -0.0006 |          74.5675 |           1.7094 |
[32m[20221214 00:18:47 @agent_ppo2.py:185][0m |          -0.0097 |          67.4290 |           1.8360 |
[32m[20221214 00:18:47 @agent_ppo2.py:185][0m |          -0.0021 |          65.4422 |           1.8941 |
[32m[20221214 00:18:47 @agent_ppo2.py:185][0m |          -0.0122 |          63.6820 |           1.8585 |
[32m[20221214 00:18:47 @agent_ppo2.py:185][0m |          -0.0190 |          63.0006 |           1.8969 |
[32m[20221214 00:18:47 @agent_ppo2.py:185][0m |          -0.0181 |          62.2506 |           1.9579 |
[32m[20221214 00:18:48 @agent_ppo2.py:185][0m |          -0.0188 |          61.7170 |           1.9692 |
[32m[20221214 00:18:48 @agent_ppo2.py:185][0m |          -0.0191 |          61.1339 |           1.9661 |
[32m[20221214 00:18:48 @agent_ppo2.py:185][0m |          -0.0181 |          60.7632 |           2.0160 |
[32m[20221214 00:18:48 @agent_ppo2.py:185][0m |          -0.0198 |          60.3121 |           2.0894 |
[32m[20221214 00:18:48 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:18:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 427.34
[32m[20221214 00:18:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 484.29
[32m[20221214 00:18:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 491.80
[32m[20221214 00:18:48 @agent_ppo2.py:143][0m Total time:      21.29 min
[32m[20221214 00:18:48 @agent_ppo2.py:145][0m 1959936 total steps have happened
[32m[20221214 00:18:48 @agent_ppo2.py:121][0m #------------------------ Iteration 4957 --------------------------#
[32m[20221214 00:18:48 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:18:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:18:48 @agent_ppo2.py:185][0m |          -0.0017 |          61.2749 |           0.9792 |
[32m[20221214 00:18:48 @agent_ppo2.py:185][0m |          -0.0061 |          56.2427 |           0.9601 |
[32m[20221214 00:18:49 @agent_ppo2.py:185][0m |          -0.0034 |          58.1267 |           0.9685 |
[32m[20221214 00:18:49 @agent_ppo2.py:185][0m |          -0.0062 |          57.4152 |           0.9881 |
[32m[20221214 00:18:49 @agent_ppo2.py:185][0m |          -0.0067 |          53.9167 |           1.0562 |
[32m[20221214 00:18:49 @agent_ppo2.py:185][0m |          -0.0177 |          52.1403 |           1.0518 |
[32m[20221214 00:18:49 @agent_ppo2.py:185][0m |          -0.0199 |          51.5693 |           1.0548 |
[32m[20221214 00:18:49 @agent_ppo2.py:185][0m |          -0.0189 |          51.0619 |           1.1003 |
[32m[20221214 00:18:49 @agent_ppo2.py:185][0m |          -0.0190 |          50.7022 |           1.0695 |
[32m[20221214 00:18:49 @agent_ppo2.py:185][0m |          -0.0193 |          50.4544 |           1.1660 |
[32m[20221214 00:18:49 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:18:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 435.16
[32m[20221214 00:18:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 552.28
[32m[20221214 00:18:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 550.99
[32m[20221214 00:18:49 @agent_ppo2.py:143][0m Total time:      21.32 min
[32m[20221214 00:18:49 @agent_ppo2.py:145][0m 1961984 total steps have happened
[32m[20221214 00:18:49 @agent_ppo2.py:121][0m #------------------------ Iteration 4958 --------------------------#
[32m[20221214 00:18:50 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:18:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:18:50 @agent_ppo2.py:185][0m |           0.0078 |          72.3442 |           1.6292 |
[32m[20221214 00:18:50 @agent_ppo2.py:185][0m |          -0.0012 |          62.3001 |           1.6240 |
[32m[20221214 00:18:50 @agent_ppo2.py:185][0m |          -0.0101 |          59.6739 |           1.5565 |
[32m[20221214 00:18:50 @agent_ppo2.py:185][0m |          -0.0028 |          58.9436 |           1.5722 |
[32m[20221214 00:18:50 @agent_ppo2.py:185][0m |          -0.0115 |          57.1226 |           1.4580 |
[32m[20221214 00:18:50 @agent_ppo2.py:185][0m |          -0.0009 |          60.3003 |           1.4896 |
[32m[20221214 00:18:50 @agent_ppo2.py:185][0m |          -0.0147 |          55.6927 |           1.4732 |
[32m[20221214 00:18:50 @agent_ppo2.py:185][0m |          -0.0135 |          55.0110 |           1.4301 |
[32m[20221214 00:18:51 @agent_ppo2.py:185][0m |          -0.0152 |          54.5292 |           1.4591 |
[32m[20221214 00:18:51 @agent_ppo2.py:185][0m |          -0.0149 |          54.1172 |           1.3980 |
[32m[20221214 00:18:51 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:18:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 459.89
[32m[20221214 00:18:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 530.29
[32m[20221214 00:18:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 496.08
[32m[20221214 00:18:51 @agent_ppo2.py:143][0m Total time:      21.34 min
[32m[20221214 00:18:51 @agent_ppo2.py:145][0m 1964032 total steps have happened
[32m[20221214 00:18:51 @agent_ppo2.py:121][0m #------------------------ Iteration 4959 --------------------------#
[32m[20221214 00:18:51 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:18:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:18:51 @agent_ppo2.py:185][0m |           0.0025 |          71.6873 |           0.8771 |
[32m[20221214 00:18:51 @agent_ppo2.py:185][0m |           0.0087 |          74.1305 |           0.7260 |
[32m[20221214 00:18:51 @agent_ppo2.py:185][0m |          -0.0105 |          66.0150 |           0.8734 |
[32m[20221214 00:18:51 @agent_ppo2.py:185][0m |          -0.0115 |          65.0136 |           0.8792 |
[32m[20221214 00:18:52 @agent_ppo2.py:185][0m |          -0.0111 |          64.5957 |           0.8511 |
[32m[20221214 00:18:52 @agent_ppo2.py:185][0m |          -0.0039 |          67.3536 |           0.8678 |
[32m[20221214 00:18:52 @agent_ppo2.py:185][0m |          -0.0127 |          64.2031 |           0.8326 |
[32m[20221214 00:18:52 @agent_ppo2.py:185][0m |          -0.0144 |          63.4217 |           0.9174 |
[32m[20221214 00:18:52 @agent_ppo2.py:185][0m |          -0.0162 |          63.2464 |           0.8709 |
[32m[20221214 00:18:52 @agent_ppo2.py:185][0m |          -0.0169 |          63.2067 |           0.8358 |
[32m[20221214 00:18:52 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:18:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 547.33
[32m[20221214 00:18:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 592.42
[32m[20221214 00:18:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 240.28
[32m[20221214 00:18:52 @agent_ppo2.py:143][0m Total time:      21.36 min
[32m[20221214 00:18:52 @agent_ppo2.py:145][0m 1966080 total steps have happened
[32m[20221214 00:18:52 @agent_ppo2.py:121][0m #------------------------ Iteration 4960 --------------------------#
[32m[20221214 00:18:52 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:18:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:18:53 @agent_ppo2.py:185][0m |          -0.0002 |          65.4895 |           0.4801 |
[32m[20221214 00:18:53 @agent_ppo2.py:185][0m |          -0.0080 |          60.7444 |           0.5347 |
[32m[20221214 00:18:53 @agent_ppo2.py:185][0m |          -0.0098 |          59.5331 |           0.5254 |
[32m[20221214 00:18:53 @agent_ppo2.py:185][0m |          -0.0044 |          59.2177 |           0.5431 |
[32m[20221214 00:18:53 @agent_ppo2.py:185][0m |          -0.0112 |          57.7998 |           0.5265 |
[32m[20221214 00:18:53 @agent_ppo2.py:185][0m |          -0.0147 |          57.1797 |           0.5038 |
[32m[20221214 00:18:53 @agent_ppo2.py:185][0m |          -0.0167 |          56.6878 |           0.4378 |
[32m[20221214 00:18:53 @agent_ppo2.py:185][0m |          -0.0147 |          56.2369 |           0.4896 |
[32m[20221214 00:18:53 @agent_ppo2.py:185][0m |          -0.0175 |          55.7937 |           0.4334 |
[32m[20221214 00:18:53 @agent_ppo2.py:185][0m |          -0.0193 |          55.3929 |           0.4151 |
[32m[20221214 00:18:53 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:18:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 446.11
[32m[20221214 00:18:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 500.71
[32m[20221214 00:18:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 333.50
[32m[20221214 00:18:54 @agent_ppo2.py:143][0m Total time:      21.39 min
[32m[20221214 00:18:54 @agent_ppo2.py:145][0m 1968128 total steps have happened
[32m[20221214 00:18:54 @agent_ppo2.py:121][0m #------------------------ Iteration 4961 --------------------------#
[32m[20221214 00:18:54 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:18:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:18:54 @agent_ppo2.py:185][0m |          -0.0009 |          65.0793 |           0.6435 |
[32m[20221214 00:18:54 @agent_ppo2.py:185][0m |          -0.0087 |          58.7575 |           0.7433 |
[32m[20221214 00:18:54 @agent_ppo2.py:185][0m |          -0.0105 |          55.8256 |           0.7119 |
[32m[20221214 00:18:54 @agent_ppo2.py:185][0m |          -0.0126 |          54.0427 |           0.5901 |
[32m[20221214 00:18:54 @agent_ppo2.py:185][0m |          -0.0149 |          53.1486 |           0.6890 |
[32m[20221214 00:18:54 @agent_ppo2.py:185][0m |          -0.0059 |          56.0541 |           0.5849 |
[32m[20221214 00:18:55 @agent_ppo2.py:185][0m |          -0.0144 |          51.4992 |           0.6316 |
[32m[20221214 00:18:55 @agent_ppo2.py:185][0m |          -0.0143 |          50.7777 |           0.6174 |
[32m[20221214 00:18:55 @agent_ppo2.py:185][0m |          -0.0162 |          50.5677 |           0.6264 |
[32m[20221214 00:18:55 @agent_ppo2.py:185][0m |          -0.0157 |          49.8922 |           0.6433 |
[32m[20221214 00:18:55 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:18:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 477.62
[32m[20221214 00:18:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 537.59
[32m[20221214 00:18:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 463.38
[32m[20221214 00:18:55 @agent_ppo2.py:143][0m Total time:      21.41 min
[32m[20221214 00:18:55 @agent_ppo2.py:145][0m 1970176 total steps have happened
[32m[20221214 00:18:55 @agent_ppo2.py:121][0m #------------------------ Iteration 4962 --------------------------#
[32m[20221214 00:18:55 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:18:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:18:55 @agent_ppo2.py:185][0m |           0.0043 |          60.2000 |           0.7792 |
[32m[20221214 00:18:55 @agent_ppo2.py:185][0m |          -0.0033 |          52.8861 |           0.7138 |
[32m[20221214 00:18:55 @agent_ppo2.py:185][0m |          -0.0085 |          50.8542 |           0.7856 |
[32m[20221214 00:18:56 @agent_ppo2.py:185][0m |          -0.0088 |          49.4189 |           0.7327 |
[32m[20221214 00:18:56 @agent_ppo2.py:185][0m |          -0.0131 |          48.6539 |           0.6386 |
[32m[20221214 00:18:56 @agent_ppo2.py:185][0m |          -0.0136 |          47.9420 |           0.7285 |
[32m[20221214 00:18:56 @agent_ppo2.py:185][0m |          -0.0164 |          47.5258 |           0.7184 |
[32m[20221214 00:18:56 @agent_ppo2.py:185][0m |          -0.0194 |          46.9861 |           0.7190 |
[32m[20221214 00:18:56 @agent_ppo2.py:185][0m |          -0.0126 |          48.0419 |           0.7211 |
[32m[20221214 00:18:56 @agent_ppo2.py:185][0m |          -0.0183 |          46.4179 |           0.6621 |
[32m[20221214 00:18:56 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:18:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 480.00
[32m[20221214 00:18:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 570.72
[32m[20221214 00:18:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 515.82
[32m[20221214 00:18:56 @agent_ppo2.py:143][0m Total time:      21.43 min
[32m[20221214 00:18:56 @agent_ppo2.py:145][0m 1972224 total steps have happened
[32m[20221214 00:18:56 @agent_ppo2.py:121][0m #------------------------ Iteration 4963 --------------------------#
[32m[20221214 00:18:56 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:18:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:18:57 @agent_ppo2.py:185][0m |           0.0009 |          67.0711 |          -0.1954 |
[32m[20221214 00:18:57 @agent_ppo2.py:185][0m |          -0.0078 |          57.0952 |          -0.1055 |
[32m[20221214 00:18:57 @agent_ppo2.py:185][0m |          -0.0103 |          53.2753 |          -0.0411 |
[32m[20221214 00:18:57 @agent_ppo2.py:185][0m |          -0.0105 |          51.5596 |           0.0217 |
[32m[20221214 00:18:57 @agent_ppo2.py:185][0m |          -0.0130 |          50.2006 |           0.0492 |
[32m[20221214 00:18:57 @agent_ppo2.py:185][0m |          -0.0160 |          48.8851 |           0.0620 |
[32m[20221214 00:18:57 @agent_ppo2.py:185][0m |          -0.0102 |          48.3098 |           0.1787 |
[32m[20221214 00:18:57 @agent_ppo2.py:185][0m |          -0.0198 |          47.9420 |           0.1700 |
[32m[20221214 00:18:57 @agent_ppo2.py:185][0m |          -0.0126 |          46.7953 |           0.2572 |
[32m[20221214 00:18:58 @agent_ppo2.py:185][0m |          -0.0170 |          46.1539 |           0.1577 |
[32m[20221214 00:18:58 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:18:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 482.51
[32m[20221214 00:18:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 557.70
[32m[20221214 00:18:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 395.73
[32m[20221214 00:18:58 @agent_ppo2.py:143][0m Total time:      21.46 min
[32m[20221214 00:18:58 @agent_ppo2.py:145][0m 1974272 total steps have happened
[32m[20221214 00:18:58 @agent_ppo2.py:121][0m #------------------------ Iteration 4964 --------------------------#
[32m[20221214 00:18:58 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:18:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:18:58 @agent_ppo2.py:185][0m |          -0.0016 |          71.8603 |           0.5612 |
[32m[20221214 00:18:58 @agent_ppo2.py:185][0m |          -0.0076 |          62.2332 |           0.5953 |
[32m[20221214 00:18:58 @agent_ppo2.py:185][0m |           0.0089 |          67.5193 |           0.5036 |
[32m[20221214 00:18:58 @agent_ppo2.py:185][0m |          -0.0121 |          57.3852 |           0.5844 |
[32m[20221214 00:18:58 @agent_ppo2.py:185][0m |          -0.0066 |          56.8894 |           0.5570 |
[32m[20221214 00:18:59 @agent_ppo2.py:185][0m |           0.0000 |          67.3178 |           0.5720 |
[32m[20221214 00:18:59 @agent_ppo2.py:185][0m |          -0.0109 |          55.8160 |           0.7109 |
[32m[20221214 00:18:59 @agent_ppo2.py:185][0m |          -0.0127 |          54.8675 |           0.5442 |
[32m[20221214 00:18:59 @agent_ppo2.py:185][0m |          -0.0154 |          54.5889 |           0.5785 |
[32m[20221214 00:18:59 @agent_ppo2.py:185][0m |          -0.0054 |          63.2440 |           0.6610 |
[32m[20221214 00:18:59 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:18:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 532.11
[32m[20221214 00:18:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 581.91
[32m[20221214 00:18:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 454.62
[32m[20221214 00:18:59 @agent_ppo2.py:143][0m Total time:      21.48 min
[32m[20221214 00:18:59 @agent_ppo2.py:145][0m 1976320 total steps have happened
[32m[20221214 00:18:59 @agent_ppo2.py:121][0m #------------------------ Iteration 4965 --------------------------#
[32m[20221214 00:18:59 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:18:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:18:59 @agent_ppo2.py:185][0m |           0.0007 |          82.0538 |           0.7978 |
[32m[20221214 00:19:00 @agent_ppo2.py:185][0m |           0.0080 |          80.4433 |           0.8955 |
[32m[20221214 00:19:00 @agent_ppo2.py:185][0m |          -0.0050 |          67.6369 |           0.9003 |
[32m[20221214 00:19:00 @agent_ppo2.py:185][0m |          -0.0034 |          65.7772 |           0.9416 |
[32m[20221214 00:19:00 @agent_ppo2.py:185][0m |          -0.0122 |          64.3287 |           0.9618 |
[32m[20221214 00:19:00 @agent_ppo2.py:185][0m |          -0.0147 |          63.1281 |           1.0224 |
[32m[20221214 00:19:00 @agent_ppo2.py:185][0m |          -0.0115 |          62.3325 |           0.9962 |
[32m[20221214 00:19:00 @agent_ppo2.py:185][0m |          -0.0167 |          61.8567 |           0.9572 |
[32m[20221214 00:19:00 @agent_ppo2.py:185][0m |          -0.0137 |          60.8459 |           0.9677 |
[32m[20221214 00:19:00 @agent_ppo2.py:185][0m |          -0.0173 |          60.5537 |           0.9788 |
[32m[20221214 00:19:00 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:19:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 552.66
[32m[20221214 00:19:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 625.06
[32m[20221214 00:19:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 522.39
[32m[20221214 00:19:01 @agent_ppo2.py:143][0m Total time:      21.50 min
[32m[20221214 00:19:01 @agent_ppo2.py:145][0m 1978368 total steps have happened
[32m[20221214 00:19:01 @agent_ppo2.py:121][0m #------------------------ Iteration 4966 --------------------------#
[32m[20221214 00:19:01 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:19:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:19:01 @agent_ppo2.py:185][0m |           0.0000 |          75.6211 |           0.8874 |
[32m[20221214 00:19:01 @agent_ppo2.py:185][0m |          -0.0074 |          65.0047 |           0.8533 |
[32m[20221214 00:19:01 @agent_ppo2.py:185][0m |          -0.0048 |          63.0492 |           0.9074 |
[32m[20221214 00:19:01 @agent_ppo2.py:185][0m |          -0.0155 |          61.8005 |           0.8737 |
[32m[20221214 00:19:01 @agent_ppo2.py:185][0m |          -0.0113 |          61.1254 |           0.9026 |
[32m[20221214 00:19:01 @agent_ppo2.py:185][0m |          -0.0168 |          60.3028 |           0.9059 |
[32m[20221214 00:19:01 @agent_ppo2.py:185][0m |          -0.0117 |          61.4804 |           0.8638 |
[32m[20221214 00:19:02 @agent_ppo2.py:185][0m |          -0.0165 |          59.5283 |           0.8260 |
[32m[20221214 00:19:02 @agent_ppo2.py:185][0m |          -0.0154 |          59.0258 |           0.7715 |
[32m[20221214 00:19:02 @agent_ppo2.py:185][0m |          -0.0191 |          58.2587 |           0.8562 |
[32m[20221214 00:19:02 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:19:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 474.84
[32m[20221214 00:19:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 536.83
[32m[20221214 00:19:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 545.59
[32m[20221214 00:19:02 @agent_ppo2.py:143][0m Total time:      21.53 min
[32m[20221214 00:19:02 @agent_ppo2.py:145][0m 1980416 total steps have happened
[32m[20221214 00:19:02 @agent_ppo2.py:121][0m #------------------------ Iteration 4967 --------------------------#
[32m[20221214 00:19:02 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:19:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:19:02 @agent_ppo2.py:185][0m |           0.0009 |          57.1552 |           0.1397 |
[32m[20221214 00:19:02 @agent_ppo2.py:185][0m |          -0.0076 |          49.6473 |           0.1325 |
[32m[20221214 00:19:02 @agent_ppo2.py:185][0m |          -0.0134 |          47.4167 |           0.1516 |
[32m[20221214 00:19:03 @agent_ppo2.py:185][0m |          -0.0137 |          46.2791 |           0.1841 |
[32m[20221214 00:19:03 @agent_ppo2.py:185][0m |          -0.0136 |          45.8135 |           0.1720 |
[32m[20221214 00:19:03 @agent_ppo2.py:185][0m |          -0.0070 |          49.1012 |           0.1745 |
[32m[20221214 00:19:03 @agent_ppo2.py:185][0m |          -0.0150 |          44.2312 |           0.2034 |
[32m[20221214 00:19:03 @agent_ppo2.py:185][0m |          -0.0196 |          43.9043 |           0.2526 |
[32m[20221214 00:19:03 @agent_ppo2.py:185][0m |          -0.0199 |          43.3884 |           0.2490 |
[32m[20221214 00:19:03 @agent_ppo2.py:185][0m |          -0.0178 |          43.2232 |           0.2252 |
[32m[20221214 00:19:03 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:19:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 496.16
[32m[20221214 00:19:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 593.44
[32m[20221214 00:19:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 634.37
[32m[20221214 00:19:03 @agent_ppo2.py:143][0m Total time:      21.55 min
[32m[20221214 00:19:03 @agent_ppo2.py:145][0m 1982464 total steps have happened
[32m[20221214 00:19:03 @agent_ppo2.py:121][0m #------------------------ Iteration 4968 --------------------------#
[32m[20221214 00:19:03 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:19:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:19:04 @agent_ppo2.py:185][0m |          -0.0014 |          79.6320 |          -0.3342 |
[32m[20221214 00:19:04 @agent_ppo2.py:185][0m |          -0.0043 |          69.7201 |          -0.3812 |
[32m[20221214 00:19:04 @agent_ppo2.py:185][0m |           0.0017 |          66.6828 |          -0.4073 |
[32m[20221214 00:19:04 @agent_ppo2.py:185][0m |          -0.0067 |          64.2707 |          -0.4022 |
[32m[20221214 00:19:04 @agent_ppo2.py:185][0m |          -0.0097 |          62.2535 |          -0.3812 |
[32m[20221214 00:19:04 @agent_ppo2.py:185][0m |          -0.0090 |          60.6455 |          -0.3280 |
[32m[20221214 00:19:04 @agent_ppo2.py:185][0m |          -0.0089 |          59.4685 |          -0.2377 |
[32m[20221214 00:19:04 @agent_ppo2.py:185][0m |          -0.0129 |          58.2264 |          -0.3753 |
[32m[20221214 00:19:04 @agent_ppo2.py:185][0m |          -0.0119 |          57.1430 |          -0.1589 |
[32m[20221214 00:19:05 @agent_ppo2.py:185][0m |          -0.0138 |          56.2444 |          -0.1996 |
[32m[20221214 00:19:05 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:19:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 544.66
[32m[20221214 00:19:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 628.78
[32m[20221214 00:19:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 561.75
[32m[20221214 00:19:05 @agent_ppo2.py:143][0m Total time:      21.57 min
[32m[20221214 00:19:05 @agent_ppo2.py:145][0m 1984512 total steps have happened
[32m[20221214 00:19:05 @agent_ppo2.py:121][0m #------------------------ Iteration 4969 --------------------------#
[32m[20221214 00:19:05 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:19:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:19:05 @agent_ppo2.py:185][0m |           0.0069 |          57.4033 |           1.1646 |
[32m[20221214 00:19:05 @agent_ppo2.py:185][0m |           0.0003 |          49.1683 |           1.1737 |
[32m[20221214 00:19:05 @agent_ppo2.py:185][0m |          -0.0045 |          46.4965 |           1.1787 |
[32m[20221214 00:19:05 @agent_ppo2.py:185][0m |          -0.0108 |          45.0286 |           1.1161 |
[32m[20221214 00:19:05 @agent_ppo2.py:185][0m |          -0.0158 |          44.0992 |           1.1255 |
[32m[20221214 00:19:06 @agent_ppo2.py:185][0m |          -0.0215 |          43.7772 |           1.1450 |
[32m[20221214 00:19:06 @agent_ppo2.py:185][0m |          -0.0111 |          42.8310 |           1.1763 |
[32m[20221214 00:19:06 @agent_ppo2.py:185][0m |          -0.0125 |          42.2742 |           1.1810 |
[32m[20221214 00:19:06 @agent_ppo2.py:185][0m |          -0.0202 |          41.9678 |           1.1662 |
[32m[20221214 00:19:06 @agent_ppo2.py:185][0m |          -0.0182 |          41.6106 |           1.1701 |
[32m[20221214 00:19:06 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:19:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 450.56
[32m[20221214 00:19:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 565.27
[32m[20221214 00:19:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 491.33
[32m[20221214 00:19:06 @agent_ppo2.py:143][0m Total time:      21.60 min
[32m[20221214 00:19:06 @agent_ppo2.py:145][0m 1986560 total steps have happened
[32m[20221214 00:19:06 @agent_ppo2.py:121][0m #------------------------ Iteration 4970 --------------------------#
[32m[20221214 00:19:06 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:19:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:19:06 @agent_ppo2.py:185][0m |           0.0022 |          90.6108 |           0.7148 |
[32m[20221214 00:19:07 @agent_ppo2.py:185][0m |          -0.0049 |          78.6897 |           0.7343 |
[32m[20221214 00:19:07 @agent_ppo2.py:185][0m |           0.0015 |          81.5615 |           0.7077 |
[32m[20221214 00:19:07 @agent_ppo2.py:185][0m |          -0.0121 |          71.7821 |           0.7616 |
[32m[20221214 00:19:07 @agent_ppo2.py:185][0m |          -0.0165 |          69.6669 |           0.8080 |
[32m[20221214 00:19:07 @agent_ppo2.py:185][0m |          -0.0141 |          67.6556 |           0.7707 |
[32m[20221214 00:19:07 @agent_ppo2.py:185][0m |          -0.0159 |          66.2469 |           0.7299 |
[32m[20221214 00:19:07 @agent_ppo2.py:185][0m |          -0.0163 |          65.4456 |           0.7430 |
[32m[20221214 00:19:07 @agent_ppo2.py:185][0m |          -0.0152 |          64.7704 |           0.7042 |
[32m[20221214 00:19:07 @agent_ppo2.py:185][0m |          -0.0182 |          63.5058 |           0.7404 |
[32m[20221214 00:19:07 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:19:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 339.02
[32m[20221214 00:19:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 517.44
[32m[20221214 00:19:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 358.17
[32m[20221214 00:19:08 @agent_ppo2.py:143][0m Total time:      21.62 min
[32m[20221214 00:19:08 @agent_ppo2.py:145][0m 1988608 total steps have happened
[32m[20221214 00:19:08 @agent_ppo2.py:121][0m #------------------------ Iteration 4971 --------------------------#
[32m[20221214 00:19:08 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:19:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:19:08 @agent_ppo2.py:185][0m |           0.0105 |          96.4530 |           0.5579 |
[32m[20221214 00:19:08 @agent_ppo2.py:185][0m |          -0.0023 |          89.5817 |           0.5415 |
[32m[20221214 00:19:08 @agent_ppo2.py:185][0m |          -0.0051 |          87.7182 |           0.6265 |
[32m[20221214 00:19:08 @agent_ppo2.py:185][0m |          -0.0064 |          86.4460 |           0.5751 |
[32m[20221214 00:19:08 @agent_ppo2.py:185][0m |          -0.0039 |          87.8892 |           0.5275 |
[32m[20221214 00:19:08 @agent_ppo2.py:185][0m |          -0.0096 |          85.2406 |           0.5728 |
[32m[20221214 00:19:08 @agent_ppo2.py:185][0m |          -0.0102 |          84.4926 |           0.4696 |
[32m[20221214 00:19:09 @agent_ppo2.py:185][0m |          -0.0116 |          84.0412 |           0.5035 |
[32m[20221214 00:19:09 @agent_ppo2.py:185][0m |          -0.0115 |          83.6742 |           0.4754 |
[32m[20221214 00:19:09 @agent_ppo2.py:185][0m |          -0.0114 |          83.2542 |           0.4795 |
[32m[20221214 00:19:09 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:19:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 547.14
[32m[20221214 00:19:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 565.72
[32m[20221214 00:19:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 603.85
[32m[20221214 00:19:09 @agent_ppo2.py:143][0m Total time:      21.64 min
[32m[20221214 00:19:09 @agent_ppo2.py:145][0m 1990656 total steps have happened
[32m[20221214 00:19:09 @agent_ppo2.py:121][0m #------------------------ Iteration 4972 --------------------------#
[32m[20221214 00:19:09 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:19:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:19:09 @agent_ppo2.py:185][0m |           0.0060 |          80.1680 |           0.4057 |
[32m[20221214 00:19:09 @agent_ppo2.py:185][0m |          -0.0019 |          74.6144 |           0.4446 |
[32m[20221214 00:19:09 @agent_ppo2.py:185][0m |          -0.0102 |          72.6860 |           0.3371 |
[32m[20221214 00:19:10 @agent_ppo2.py:185][0m |          -0.0102 |          72.0060 |           0.5440 |
[32m[20221214 00:19:10 @agent_ppo2.py:185][0m |          -0.0081 |          71.0268 |           0.5407 |
[32m[20221214 00:19:10 @agent_ppo2.py:185][0m |          -0.0120 |          70.5885 |           0.4511 |
[32m[20221214 00:19:10 @agent_ppo2.py:185][0m |          -0.0095 |          70.0262 |           0.5230 |
[32m[20221214 00:19:10 @agent_ppo2.py:185][0m |          -0.0121 |          69.7512 |           0.5158 |
[32m[20221214 00:19:10 @agent_ppo2.py:185][0m |          -0.0111 |          69.4255 |           0.5389 |
[32m[20221214 00:19:10 @agent_ppo2.py:185][0m |          -0.0126 |          68.9909 |           0.4923 |
[32m[20221214 00:19:10 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:19:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 570.81
[32m[20221214 00:19:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 614.73
[32m[20221214 00:19:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 582.88
[32m[20221214 00:19:10 @agent_ppo2.py:143][0m Total time:      21.66 min
[32m[20221214 00:19:10 @agent_ppo2.py:145][0m 1992704 total steps have happened
[32m[20221214 00:19:10 @agent_ppo2.py:121][0m #------------------------ Iteration 4973 --------------------------#
[32m[20221214 00:19:10 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:19:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:19:11 @agent_ppo2.py:185][0m |          -0.0004 |          59.6188 |           0.5077 |
[32m[20221214 00:19:11 @agent_ppo2.py:185][0m |          -0.0059 |          43.7648 |           0.5685 |
[32m[20221214 00:19:11 @agent_ppo2.py:185][0m |           0.0003 |          41.8222 |           0.6909 |
[32m[20221214 00:19:11 @agent_ppo2.py:185][0m |          -0.0055 |          40.3242 |           0.6916 |
[32m[20221214 00:19:11 @agent_ppo2.py:185][0m |          -0.0105 |          39.4712 |           0.7277 |
[32m[20221214 00:19:11 @agent_ppo2.py:185][0m |          -0.0144 |          38.9590 |           0.7246 |
[32m[20221214 00:19:11 @agent_ppo2.py:185][0m |          -0.0154 |          38.3796 |           0.8266 |
[32m[20221214 00:19:11 @agent_ppo2.py:185][0m |          -0.0063 |          41.2581 |           0.7933 |
[32m[20221214 00:19:11 @agent_ppo2.py:185][0m |          -0.0138 |          39.5266 |           0.7657 |
[32m[20221214 00:19:11 @agent_ppo2.py:185][0m |          -0.0152 |          37.3847 |           0.8493 |
[32m[20221214 00:19:11 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:19:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 476.51
[32m[20221214 00:19:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 538.45
[32m[20221214 00:19:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 542.48
[32m[20221214 00:19:12 @agent_ppo2.py:143][0m Total time:      21.69 min
[32m[20221214 00:19:12 @agent_ppo2.py:145][0m 1994752 total steps have happened
[32m[20221214 00:19:12 @agent_ppo2.py:121][0m #------------------------ Iteration 4974 --------------------------#
[32m[20221214 00:19:12 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:19:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:19:12 @agent_ppo2.py:185][0m |          -0.0013 |          86.0055 |           0.3424 |
[32m[20221214 00:19:12 @agent_ppo2.py:185][0m |          -0.0064 |          80.9594 |           0.3800 |
[32m[20221214 00:19:12 @agent_ppo2.py:185][0m |          -0.0102 |          79.4123 |           0.3181 |
[32m[20221214 00:19:12 @agent_ppo2.py:185][0m |          -0.0104 |          78.2000 |           0.4630 |
[32m[20221214 00:19:12 @agent_ppo2.py:185][0m |          -0.0127 |          76.9689 |           0.4946 |
[32m[20221214 00:19:13 @agent_ppo2.py:185][0m |          -0.0158 |          76.6828 |           0.4857 |
[32m[20221214 00:19:13 @agent_ppo2.py:185][0m |          -0.0082 |          76.7079 |           0.4554 |
[32m[20221214 00:19:13 @agent_ppo2.py:185][0m |          -0.0123 |          75.6982 |           0.4802 |
[32m[20221214 00:19:13 @agent_ppo2.py:185][0m |          -0.0174 |          75.1560 |           0.4549 |
[32m[20221214 00:19:13 @agent_ppo2.py:185][0m |          -0.0213 |          74.8367 |           0.4995 |
[32m[20221214 00:19:13 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:19:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 515.95
[32m[20221214 00:19:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 587.36
[32m[20221214 00:19:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 629.99
[32m[20221214 00:19:13 @agent_ppo2.py:143][0m Total time:      21.71 min
[32m[20221214 00:19:13 @agent_ppo2.py:145][0m 1996800 total steps have happened
[32m[20221214 00:19:13 @agent_ppo2.py:121][0m #------------------------ Iteration 4975 --------------------------#
[32m[20221214 00:19:13 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:19:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:19:13 @agent_ppo2.py:185][0m |           0.0113 |          94.5987 |          -0.0154 |
[32m[20221214 00:19:13 @agent_ppo2.py:185][0m |          -0.0027 |          83.4092 |           0.1491 |
[32m[20221214 00:19:14 @agent_ppo2.py:185][0m |          -0.0092 |          80.5899 |           0.0893 |
[32m[20221214 00:19:14 @agent_ppo2.py:185][0m |          -0.0083 |          78.3621 |           0.0382 |
[32m[20221214 00:19:14 @agent_ppo2.py:185][0m |          -0.0100 |          77.3044 |           0.0176 |
[32m[20221214 00:19:14 @agent_ppo2.py:185][0m |          -0.0168 |          75.8899 |           0.0373 |
[32m[20221214 00:19:14 @agent_ppo2.py:185][0m |          -0.0083 |          74.5752 |           0.0911 |
[32m[20221214 00:19:14 @agent_ppo2.py:185][0m |          -0.0204 |          74.0419 |           0.1258 |
[32m[20221214 00:19:14 @agent_ppo2.py:185][0m |          -0.0136 |          73.1841 |           0.1456 |
[32m[20221214 00:19:14 @agent_ppo2.py:185][0m |          -0.0215 |          72.7791 |           0.0904 |
[32m[20221214 00:19:14 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:19:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 356.49
[32m[20221214 00:19:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 549.80
[32m[20221214 00:19:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 568.11
[32m[20221214 00:19:14 @agent_ppo2.py:143][0m Total time:      21.73 min
[32m[20221214 00:19:14 @agent_ppo2.py:145][0m 1998848 total steps have happened
[32m[20221214 00:19:14 @agent_ppo2.py:121][0m #------------------------ Iteration 4976 --------------------------#
[32m[20221214 00:19:15 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:19:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:19:15 @agent_ppo2.py:185][0m |          -0.0034 |          53.7368 |          -0.0377 |
[32m[20221214 00:19:15 @agent_ppo2.py:185][0m |          -0.0070 |          39.1492 |          -0.0603 |
[32m[20221214 00:19:15 @agent_ppo2.py:185][0m |          -0.0065 |          36.2313 |          -0.0102 |
[32m[20221214 00:19:15 @agent_ppo2.py:185][0m |          -0.0127 |          34.4038 |           0.0254 |
[32m[20221214 00:19:15 @agent_ppo2.py:185][0m |          -0.0048 |          33.0855 |          -0.0030 |
[32m[20221214 00:19:15 @agent_ppo2.py:185][0m |          -0.0141 |          32.0850 |          -0.0088 |
[32m[20221214 00:19:15 @agent_ppo2.py:185][0m |          -0.0143 |          31.3373 |           0.0496 |
[32m[20221214 00:19:15 @agent_ppo2.py:185][0m |          -0.0200 |          31.0120 |           0.0784 |
[32m[20221214 00:19:16 @agent_ppo2.py:185][0m |          -0.0234 |          30.3537 |           0.0516 |
[32m[20221214 00:19:16 @agent_ppo2.py:185][0m |          -0.0154 |          31.3934 |          -0.0077 |
[32m[20221214 00:19:16 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:19:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 436.27
[32m[20221214 00:19:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 524.28
[32m[20221214 00:19:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 579.72
[32m[20221214 00:19:16 @agent_ppo2.py:143][0m Total time:      21.76 min
[32m[20221214 00:19:16 @agent_ppo2.py:145][0m 2000896 total steps have happened
[32m[20221214 00:19:16 @agent_ppo2.py:121][0m #------------------------ Iteration 4977 --------------------------#
[32m[20221214 00:19:16 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:19:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:19:16 @agent_ppo2.py:185][0m |           0.0079 |          52.9593 |           0.2328 |
[32m[20221214 00:19:16 @agent_ppo2.py:185][0m |          -0.0044 |          44.1243 |           0.1924 |
[32m[20221214 00:19:16 @agent_ppo2.py:185][0m |          -0.0066 |          42.1582 |           0.3312 |
[32m[20221214 00:19:16 @agent_ppo2.py:185][0m |          -0.0112 |          40.8902 |           0.2509 |
[32m[20221214 00:19:17 @agent_ppo2.py:185][0m |          -0.0112 |          40.0255 |           0.2155 |
[32m[20221214 00:19:17 @agent_ppo2.py:185][0m |          -0.0089 |          39.0719 |           0.2054 |
[32m[20221214 00:19:17 @agent_ppo2.py:185][0m |          -0.0097 |          38.9082 |           0.0877 |
[32m[20221214 00:19:17 @agent_ppo2.py:185][0m |          -0.0152 |          38.1696 |           0.1152 |
[32m[20221214 00:19:17 @agent_ppo2.py:185][0m |          -0.0146 |          37.5297 |           0.0789 |
[32m[20221214 00:19:17 @agent_ppo2.py:185][0m |          -0.0127 |          37.1786 |           0.1742 |
[32m[20221214 00:19:17 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:19:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 456.15
[32m[20221214 00:19:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 478.20
[32m[20221214 00:19:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 561.39
[32m[20221214 00:19:17 @agent_ppo2.py:143][0m Total time:      21.78 min
[32m[20221214 00:19:17 @agent_ppo2.py:145][0m 2002944 total steps have happened
[32m[20221214 00:19:17 @agent_ppo2.py:121][0m #------------------------ Iteration 4978 --------------------------#
[32m[20221214 00:19:17 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:19:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:19:18 @agent_ppo2.py:185][0m |           0.0055 |          68.9702 |          -0.6227 |
[32m[20221214 00:19:18 @agent_ppo2.py:185][0m |          -0.0083 |          61.2382 |          -0.5631 |
[32m[20221214 00:19:18 @agent_ppo2.py:185][0m |          -0.0113 |          58.6114 |          -0.6504 |
[32m[20221214 00:19:18 @agent_ppo2.py:185][0m |          -0.0078 |          58.8362 |          -0.6616 |
[32m[20221214 00:19:18 @agent_ppo2.py:185][0m |          -0.0129 |          56.4755 |          -0.6640 |
[32m[20221214 00:19:18 @agent_ppo2.py:185][0m |          -0.0079 |          56.8440 |          -0.6833 |
[32m[20221214 00:19:18 @agent_ppo2.py:185][0m |          -0.0190 |          54.1854 |          -0.6848 |
[32m[20221214 00:19:18 @agent_ppo2.py:185][0m |          -0.0231 |          53.3885 |          -0.7145 |
[32m[20221214 00:19:18 @agent_ppo2.py:185][0m |          -0.0226 |          52.9657 |          -0.6827 |
[32m[20221214 00:19:18 @agent_ppo2.py:185][0m |          -0.0196 |          52.5315 |          -0.7068 |
[32m[20221214 00:19:18 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:19:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 446.37
[32m[20221214 00:19:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 554.30
[32m[20221214 00:19:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 545.14
[32m[20221214 00:19:19 @agent_ppo2.py:143][0m Total time:      21.80 min
[32m[20221214 00:19:19 @agent_ppo2.py:145][0m 2004992 total steps have happened
[32m[20221214 00:19:19 @agent_ppo2.py:121][0m #------------------------ Iteration 4979 --------------------------#
[32m[20221214 00:19:19 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:19:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:19:19 @agent_ppo2.py:185][0m |           0.0024 |          50.1734 |           0.2844 |
[32m[20221214 00:19:19 @agent_ppo2.py:185][0m |          -0.0133 |          46.7707 |           0.3532 |
[32m[20221214 00:19:19 @agent_ppo2.py:185][0m |          -0.0093 |          45.1712 |           0.4776 |
[32m[20221214 00:19:19 @agent_ppo2.py:185][0m |          -0.0115 |          44.3319 |           0.4935 |
[32m[20221214 00:19:19 @agent_ppo2.py:185][0m |          -0.0180 |          43.8409 |           0.5495 |
[32m[20221214 00:19:19 @agent_ppo2.py:185][0m |          -0.0117 |          43.2813 |           0.6630 |
[32m[20221214 00:19:20 @agent_ppo2.py:185][0m |          -0.0151 |          43.0763 |           0.6260 |
[32m[20221214 00:19:20 @agent_ppo2.py:185][0m |          -0.0123 |          43.9247 |           0.7787 |
[32m[20221214 00:19:20 @agent_ppo2.py:185][0m |          -0.0165 |          42.3319 |           0.7867 |
[32m[20221214 00:19:20 @agent_ppo2.py:185][0m |          -0.0207 |          42.0146 |           0.7772 |
[32m[20221214 00:19:20 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:19:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 443.91
[32m[20221214 00:19:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 481.79
[32m[20221214 00:19:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 640.40
[32m[20221214 00:19:20 @agent_ppo2.py:143][0m Total time:      21.83 min
[32m[20221214 00:19:20 @agent_ppo2.py:145][0m 2007040 total steps have happened
[32m[20221214 00:19:20 @agent_ppo2.py:121][0m #------------------------ Iteration 4980 --------------------------#
[32m[20221214 00:19:20 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:19:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:19:20 @agent_ppo2.py:185][0m |          -0.0020 |          58.6509 |          -0.6475 |
[32m[20221214 00:19:20 @agent_ppo2.py:185][0m |          -0.0050 |          52.7778 |          -0.5596 |
[32m[20221214 00:19:21 @agent_ppo2.py:185][0m |          -0.0119 |          50.8935 |          -0.5769 |
[32m[20221214 00:19:21 @agent_ppo2.py:185][0m |          -0.0136 |          50.1431 |          -0.6859 |
[32m[20221214 00:19:21 @agent_ppo2.py:185][0m |          -0.0135 |          49.3763 |          -0.6172 |
[32m[20221214 00:19:21 @agent_ppo2.py:185][0m |          -0.0201 |          48.8933 |          -0.5966 |
[32m[20221214 00:19:21 @agent_ppo2.py:185][0m |          -0.0192 |          48.2733 |          -0.5907 |
[32m[20221214 00:19:21 @agent_ppo2.py:185][0m |          -0.0175 |          47.7226 |          -0.6564 |
[32m[20221214 00:19:21 @agent_ppo2.py:185][0m |          -0.0178 |          47.2137 |          -0.6567 |
[32m[20221214 00:19:21 @agent_ppo2.py:185][0m |          -0.0223 |          47.1338 |          -0.6454 |
[32m[20221214 00:19:21 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:19:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 422.58
[32m[20221214 00:19:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 453.51
[32m[20221214 00:19:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 492.89
[32m[20221214 00:19:21 @agent_ppo2.py:143][0m Total time:      21.85 min
[32m[20221214 00:19:21 @agent_ppo2.py:145][0m 2009088 total steps have happened
[32m[20221214 00:19:21 @agent_ppo2.py:121][0m #------------------------ Iteration 4981 --------------------------#
[32m[20221214 00:19:22 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:19:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:19:22 @agent_ppo2.py:185][0m |           0.0006 |          45.6033 |           0.6348 |
[32m[20221214 00:19:22 @agent_ppo2.py:185][0m |          -0.0091 |          37.7297 |           0.6287 |
[32m[20221214 00:19:22 @agent_ppo2.py:185][0m |          -0.0110 |          35.2073 |           0.6158 |
[32m[20221214 00:19:22 @agent_ppo2.py:185][0m |          -0.0180 |          33.8426 |           0.6513 |
[32m[20221214 00:19:22 @agent_ppo2.py:185][0m |          -0.0189 |          32.2494 |           0.6632 |
[32m[20221214 00:19:22 @agent_ppo2.py:185][0m |          -0.0151 |          31.3033 |           0.7564 |
[32m[20221214 00:19:22 @agent_ppo2.py:185][0m |          -0.0248 |          30.7612 |           0.8411 |
[32m[20221214 00:19:22 @agent_ppo2.py:185][0m |          -0.0218 |          29.7962 |           0.8256 |
[32m[20221214 00:19:23 @agent_ppo2.py:185][0m |          -0.0218 |          29.0954 |           0.7869 |
[32m[20221214 00:19:23 @agent_ppo2.py:185][0m |          -0.0278 |          28.7507 |           0.8420 |
[32m[20221214 00:19:23 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:19:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 328.78
[32m[20221214 00:19:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 430.61
[32m[20221214 00:19:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 483.45
[32m[20221214 00:19:23 @agent_ppo2.py:143][0m Total time:      21.87 min
[32m[20221214 00:19:23 @agent_ppo2.py:145][0m 2011136 total steps have happened
[32m[20221214 00:19:23 @agent_ppo2.py:121][0m #------------------------ Iteration 4982 --------------------------#
[32m[20221214 00:19:23 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:19:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:19:23 @agent_ppo2.py:185][0m |           0.0026 |          44.6965 |           0.3444 |
[32m[20221214 00:19:23 @agent_ppo2.py:185][0m |          -0.0038 |          37.8346 |           0.3734 |
[32m[20221214 00:19:23 @agent_ppo2.py:185][0m |          -0.0082 |          35.5784 |           0.3455 |
[32m[20221214 00:19:23 @agent_ppo2.py:185][0m |          -0.0128 |          34.4942 |           0.3949 |
[32m[20221214 00:19:24 @agent_ppo2.py:185][0m |          -0.0113 |          33.7364 |           0.3286 |
[32m[20221214 00:19:24 @agent_ppo2.py:185][0m |          -0.0133 |          32.9214 |           0.4256 |
[32m[20221214 00:19:24 @agent_ppo2.py:185][0m |          -0.0131 |          32.4583 |           0.3549 |
[32m[20221214 00:19:24 @agent_ppo2.py:185][0m |          -0.0171 |          32.0239 |           0.3415 |
[32m[20221214 00:19:24 @agent_ppo2.py:185][0m |          -0.0201 |          31.4932 |           0.4257 |
[32m[20221214 00:19:24 @agent_ppo2.py:185][0m |          -0.0180 |          31.7130 |           0.3187 |
[32m[20221214 00:19:24 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:19:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 495.95
[32m[20221214 00:19:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 545.25
[32m[20221214 00:19:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 441.55
[32m[20221214 00:19:24 @agent_ppo2.py:143][0m Total time:      21.90 min
[32m[20221214 00:19:24 @agent_ppo2.py:145][0m 2013184 total steps have happened
[32m[20221214 00:19:24 @agent_ppo2.py:121][0m #------------------------ Iteration 4983 --------------------------#
[32m[20221214 00:19:24 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:19:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:19:25 @agent_ppo2.py:185][0m |           0.0018 |          67.8812 |           1.1237 |
[32m[20221214 00:19:25 @agent_ppo2.py:185][0m |          -0.0074 |          58.9405 |           1.2683 |
[32m[20221214 00:19:25 @agent_ppo2.py:185][0m |          -0.0091 |          55.0501 |           1.1282 |
[32m[20221214 00:19:25 @agent_ppo2.py:185][0m |          -0.0089 |          52.4600 |           1.0946 |
[32m[20221214 00:19:25 @agent_ppo2.py:185][0m |          -0.0105 |          52.0479 |           1.0750 |
[32m[20221214 00:19:25 @agent_ppo2.py:185][0m |          -0.0152 |          50.1927 |           1.1013 |
[32m[20221214 00:19:25 @agent_ppo2.py:185][0m |          -0.0147 |          48.4331 |           1.0653 |
[32m[20221214 00:19:25 @agent_ppo2.py:185][0m |          -0.0159 |          47.5517 |           1.1755 |
[32m[20221214 00:19:25 @agent_ppo2.py:185][0m |          -0.0172 |          46.7333 |           1.0826 |
[32m[20221214 00:19:25 @agent_ppo2.py:185][0m |          -0.0223 |          46.4359 |           1.0844 |
[32m[20221214 00:19:25 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:19:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 452.00
[32m[20221214 00:19:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 561.40
[32m[20221214 00:19:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 421.78
[32m[20221214 00:19:26 @agent_ppo2.py:143][0m Total time:      21.92 min
[32m[20221214 00:19:26 @agent_ppo2.py:145][0m 2015232 total steps have happened
[32m[20221214 00:19:26 @agent_ppo2.py:121][0m #------------------------ Iteration 4984 --------------------------#
[32m[20221214 00:19:26 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:19:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:19:26 @agent_ppo2.py:185][0m |           0.0009 |          60.4956 |           0.5011 |
[32m[20221214 00:19:26 @agent_ppo2.py:185][0m |          -0.0074 |          53.8042 |           0.5743 |
[32m[20221214 00:19:26 @agent_ppo2.py:185][0m |          -0.0105 |          51.1090 |           0.4694 |
[32m[20221214 00:19:26 @agent_ppo2.py:185][0m |          -0.0101 |          49.6070 |           0.3739 |
[32m[20221214 00:19:26 @agent_ppo2.py:185][0m |          -0.0177 |          48.6126 |           0.4490 |
[32m[20221214 00:19:27 @agent_ppo2.py:185][0m |          -0.0102 |          48.0864 |           0.3895 |
[32m[20221214 00:19:27 @agent_ppo2.py:185][0m |          -0.0146 |          46.8079 |           0.3483 |
[32m[20221214 00:19:27 @agent_ppo2.py:185][0m |          -0.0144 |          46.8097 |           0.3002 |
[32m[20221214 00:19:27 @agent_ppo2.py:185][0m |          -0.0193 |          45.8783 |           0.2634 |
[32m[20221214 00:19:27 @agent_ppo2.py:185][0m |          -0.0185 |          45.7701 |           0.2480 |
[32m[20221214 00:19:27 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221214 00:19:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 475.48
[32m[20221214 00:19:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 605.69
[32m[20221214 00:19:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 401.30
[32m[20221214 00:19:27 @agent_ppo2.py:143][0m Total time:      21.95 min
[32m[20221214 00:19:27 @agent_ppo2.py:145][0m 2017280 total steps have happened
[32m[20221214 00:19:27 @agent_ppo2.py:121][0m #------------------------ Iteration 4985 --------------------------#
[32m[20221214 00:19:27 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:19:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:19:27 @agent_ppo2.py:185][0m |           0.0071 |          62.4752 |          -0.6098 |
[32m[20221214 00:19:28 @agent_ppo2.py:185][0m |          -0.0013 |          56.8390 |          -0.5414 |
[32m[20221214 00:19:28 @agent_ppo2.py:185][0m |          -0.0056 |          53.8595 |          -0.5578 |
[32m[20221214 00:19:28 @agent_ppo2.py:185][0m |          -0.0137 |          51.5050 |          -0.5437 |
[32m[20221214 00:19:28 @agent_ppo2.py:185][0m |          -0.0109 |          50.4701 |          -0.5783 |
[32m[20221214 00:19:28 @agent_ppo2.py:185][0m |          -0.0157 |          48.8847 |          -0.6119 |
[32m[20221214 00:19:28 @agent_ppo2.py:185][0m |          -0.0174 |          48.2732 |          -0.6911 |
[32m[20221214 00:19:28 @agent_ppo2.py:185][0m |          -0.0170 |          47.2726 |          -0.5864 |
[32m[20221214 00:19:28 @agent_ppo2.py:185][0m |          -0.0195 |          46.3062 |          -0.7347 |
[32m[20221214 00:19:28 @agent_ppo2.py:185][0m |          -0.0145 |          46.7011 |          -0.6866 |
[32m[20221214 00:19:28 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:19:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 453.57
[32m[20221214 00:19:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 548.55
[32m[20221214 00:19:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 316.30
[32m[20221214 00:19:29 @agent_ppo2.py:143][0m Total time:      21.97 min
[32m[20221214 00:19:29 @agent_ppo2.py:145][0m 2019328 total steps have happened
[32m[20221214 00:19:29 @agent_ppo2.py:121][0m #------------------------ Iteration 4986 --------------------------#
[32m[20221214 00:19:29 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:19:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:19:29 @agent_ppo2.py:185][0m |           0.0152 |          41.2988 |          -1.7299 |
[32m[20221214 00:19:29 @agent_ppo2.py:185][0m |          -0.0007 |          30.4181 |          -1.7728 |
[32m[20221214 00:19:29 @agent_ppo2.py:185][0m |          -0.0082 |          27.9800 |          -1.8682 |
[32m[20221214 00:19:29 @agent_ppo2.py:185][0m |          -0.0126 |          26.3735 |          -1.9664 |
[32m[20221214 00:19:29 @agent_ppo2.py:185][0m |          -0.0094 |          25.4440 |          -1.9388 |
[32m[20221214 00:19:29 @agent_ppo2.py:185][0m |          -0.0105 |          24.4308 |          -2.0557 |
[32m[20221214 00:19:29 @agent_ppo2.py:185][0m |          -0.0126 |          23.8927 |          -2.1170 |
[32m[20221214 00:19:30 @agent_ppo2.py:185][0m |          -0.0208 |          23.3972 |          -2.1440 |
[32m[20221214 00:19:30 @agent_ppo2.py:185][0m |          -0.0178 |          22.7907 |          -2.2130 |
[32m[20221214 00:19:30 @agent_ppo2.py:185][0m |          -0.0197 |          22.5958 |          -2.2943 |
[32m[20221214 00:19:30 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:19:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 457.86
[32m[20221214 00:19:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 565.92
[32m[20221214 00:19:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 645.85
[32m[20221214 00:19:30 @agent_ppo2.py:143][0m Total time:      21.99 min
[32m[20221214 00:19:30 @agent_ppo2.py:145][0m 2021376 total steps have happened
[32m[20221214 00:19:30 @agent_ppo2.py:121][0m #------------------------ Iteration 4987 --------------------------#
[32m[20221214 00:19:30 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:19:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:19:30 @agent_ppo2.py:185][0m |          -0.0002 |          71.3606 |          -0.9918 |
[32m[20221214 00:19:30 @agent_ppo2.py:185][0m |          -0.0088 |          65.0005 |          -0.9724 |
[32m[20221214 00:19:30 @agent_ppo2.py:185][0m |          -0.0090 |          62.9268 |          -1.0822 |
[32m[20221214 00:19:31 @agent_ppo2.py:185][0m |          -0.0017 |          63.2883 |          -1.0134 |
[32m[20221214 00:19:31 @agent_ppo2.py:185][0m |          -0.0104 |          61.6570 |          -0.9142 |
[32m[20221214 00:19:31 @agent_ppo2.py:185][0m |          -0.0058 |          61.3692 |          -1.0596 |
[32m[20221214 00:19:31 @agent_ppo2.py:185][0m |          -0.0108 |          60.3446 |          -0.9721 |
[32m[20221214 00:19:31 @agent_ppo2.py:185][0m |          -0.0123 |          59.8110 |          -1.0737 |
[32m[20221214 00:19:31 @agent_ppo2.py:185][0m |          -0.0134 |          59.4258 |          -1.0525 |
[32m[20221214 00:19:31 @agent_ppo2.py:185][0m |          -0.0158 |          59.2123 |          -1.0689 |
[32m[20221214 00:19:31 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:19:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 557.76
[32m[20221214 00:19:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 648.63
[32m[20221214 00:19:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 555.85
[32m[20221214 00:19:31 @agent_ppo2.py:143][0m Total time:      22.02 min
[32m[20221214 00:19:31 @agent_ppo2.py:145][0m 2023424 total steps have happened
[32m[20221214 00:19:31 @agent_ppo2.py:121][0m #------------------------ Iteration 4988 --------------------------#
[32m[20221214 00:19:31 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:19:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:19:32 @agent_ppo2.py:185][0m |           0.0034 |          56.2287 |          -0.5218 |
[32m[20221214 00:19:32 @agent_ppo2.py:185][0m |          -0.0066 |          49.0422 |          -0.4309 |
[32m[20221214 00:19:32 @agent_ppo2.py:185][0m |          -0.0059 |          45.5303 |          -0.4738 |
[32m[20221214 00:19:32 @agent_ppo2.py:185][0m |          -0.0122 |          44.1583 |          -0.3797 |
[32m[20221214 00:19:32 @agent_ppo2.py:185][0m |          -0.0138 |          42.7222 |          -0.4578 |
[32m[20221214 00:19:32 @agent_ppo2.py:185][0m |          -0.0161 |          41.5378 |          -0.3840 |
[32m[20221214 00:19:32 @agent_ppo2.py:185][0m |          -0.0215 |          40.7103 |          -0.3854 |
[32m[20221214 00:19:32 @agent_ppo2.py:185][0m |          -0.0153 |          40.0822 |          -0.3318 |
[32m[20221214 00:19:32 @agent_ppo2.py:185][0m |          -0.0181 |          39.4465 |          -0.4288 |
[32m[20221214 00:19:33 @agent_ppo2.py:185][0m |          -0.0215 |          38.8734 |          -0.3689 |
[32m[20221214 00:19:33 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:19:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 437.53
[32m[20221214 00:19:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 498.32
[32m[20221214 00:19:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 517.60
[32m[20221214 00:19:33 @agent_ppo2.py:143][0m Total time:      22.04 min
[32m[20221214 00:19:33 @agent_ppo2.py:145][0m 2025472 total steps have happened
[32m[20221214 00:19:33 @agent_ppo2.py:121][0m #------------------------ Iteration 4989 --------------------------#
[32m[20221214 00:19:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:19:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:19:33 @agent_ppo2.py:185][0m |           0.0000 |          82.7500 |          -1.7379 |
[32m[20221214 00:19:33 @agent_ppo2.py:185][0m |          -0.0052 |          74.8743 |          -1.6005 |
[32m[20221214 00:19:33 @agent_ppo2.py:185][0m |          -0.0178 |          72.3043 |          -1.6509 |
[32m[20221214 00:19:33 @agent_ppo2.py:185][0m |          -0.0114 |          70.0321 |          -1.7068 |
[32m[20221214 00:19:33 @agent_ppo2.py:185][0m |          -0.0106 |          68.8865 |          -1.5735 |
[32m[20221214 00:19:34 @agent_ppo2.py:185][0m |          -0.0100 |          68.7987 |          -1.5977 |
[32m[20221214 00:19:34 @agent_ppo2.py:185][0m |          -0.0198 |          66.6179 |          -1.5005 |
[32m[20221214 00:19:34 @agent_ppo2.py:185][0m |          -0.0203 |          65.5453 |          -1.4037 |
[32m[20221214 00:19:34 @agent_ppo2.py:185][0m |          -0.0170 |          64.6218 |          -1.4054 |
[32m[20221214 00:19:34 @agent_ppo2.py:185][0m |          -0.0237 |          63.8893 |          -1.3383 |
[32m[20221214 00:19:34 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:19:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 519.10
[32m[20221214 00:19:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 623.22
[32m[20221214 00:19:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 441.26
[32m[20221214 00:19:34 @agent_ppo2.py:143][0m Total time:      22.06 min
[32m[20221214 00:19:34 @agent_ppo2.py:145][0m 2027520 total steps have happened
[32m[20221214 00:19:34 @agent_ppo2.py:121][0m #------------------------ Iteration 4990 --------------------------#
[32m[20221214 00:19:34 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:19:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:19:34 @agent_ppo2.py:185][0m |          -0.0006 |          77.7222 |          -1.5656 |
[32m[20221214 00:19:35 @agent_ppo2.py:185][0m |          -0.0078 |          68.3596 |          -1.4855 |
[32m[20221214 00:19:35 @agent_ppo2.py:185][0m |          -0.0075 |          64.2523 |          -1.4277 |
[32m[20221214 00:19:35 @agent_ppo2.py:185][0m |          -0.0122 |          61.9256 |          -1.3344 |
[32m[20221214 00:19:35 @agent_ppo2.py:185][0m |          -0.0107 |          60.1832 |          -1.2484 |
[32m[20221214 00:19:35 @agent_ppo2.py:185][0m |          -0.0112 |          58.8361 |          -1.3223 |
[32m[20221214 00:19:35 @agent_ppo2.py:185][0m |           0.0070 |          60.9099 |          -1.2355 |
[32m[20221214 00:19:35 @agent_ppo2.py:185][0m |          -0.0143 |          57.3687 |          -1.2379 |
[32m[20221214 00:19:35 @agent_ppo2.py:185][0m |          -0.0162 |          56.1753 |          -1.1697 |
[32m[20221214 00:19:35 @agent_ppo2.py:185][0m |          -0.0163 |          55.7219 |          -1.2015 |
[32m[20221214 00:19:35 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:19:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 479.31
[32m[20221214 00:19:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 531.70
[32m[20221214 00:19:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 577.78
[32m[20221214 00:19:36 @agent_ppo2.py:143][0m Total time:      22.09 min
[32m[20221214 00:19:36 @agent_ppo2.py:145][0m 2029568 total steps have happened
[32m[20221214 00:19:36 @agent_ppo2.py:121][0m #------------------------ Iteration 4991 --------------------------#
[32m[20221214 00:19:36 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:19:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:19:36 @agent_ppo2.py:185][0m |           0.0058 |          68.8302 |          -1.2592 |
[32m[20221214 00:19:36 @agent_ppo2.py:185][0m |          -0.0094 |          58.4483 |          -1.2625 |
[32m[20221214 00:19:36 @agent_ppo2.py:185][0m |          -0.0074 |          61.2339 |          -1.1742 |
[32m[20221214 00:19:36 @agent_ppo2.py:185][0m |          -0.0137 |          53.9366 |          -1.2227 |
[32m[20221214 00:19:36 @agent_ppo2.py:185][0m |          -0.0214 |          52.6574 |          -1.3285 |
[32m[20221214 00:19:36 @agent_ppo2.py:185][0m |          -0.0254 |          51.7802 |          -1.3740 |
[32m[20221214 00:19:36 @agent_ppo2.py:185][0m |          -0.0217 |          50.9592 |          -1.3463 |
[32m[20221214 00:19:37 @agent_ppo2.py:185][0m |          -0.0233 |          50.1868 |          -1.4013 |
[32m[20221214 00:19:37 @agent_ppo2.py:185][0m |          -0.0181 |          49.7120 |          -1.4032 |
[32m[20221214 00:19:37 @agent_ppo2.py:185][0m |          -0.0244 |          48.9622 |          -1.4556 |
[32m[20221214 00:19:37 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:19:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 458.21
[32m[20221214 00:19:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 507.35
[32m[20221214 00:19:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 450.08
[32m[20221214 00:19:37 @agent_ppo2.py:143][0m Total time:      22.11 min
[32m[20221214 00:19:37 @agent_ppo2.py:145][0m 2031616 total steps have happened
[32m[20221214 00:19:37 @agent_ppo2.py:121][0m #------------------------ Iteration 4992 --------------------------#
[32m[20221214 00:19:37 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:19:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:19:37 @agent_ppo2.py:185][0m |          -0.0034 |          45.8701 |          -1.1657 |
[32m[20221214 00:19:37 @agent_ppo2.py:185][0m |          -0.0028 |          38.7160 |          -1.0406 |
[32m[20221214 00:19:37 @agent_ppo2.py:185][0m |          -0.0058 |          34.7391 |          -1.0763 |
[32m[20221214 00:19:38 @agent_ppo2.py:185][0m |          -0.0112 |          32.5149 |          -0.9745 |
[32m[20221214 00:19:38 @agent_ppo2.py:185][0m |          -0.0165 |          31.2911 |          -1.0077 |
[32m[20221214 00:19:38 @agent_ppo2.py:185][0m |          -0.0164 |          30.2061 |          -0.9803 |
[32m[20221214 00:19:38 @agent_ppo2.py:185][0m |          -0.0207 |          29.5317 |          -0.8390 |
[32m[20221214 00:19:38 @agent_ppo2.py:185][0m |          -0.0174 |          28.8958 |          -1.0804 |
[32m[20221214 00:19:38 @agent_ppo2.py:185][0m |          -0.0152 |          29.8608 |          -0.9515 |
[32m[20221214 00:19:38 @agent_ppo2.py:185][0m |          -0.0252 |          27.5531 |          -0.9535 |
[32m[20221214 00:19:38 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:19:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 378.36
[32m[20221214 00:19:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 502.53
[32m[20221214 00:19:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 582.06
[32m[20221214 00:19:38 @agent_ppo2.py:143][0m Total time:      22.13 min
[32m[20221214 00:19:38 @agent_ppo2.py:145][0m 2033664 total steps have happened
[32m[20221214 00:19:38 @agent_ppo2.py:121][0m #------------------------ Iteration 4993 --------------------------#
[32m[20221214 00:19:38 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:19:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:19:39 @agent_ppo2.py:185][0m |           0.0075 |          65.5121 |          -0.8755 |
[32m[20221214 00:19:39 @agent_ppo2.py:185][0m |          -0.0016 |          60.2188 |          -0.8577 |
[32m[20221214 00:19:39 @agent_ppo2.py:185][0m |          -0.0089 |          56.4373 |          -0.7903 |
[32m[20221214 00:19:39 @agent_ppo2.py:185][0m |          -0.0093 |          55.2671 |          -0.9549 |
[32m[20221214 00:19:39 @agent_ppo2.py:185][0m |          -0.0159 |          53.6336 |          -0.9827 |
[32m[20221214 00:19:39 @agent_ppo2.py:185][0m |          -0.0146 |          52.7644 |          -0.9095 |
[32m[20221214 00:19:39 @agent_ppo2.py:185][0m |          -0.0159 |          52.2420 |          -0.8912 |
[32m[20221214 00:19:39 @agent_ppo2.py:185][0m |          -0.0178 |          52.0793 |          -0.8202 |
[32m[20221214 00:19:39 @agent_ppo2.py:185][0m |          -0.0192 |          51.5275 |          -0.9670 |
[32m[20221214 00:19:40 @agent_ppo2.py:185][0m |          -0.0225 |          50.9984 |          -0.9843 |
[32m[20221214 00:19:40 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:19:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 482.86
[32m[20221214 00:19:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 536.97
[32m[20221214 00:19:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 598.62
[32m[20221214 00:19:40 @agent_ppo2.py:143][0m Total time:      22.16 min
[32m[20221214 00:19:40 @agent_ppo2.py:145][0m 2035712 total steps have happened
[32m[20221214 00:19:40 @agent_ppo2.py:121][0m #------------------------ Iteration 4994 --------------------------#
[32m[20221214 00:19:40 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:19:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:19:40 @agent_ppo2.py:185][0m |           0.0009 |          71.0877 |          -1.0629 |
[32m[20221214 00:19:40 @agent_ppo2.py:185][0m |          -0.0063 |          64.5752 |          -1.0628 |
[32m[20221214 00:19:40 @agent_ppo2.py:185][0m |          -0.0114 |          62.6635 |          -1.0010 |
[32m[20221214 00:19:40 @agent_ppo2.py:185][0m |          -0.0064 |          66.5144 |          -1.0613 |
[32m[20221214 00:19:40 @agent_ppo2.py:185][0m |          -0.0148 |          60.5219 |          -1.0717 |
[32m[20221214 00:19:41 @agent_ppo2.py:185][0m |          -0.0121 |          61.5877 |          -1.0705 |
[32m[20221214 00:19:41 @agent_ppo2.py:185][0m |          -0.0198 |          58.7451 |          -1.1290 |
[32m[20221214 00:19:41 @agent_ppo2.py:185][0m |          -0.0186 |          58.5520 |          -1.0647 |
[32m[20221214 00:19:41 @agent_ppo2.py:185][0m |          -0.0256 |          57.8646 |          -1.0808 |
[32m[20221214 00:19:41 @agent_ppo2.py:185][0m |          -0.0248 |          57.4123 |          -1.1054 |
[32m[20221214 00:19:41 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:19:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 381.00
[32m[20221214 00:19:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 472.74
[32m[20221214 00:19:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 704.90
[32m[20221214 00:19:41 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 704.90
[32m[20221214 00:19:41 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 704.90
[32m[20221214 00:19:41 @agent_ppo2.py:143][0m Total time:      22.18 min
[32m[20221214 00:19:41 @agent_ppo2.py:145][0m 2037760 total steps have happened
[32m[20221214 00:19:41 @agent_ppo2.py:121][0m #------------------------ Iteration 4995 --------------------------#
[32m[20221214 00:19:41 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:19:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:19:41 @agent_ppo2.py:185][0m |           0.0045 |          65.8197 |          -0.8576 |
[32m[20221214 00:19:42 @agent_ppo2.py:185][0m |           0.0045 |          65.6526 |          -0.8819 |
[32m[20221214 00:19:42 @agent_ppo2.py:185][0m |          -0.0038 |          56.6936 |          -0.8258 |
[32m[20221214 00:19:42 @agent_ppo2.py:185][0m |          -0.0126 |          54.8618 |          -0.8898 |
[32m[20221214 00:19:42 @agent_ppo2.py:185][0m |          -0.0076 |          54.0769 |          -0.9294 |
[32m[20221214 00:19:42 @agent_ppo2.py:185][0m |          -0.0096 |          52.7851 |          -0.9068 |
[32m[20221214 00:19:42 @agent_ppo2.py:185][0m |          -0.0104 |          52.8711 |          -0.9079 |
[32m[20221214 00:19:42 @agent_ppo2.py:185][0m |          -0.0127 |          51.1929 |          -0.9288 |
[32m[20221214 00:19:42 @agent_ppo2.py:185][0m |          -0.0147 |          50.4890 |          -0.8859 |
[32m[20221214 00:19:42 @agent_ppo2.py:185][0m |          -0.0125 |          49.9540 |          -0.9835 |
[32m[20221214 00:19:42 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:19:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 395.88
[32m[20221214 00:19:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 459.02
[32m[20221214 00:19:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 560.28
[32m[20221214 00:19:42 @agent_ppo2.py:143][0m Total time:      22.20 min
[32m[20221214 00:19:42 @agent_ppo2.py:145][0m 2039808 total steps have happened
[32m[20221214 00:19:42 @agent_ppo2.py:121][0m #------------------------ Iteration 4996 --------------------------#
[32m[20221214 00:19:43 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:19:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:19:43 @agent_ppo2.py:185][0m |          -0.0019 |          83.5376 |          -1.8113 |
[32m[20221214 00:19:43 @agent_ppo2.py:185][0m |          -0.0056 |          76.0674 |          -1.9930 |
[32m[20221214 00:19:43 @agent_ppo2.py:185][0m |          -0.0104 |          73.5202 |          -1.8035 |
[32m[20221214 00:19:43 @agent_ppo2.py:185][0m |          -0.0077 |          74.9775 |          -1.8991 |
[32m[20221214 00:19:43 @agent_ppo2.py:185][0m |          -0.0168 |          71.0241 |          -1.9651 |
[32m[20221214 00:19:43 @agent_ppo2.py:185][0m |          -0.0159 |          69.8117 |          -1.8617 |
[32m[20221214 00:19:43 @agent_ppo2.py:185][0m |          -0.0184 |          69.2670 |          -1.9239 |
[32m[20221214 00:19:44 @agent_ppo2.py:185][0m |          -0.0169 |          68.8459 |          -1.8290 |
[32m[20221214 00:19:44 @agent_ppo2.py:185][0m |          -0.0201 |          68.3720 |          -1.8477 |
[32m[20221214 00:19:44 @agent_ppo2.py:185][0m |          -0.0190 |          67.8227 |          -1.8530 |
[32m[20221214 00:19:44 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:19:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 510.38
[32m[20221214 00:19:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 642.19
[32m[20221214 00:19:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 317.85
[32m[20221214 00:19:44 @agent_ppo2.py:143][0m Total time:      22.23 min
[32m[20221214 00:19:44 @agent_ppo2.py:145][0m 2041856 total steps have happened
[32m[20221214 00:19:44 @agent_ppo2.py:121][0m #------------------------ Iteration 4997 --------------------------#
[32m[20221214 00:19:44 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:19:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:19:44 @agent_ppo2.py:185][0m |           0.0017 |          63.1940 |          -1.6711 |
[32m[20221214 00:19:44 @agent_ppo2.py:185][0m |          -0.0075 |          56.5891 |          -1.6576 |
[32m[20221214 00:19:44 @agent_ppo2.py:185][0m |          -0.0082 |          54.4533 |          -1.6853 |
[32m[20221214 00:19:45 @agent_ppo2.py:185][0m |          -0.0080 |          53.3045 |          -1.7232 |
[32m[20221214 00:19:45 @agent_ppo2.py:185][0m |          -0.0102 |          52.3878 |          -1.6926 |
[32m[20221214 00:19:45 @agent_ppo2.py:185][0m |          -0.0147 |          51.7709 |          -1.7803 |
[32m[20221214 00:19:45 @agent_ppo2.py:185][0m |          -0.0161 |          51.3784 |          -1.8552 |
[32m[20221214 00:19:45 @agent_ppo2.py:185][0m |          -0.0169 |          50.7692 |          -1.7701 |
[32m[20221214 00:19:45 @agent_ppo2.py:185][0m |          -0.0157 |          50.4469 |          -1.8857 |
[32m[20221214 00:19:45 @agent_ppo2.py:185][0m |          -0.0208 |          50.1767 |          -1.9958 |
[32m[20221214 00:19:45 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:19:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 462.62
[32m[20221214 00:19:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 532.24
[32m[20221214 00:19:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 580.13
[32m[20221214 00:19:45 @agent_ppo2.py:143][0m Total time:      22.25 min
[32m[20221214 00:19:45 @agent_ppo2.py:145][0m 2043904 total steps have happened
[32m[20221214 00:19:45 @agent_ppo2.py:121][0m #------------------------ Iteration 4998 --------------------------#
[32m[20221214 00:19:45 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:19:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:19:46 @agent_ppo2.py:185][0m |           0.0007 |          84.4466 |          -2.9898 |
[32m[20221214 00:19:46 @agent_ppo2.py:185][0m |          -0.0033 |          76.4532 |          -2.9090 |
[32m[20221214 00:19:46 @agent_ppo2.py:185][0m |          -0.0075 |          73.2653 |          -2.9317 |
[32m[20221214 00:19:46 @agent_ppo2.py:185][0m |          -0.0076 |          71.3140 |          -2.9292 |
[32m[20221214 00:19:46 @agent_ppo2.py:185][0m |          -0.0018 |          71.9257 |          -2.8871 |
[32m[20221214 00:19:46 @agent_ppo2.py:185][0m |          -0.0125 |          69.3183 |          -2.8657 |
[32m[20221214 00:19:46 @agent_ppo2.py:185][0m |          -0.0089 |          68.6675 |          -2.9138 |
[32m[20221214 00:19:46 @agent_ppo2.py:185][0m |          -0.0114 |          67.9735 |          -2.8897 |
[32m[20221214 00:19:46 @agent_ppo2.py:185][0m |          -0.0068 |          67.3108 |          -2.9027 |
[32m[20221214 00:19:47 @agent_ppo2.py:185][0m |          -0.0140 |          66.8467 |          -2.8514 |
[32m[20221214 00:19:47 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:19:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 581.77
[32m[20221214 00:19:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 627.11
[32m[20221214 00:19:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 481.27
[32m[20221214 00:19:47 @agent_ppo2.py:143][0m Total time:      22.27 min
[32m[20221214 00:19:47 @agent_ppo2.py:145][0m 2045952 total steps have happened
[32m[20221214 00:19:47 @agent_ppo2.py:121][0m #------------------------ Iteration 4999 --------------------------#
[32m[20221214 00:19:47 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:19:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:19:47 @agent_ppo2.py:185][0m |           0.0015 |          83.2160 |          -1.6093 |
[32m[20221214 00:19:47 @agent_ppo2.py:185][0m |          -0.0035 |          71.1309 |          -1.5165 |
[32m[20221214 00:19:47 @agent_ppo2.py:185][0m |          -0.0086 |          66.2161 |          -1.5623 |
[32m[20221214 00:19:47 @agent_ppo2.py:185][0m |          -0.0101 |          63.4275 |          -1.5991 |
[32m[20221214 00:19:47 @agent_ppo2.py:185][0m |          -0.0127 |          61.5996 |          -1.6261 |
[32m[20221214 00:19:48 @agent_ppo2.py:185][0m |          -0.0141 |          59.5553 |          -1.6626 |
[32m[20221214 00:19:48 @agent_ppo2.py:185][0m |          -0.0160 |          58.4074 |          -1.6800 |
[32m[20221214 00:19:48 @agent_ppo2.py:185][0m |          -0.0137 |          57.6324 |          -1.6720 |
[32m[20221214 00:19:48 @agent_ppo2.py:185][0m |          -0.0168 |          56.6425 |          -1.7978 |
[32m[20221214 00:19:48 @agent_ppo2.py:185][0m |          -0.0182 |          55.8974 |          -1.7687 |
[32m[20221214 00:19:48 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:19:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 505.45
[32m[20221214 00:19:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 588.23
[32m[20221214 00:19:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 449.01
[32m[20221214 00:19:48 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Saving the interval checkpoint with rewards 704.90
[32m[20221214 00:19:48 @agent_ppo2.py:143][0m Total time:      22.29 min
[32m[20221214 00:19:48 @agent_ppo2.py:145][0m 2048000 total steps have happened
[32m[20221214 00:19:48 @agent_ppo2.py:121][0m #------------------------ Iteration 5000 --------------------------#
[32m[20221214 00:19:48 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:19:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:19:48 @agent_ppo2.py:185][0m |           0.0036 |          83.2381 |          -3.4227 |
[32m[20221214 00:19:49 @agent_ppo2.py:185][0m |          -0.0020 |          80.0327 |          -3.4140 |
[32m[20221214 00:19:49 @agent_ppo2.py:185][0m |          -0.0082 |          74.6302 |          -3.4733 |
[32m[20221214 00:19:49 @agent_ppo2.py:185][0m |          -0.0078 |          72.5096 |          -3.4344 |
[32m[20221214 00:19:49 @agent_ppo2.py:185][0m |          -0.0095 |          70.8089 |          -3.4907 |
[32m[20221214 00:19:49 @agent_ppo2.py:185][0m |          -0.0115 |          70.2358 |          -3.5182 |
[32m[20221214 00:19:49 @agent_ppo2.py:185][0m |          -0.0142 |          69.2684 |          -3.6012 |
[32m[20221214 00:19:49 @agent_ppo2.py:185][0m |          -0.0127 |          68.5031 |          -3.5179 |
[32m[20221214 00:19:49 @agent_ppo2.py:185][0m |          -0.0133 |          68.0614 |          -3.6352 |
[32m[20221214 00:19:49 @agent_ppo2.py:185][0m |          -0.0140 |          67.5572 |          -3.6244 |
[32m[20221214 00:19:49 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:19:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 510.83
[32m[20221214 00:19:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 551.64
[32m[20221214 00:19:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 623.32
[32m[20221214 00:19:49 @agent_ppo2.py:143][0m Total time:      22.32 min
[32m[20221214 00:19:49 @agent_ppo2.py:145][0m 2050048 total steps have happened
[32m[20221214 00:19:49 @agent_ppo2.py:121][0m #------------------------ Iteration 5001 --------------------------#
[32m[20221214 00:19:50 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:19:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:19:50 @agent_ppo2.py:185][0m |           0.0012 |          78.4998 |          -3.2335 |
[32m[20221214 00:19:50 @agent_ppo2.py:185][0m |          -0.0096 |          70.6372 |          -3.1196 |
[32m[20221214 00:19:50 @agent_ppo2.py:185][0m |          -0.0063 |          68.4460 |          -3.1320 |
[32m[20221214 00:19:50 @agent_ppo2.py:185][0m |          -0.0054 |          67.2984 |          -3.1168 |
[32m[20221214 00:19:50 @agent_ppo2.py:185][0m |          -0.0114 |          65.4160 |          -3.0857 |
[32m[20221214 00:19:50 @agent_ppo2.py:185][0m |          -0.0101 |          64.7233 |          -3.0882 |
[32m[20221214 00:19:50 @agent_ppo2.py:185][0m |          -0.0053 |          68.7139 |          -3.0680 |
[32m[20221214 00:19:51 @agent_ppo2.py:185][0m |          -0.0173 |          64.0435 |          -2.9989 |
[32m[20221214 00:19:51 @agent_ppo2.py:185][0m |          -0.0144 |          63.0962 |          -3.1812 |
[32m[20221214 00:19:51 @agent_ppo2.py:185][0m |          -0.0167 |          62.7819 |          -3.1052 |
[32m[20221214 00:19:51 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:19:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 542.72
[32m[20221214 00:19:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 633.43
[32m[20221214 00:19:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 510.35
[32m[20221214 00:19:51 @agent_ppo2.py:143][0m Total time:      22.34 min
[32m[20221214 00:19:51 @agent_ppo2.py:145][0m 2052096 total steps have happened
[32m[20221214 00:19:51 @agent_ppo2.py:121][0m #------------------------ Iteration 5002 --------------------------#
[32m[20221214 00:19:51 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:19:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:19:51 @agent_ppo2.py:185][0m |           0.0019 |          92.3020 |          -3.3077 |
[32m[20221214 00:19:51 @agent_ppo2.py:185][0m |          -0.0136 |          82.6015 |          -3.0326 |
[32m[20221214 00:19:51 @agent_ppo2.py:185][0m |          -0.0095 |          80.4375 |          -3.1582 |
[32m[20221214 00:19:51 @agent_ppo2.py:185][0m |          -0.0120 |          78.7915 |          -3.0766 |
[32m[20221214 00:19:52 @agent_ppo2.py:185][0m |          -0.0103 |          78.8339 |          -3.0721 |
[32m[20221214 00:19:52 @agent_ppo2.py:185][0m |          -0.0129 |          77.4568 |          -2.9902 |
[32m[20221214 00:19:52 @agent_ppo2.py:185][0m |          -0.0098 |          81.3445 |          -3.0772 |
[32m[20221214 00:19:52 @agent_ppo2.py:185][0m |          -0.0187 |          75.6196 |          -2.9524 |
[32m[20221214 00:19:52 @agent_ppo2.py:185][0m |          -0.0139 |          74.9407 |          -2.9395 |
[32m[20221214 00:19:52 @agent_ppo2.py:185][0m |          -0.0214 |          73.9655 |          -3.0071 |
[32m[20221214 00:19:52 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:19:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 581.04
[32m[20221214 00:19:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 588.62
[32m[20221214 00:19:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 557.54
[32m[20221214 00:19:52 @agent_ppo2.py:143][0m Total time:      22.36 min
[32m[20221214 00:19:52 @agent_ppo2.py:145][0m 2054144 total steps have happened
[32m[20221214 00:19:52 @agent_ppo2.py:121][0m #------------------------ Iteration 5003 --------------------------#
[32m[20221214 00:19:52 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:19:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:19:53 @agent_ppo2.py:185][0m |           0.0020 |          81.3923 |          -4.2996 |
[32m[20221214 00:19:53 @agent_ppo2.py:185][0m |          -0.0046 |          73.6772 |          -4.1954 |
[32m[20221214 00:19:53 @agent_ppo2.py:185][0m |          -0.0045 |          69.9321 |          -4.2632 |
[32m[20221214 00:19:53 @agent_ppo2.py:185][0m |          -0.0100 |          67.4696 |          -4.3662 |
[32m[20221214 00:19:53 @agent_ppo2.py:185][0m |          -0.0109 |          66.0806 |          -4.3825 |
[32m[20221214 00:19:53 @agent_ppo2.py:185][0m |          -0.0107 |          64.4205 |          -4.3804 |
[32m[20221214 00:19:53 @agent_ppo2.py:185][0m |          -0.0144 |          63.3236 |          -4.4482 |
[32m[20221214 00:19:53 @agent_ppo2.py:185][0m |          -0.0142 |          62.6922 |          -4.4676 |
[32m[20221214 00:19:53 @agent_ppo2.py:185][0m |          -0.0160 |          61.2408 |          -4.5221 |
[32m[20221214 00:19:53 @agent_ppo2.py:185][0m |          -0.0112 |          60.3241 |          -4.5434 |
[32m[20221214 00:19:53 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:19:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 541.59
[32m[20221214 00:19:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 600.74
[32m[20221214 00:19:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 624.67
[32m[20221214 00:19:54 @agent_ppo2.py:143][0m Total time:      22.39 min
[32m[20221214 00:19:54 @agent_ppo2.py:145][0m 2056192 total steps have happened
[32m[20221214 00:19:54 @agent_ppo2.py:121][0m #------------------------ Iteration 5004 --------------------------#
[32m[20221214 00:19:54 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:19:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:19:54 @agent_ppo2.py:185][0m |           0.0047 |          68.4607 |          -2.8736 |
[32m[20221214 00:19:54 @agent_ppo2.py:185][0m |          -0.0033 |          60.6259 |          -2.5203 |
[32m[20221214 00:19:54 @agent_ppo2.py:185][0m |          -0.0073 |          58.2230 |          -2.4591 |
[32m[20221214 00:19:54 @agent_ppo2.py:185][0m |          -0.0111 |          56.7531 |          -2.4805 |
[32m[20221214 00:19:54 @agent_ppo2.py:185][0m |          -0.0152 |          55.5115 |          -2.3824 |
[32m[20221214 00:19:54 @agent_ppo2.py:185][0m |          -0.0115 |          55.7401 |          -2.4338 |
[32m[20221214 00:19:55 @agent_ppo2.py:185][0m |          -0.0126 |          54.0646 |          -2.3336 |
[32m[20221214 00:19:55 @agent_ppo2.py:185][0m |          -0.0167 |          53.0315 |          -2.3489 |
[32m[20221214 00:19:55 @agent_ppo2.py:185][0m |          -0.0237 |          52.9436 |          -2.3037 |
[32m[20221214 00:19:55 @agent_ppo2.py:185][0m |          -0.0210 |          52.6709 |          -2.2636 |
[32m[20221214 00:19:55 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:19:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 488.72
[32m[20221214 00:19:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 587.98
[32m[20221214 00:19:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 411.09
[32m[20221214 00:19:55 @agent_ppo2.py:143][0m Total time:      22.41 min
[32m[20221214 00:19:55 @agent_ppo2.py:145][0m 2058240 total steps have happened
[32m[20221214 00:19:55 @agent_ppo2.py:121][0m #------------------------ Iteration 5005 --------------------------#
[32m[20221214 00:19:55 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:19:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:19:55 @agent_ppo2.py:185][0m |          -0.0029 |          64.5656 |          -3.2480 |
[32m[20221214 00:19:55 @agent_ppo2.py:185][0m |          -0.0077 |          54.6828 |          -3.2185 |
[32m[20221214 00:19:56 @agent_ppo2.py:185][0m |          -0.0086 |          52.4157 |          -3.1484 |
[32m[20221214 00:19:56 @agent_ppo2.py:185][0m |          -0.0144 |          50.8648 |          -3.1561 |
[32m[20221214 00:19:56 @agent_ppo2.py:185][0m |          -0.0175 |          49.7570 |          -3.0161 |
[32m[20221214 00:19:56 @agent_ppo2.py:185][0m |          -0.0149 |          49.1799 |          -3.0938 |
[32m[20221214 00:19:56 @agent_ppo2.py:185][0m |          -0.0214 |          48.2340 |          -2.9816 |
[32m[20221214 00:19:56 @agent_ppo2.py:185][0m |          -0.0177 |          47.7056 |          -3.0052 |
[32m[20221214 00:19:56 @agent_ppo2.py:185][0m |          -0.0163 |          47.2335 |          -2.9970 |
[32m[20221214 00:19:56 @agent_ppo2.py:185][0m |          -0.0206 |          47.0908 |          -2.9248 |
[32m[20221214 00:19:56 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:19:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 413.98
[32m[20221214 00:19:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 487.68
[32m[20221214 00:19:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 568.83
[32m[20221214 00:19:56 @agent_ppo2.py:143][0m Total time:      22.43 min
[32m[20221214 00:19:56 @agent_ppo2.py:145][0m 2060288 total steps have happened
[32m[20221214 00:19:56 @agent_ppo2.py:121][0m #------------------------ Iteration 5006 --------------------------#
[32m[20221214 00:19:57 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:19:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:19:57 @agent_ppo2.py:185][0m |           0.0023 |          67.9000 |          -3.3068 |
[32m[20221214 00:19:57 @agent_ppo2.py:185][0m |          -0.0022 |          62.3712 |          -3.3706 |
[32m[20221214 00:19:57 @agent_ppo2.py:185][0m |          -0.0076 |          59.9030 |          -3.4485 |
[32m[20221214 00:19:57 @agent_ppo2.py:185][0m |          -0.0117 |          58.5991 |          -3.3014 |
[32m[20221214 00:19:57 @agent_ppo2.py:185][0m |          -0.0129 |          57.5428 |          -3.3802 |
[32m[20221214 00:19:57 @agent_ppo2.py:185][0m |          -0.0171 |          56.6595 |          -3.3410 |
[32m[20221214 00:19:57 @agent_ppo2.py:185][0m |          -0.0139 |          56.2229 |          -3.3437 |
[32m[20221214 00:19:57 @agent_ppo2.py:185][0m |          -0.0217 |          55.6553 |          -3.2817 |
[32m[20221214 00:19:58 @agent_ppo2.py:185][0m |          -0.0165 |          54.9580 |          -3.3253 |
[32m[20221214 00:19:58 @agent_ppo2.py:185][0m |          -0.0160 |          54.5965 |          -3.3537 |
[32m[20221214 00:19:58 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:19:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 478.39
[32m[20221214 00:19:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 556.42
[32m[20221214 00:19:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 508.02
[32m[20221214 00:19:58 @agent_ppo2.py:143][0m Total time:      22.46 min
[32m[20221214 00:19:58 @agent_ppo2.py:145][0m 2062336 total steps have happened
[32m[20221214 00:19:58 @agent_ppo2.py:121][0m #------------------------ Iteration 5007 --------------------------#
[32m[20221214 00:19:58 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:19:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:19:58 @agent_ppo2.py:185][0m |           0.0014 |          57.5957 |          -3.6701 |
[32m[20221214 00:19:58 @agent_ppo2.py:185][0m |           0.0031 |          50.5540 |          -3.6456 |
[32m[20221214 00:19:58 @agent_ppo2.py:185][0m |          -0.0044 |          47.1568 |          -3.6238 |
[32m[20221214 00:19:58 @agent_ppo2.py:185][0m |          -0.0120 |          45.9870 |          -3.5189 |
[32m[20221214 00:19:59 @agent_ppo2.py:185][0m |          -0.0084 |          44.8624 |          -3.6417 |
[32m[20221214 00:19:59 @agent_ppo2.py:185][0m |          -0.0071 |          44.1626 |          -3.6588 |
[32m[20221214 00:19:59 @agent_ppo2.py:185][0m |          -0.0072 |          43.4697 |          -3.6213 |
[32m[20221214 00:19:59 @agent_ppo2.py:185][0m |          -0.0087 |          44.9382 |          -3.6102 |
[32m[20221214 00:19:59 @agent_ppo2.py:185][0m |          -0.0155 |          42.5151 |          -3.5339 |
[32m[20221214 00:19:59 @agent_ppo2.py:185][0m |          -0.0156 |          42.3404 |          -3.6629 |
[32m[20221214 00:19:59 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:19:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 510.09
[32m[20221214 00:19:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 580.84
[32m[20221214 00:19:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 556.57
[32m[20221214 00:19:59 @agent_ppo2.py:143][0m Total time:      22.48 min
[32m[20221214 00:19:59 @agent_ppo2.py:145][0m 2064384 total steps have happened
[32m[20221214 00:19:59 @agent_ppo2.py:121][0m #------------------------ Iteration 5008 --------------------------#
[32m[20221214 00:19:59 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:19:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:20:00 @agent_ppo2.py:185][0m |          -0.0004 |          71.0272 |          -3.8391 |
[32m[20221214 00:20:00 @agent_ppo2.py:185][0m |          -0.0026 |          65.4870 |          -3.8718 |
[32m[20221214 00:20:00 @agent_ppo2.py:185][0m |          -0.0020 |          64.4276 |          -3.7844 |
[32m[20221214 00:20:00 @agent_ppo2.py:185][0m |          -0.0072 |          62.0314 |          -3.8452 |
[32m[20221214 00:20:00 @agent_ppo2.py:185][0m |          -0.0115 |          61.3123 |          -3.8092 |
[32m[20221214 00:20:00 @agent_ppo2.py:185][0m |          -0.0038 |          60.7431 |          -3.8422 |
[32m[20221214 00:20:00 @agent_ppo2.py:185][0m |          -0.0174 |          59.8790 |          -3.7806 |
[32m[20221214 00:20:00 @agent_ppo2.py:185][0m |          -0.0066 |          60.2614 |          -3.7149 |
[32m[20221214 00:20:00 @agent_ppo2.py:185][0m |          -0.0132 |          59.3277 |          -3.7758 |
[32m[20221214 00:20:00 @agent_ppo2.py:185][0m |          -0.0079 |          59.8231 |          -3.5559 |
[32m[20221214 00:20:00 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:20:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 457.21
[32m[20221214 00:20:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 509.12
[32m[20221214 00:20:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 626.41
[32m[20221214 00:20:01 @agent_ppo2.py:143][0m Total time:      22.50 min
[32m[20221214 00:20:01 @agent_ppo2.py:145][0m 2066432 total steps have happened
[32m[20221214 00:20:01 @agent_ppo2.py:121][0m #------------------------ Iteration 5009 --------------------------#
[32m[20221214 00:20:01 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:20:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:20:01 @agent_ppo2.py:185][0m |           0.0073 |          50.4260 |          -3.6118 |
[32m[20221214 00:20:01 @agent_ppo2.py:185][0m |           0.0022 |          42.4394 |          -3.4037 |
[32m[20221214 00:20:01 @agent_ppo2.py:185][0m |          -0.0060 |          40.3466 |          -3.3562 |
[32m[20221214 00:20:01 @agent_ppo2.py:185][0m |          -0.0083 |          39.1735 |          -3.3567 |
[32m[20221214 00:20:01 @agent_ppo2.py:185][0m |          -0.0115 |          37.9336 |          -3.2881 |
[32m[20221214 00:20:01 @agent_ppo2.py:185][0m |          -0.0152 |          36.8298 |          -3.3153 |
[32m[20221214 00:20:02 @agent_ppo2.py:185][0m |          -0.0136 |          36.0019 |          -3.2864 |
[32m[20221214 00:20:02 @agent_ppo2.py:185][0m |          -0.0126 |          35.4034 |          -3.2261 |
[32m[20221214 00:20:02 @agent_ppo2.py:185][0m |          -0.0140 |          34.8966 |          -3.1984 |
[32m[20221214 00:20:02 @agent_ppo2.py:185][0m |          -0.0173 |          34.5425 |          -3.1715 |
[32m[20221214 00:20:02 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:20:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 439.75
[32m[20221214 00:20:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 505.68
[32m[20221214 00:20:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 592.80
[32m[20221214 00:20:02 @agent_ppo2.py:143][0m Total time:      22.53 min
[32m[20221214 00:20:02 @agent_ppo2.py:145][0m 2068480 total steps have happened
[32m[20221214 00:20:02 @agent_ppo2.py:121][0m #------------------------ Iteration 5010 --------------------------#
[32m[20221214 00:20:02 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:20:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:20:02 @agent_ppo2.py:185][0m |           0.0016 |          66.7868 |          -2.8943 |
[32m[20221214 00:20:02 @agent_ppo2.py:185][0m |          -0.0098 |          61.2407 |          -2.9436 |
[32m[20221214 00:20:03 @agent_ppo2.py:185][0m |          -0.0085 |          59.4009 |          -2.8355 |
[32m[20221214 00:20:03 @agent_ppo2.py:185][0m |          -0.0126 |          57.8473 |          -3.0544 |
[32m[20221214 00:20:03 @agent_ppo2.py:185][0m |          -0.0149 |          56.9069 |          -2.9457 |
[32m[20221214 00:20:03 @agent_ppo2.py:185][0m |          -0.0148 |          55.6705 |          -3.0137 |
[32m[20221214 00:20:03 @agent_ppo2.py:185][0m |          -0.0063 |          56.6271 |          -3.0176 |
[32m[20221214 00:20:03 @agent_ppo2.py:185][0m |          -0.0198 |          54.1749 |          -3.0626 |
[32m[20221214 00:20:03 @agent_ppo2.py:185][0m |          -0.0195 |          53.4377 |          -3.0355 |
[32m[20221214 00:20:03 @agent_ppo2.py:185][0m |          -0.0178 |          52.8070 |          -3.0559 |
[32m[20221214 00:20:03 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:20:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 446.27
[32m[20221214 00:20:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 519.15
[32m[20221214 00:20:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 571.31
[32m[20221214 00:20:03 @agent_ppo2.py:143][0m Total time:      22.55 min
[32m[20221214 00:20:03 @agent_ppo2.py:145][0m 2070528 total steps have happened
[32m[20221214 00:20:03 @agent_ppo2.py:121][0m #------------------------ Iteration 5011 --------------------------#
[32m[20221214 00:20:04 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:20:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:20:04 @agent_ppo2.py:185][0m |           0.0002 |          64.3573 |          -3.2662 |
[32m[20221214 00:20:04 @agent_ppo2.py:185][0m |          -0.0111 |          58.2708 |          -3.4197 |
[32m[20221214 00:20:04 @agent_ppo2.py:185][0m |          -0.0103 |          55.2020 |          -3.2011 |
[32m[20221214 00:20:04 @agent_ppo2.py:185][0m |          -0.0118 |          53.5744 |          -3.3620 |
[32m[20221214 00:20:04 @agent_ppo2.py:185][0m |          -0.0134 |          52.0926 |          -3.2786 |
[32m[20221214 00:20:04 @agent_ppo2.py:185][0m |          -0.0126 |          51.1363 |          -3.3501 |
[32m[20221214 00:20:04 @agent_ppo2.py:185][0m |          -0.0115 |          50.3997 |          -3.2312 |
[32m[20221214 00:20:04 @agent_ppo2.py:185][0m |          -0.0209 |          49.6403 |          -3.3916 |
[32m[20221214 00:20:05 @agent_ppo2.py:185][0m |          -0.0142 |          49.4268 |          -3.3216 |
[32m[20221214 00:20:05 @agent_ppo2.py:185][0m |          -0.0190 |          48.7422 |          -3.3816 |
[32m[20221214 00:20:05 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:20:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 455.78
[32m[20221214 00:20:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 513.20
[32m[20221214 00:20:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 567.09
[32m[20221214 00:20:05 @agent_ppo2.py:143][0m Total time:      22.57 min
[32m[20221214 00:20:05 @agent_ppo2.py:145][0m 2072576 total steps have happened
[32m[20221214 00:20:05 @agent_ppo2.py:121][0m #------------------------ Iteration 5012 --------------------------#
[32m[20221214 00:20:05 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:20:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:20:05 @agent_ppo2.py:185][0m |           0.0007 |          75.3133 |          -3.6333 |
[32m[20221214 00:20:05 @agent_ppo2.py:185][0m |          -0.0087 |          70.1264 |          -3.5929 |
[32m[20221214 00:20:05 @agent_ppo2.py:185][0m |          -0.0075 |          67.7004 |          -3.4348 |
[32m[20221214 00:20:05 @agent_ppo2.py:185][0m |          -0.0087 |          66.7790 |          -3.3088 |
[32m[20221214 00:20:06 @agent_ppo2.py:185][0m |          -0.0058 |          71.2027 |          -3.3536 |
[32m[20221214 00:20:06 @agent_ppo2.py:185][0m |          -0.0159 |          63.5924 |          -3.2419 |
[32m[20221214 00:20:06 @agent_ppo2.py:185][0m |          -0.0173 |          62.7567 |          -3.1274 |
[32m[20221214 00:20:06 @agent_ppo2.py:185][0m |          -0.0171 |          61.7136 |          -3.1574 |
[32m[20221214 00:20:06 @agent_ppo2.py:185][0m |          -0.0151 |          61.2843 |          -3.1157 |
[32m[20221214 00:20:06 @agent_ppo2.py:185][0m |          -0.0224 |          60.3812 |          -3.0554 |
[32m[20221214 00:20:06 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:20:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 406.84
[32m[20221214 00:20:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 532.82
[32m[20221214 00:20:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 567.03
[32m[20221214 00:20:06 @agent_ppo2.py:143][0m Total time:      22.60 min
[32m[20221214 00:20:06 @agent_ppo2.py:145][0m 2074624 total steps have happened
[32m[20221214 00:20:06 @agent_ppo2.py:121][0m #------------------------ Iteration 5013 --------------------------#
[32m[20221214 00:20:06 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:20:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:20:07 @agent_ppo2.py:185][0m |           0.0038 |          69.3525 |          -2.9485 |
[32m[20221214 00:20:07 @agent_ppo2.py:185][0m |          -0.0051 |          61.5732 |          -2.7564 |
[32m[20221214 00:20:07 @agent_ppo2.py:185][0m |          -0.0084 |          59.6771 |          -2.8505 |
[32m[20221214 00:20:07 @agent_ppo2.py:185][0m |          -0.0014 |          59.6430 |          -2.8477 |
[32m[20221214 00:20:07 @agent_ppo2.py:185][0m |          -0.0020 |          58.5448 |          -2.9105 |
[32m[20221214 00:20:07 @agent_ppo2.py:185][0m |          -0.0086 |          57.2369 |          -2.9611 |
[32m[20221214 00:20:07 @agent_ppo2.py:185][0m |          -0.0055 |          58.4565 |          -2.9846 |
[32m[20221214 00:20:07 @agent_ppo2.py:185][0m |          -0.0116 |          56.6405 |          -2.9189 |
[32m[20221214 00:20:07 @agent_ppo2.py:185][0m |          -0.0130 |          56.1780 |          -2.9929 |
[32m[20221214 00:20:07 @agent_ppo2.py:185][0m |          -0.0129 |          55.7424 |          -2.8993 |
[32m[20221214 00:20:07 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:20:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 581.75
[32m[20221214 00:20:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 617.71
[32m[20221214 00:20:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 552.61
[32m[20221214 00:20:08 @agent_ppo2.py:143][0m Total time:      22.62 min
[32m[20221214 00:20:08 @agent_ppo2.py:145][0m 2076672 total steps have happened
[32m[20221214 00:20:08 @agent_ppo2.py:121][0m #------------------------ Iteration 5014 --------------------------#
[32m[20221214 00:20:08 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:20:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:20:08 @agent_ppo2.py:185][0m |           0.0002 |          53.5132 |          -2.7047 |
[32m[20221214 00:20:08 @agent_ppo2.py:185][0m |          -0.0090 |          46.8632 |          -2.6759 |
[32m[20221214 00:20:08 @agent_ppo2.py:185][0m |          -0.0169 |          44.0574 |          -2.6812 |
[32m[20221214 00:20:08 @agent_ppo2.py:185][0m |          -0.0209 |          42.4968 |          -2.6041 |
[32m[20221214 00:20:08 @agent_ppo2.py:185][0m |          -0.0155 |          41.5688 |          -2.6529 |
[32m[20221214 00:20:08 @agent_ppo2.py:185][0m |          -0.0198 |          40.1461 |          -2.5770 |
[32m[20221214 00:20:09 @agent_ppo2.py:185][0m |          -0.0187 |          40.2555 |          -2.6493 |
[32m[20221214 00:20:09 @agent_ppo2.py:185][0m |          -0.0212 |          38.6048 |          -2.6208 |
[32m[20221214 00:20:09 @agent_ppo2.py:185][0m |          -0.0225 |          38.2477 |          -2.5944 |
[32m[20221214 00:20:09 @agent_ppo2.py:185][0m |          -0.0218 |          38.0619 |          -2.6486 |
[32m[20221214 00:20:09 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:20:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 488.63
[32m[20221214 00:20:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 555.50
[32m[20221214 00:20:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 471.18
[32m[20221214 00:20:09 @agent_ppo2.py:143][0m Total time:      22.64 min
[32m[20221214 00:20:09 @agent_ppo2.py:145][0m 2078720 total steps have happened
[32m[20221214 00:20:09 @agent_ppo2.py:121][0m #------------------------ Iteration 5015 --------------------------#
[32m[20221214 00:20:09 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:20:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:20:09 @agent_ppo2.py:185][0m |          -0.0016 |          74.7484 |          -2.7242 |
[32m[20221214 00:20:09 @agent_ppo2.py:185][0m |          -0.0106 |          67.2629 |          -2.6169 |
[32m[20221214 00:20:10 @agent_ppo2.py:185][0m |          -0.0142 |          65.6047 |          -2.5958 |
[32m[20221214 00:20:10 @agent_ppo2.py:185][0m |          -0.0094 |          65.3277 |          -2.5586 |
[32m[20221214 00:20:10 @agent_ppo2.py:185][0m |          -0.0182 |          62.3374 |          -2.6102 |
[32m[20221214 00:20:10 @agent_ppo2.py:185][0m |          -0.0181 |          61.1497 |          -2.4965 |
[32m[20221214 00:20:10 @agent_ppo2.py:185][0m |          -0.0174 |          60.4442 |          -2.4130 |
[32m[20221214 00:20:10 @agent_ppo2.py:185][0m |          -0.0175 |          59.7137 |          -2.4915 |
[32m[20221214 00:20:10 @agent_ppo2.py:185][0m |          -0.0226 |          58.7864 |          -2.3374 |
[32m[20221214 00:20:10 @agent_ppo2.py:185][0m |          -0.0200 |          58.2929 |          -2.2908 |
[32m[20221214 00:20:10 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:20:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 523.70
[32m[20221214 00:20:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 600.14
[32m[20221214 00:20:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 375.05
[32m[20221214 00:20:10 @agent_ppo2.py:143][0m Total time:      22.67 min
[32m[20221214 00:20:10 @agent_ppo2.py:145][0m 2080768 total steps have happened
[32m[20221214 00:20:10 @agent_ppo2.py:121][0m #------------------------ Iteration 5016 --------------------------#
[32m[20221214 00:20:11 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:20:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:20:11 @agent_ppo2.py:185][0m |           0.0068 |          77.5933 |          -3.6017 |
[32m[20221214 00:20:11 @agent_ppo2.py:185][0m |          -0.0038 |          70.6702 |          -3.6369 |
[32m[20221214 00:20:11 @agent_ppo2.py:185][0m |          -0.0063 |          67.1890 |          -3.7870 |
[32m[20221214 00:20:11 @agent_ppo2.py:185][0m |          -0.0072 |          65.0548 |          -3.7906 |
[32m[20221214 00:20:11 @agent_ppo2.py:185][0m |          -0.0108 |          63.8133 |          -3.8645 |
[32m[20221214 00:20:11 @agent_ppo2.py:185][0m |          -0.0145 |          62.9014 |          -4.0026 |
[32m[20221214 00:20:11 @agent_ppo2.py:185][0m |           0.0005 |          68.4017 |          -3.9288 |
[32m[20221214 00:20:11 @agent_ppo2.py:185][0m |          -0.0181 |          61.4656 |          -3.9969 |
[32m[20221214 00:20:12 @agent_ppo2.py:185][0m |          -0.0104 |          60.7882 |          -4.0073 |
[32m[20221214 00:20:12 @agent_ppo2.py:185][0m |          -0.0166 |          60.1195 |          -3.9982 |
[32m[20221214 00:20:12 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:20:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 542.45
[32m[20221214 00:20:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 574.89
[32m[20221214 00:20:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 590.21
[32m[20221214 00:20:12 @agent_ppo2.py:143][0m Total time:      22.69 min
[32m[20221214 00:20:12 @agent_ppo2.py:145][0m 2082816 total steps have happened
[32m[20221214 00:20:12 @agent_ppo2.py:121][0m #------------------------ Iteration 5017 --------------------------#
[32m[20221214 00:20:12 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:20:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:20:12 @agent_ppo2.py:185][0m |           0.0038 |          86.5222 |          -2.6080 |
[32m[20221214 00:20:12 @agent_ppo2.py:185][0m |          -0.0045 |          83.2460 |          -2.6048 |
[32m[20221214 00:20:12 @agent_ppo2.py:185][0m |          -0.0074 |          81.9110 |          -2.5765 |
[32m[20221214 00:20:12 @agent_ppo2.py:185][0m |          -0.0109 |          81.3553 |          -2.6322 |
[32m[20221214 00:20:13 @agent_ppo2.py:185][0m |          -0.0077 |          80.8505 |          -2.5755 |
[32m[20221214 00:20:13 @agent_ppo2.py:185][0m |          -0.0121 |          80.1337 |          -2.5916 |
[32m[20221214 00:20:13 @agent_ppo2.py:185][0m |          -0.0122 |          79.6982 |          -2.5889 |
[32m[20221214 00:20:13 @agent_ppo2.py:185][0m |          -0.0134 |          79.6071 |          -2.5817 |
[32m[20221214 00:20:13 @agent_ppo2.py:185][0m |          -0.0172 |          79.4252 |          -2.6127 |
[32m[20221214 00:20:13 @agent_ppo2.py:185][0m |          -0.0111 |          80.5666 |          -2.6237 |
[32m[20221214 00:20:13 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:20:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 576.74
[32m[20221214 00:20:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 631.45
[32m[20221214 00:20:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 445.01
[32m[20221214 00:20:13 @agent_ppo2.py:143][0m Total time:      22.71 min
[32m[20221214 00:20:13 @agent_ppo2.py:145][0m 2084864 total steps have happened
[32m[20221214 00:20:13 @agent_ppo2.py:121][0m #------------------------ Iteration 5018 --------------------------#
[32m[20221214 00:20:13 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:20:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:20:14 @agent_ppo2.py:185][0m |           0.0033 |          80.1962 |          -1.8707 |
[32m[20221214 00:20:14 @agent_ppo2.py:185][0m |          -0.0027 |          74.4766 |          -2.0049 |
[32m[20221214 00:20:14 @agent_ppo2.py:185][0m |          -0.0087 |          72.1276 |          -1.8964 |
[32m[20221214 00:20:14 @agent_ppo2.py:185][0m |          -0.0139 |          71.1690 |          -2.0278 |
[32m[20221214 00:20:14 @agent_ppo2.py:185][0m |          -0.0154 |          69.9058 |          -2.0458 |
[32m[20221214 00:20:14 @agent_ppo2.py:185][0m |          -0.0120 |          69.4913 |          -2.1176 |
[32m[20221214 00:20:14 @agent_ppo2.py:185][0m |          -0.0128 |          68.6536 |          -2.2280 |
[32m[20221214 00:20:14 @agent_ppo2.py:185][0m |          -0.0179 |          67.9956 |          -2.1664 |
[32m[20221214 00:20:14 @agent_ppo2.py:185][0m |          -0.0200 |          67.5613 |          -2.2195 |
[32m[20221214 00:20:14 @agent_ppo2.py:185][0m |          -0.0198 |          67.0195 |          -2.2484 |
[32m[20221214 00:20:14 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:20:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 473.67
[32m[20221214 00:20:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 522.80
[32m[20221214 00:20:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 584.34
[32m[20221214 00:20:15 @agent_ppo2.py:143][0m Total time:      22.74 min
[32m[20221214 00:20:15 @agent_ppo2.py:145][0m 2086912 total steps have happened
[32m[20221214 00:20:15 @agent_ppo2.py:121][0m #------------------------ Iteration 5019 --------------------------#
[32m[20221214 00:20:15 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:20:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:20:15 @agent_ppo2.py:185][0m |           0.0084 |          54.7951 |          -3.1065 |
[32m[20221214 00:20:15 @agent_ppo2.py:185][0m |          -0.0050 |          48.8430 |          -2.8999 |
[32m[20221214 00:20:15 @agent_ppo2.py:185][0m |          -0.0062 |          47.2687 |          -2.9435 |
[32m[20221214 00:20:15 @agent_ppo2.py:185][0m |          -0.0128 |          46.1465 |          -3.0121 |
[32m[20221214 00:20:15 @agent_ppo2.py:185][0m |          -0.0132 |          45.4886 |          -3.0522 |
[32m[20221214 00:20:15 @agent_ppo2.py:185][0m |          -0.0118 |          44.9136 |          -2.9449 |
[32m[20221214 00:20:15 @agent_ppo2.py:185][0m |          -0.0162 |          44.4378 |          -2.9089 |
[32m[20221214 00:20:16 @agent_ppo2.py:185][0m |          -0.0155 |          44.1902 |          -2.8997 |
[32m[20221214 00:20:16 @agent_ppo2.py:185][0m |          -0.0183 |          43.8592 |          -2.8605 |
[32m[20221214 00:20:16 @agent_ppo2.py:185][0m |          -0.0205 |          43.5182 |          -2.8786 |
[32m[20221214 00:20:16 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:20:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 431.06
[32m[20221214 00:20:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 536.69
[32m[20221214 00:20:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 469.45
[32m[20221214 00:20:16 @agent_ppo2.py:143][0m Total time:      22.76 min
[32m[20221214 00:20:16 @agent_ppo2.py:145][0m 2088960 total steps have happened
[32m[20221214 00:20:16 @agent_ppo2.py:121][0m #------------------------ Iteration 5020 --------------------------#
[32m[20221214 00:20:16 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:20:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:20:16 @agent_ppo2.py:185][0m |           0.0100 |          94.8570 |          -3.6237 |
[32m[20221214 00:20:16 @agent_ppo2.py:185][0m |           0.0110 |          75.5382 |          -3.5861 |
[32m[20221214 00:20:17 @agent_ppo2.py:185][0m |          -0.0007 |          72.2805 |          -3.5412 |
[32m[20221214 00:20:17 @agent_ppo2.py:185][0m |          -0.0079 |          68.9791 |          -3.4969 |
[32m[20221214 00:20:17 @agent_ppo2.py:185][0m |          -0.0114 |          67.6314 |          -3.4829 |
[32m[20221214 00:20:17 @agent_ppo2.py:185][0m |          -0.0053 |          66.5557 |          -3.6048 |
[32m[20221214 00:20:17 @agent_ppo2.py:185][0m |          -0.0143 |          65.7907 |          -3.5163 |
[32m[20221214 00:20:17 @agent_ppo2.py:185][0m |          -0.0122 |          65.2692 |          -3.4790 |
[32m[20221214 00:20:17 @agent_ppo2.py:185][0m |          -0.0154 |          64.8118 |          -3.4911 |
[32m[20221214 00:20:17 @agent_ppo2.py:185][0m |          -0.0161 |          63.9888 |          -3.6891 |
[32m[20221214 00:20:17 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:20:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 545.78
[32m[20221214 00:20:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 609.89
[32m[20221214 00:20:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 280.11
[32m[20221214 00:20:17 @agent_ppo2.py:143][0m Total time:      22.78 min
[32m[20221214 00:20:17 @agent_ppo2.py:145][0m 2091008 total steps have happened
[32m[20221214 00:20:17 @agent_ppo2.py:121][0m #------------------------ Iteration 5021 --------------------------#
[32m[20221214 00:20:18 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:20:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:20:18 @agent_ppo2.py:185][0m |           0.0117 |          65.9310 |          -2.7988 |
[32m[20221214 00:20:18 @agent_ppo2.py:185][0m |          -0.0044 |          52.3634 |          -2.7161 |
[32m[20221214 00:20:18 @agent_ppo2.py:185][0m |          -0.0121 |          48.7860 |          -2.7001 |
[32m[20221214 00:20:18 @agent_ppo2.py:185][0m |          -0.0115 |          46.6982 |          -2.6807 |
[32m[20221214 00:20:18 @agent_ppo2.py:185][0m |          -0.0121 |          45.7078 |          -2.7310 |
[32m[20221214 00:20:18 @agent_ppo2.py:185][0m |          -0.0169 |          44.6343 |          -2.6545 |
[32m[20221214 00:20:18 @agent_ppo2.py:185][0m |          -0.0192 |          43.8080 |          -2.6261 |
[32m[20221214 00:20:18 @agent_ppo2.py:185][0m |          -0.0184 |          43.0657 |          -2.5237 |
[32m[20221214 00:20:18 @agent_ppo2.py:185][0m |          -0.0202 |          42.0083 |          -2.5145 |
[32m[20221214 00:20:19 @agent_ppo2.py:185][0m |          -0.0239 |          41.4989 |          -2.5322 |
[32m[20221214 00:20:19 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:20:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 503.14
[32m[20221214 00:20:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 580.30
[32m[20221214 00:20:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 402.28
[32m[20221214 00:20:19 @agent_ppo2.py:143][0m Total time:      22.81 min
[32m[20221214 00:20:19 @agent_ppo2.py:145][0m 2093056 total steps have happened
[32m[20221214 00:20:19 @agent_ppo2.py:121][0m #------------------------ Iteration 5022 --------------------------#
[32m[20221214 00:20:19 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:20:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:20:19 @agent_ppo2.py:185][0m |           0.0023 |          64.5216 |          -3.5186 |
[32m[20221214 00:20:19 @agent_ppo2.py:185][0m |           0.0025 |          54.1758 |          -3.7139 |
[32m[20221214 00:20:19 @agent_ppo2.py:185][0m |          -0.0053 |          51.0135 |          -3.3337 |
[32m[20221214 00:20:19 @agent_ppo2.py:185][0m |           0.0003 |          51.9404 |          -3.7116 |
[32m[20221214 00:20:19 @agent_ppo2.py:185][0m |          -0.0040 |          47.7309 |          -3.6759 |
[32m[20221214 00:20:20 @agent_ppo2.py:185][0m |          -0.0119 |          46.0477 |          -3.6070 |
[32m[20221214 00:20:20 @agent_ppo2.py:185][0m |          -0.0094 |          45.3715 |          -3.7955 |
[32m[20221214 00:20:20 @agent_ppo2.py:185][0m |          -0.0123 |          44.6960 |          -3.6854 |
[32m[20221214 00:20:20 @agent_ppo2.py:185][0m |          -0.0099 |          43.9733 |          -3.7950 |
[32m[20221214 00:20:20 @agent_ppo2.py:185][0m |          -0.0084 |          43.4908 |          -3.6222 |
[32m[20221214 00:20:20 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:20:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 482.33
[32m[20221214 00:20:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 553.64
[32m[20221214 00:20:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 349.80
[32m[20221214 00:20:20 @agent_ppo2.py:143][0m Total time:      22.83 min
[32m[20221214 00:20:20 @agent_ppo2.py:145][0m 2095104 total steps have happened
[32m[20221214 00:20:20 @agent_ppo2.py:121][0m #------------------------ Iteration 5023 --------------------------#
[32m[20221214 00:20:20 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:20:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:20:20 @agent_ppo2.py:185][0m |           0.0076 |          32.3940 |          -4.2529 |
[32m[20221214 00:20:21 @agent_ppo2.py:185][0m |          -0.0058 |          26.9890 |          -4.2579 |
[32m[20221214 00:20:21 @agent_ppo2.py:185][0m |          -0.0122 |          23.8954 |          -4.1853 |
[32m[20221214 00:20:21 @agent_ppo2.py:185][0m |          -0.0076 |          22.6060 |          -4.2050 |
[32m[20221214 00:20:21 @agent_ppo2.py:185][0m |          -0.0160 |          21.0261 |          -4.1889 |
[32m[20221214 00:20:21 @agent_ppo2.py:185][0m |          -0.0187 |          20.2553 |          -4.2227 |
[32m[20221214 00:20:21 @agent_ppo2.py:185][0m |          -0.0153 |          20.0736 |          -4.3607 |
[32m[20221214 00:20:21 @agent_ppo2.py:185][0m |          -0.0217 |          19.0942 |          -4.2873 |
[32m[20221214 00:20:21 @agent_ppo2.py:185][0m |          -0.0195 |          18.6471 |          -4.2814 |
[32m[20221214 00:20:21 @agent_ppo2.py:185][0m |          -0.0212 |          18.2943 |          -4.4105 |
[32m[20221214 00:20:21 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:20:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 413.19
[32m[20221214 00:20:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 539.80
[32m[20221214 00:20:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 593.45
[32m[20221214 00:20:22 @agent_ppo2.py:143][0m Total time:      22.85 min
[32m[20221214 00:20:22 @agent_ppo2.py:145][0m 2097152 total steps have happened
[32m[20221214 00:20:22 @agent_ppo2.py:121][0m #------------------------ Iteration 5024 --------------------------#
[32m[20221214 00:20:22 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:20:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:20:22 @agent_ppo2.py:185][0m |          -0.0003 |          39.8347 |          -3.6257 |
[32m[20221214 00:20:22 @agent_ppo2.py:185][0m |          -0.0083 |          33.3021 |          -3.7662 |
[32m[20221214 00:20:22 @agent_ppo2.py:185][0m |          -0.0083 |          31.3918 |          -3.7972 |
[32m[20221214 00:20:22 @agent_ppo2.py:185][0m |          -0.0150 |          29.4463 |          -3.8839 |
[32m[20221214 00:20:22 @agent_ppo2.py:185][0m |          -0.0186 |          28.1162 |          -3.9487 |
[32m[20221214 00:20:22 @agent_ppo2.py:185][0m |          -0.0189 |          26.9213 |          -3.9241 |
[32m[20221214 00:20:22 @agent_ppo2.py:185][0m |          -0.0218 |          26.3078 |          -4.0100 |
[32m[20221214 00:20:23 @agent_ppo2.py:185][0m |          -0.0201 |          25.7403 |          -4.1134 |
[32m[20221214 00:20:23 @agent_ppo2.py:185][0m |          -0.0252 |          25.1007 |          -4.0619 |
[32m[20221214 00:20:23 @agent_ppo2.py:185][0m |          -0.0194 |          25.4629 |          -4.1223 |
[32m[20221214 00:20:23 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:20:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 419.89
[32m[20221214 00:20:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 519.61
[32m[20221214 00:20:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 220.36
[32m[20221214 00:20:23 @agent_ppo2.py:143][0m Total time:      22.88 min
[32m[20221214 00:20:23 @agent_ppo2.py:145][0m 2099200 total steps have happened
[32m[20221214 00:20:23 @agent_ppo2.py:121][0m #------------------------ Iteration 5025 --------------------------#
[32m[20221214 00:20:23 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:20:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:20:23 @agent_ppo2.py:185][0m |          -0.0011 |          85.3234 |          -4.5712 |
[32m[20221214 00:20:23 @agent_ppo2.py:185][0m |          -0.0085 |          72.9706 |          -4.4870 |
[32m[20221214 00:20:23 @agent_ppo2.py:185][0m |          -0.0117 |          68.4992 |          -4.5562 |
[32m[20221214 00:20:24 @agent_ppo2.py:185][0m |          -0.0092 |          67.6808 |          -4.4835 |
[32m[20221214 00:20:24 @agent_ppo2.py:185][0m |          -0.0173 |          65.6326 |          -4.4573 |
[32m[20221214 00:20:24 @agent_ppo2.py:185][0m |          -0.0136 |          63.4799 |          -4.3518 |
[32m[20221214 00:20:24 @agent_ppo2.py:185][0m |          -0.0181 |          61.4981 |          -4.2994 |
[32m[20221214 00:20:24 @agent_ppo2.py:185][0m |          -0.0188 |          60.6008 |          -4.1915 |
[32m[20221214 00:20:24 @agent_ppo2.py:185][0m |          -0.0111 |          60.4418 |          -4.2182 |
[32m[20221214 00:20:24 @agent_ppo2.py:185][0m |          -0.0156 |          59.5120 |          -4.1410 |
[32m[20221214 00:20:24 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:20:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 508.36
[32m[20221214 00:20:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 612.14
[32m[20221214 00:20:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 222.74
[32m[20221214 00:20:24 @agent_ppo2.py:143][0m Total time:      22.90 min
[32m[20221214 00:20:24 @agent_ppo2.py:145][0m 2101248 total steps have happened
[32m[20221214 00:20:24 @agent_ppo2.py:121][0m #------------------------ Iteration 5026 --------------------------#
[32m[20221214 00:20:24 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:20:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:20:25 @agent_ppo2.py:185][0m |          -0.0001 |          70.3028 |          -4.6497 |
[32m[20221214 00:20:25 @agent_ppo2.py:185][0m |          -0.0012 |          59.8817 |          -4.7259 |
[32m[20221214 00:20:25 @agent_ppo2.py:185][0m |          -0.0062 |          54.5118 |          -4.6184 |
[32m[20221214 00:20:25 @agent_ppo2.py:185][0m |          -0.0155 |          51.3803 |          -4.5760 |
[32m[20221214 00:20:25 @agent_ppo2.py:185][0m |          -0.0141 |          49.9865 |          -4.6919 |
[32m[20221214 00:20:25 @agent_ppo2.py:185][0m |          -0.0220 |          48.5945 |          -4.6402 |
[32m[20221214 00:20:25 @agent_ppo2.py:185][0m |          -0.0124 |          47.8145 |          -4.6845 |
[32m[20221214 00:20:25 @agent_ppo2.py:185][0m |          -0.0130 |          48.8034 |          -4.7014 |
[32m[20221214 00:20:25 @agent_ppo2.py:185][0m |          -0.0187 |          46.1641 |          -4.7095 |
[32m[20221214 00:20:26 @agent_ppo2.py:185][0m |          -0.0226 |          45.4146 |          -4.7143 |
[32m[20221214 00:20:26 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:20:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 534.43
[32m[20221214 00:20:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 631.34
[32m[20221214 00:20:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 524.03
[32m[20221214 00:20:26 @agent_ppo2.py:143][0m Total time:      22.92 min
[32m[20221214 00:20:26 @agent_ppo2.py:145][0m 2103296 total steps have happened
[32m[20221214 00:20:26 @agent_ppo2.py:121][0m #------------------------ Iteration 5027 --------------------------#
[32m[20221214 00:20:26 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:20:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:20:26 @agent_ppo2.py:185][0m |          -0.0013 |          94.5065 |          -3.7490 |
[32m[20221214 00:20:26 @agent_ppo2.py:185][0m |          -0.0035 |          82.4868 |          -3.5796 |
[32m[20221214 00:20:26 @agent_ppo2.py:185][0m |          -0.0099 |          78.2481 |          -3.5746 |
[32m[20221214 00:20:26 @agent_ppo2.py:185][0m |          -0.0123 |          76.1036 |          -3.4229 |
[32m[20221214 00:20:26 @agent_ppo2.py:185][0m |          -0.0114 |          74.6514 |          -3.4910 |
[32m[20221214 00:20:27 @agent_ppo2.py:185][0m |          -0.0152 |          73.2259 |          -3.4087 |
[32m[20221214 00:20:27 @agent_ppo2.py:185][0m |          -0.0173 |          72.2104 |          -3.4050 |
[32m[20221214 00:20:27 @agent_ppo2.py:185][0m |          -0.0202 |          71.7459 |          -3.4104 |
[32m[20221214 00:20:27 @agent_ppo2.py:185][0m |          -0.0181 |          70.5390 |          -3.3210 |
[32m[20221214 00:20:27 @agent_ppo2.py:185][0m |          -0.0163 |          70.1407 |          -3.3226 |
[32m[20221214 00:20:27 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:20:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 514.62
[32m[20221214 00:20:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 554.39
[32m[20221214 00:20:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 545.47
[32m[20221214 00:20:27 @agent_ppo2.py:143][0m Total time:      22.95 min
[32m[20221214 00:20:27 @agent_ppo2.py:145][0m 2105344 total steps have happened
[32m[20221214 00:20:27 @agent_ppo2.py:121][0m #------------------------ Iteration 5028 --------------------------#
[32m[20221214 00:20:27 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:20:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:20:27 @agent_ppo2.py:185][0m |           0.0044 |          72.0693 |          -3.2719 |
[32m[20221214 00:20:28 @agent_ppo2.py:185][0m |          -0.0034 |          68.0782 |          -3.2848 |
[32m[20221214 00:20:28 @agent_ppo2.py:185][0m |          -0.0049 |          66.7002 |          -3.1788 |
[32m[20221214 00:20:28 @agent_ppo2.py:185][0m |          -0.0080 |          66.0147 |          -3.0863 |
[32m[20221214 00:20:28 @agent_ppo2.py:185][0m |          -0.0085 |          64.9885 |          -2.9346 |
[32m[20221214 00:20:28 @agent_ppo2.py:185][0m |          -0.0104 |          64.5827 |          -2.9848 |
[32m[20221214 00:20:28 @agent_ppo2.py:185][0m |          -0.0102 |          64.1631 |          -2.8443 |
[32m[20221214 00:20:28 @agent_ppo2.py:185][0m |          -0.0128 |          63.6295 |          -2.8944 |
[32m[20221214 00:20:28 @agent_ppo2.py:185][0m |          -0.0117 |          63.2160 |          -2.8466 |
[32m[20221214 00:20:28 @agent_ppo2.py:185][0m |          -0.0153 |          63.1559 |          -2.8499 |
[32m[20221214 00:20:28 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221214 00:20:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 614.97
[32m[20221214 00:20:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 651.05
[32m[20221214 00:20:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 482.89
[32m[20221214 00:20:29 @agent_ppo2.py:143][0m Total time:      22.97 min
[32m[20221214 00:20:29 @agent_ppo2.py:145][0m 2107392 total steps have happened
[32m[20221214 00:20:29 @agent_ppo2.py:121][0m #------------------------ Iteration 5029 --------------------------#
[32m[20221214 00:20:29 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221214 00:20:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:20:29 @agent_ppo2.py:185][0m |          -0.0004 |          39.5598 |          -2.7661 |
[32m[20221214 00:20:29 @agent_ppo2.py:185][0m |          -0.0054 |          29.2401 |          -2.7261 |
[32m[20221214 00:20:29 @agent_ppo2.py:185][0m |          -0.0092 |          25.8471 |          -2.7197 |
[32m[20221214 00:20:29 @agent_ppo2.py:185][0m |          -0.0124 |          24.4609 |          -2.6771 |
[32m[20221214 00:20:30 @agent_ppo2.py:185][0m |          -0.0173 |          23.6573 |          -2.5360 |
[32m[20221214 00:20:30 @agent_ppo2.py:185][0m |          -0.0172 |          22.8101 |          -2.6804 |
[32m[20221214 00:20:30 @agent_ppo2.py:185][0m |          -0.0196 |          22.5294 |          -2.5814 |
[32m[20221214 00:20:30 @agent_ppo2.py:185][0m |          -0.0211 |          21.8646 |          -2.6013 |
[32m[20221214 00:20:30 @agent_ppo2.py:185][0m |          -0.0169 |          21.5873 |          -2.5864 |
[32m[20221214 00:20:30 @agent_ppo2.py:185][0m |          -0.0157 |          22.3948 |          -2.5683 |
[32m[20221214 00:20:30 @agent_ppo2.py:130][0m Policy update time: 1.30 s
[32m[20221214 00:20:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 407.85
[32m[20221214 00:20:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 512.07
[32m[20221214 00:20:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 629.49
[32m[20221214 00:20:30 @agent_ppo2.py:143][0m Total time:      23.00 min
[32m[20221214 00:20:30 @agent_ppo2.py:145][0m 2109440 total steps have happened
[32m[20221214 00:20:30 @agent_ppo2.py:121][0m #------------------------ Iteration 5030 --------------------------#
[32m[20221214 00:20:30 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:20:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:20:31 @agent_ppo2.py:185][0m |          -0.0003 |          83.9881 |          -3.5826 |
[32m[20221214 00:20:31 @agent_ppo2.py:185][0m |          -0.0038 |          77.2048 |          -3.5938 |
[32m[20221214 00:20:31 @agent_ppo2.py:185][0m |          -0.0077 |          74.0228 |          -3.4997 |
[32m[20221214 00:20:31 @agent_ppo2.py:185][0m |          -0.0094 |          72.1854 |          -3.6633 |
[32m[20221214 00:20:31 @agent_ppo2.py:185][0m |          -0.0081 |          70.6994 |          -3.6008 |
[32m[20221214 00:20:31 @agent_ppo2.py:185][0m |          -0.0064 |          70.1253 |          -3.6840 |
[32m[20221214 00:20:31 @agent_ppo2.py:185][0m |          -0.0055 |          69.7778 |          -3.6076 |
[32m[20221214 00:20:31 @agent_ppo2.py:185][0m |          -0.0121 |          68.5492 |          -3.5235 |
[32m[20221214 00:20:31 @agent_ppo2.py:185][0m |          -0.0064 |          70.2397 |          -3.5533 |
[32m[20221214 00:20:32 @agent_ppo2.py:185][0m |          -0.0121 |          67.1993 |          -3.5956 |
[32m[20221214 00:20:32 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:20:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 535.38
[32m[20221214 00:20:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 608.53
[32m[20221214 00:20:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 500.87
[32m[20221214 00:20:32 @agent_ppo2.py:143][0m Total time:      23.02 min
[32m[20221214 00:20:32 @agent_ppo2.py:145][0m 2111488 total steps have happened
[32m[20221214 00:20:32 @agent_ppo2.py:121][0m #------------------------ Iteration 5031 --------------------------#
[32m[20221214 00:20:32 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:20:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:20:32 @agent_ppo2.py:185][0m |           0.0027 |          54.9346 |          -3.9390 |
[32m[20221214 00:20:32 @agent_ppo2.py:185][0m |          -0.0066 |          45.2808 |          -4.0358 |
[32m[20221214 00:20:32 @agent_ppo2.py:185][0m |          -0.0103 |          42.8570 |          -3.9285 |
[32m[20221214 00:20:32 @agent_ppo2.py:185][0m |          -0.0112 |          41.5488 |          -3.9185 |
[32m[20221214 00:20:32 @agent_ppo2.py:185][0m |          -0.0118 |          40.8522 |          -3.9155 |
[32m[20221214 00:20:33 @agent_ppo2.py:185][0m |          -0.0137 |          40.2134 |          -3.8429 |
[32m[20221214 00:20:33 @agent_ppo2.py:185][0m |          -0.0097 |          40.1084 |          -3.8745 |
[32m[20221214 00:20:33 @agent_ppo2.py:185][0m |          -0.0198 |          38.8643 |          -3.6340 |
[32m[20221214 00:20:33 @agent_ppo2.py:185][0m |          -0.0150 |          38.6457 |          -3.7791 |
[32m[20221214 00:20:33 @agent_ppo2.py:185][0m |          -0.0143 |          38.1988 |          -3.7430 |
[32m[20221214 00:20:33 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:20:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 467.10
[32m[20221214 00:20:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 521.88
[32m[20221214 00:20:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 636.00
[32m[20221214 00:20:33 @agent_ppo2.py:143][0m Total time:      23.05 min
[32m[20221214 00:20:33 @agent_ppo2.py:145][0m 2113536 total steps have happened
[32m[20221214 00:20:33 @agent_ppo2.py:121][0m #------------------------ Iteration 5032 --------------------------#
[32m[20221214 00:20:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:20:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:20:33 @agent_ppo2.py:185][0m |           0.0141 |          43.8539 |          -4.0505 |
[32m[20221214 00:20:34 @agent_ppo2.py:185][0m |          -0.0013 |          34.6932 |          -4.0255 |
[32m[20221214 00:20:34 @agent_ppo2.py:185][0m |          -0.0086 |          33.3495 |          -3.9621 |
[32m[20221214 00:20:34 @agent_ppo2.py:185][0m |          -0.0127 |          32.2897 |          -3.9407 |
[32m[20221214 00:20:34 @agent_ppo2.py:185][0m |          -0.0114 |          31.1286 |          -3.9425 |
[32m[20221214 00:20:34 @agent_ppo2.py:185][0m |          -0.0104 |          30.2337 |          -3.8449 |
[32m[20221214 00:20:34 @agent_ppo2.py:185][0m |          -0.0090 |          29.8506 |          -3.9026 |
[32m[20221214 00:20:34 @agent_ppo2.py:185][0m |          -0.0116 |          29.3457 |          -3.7031 |
[32m[20221214 00:20:34 @agent_ppo2.py:185][0m |          -0.0126 |          28.8953 |          -3.8966 |
[32m[20221214 00:20:34 @agent_ppo2.py:185][0m |          -0.0189 |          28.8661 |          -3.8170 |
[32m[20221214 00:20:34 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:20:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 490.73
[32m[20221214 00:20:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 525.01
[32m[20221214 00:20:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 586.00
[32m[20221214 00:20:34 @agent_ppo2.py:143][0m Total time:      23.07 min
[32m[20221214 00:20:34 @agent_ppo2.py:145][0m 2115584 total steps have happened
[32m[20221214 00:20:35 @agent_ppo2.py:121][0m #------------------------ Iteration 5033 --------------------------#
[32m[20221214 00:20:35 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:20:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:20:35 @agent_ppo2.py:185][0m |           0.0033 |          67.3423 |          -4.2477 |
[32m[20221214 00:20:35 @agent_ppo2.py:185][0m |          -0.0038 |          62.2752 |          -4.2403 |
[32m[20221214 00:20:35 @agent_ppo2.py:185][0m |          -0.0088 |          60.2109 |          -4.2144 |
[32m[20221214 00:20:35 @agent_ppo2.py:185][0m |          -0.0104 |          59.2642 |          -4.2065 |
[32m[20221214 00:20:35 @agent_ppo2.py:185][0m |          -0.0128 |          58.2023 |          -4.2556 |
[32m[20221214 00:20:35 @agent_ppo2.py:185][0m |          -0.0112 |          57.3765 |          -4.2634 |
[32m[20221214 00:20:35 @agent_ppo2.py:185][0m |          -0.0093 |          57.8457 |          -4.2840 |
[32m[20221214 00:20:36 @agent_ppo2.py:185][0m |          -0.0094 |          57.1481 |          -4.2977 |
[32m[20221214 00:20:36 @agent_ppo2.py:185][0m |          -0.0118 |          55.8812 |          -4.2759 |
[32m[20221214 00:20:36 @agent_ppo2.py:185][0m |          -0.0109 |          55.2937 |          -4.3365 |
[32m[20221214 00:20:36 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 00:20:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 490.62
[32m[20221214 00:20:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 528.96
[32m[20221214 00:20:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 601.49
[32m[20221214 00:20:36 @agent_ppo2.py:143][0m Total time:      23.09 min
[32m[20221214 00:20:36 @agent_ppo2.py:145][0m 2117632 total steps have happened
[32m[20221214 00:20:36 @agent_ppo2.py:121][0m #------------------------ Iteration 5034 --------------------------#
[32m[20221214 00:20:36 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:20:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:20:36 @agent_ppo2.py:185][0m |           0.0035 |          56.9447 |          -4.2544 |
[32m[20221214 00:20:36 @agent_ppo2.py:185][0m |          -0.0070 |          49.3671 |          -4.2140 |
[32m[20221214 00:20:37 @agent_ppo2.py:185][0m |          -0.0113 |          46.9669 |          -4.0735 |
[32m[20221214 00:20:37 @agent_ppo2.py:185][0m |          -0.0094 |          45.2055 |          -4.2817 |
[32m[20221214 00:20:37 @agent_ppo2.py:185][0m |          -0.0146 |          44.1541 |          -4.2585 |
[32m[20221214 00:20:37 @agent_ppo2.py:185][0m |          -0.0108 |          43.2212 |          -4.2831 |
[32m[20221214 00:20:37 @agent_ppo2.py:185][0m |          -0.0124 |          42.5802 |          -4.3501 |
[32m[20221214 00:20:37 @agent_ppo2.py:185][0m |          -0.0178 |          41.7986 |          -4.4061 |
[32m[20221214 00:20:37 @agent_ppo2.py:185][0m |          -0.0197 |          41.6244 |          -4.4960 |
[32m[20221214 00:20:37 @agent_ppo2.py:185][0m |          -0.0203 |          40.8543 |          -4.5779 |
[32m[20221214 00:20:37 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221214 00:20:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 407.70
[32m[20221214 00:20:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 543.51
[32m[20221214 00:20:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 649.33
[32m[20221214 00:20:37 @agent_ppo2.py:143][0m Total time:      23.12 min
[32m[20221214 00:20:37 @agent_ppo2.py:145][0m 2119680 total steps have happened
[32m[20221214 00:20:37 @agent_ppo2.py:121][0m #------------------------ Iteration 5035 --------------------------#
[32m[20221214 00:20:38 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:20:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:20:38 @agent_ppo2.py:185][0m |           0.0056 |          84.3928 |          -3.7935 |
[32m[20221214 00:20:38 @agent_ppo2.py:185][0m |          -0.0021 |          76.9573 |          -3.8516 |
[32m[20221214 00:20:38 @agent_ppo2.py:185][0m |          -0.0047 |          74.1534 |          -3.8101 |
[32m[20221214 00:20:38 @agent_ppo2.py:185][0m |          -0.0067 |          72.1334 |          -3.7423 |
[32m[20221214 00:20:38 @agent_ppo2.py:185][0m |          -0.0090 |          70.6632 |          -3.7790 |
[32m[20221214 00:20:38 @agent_ppo2.py:185][0m |          -0.0108 |          70.4443 |          -3.7519 |
[32m[20221214 00:20:38 @agent_ppo2.py:185][0m |          -0.0117 |          69.1053 |          -3.7525 |
[32m[20221214 00:20:38 @agent_ppo2.py:185][0m |          -0.0113 |          68.3676 |          -3.6154 |
[32m[20221214 00:20:39 @agent_ppo2.py:185][0m |          -0.0114 |          68.1957 |          -3.7291 |
[32m[20221214 00:20:39 @agent_ppo2.py:185][0m |          -0.0099 |          67.2010 |          -3.6209 |
[32m[20221214 00:20:39 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:20:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 554.51
[32m[20221214 00:20:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 626.56
[32m[20221214 00:20:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 641.93
[32m[20221214 00:20:39 @agent_ppo2.py:143][0m Total time:      23.14 min
[32m[20221214 00:20:39 @agent_ppo2.py:145][0m 2121728 total steps have happened
[32m[20221214 00:20:39 @agent_ppo2.py:121][0m #------------------------ Iteration 5036 --------------------------#
[32m[20221214 00:20:39 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:20:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:20:39 @agent_ppo2.py:185][0m |          -0.0027 |          91.0097 |          -5.7661 |
[32m[20221214 00:20:39 @agent_ppo2.py:185][0m |          -0.0088 |          81.3887 |          -5.6824 |
[32m[20221214 00:20:39 @agent_ppo2.py:185][0m |          -0.0126 |          78.7492 |          -5.6995 |
[32m[20221214 00:20:39 @agent_ppo2.py:185][0m |          -0.0151 |          76.8281 |          -5.6261 |
[32m[20221214 00:20:40 @agent_ppo2.py:185][0m |          -0.0143 |          75.8330 |          -5.5688 |
[32m[20221214 00:20:40 @agent_ppo2.py:185][0m |          -0.0165 |          74.6416 |          -5.6068 |
[32m[20221214 00:20:40 @agent_ppo2.py:185][0m |          -0.0189 |          73.8152 |          -5.5999 |
[32m[20221214 00:20:40 @agent_ppo2.py:185][0m |          -0.0176 |          73.1609 |          -5.5798 |
[32m[20221214 00:20:40 @agent_ppo2.py:185][0m |          -0.0187 |          72.4881 |          -5.5031 |
[32m[20221214 00:20:40 @agent_ppo2.py:185][0m |          -0.0185 |          72.3215 |          -5.3677 |
[32m[20221214 00:20:40 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:20:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 564.14
[32m[20221214 00:20:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 587.96
[32m[20221214 00:20:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 313.60
[32m[20221214 00:20:40 @agent_ppo2.py:143][0m Total time:      23.17 min
[32m[20221214 00:20:40 @agent_ppo2.py:145][0m 2123776 total steps have happened
[32m[20221214 00:20:40 @agent_ppo2.py:121][0m #------------------------ Iteration 5037 --------------------------#
[32m[20221214 00:20:40 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:20:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:20:41 @agent_ppo2.py:185][0m |          -0.0011 |          69.8583 |          -3.2337 |
[32m[20221214 00:20:41 @agent_ppo2.py:185][0m |          -0.0119 |          63.8770 |          -3.2023 |
[32m[20221214 00:20:41 @agent_ppo2.py:185][0m |          -0.0034 |          61.6739 |          -3.1134 |
[32m[20221214 00:20:41 @agent_ppo2.py:185][0m |           0.0039 |          62.1133 |          -2.9948 |
[32m[20221214 00:20:41 @agent_ppo2.py:185][0m |          -0.0111 |          59.2101 |          -2.9575 |
[32m[20221214 00:20:41 @agent_ppo2.py:185][0m |          -0.0122 |          58.3951 |          -2.8671 |
[32m[20221214 00:20:41 @agent_ppo2.py:185][0m |          -0.0172 |          57.6735 |          -2.8793 |
[32m[20221214 00:20:41 @agent_ppo2.py:185][0m |          -0.0128 |          56.8759 |          -2.8901 |
[32m[20221214 00:20:41 @agent_ppo2.py:185][0m |          -0.0161 |          56.3235 |          -2.7635 |
[32m[20221214 00:20:42 @agent_ppo2.py:185][0m |          -0.0133 |          55.8602 |          -2.7993 |
[32m[20221214 00:20:42 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:20:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 484.30
[32m[20221214 00:20:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 510.52
[32m[20221214 00:20:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 617.28
[32m[20221214 00:20:42 @agent_ppo2.py:143][0m Total time:      23.19 min
[32m[20221214 00:20:42 @agent_ppo2.py:145][0m 2125824 total steps have happened
[32m[20221214 00:20:42 @agent_ppo2.py:121][0m #------------------------ Iteration 5038 --------------------------#
[32m[20221214 00:20:42 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:20:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:20:42 @agent_ppo2.py:185][0m |           0.0017 |          62.9578 |          -3.8811 |
[32m[20221214 00:20:42 @agent_ppo2.py:185][0m |          -0.0046 |          59.8605 |          -3.9369 |
[32m[20221214 00:20:42 @agent_ppo2.py:185][0m |          -0.0122 |          56.1589 |          -4.0521 |
[32m[20221214 00:20:42 @agent_ppo2.py:185][0m |          -0.0132 |          54.0315 |          -4.0842 |
[32m[20221214 00:20:42 @agent_ppo2.py:185][0m |          -0.0088 |          53.0548 |          -4.0740 |
[32m[20221214 00:20:43 @agent_ppo2.py:185][0m |          -0.0181 |          52.1158 |          -4.0535 |
[32m[20221214 00:20:43 @agent_ppo2.py:185][0m |          -0.0093 |          51.4106 |          -4.0950 |
[32m[20221214 00:20:43 @agent_ppo2.py:185][0m |          -0.0086 |          54.6159 |          -4.0689 |
[32m[20221214 00:20:43 @agent_ppo2.py:185][0m |          -0.0139 |          50.0638 |          -4.1694 |
[32m[20221214 00:20:43 @agent_ppo2.py:185][0m |          -0.0228 |          49.0435 |          -4.1385 |
[32m[20221214 00:20:43 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:20:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 382.97
[32m[20221214 00:20:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 557.23
[32m[20221214 00:20:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 585.38
[32m[20221214 00:20:43 @agent_ppo2.py:143][0m Total time:      23.21 min
[32m[20221214 00:20:43 @agent_ppo2.py:145][0m 2127872 total steps have happened
[32m[20221214 00:20:43 @agent_ppo2.py:121][0m #------------------------ Iteration 5039 --------------------------#
[32m[20221214 00:20:43 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:20:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:20:43 @agent_ppo2.py:185][0m |           0.0104 |          78.0834 |          -4.2936 |
[32m[20221214 00:20:44 @agent_ppo2.py:185][0m |          -0.0079 |          66.2614 |          -4.1757 |
[32m[20221214 00:20:44 @agent_ppo2.py:185][0m |          -0.0074 |          64.9970 |          -4.1959 |
[32m[20221214 00:20:44 @agent_ppo2.py:185][0m |          -0.0082 |          64.2779 |          -4.0869 |
[32m[20221214 00:20:44 @agent_ppo2.py:185][0m |          -0.0163 |          63.0640 |          -4.1045 |
[32m[20221214 00:20:44 @agent_ppo2.py:185][0m |          -0.0110 |          62.6190 |          -4.1009 |
[32m[20221214 00:20:44 @agent_ppo2.py:185][0m |          -0.0174 |          61.8567 |          -4.0188 |
[32m[20221214 00:20:44 @agent_ppo2.py:185][0m |          -0.0151 |          61.8309 |          -4.0952 |
[32m[20221214 00:20:44 @agent_ppo2.py:185][0m |          -0.0132 |          61.3547 |          -4.0278 |
[32m[20221214 00:20:44 @agent_ppo2.py:185][0m |          -0.0200 |          60.9923 |          -4.0669 |
[32m[20221214 00:20:44 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:20:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 480.64
[32m[20221214 00:20:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 544.07
[32m[20221214 00:20:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 628.53
[32m[20221214 00:20:44 @agent_ppo2.py:143][0m Total time:      23.24 min
[32m[20221214 00:20:44 @agent_ppo2.py:145][0m 2129920 total steps have happened
[32m[20221214 00:20:44 @agent_ppo2.py:121][0m #------------------------ Iteration 5040 --------------------------#
[32m[20221214 00:20:45 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:20:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:20:45 @agent_ppo2.py:185][0m |          -0.0021 |          87.0157 |          -4.4635 |
[32m[20221214 00:20:45 @agent_ppo2.py:185][0m |          -0.0089 |          83.2559 |          -4.4903 |
[32m[20221214 00:20:45 @agent_ppo2.py:185][0m |          -0.0110 |          81.6434 |          -4.4532 |
[32m[20221214 00:20:45 @agent_ppo2.py:185][0m |          -0.0103 |          79.9423 |          -4.5567 |
[32m[20221214 00:20:45 @agent_ppo2.py:185][0m |          -0.0131 |          79.9714 |          -4.4561 |
[32m[20221214 00:20:45 @agent_ppo2.py:185][0m |          -0.0022 |          88.0371 |          -4.6381 |
[32m[20221214 00:20:45 @agent_ppo2.py:185][0m |          -0.0156 |          78.2870 |          -4.5072 |
[32m[20221214 00:20:46 @agent_ppo2.py:185][0m |          -0.0158 |          78.1023 |          -4.6189 |
[32m[20221214 00:20:46 @agent_ppo2.py:185][0m |          -0.0178 |          77.5771 |          -4.6075 |
[32m[20221214 00:20:46 @agent_ppo2.py:185][0m |          -0.0173 |          77.2025 |          -4.5943 |
[32m[20221214 00:20:46 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:20:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 546.24
[32m[20221214 00:20:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 605.13
[32m[20221214 00:20:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 576.97
[32m[20221214 00:20:46 @agent_ppo2.py:143][0m Total time:      23.26 min
[32m[20221214 00:20:46 @agent_ppo2.py:145][0m 2131968 total steps have happened
[32m[20221214 00:20:46 @agent_ppo2.py:121][0m #------------------------ Iteration 5041 --------------------------#
[32m[20221214 00:20:46 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:20:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:20:46 @agent_ppo2.py:185][0m |           0.0030 |          69.3852 |          -4.9508 |
[32m[20221214 00:20:46 @agent_ppo2.py:185][0m |          -0.0093 |          61.3156 |          -4.9427 |
[32m[20221214 00:20:46 @agent_ppo2.py:185][0m |          -0.0104 |          58.7896 |          -4.9468 |
[32m[20221214 00:20:47 @agent_ppo2.py:185][0m |          -0.0126 |          57.3897 |          -5.0201 |
[32m[20221214 00:20:47 @agent_ppo2.py:185][0m |          -0.0160 |          56.4848 |          -5.0635 |
[32m[20221214 00:20:47 @agent_ppo2.py:185][0m |          -0.0178 |          55.8223 |          -5.1020 |
[32m[20221214 00:20:47 @agent_ppo2.py:185][0m |          -0.0189 |          55.1108 |          -5.1322 |
[32m[20221214 00:20:47 @agent_ppo2.py:185][0m |          -0.0163 |          54.6853 |          -5.2188 |
[32m[20221214 00:20:47 @agent_ppo2.py:185][0m |          -0.0181 |          54.1209 |          -5.2584 |
[32m[20221214 00:20:47 @agent_ppo2.py:185][0m |          -0.0071 |          61.6556 |          -5.3484 |
[32m[20221214 00:20:47 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:20:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 389.09
[32m[20221214 00:20:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 474.10
[32m[20221214 00:20:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 542.94
[32m[20221214 00:20:47 @agent_ppo2.py:143][0m Total time:      23.28 min
[32m[20221214 00:20:47 @agent_ppo2.py:145][0m 2134016 total steps have happened
[32m[20221214 00:20:47 @agent_ppo2.py:121][0m #------------------------ Iteration 5042 --------------------------#
[32m[20221214 00:20:47 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:20:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:20:48 @agent_ppo2.py:185][0m |           0.0040 |          85.1626 |          -4.9697 |
[32m[20221214 00:20:48 @agent_ppo2.py:185][0m |          -0.0051 |          76.4748 |          -4.8370 |
[32m[20221214 00:20:48 @agent_ppo2.py:185][0m |          -0.0081 |          72.9380 |          -4.9173 |
[32m[20221214 00:20:48 @agent_ppo2.py:185][0m |          -0.0091 |          70.5851 |          -4.8552 |
[32m[20221214 00:20:48 @agent_ppo2.py:185][0m |          -0.0136 |          69.3122 |          -4.7743 |
[32m[20221214 00:20:48 @agent_ppo2.py:185][0m |          -0.0147 |          67.8367 |          -4.7638 |
[32m[20221214 00:20:48 @agent_ppo2.py:185][0m |          -0.0158 |          66.7902 |          -4.6704 |
[32m[20221214 00:20:48 @agent_ppo2.py:185][0m |          -0.0174 |          65.5504 |          -4.7489 |
[32m[20221214 00:20:48 @agent_ppo2.py:185][0m |          -0.0150 |          65.5232 |          -4.6781 |
[32m[20221214 00:20:49 @agent_ppo2.py:185][0m |          -0.0189 |          64.0152 |          -4.6060 |
[32m[20221214 00:20:49 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:20:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 520.90
[32m[20221214 00:20:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 601.64
[32m[20221214 00:20:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 328.00
[32m[20221214 00:20:49 @agent_ppo2.py:143][0m Total time:      23.30 min
[32m[20221214 00:20:49 @agent_ppo2.py:145][0m 2136064 total steps have happened
[32m[20221214 00:20:49 @agent_ppo2.py:121][0m #------------------------ Iteration 5043 --------------------------#
[32m[20221214 00:20:49 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:20:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:20:49 @agent_ppo2.py:185][0m |           0.0016 |          62.2756 |          -4.6836 |
[32m[20221214 00:20:49 @agent_ppo2.py:185][0m |          -0.0070 |          53.2625 |          -5.0160 |
[32m[20221214 00:20:49 @agent_ppo2.py:185][0m |          -0.0036 |          49.8684 |          -4.9326 |
[32m[20221214 00:20:49 @agent_ppo2.py:185][0m |           0.0017 |          47.7592 |          -4.9704 |
[32m[20221214 00:20:49 @agent_ppo2.py:185][0m |          -0.0078 |          46.4758 |          -4.7820 |
[32m[20221214 00:20:49 @agent_ppo2.py:185][0m |          -0.0077 |          45.7884 |          -4.9372 |
[32m[20221214 00:20:50 @agent_ppo2.py:185][0m |          -0.0121 |          45.0186 |          -4.9055 |
[32m[20221214 00:20:50 @agent_ppo2.py:185][0m |          -0.0100 |          44.2837 |          -4.8845 |
[32m[20221214 00:20:50 @agent_ppo2.py:185][0m |          -0.0117 |          43.5752 |          -4.8935 |
[32m[20221214 00:20:50 @agent_ppo2.py:185][0m |          -0.0106 |          43.2340 |          -4.9138 |
[32m[20221214 00:20:50 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:20:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 611.03
[32m[20221214 00:20:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 652.24
[32m[20221214 00:20:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 273.20
[32m[20221214 00:20:50 @agent_ppo2.py:143][0m Total time:      23.33 min
[32m[20221214 00:20:50 @agent_ppo2.py:145][0m 2138112 total steps have happened
[32m[20221214 00:20:50 @agent_ppo2.py:121][0m #------------------------ Iteration 5044 --------------------------#
[32m[20221214 00:20:50 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:20:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:20:50 @agent_ppo2.py:185][0m |           0.0037 |          78.5941 |          -4.4724 |
[32m[20221214 00:20:50 @agent_ppo2.py:185][0m |           0.0083 |          74.8838 |          -4.2963 |
[32m[20221214 00:20:51 @agent_ppo2.py:185][0m |          -0.0087 |          65.4533 |          -4.1515 |
[32m[20221214 00:20:51 @agent_ppo2.py:185][0m |          -0.0099 |          63.0215 |          -4.0273 |
[32m[20221214 00:20:51 @agent_ppo2.py:185][0m |          -0.0143 |          61.3346 |          -3.9151 |
[32m[20221214 00:20:51 @agent_ppo2.py:185][0m |          -0.0103 |          60.2556 |          -3.9412 |
[32m[20221214 00:20:51 @agent_ppo2.py:185][0m |          -0.0123 |          58.8942 |          -3.8010 |
[32m[20221214 00:20:51 @agent_ppo2.py:185][0m |          -0.0091 |          58.4508 |          -3.7391 |
[32m[20221214 00:20:51 @agent_ppo2.py:185][0m |          -0.0148 |          57.4867 |          -3.6051 |
[32m[20221214 00:20:51 @agent_ppo2.py:185][0m |          -0.0180 |          56.6602 |          -3.6032 |
[32m[20221214 00:20:51 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:20:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 546.33
[32m[20221214 00:20:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 596.81
[32m[20221214 00:20:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 399.93
[32m[20221214 00:20:51 @agent_ppo2.py:143][0m Total time:      23.35 min
[32m[20221214 00:20:51 @agent_ppo2.py:145][0m 2140160 total steps have happened
[32m[20221214 00:20:51 @agent_ppo2.py:121][0m #------------------------ Iteration 5045 --------------------------#
[32m[20221214 00:20:52 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:20:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:20:52 @agent_ppo2.py:185][0m |           0.0099 |          44.8258 |          -4.1508 |
[32m[20221214 00:20:52 @agent_ppo2.py:185][0m |          -0.0073 |          31.6263 |          -4.1149 |
[32m[20221214 00:20:52 @agent_ppo2.py:185][0m |          -0.0038 |          28.8952 |          -4.1596 |
[32m[20221214 00:20:52 @agent_ppo2.py:185][0m |          -0.0090 |          27.5822 |          -3.9746 |
[32m[20221214 00:20:52 @agent_ppo2.py:185][0m |          -0.0093 |          26.6381 |          -4.1239 |
[32m[20221214 00:20:52 @agent_ppo2.py:185][0m |          -0.0142 |          25.8110 |          -4.0529 |
[32m[20221214 00:20:52 @agent_ppo2.py:185][0m |          -0.0193 |          25.3282 |          -4.0382 |
[32m[20221214 00:20:52 @agent_ppo2.py:185][0m |          -0.0185 |          24.8335 |          -3.9875 |
[32m[20221214 00:20:53 @agent_ppo2.py:185][0m |          -0.0152 |          24.2615 |          -4.0008 |
[32m[20221214 00:20:53 @agent_ppo2.py:185][0m |          -0.0155 |          24.2267 |          -4.0612 |
[32m[20221214 00:20:53 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:20:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 445.47
[32m[20221214 00:20:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 520.45
[32m[20221214 00:20:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 216.46
[32m[20221214 00:20:53 @agent_ppo2.py:143][0m Total time:      23.37 min
[32m[20221214 00:20:53 @agent_ppo2.py:145][0m 2142208 total steps have happened
[32m[20221214 00:20:53 @agent_ppo2.py:121][0m #------------------------ Iteration 5046 --------------------------#
[32m[20221214 00:20:53 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:20:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:20:53 @agent_ppo2.py:185][0m |           0.0017 |          41.7996 |          -3.9966 |
[32m[20221214 00:20:53 @agent_ppo2.py:185][0m |          -0.0035 |          34.3250 |          -3.8036 |
[32m[20221214 00:20:53 @agent_ppo2.py:185][0m |          -0.0119 |          32.0933 |          -3.8060 |
[32m[20221214 00:20:53 @agent_ppo2.py:185][0m |          -0.0160 |          31.1069 |          -3.7429 |
[32m[20221214 00:20:53 @agent_ppo2.py:185][0m |          -0.0166 |          30.1156 |          -3.7149 |
[32m[20221214 00:20:54 @agent_ppo2.py:185][0m |          -0.0224 |          29.1275 |          -3.6860 |
[32m[20221214 00:20:54 @agent_ppo2.py:185][0m |          -0.0172 |          28.5295 |          -3.6740 |
[32m[20221214 00:20:54 @agent_ppo2.py:185][0m |          -0.0114 |          30.3091 |          -3.7268 |
[32m[20221214 00:20:54 @agent_ppo2.py:185][0m |          -0.0201 |          28.0026 |          -3.6758 |
[32m[20221214 00:20:54 @agent_ppo2.py:185][0m |          -0.0301 |          27.3116 |          -3.5994 |
[32m[20221214 00:20:54 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:20:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 412.59
[32m[20221214 00:20:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 467.12
[32m[20221214 00:20:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 438.21
[32m[20221214 00:20:54 @agent_ppo2.py:143][0m Total time:      23.40 min
[32m[20221214 00:20:54 @agent_ppo2.py:145][0m 2144256 total steps have happened
[32m[20221214 00:20:54 @agent_ppo2.py:121][0m #------------------------ Iteration 5047 --------------------------#
[32m[20221214 00:20:54 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:20:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:20:55 @agent_ppo2.py:185][0m |           0.0005 |          57.3959 |          -3.0432 |
[32m[20221214 00:20:55 @agent_ppo2.py:185][0m |          -0.0043 |          49.0790 |          -3.0250 |
[32m[20221214 00:20:55 @agent_ppo2.py:185][0m |          -0.0099 |          45.6086 |          -2.9901 |
[32m[20221214 00:20:55 @agent_ppo2.py:185][0m |          -0.0132 |          43.4915 |          -2.9236 |
[32m[20221214 00:20:55 @agent_ppo2.py:185][0m |          -0.0154 |          42.6910 |          -3.0175 |
[32m[20221214 00:20:55 @agent_ppo2.py:185][0m |          -0.0129 |          41.4018 |          -2.9770 |
[32m[20221214 00:20:55 @agent_ppo2.py:185][0m |          -0.0144 |          40.7759 |          -2.8884 |
[32m[20221214 00:20:55 @agent_ppo2.py:185][0m |          -0.0167 |          40.0459 |          -2.9983 |
[32m[20221214 00:20:55 @agent_ppo2.py:185][0m |          -0.0146 |          39.2505 |          -2.8996 |
[32m[20221214 00:20:55 @agent_ppo2.py:185][0m |          -0.0136 |          39.1616 |          -2.9256 |
[32m[20221214 00:20:55 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:20:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 494.64
[32m[20221214 00:20:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 584.11
[32m[20221214 00:20:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 396.59
[32m[20221214 00:20:56 @agent_ppo2.py:143][0m Total time:      23.42 min
[32m[20221214 00:20:56 @agent_ppo2.py:145][0m 2146304 total steps have happened
[32m[20221214 00:20:56 @agent_ppo2.py:121][0m #------------------------ Iteration 5048 --------------------------#
[32m[20221214 00:20:56 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:20:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:20:56 @agent_ppo2.py:185][0m |          -0.0058 |          67.8610 |          -3.7048 |
[32m[20221214 00:20:56 @agent_ppo2.py:185][0m |          -0.0067 |          53.3996 |          -3.6450 |
[32m[20221214 00:20:56 @agent_ppo2.py:185][0m |          -0.0117 |          47.4149 |          -3.6867 |
[32m[20221214 00:20:56 @agent_ppo2.py:185][0m |          -0.0094 |          44.7383 |          -3.6283 |
[32m[20221214 00:20:56 @agent_ppo2.py:185][0m |          -0.0116 |          41.9139 |          -3.7178 |
[32m[20221214 00:20:56 @agent_ppo2.py:185][0m |          -0.0139 |          39.9718 |          -3.6653 |
[32m[20221214 00:20:56 @agent_ppo2.py:185][0m |          -0.0171 |          38.2863 |          -3.7862 |
[32m[20221214 00:20:57 @agent_ppo2.py:185][0m |          -0.0191 |          37.4320 |          -3.7224 |
[32m[20221214 00:20:57 @agent_ppo2.py:185][0m |          -0.0120 |          36.4189 |          -3.6364 |
[32m[20221214 00:20:57 @agent_ppo2.py:185][0m |          -0.0204 |          35.4003 |          -3.8106 |
[32m[20221214 00:20:57 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:20:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 445.63
[32m[20221214 00:20:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 553.60
[32m[20221214 00:20:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 402.27
[32m[20221214 00:20:57 @agent_ppo2.py:143][0m Total time:      23.44 min
[32m[20221214 00:20:57 @agent_ppo2.py:145][0m 2148352 total steps have happened
[32m[20221214 00:20:57 @agent_ppo2.py:121][0m #------------------------ Iteration 5049 --------------------------#
[32m[20221214 00:20:57 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:20:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:20:57 @agent_ppo2.py:185][0m |           0.0036 |          39.1137 |          -4.6049 |
[32m[20221214 00:20:57 @agent_ppo2.py:185][0m |           0.0004 |          30.2061 |          -4.5767 |
[32m[20221214 00:20:57 @agent_ppo2.py:185][0m |          -0.0069 |          28.6750 |          -4.6396 |
[32m[20221214 00:20:58 @agent_ppo2.py:185][0m |          -0.0109 |          27.1161 |          -4.6365 |
[32m[20221214 00:20:58 @agent_ppo2.py:185][0m |          -0.0248 |          26.9323 |          -4.6268 |
[32m[20221214 00:20:58 @agent_ppo2.py:185][0m |          -0.0178 |          25.7144 |          -4.5599 |
[32m[20221214 00:20:58 @agent_ppo2.py:185][0m |          -0.0096 |          25.3906 |          -4.6590 |
[32m[20221214 00:20:58 @agent_ppo2.py:185][0m |          -0.0145 |          24.7889 |          -4.6289 |
[32m[20221214 00:20:58 @agent_ppo2.py:185][0m |          -0.0161 |          26.6855 |          -4.6732 |
[32m[20221214 00:20:58 @agent_ppo2.py:185][0m |          -0.0179 |          24.8167 |          -4.6352 |
[32m[20221214 00:20:58 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:20:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 453.35
[32m[20221214 00:20:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 531.06
[32m[20221214 00:20:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 421.57
[32m[20221214 00:20:58 @agent_ppo2.py:143][0m Total time:      23.46 min
[32m[20221214 00:20:58 @agent_ppo2.py:145][0m 2150400 total steps have happened
[32m[20221214 00:20:58 @agent_ppo2.py:121][0m #------------------------ Iteration 5050 --------------------------#
[32m[20221214 00:20:58 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:20:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:20:59 @agent_ppo2.py:185][0m |           0.0032 |          67.7359 |          -3.2701 |
[32m[20221214 00:20:59 @agent_ppo2.py:185][0m |          -0.0062 |          55.9463 |          -3.3490 |
[32m[20221214 00:20:59 @agent_ppo2.py:185][0m |          -0.0009 |          52.9061 |          -3.4077 |
[32m[20221214 00:20:59 @agent_ppo2.py:185][0m |           0.0007 |          51.8797 |          -3.2998 |
[32m[20221214 00:20:59 @agent_ppo2.py:185][0m |          -0.0084 |          50.6107 |          -3.3065 |
[32m[20221214 00:20:59 @agent_ppo2.py:185][0m |          -0.0078 |          48.8647 |          -3.3156 |
[32m[20221214 00:20:59 @agent_ppo2.py:185][0m |          -0.0115 |          48.2819 |          -3.2208 |
[32m[20221214 00:20:59 @agent_ppo2.py:185][0m |          -0.0021 |          52.6058 |          -3.2788 |
[32m[20221214 00:20:59 @agent_ppo2.py:185][0m |          -0.0142 |          47.6535 |          -3.3274 |
[32m[20221214 00:20:59 @agent_ppo2.py:185][0m |          -0.0129 |          46.1053 |          -3.2788 |
[32m[20221214 00:20:59 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:21:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 499.35
[32m[20221214 00:21:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 586.72
[32m[20221214 00:21:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 584.76
[32m[20221214 00:21:00 @agent_ppo2.py:143][0m Total time:      23.49 min
[32m[20221214 00:21:00 @agent_ppo2.py:145][0m 2152448 total steps have happened
[32m[20221214 00:21:00 @agent_ppo2.py:121][0m #------------------------ Iteration 5051 --------------------------#
[32m[20221214 00:21:00 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:21:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:21:00 @agent_ppo2.py:185][0m |          -0.0053 |          49.8126 |          -4.2347 |
[32m[20221214 00:21:00 @agent_ppo2.py:185][0m |           0.0047 |          35.5007 |          -4.3415 |
[32m[20221214 00:21:00 @agent_ppo2.py:185][0m |          -0.0086 |          31.3043 |          -4.3712 |
[32m[20221214 00:21:00 @agent_ppo2.py:185][0m |          -0.0111 |          29.1144 |          -4.4268 |
[32m[20221214 00:21:00 @agent_ppo2.py:185][0m |          -0.0130 |          27.7499 |          -4.4238 |
[32m[20221214 00:21:00 @agent_ppo2.py:185][0m |          -0.0213 |          27.0426 |          -4.3659 |
[32m[20221214 00:21:01 @agent_ppo2.py:185][0m |          -0.0158 |          26.2355 |          -4.4566 |
[32m[20221214 00:21:01 @agent_ppo2.py:185][0m |          -0.0134 |          25.1692 |          -4.4976 |
[32m[20221214 00:21:01 @agent_ppo2.py:185][0m |          -0.0163 |          24.7199 |          -4.5242 |
[32m[20221214 00:21:01 @agent_ppo2.py:185][0m |          -0.0129 |          25.4217 |          -4.5467 |
[32m[20221214 00:21:01 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:21:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 398.46
[32m[20221214 00:21:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 496.42
[32m[20221214 00:21:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 555.70
[32m[20221214 00:21:01 @agent_ppo2.py:143][0m Total time:      23.51 min
[32m[20221214 00:21:01 @agent_ppo2.py:145][0m 2154496 total steps have happened
[32m[20221214 00:21:01 @agent_ppo2.py:121][0m #------------------------ Iteration 5052 --------------------------#
[32m[20221214 00:21:01 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:21:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:21:01 @agent_ppo2.py:185][0m |           0.0033 |          78.8333 |          -4.8754 |
[32m[20221214 00:21:01 @agent_ppo2.py:185][0m |           0.0060 |          78.2591 |          -4.8031 |
[32m[20221214 00:21:02 @agent_ppo2.py:185][0m |          -0.0064 |          69.5095 |          -4.7769 |
[32m[20221214 00:21:02 @agent_ppo2.py:185][0m |          -0.0081 |          68.6443 |          -4.8296 |
[32m[20221214 00:21:02 @agent_ppo2.py:185][0m |          -0.0122 |          67.6273 |          -4.7730 |
[32m[20221214 00:21:02 @agent_ppo2.py:185][0m |          -0.0126 |          66.5673 |          -4.8085 |
[32m[20221214 00:21:02 @agent_ppo2.py:185][0m |          -0.0114 |          66.0424 |          -4.7942 |
[32m[20221214 00:21:02 @agent_ppo2.py:185][0m |          -0.0078 |          66.2204 |          -4.8668 |
[32m[20221214 00:21:02 @agent_ppo2.py:185][0m |          -0.0087 |          69.2239 |          -4.9408 |
[32m[20221214 00:21:02 @agent_ppo2.py:185][0m |          -0.0137 |          64.9680 |          -4.7823 |
[32m[20221214 00:21:02 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:21:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 600.07
[32m[20221214 00:21:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 671.60
[32m[20221214 00:21:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 598.22
[32m[20221214 00:21:02 @agent_ppo2.py:143][0m Total time:      23.53 min
[32m[20221214 00:21:02 @agent_ppo2.py:145][0m 2156544 total steps have happened
[32m[20221214 00:21:02 @agent_ppo2.py:121][0m #------------------------ Iteration 5053 --------------------------#
[32m[20221214 00:21:03 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:21:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:21:03 @agent_ppo2.py:185][0m |           0.0032 |          63.2593 |          -4.9233 |
[32m[20221214 00:21:03 @agent_ppo2.py:185][0m |          -0.0013 |          56.3307 |          -4.8624 |
[32m[20221214 00:21:03 @agent_ppo2.py:185][0m |           0.0061 |          55.6395 |          -4.5650 |
[32m[20221214 00:21:03 @agent_ppo2.py:185][0m |          -0.0079 |          51.3765 |          -4.6649 |
[32m[20221214 00:21:03 @agent_ppo2.py:185][0m |          -0.0130 |          50.3500 |          -4.5700 |
[32m[20221214 00:21:03 @agent_ppo2.py:185][0m |          -0.0082 |          49.9130 |          -4.5889 |
[32m[20221214 00:21:03 @agent_ppo2.py:185][0m |          -0.0161 |          49.3888 |          -4.4859 |
[32m[20221214 00:21:03 @agent_ppo2.py:185][0m |          -0.0152 |          48.4315 |          -4.5373 |
[32m[20221214 00:21:04 @agent_ppo2.py:185][0m |          -0.0163 |          48.0970 |          -4.4992 |
[32m[20221214 00:21:04 @agent_ppo2.py:185][0m |          -0.0187 |          47.6788 |          -4.4312 |
[32m[20221214 00:21:04 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:21:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 426.97
[32m[20221214 00:21:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 529.35
[32m[20221214 00:21:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 614.84
[32m[20221214 00:21:04 @agent_ppo2.py:143][0m Total time:      23.56 min
[32m[20221214 00:21:04 @agent_ppo2.py:145][0m 2158592 total steps have happened
[32m[20221214 00:21:04 @agent_ppo2.py:121][0m #------------------------ Iteration 5054 --------------------------#
[32m[20221214 00:21:04 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:21:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:21:04 @agent_ppo2.py:185][0m |          -0.0004 |          83.0842 |          -4.7497 |
[32m[20221214 00:21:04 @agent_ppo2.py:185][0m |          -0.0144 |          75.7437 |          -4.6037 |
[32m[20221214 00:21:04 @agent_ppo2.py:185][0m |          -0.0170 |          72.5976 |          -4.5820 |
[32m[20221214 00:21:05 @agent_ppo2.py:185][0m |          -0.0090 |          70.3763 |          -4.5920 |
[32m[20221214 00:21:05 @agent_ppo2.py:185][0m |          -0.0157 |          68.6250 |          -4.6102 |
[32m[20221214 00:21:05 @agent_ppo2.py:185][0m |          -0.0150 |          67.4907 |          -4.6414 |
[32m[20221214 00:21:05 @agent_ppo2.py:185][0m |          -0.0127 |          67.0316 |          -4.6316 |
[32m[20221214 00:21:05 @agent_ppo2.py:185][0m |          -0.0241 |          66.1593 |          -4.6737 |
[32m[20221214 00:21:05 @agent_ppo2.py:185][0m |          -0.0217 |          65.6243 |          -4.6926 |
[32m[20221214 00:21:05 @agent_ppo2.py:185][0m |          -0.0209 |          65.0218 |          -4.6010 |
[32m[20221214 00:21:05 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:21:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 450.61
[32m[20221214 00:21:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 495.47
[32m[20221214 00:21:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 608.91
[32m[20221214 00:21:05 @agent_ppo2.py:143][0m Total time:      23.58 min
[32m[20221214 00:21:05 @agent_ppo2.py:145][0m 2160640 total steps have happened
[32m[20221214 00:21:05 @agent_ppo2.py:121][0m #------------------------ Iteration 5055 --------------------------#
[32m[20221214 00:21:05 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:21:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:21:06 @agent_ppo2.py:185][0m |          -0.0000 |          59.4462 |          -4.8537 |
[32m[20221214 00:21:06 @agent_ppo2.py:185][0m |           0.0008 |          51.5709 |          -4.7891 |
[32m[20221214 00:21:06 @agent_ppo2.py:185][0m |          -0.0023 |          49.0512 |          -4.7043 |
[32m[20221214 00:21:06 @agent_ppo2.py:185][0m |          -0.0129 |          47.2045 |          -4.8316 |
[32m[20221214 00:21:06 @agent_ppo2.py:185][0m |          -0.0125 |          45.9268 |          -4.6642 |
[32m[20221214 00:21:06 @agent_ppo2.py:185][0m |          -0.0105 |          44.8912 |          -4.8096 |
[32m[20221214 00:21:06 @agent_ppo2.py:185][0m |          -0.0140 |          43.7450 |          -4.7098 |
[32m[20221214 00:21:06 @agent_ppo2.py:185][0m |          -0.0150 |          43.1259 |          -4.7080 |
[32m[20221214 00:21:06 @agent_ppo2.py:185][0m |          -0.0106 |          45.1889 |          -4.8455 |
[32m[20221214 00:21:06 @agent_ppo2.py:185][0m |          -0.0186 |          41.9514 |          -4.7463 |
[32m[20221214 00:21:06 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:21:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 374.26
[32m[20221214 00:21:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 459.20
[32m[20221214 00:21:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 525.63
[32m[20221214 00:21:07 @agent_ppo2.py:143][0m Total time:      23.60 min
[32m[20221214 00:21:07 @agent_ppo2.py:145][0m 2162688 total steps have happened
[32m[20221214 00:21:07 @agent_ppo2.py:121][0m #------------------------ Iteration 5056 --------------------------#
[32m[20221214 00:21:07 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:21:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:21:07 @agent_ppo2.py:185][0m |           0.0042 |          73.5242 |          -5.4573 |
[32m[20221214 00:21:07 @agent_ppo2.py:185][0m |          -0.0006 |          66.4195 |          -5.4041 |
[32m[20221214 00:21:07 @agent_ppo2.py:185][0m |          -0.0090 |          63.1306 |          -5.5394 |
[32m[20221214 00:21:07 @agent_ppo2.py:185][0m |          -0.0082 |          61.7794 |          -5.6315 |
[32m[20221214 00:21:07 @agent_ppo2.py:185][0m |          -0.0131 |          60.5955 |          -5.5990 |
[32m[20221214 00:21:07 @agent_ppo2.py:185][0m |          -0.0098 |          60.7838 |          -5.7657 |
[32m[20221214 00:21:08 @agent_ppo2.py:185][0m |          -0.0012 |          64.9644 |          -5.7701 |
[32m[20221214 00:21:08 @agent_ppo2.py:185][0m |          -0.0131 |          59.3502 |          -5.7391 |
[32m[20221214 00:21:08 @agent_ppo2.py:185][0m |          -0.0102 |          57.8527 |          -5.6816 |
[32m[20221214 00:21:08 @agent_ppo2.py:185][0m |          -0.0043 |          57.7202 |          -5.7105 |
[32m[20221214 00:21:08 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:21:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 517.83
[32m[20221214 00:21:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 606.81
[32m[20221214 00:21:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 657.04
[32m[20221214 00:21:08 @agent_ppo2.py:143][0m Total time:      23.63 min
[32m[20221214 00:21:08 @agent_ppo2.py:145][0m 2164736 total steps have happened
[32m[20221214 00:21:08 @agent_ppo2.py:121][0m #------------------------ Iteration 5057 --------------------------#
[32m[20221214 00:21:08 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:21:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:21:08 @agent_ppo2.py:185][0m |           0.0007 |          69.0799 |          -5.7654 |
[32m[20221214 00:21:09 @agent_ppo2.py:185][0m |          -0.0026 |          61.1253 |          -5.6989 |
[32m[20221214 00:21:09 @agent_ppo2.py:185][0m |          -0.0035 |          55.1992 |          -5.5907 |
[32m[20221214 00:21:09 @agent_ppo2.py:185][0m |          -0.0119 |          54.1833 |          -5.6101 |
[32m[20221214 00:21:09 @agent_ppo2.py:185][0m |          -0.0102 |          51.9092 |          -5.5066 |
[32m[20221214 00:21:09 @agent_ppo2.py:185][0m |          -0.0119 |          50.2586 |          -5.5333 |
[32m[20221214 00:21:09 @agent_ppo2.py:185][0m |          -0.0140 |          53.5094 |          -5.5891 |
[32m[20221214 00:21:09 @agent_ppo2.py:185][0m |          -0.0111 |          49.4520 |          -5.5905 |
[32m[20221214 00:21:09 @agent_ppo2.py:185][0m |          -0.0130 |          49.4907 |          -5.5790 |
[32m[20221214 00:21:09 @agent_ppo2.py:185][0m |          -0.0180 |          48.3026 |          -5.4878 |
[32m[20221214 00:21:09 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:21:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 556.16
[32m[20221214 00:21:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 632.57
[32m[20221214 00:21:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 644.53
[32m[20221214 00:21:09 @agent_ppo2.py:143][0m Total time:      23.65 min
[32m[20221214 00:21:09 @agent_ppo2.py:145][0m 2166784 total steps have happened
[32m[20221214 00:21:09 @agent_ppo2.py:121][0m #------------------------ Iteration 5058 --------------------------#
[32m[20221214 00:21:10 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:21:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:21:10 @agent_ppo2.py:185][0m |          -0.0008 |          76.3519 |          -6.3177 |
[32m[20221214 00:21:10 @agent_ppo2.py:185][0m |          -0.0022 |          66.5831 |          -6.3517 |
[32m[20221214 00:21:10 @agent_ppo2.py:185][0m |          -0.0083 |          63.5097 |          -6.2335 |
[32m[20221214 00:21:10 @agent_ppo2.py:185][0m |          -0.0067 |          60.9130 |          -6.2157 |
[32m[20221214 00:21:10 @agent_ppo2.py:185][0m |          -0.0120 |          59.1380 |          -6.1545 |
[32m[20221214 00:21:10 @agent_ppo2.py:185][0m |          -0.0145 |          57.7323 |          -6.2761 |
[32m[20221214 00:21:10 @agent_ppo2.py:185][0m |          -0.0155 |          56.8017 |          -6.1568 |
[32m[20221214 00:21:10 @agent_ppo2.py:185][0m |          -0.0169 |          55.8282 |          -6.2130 |
[32m[20221214 00:21:11 @agent_ppo2.py:185][0m |          -0.0157 |          54.7400 |          -6.1297 |
[32m[20221214 00:21:11 @agent_ppo2.py:185][0m |           0.0003 |          60.7539 |          -6.1023 |
[32m[20221214 00:21:11 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:21:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 560.11
[32m[20221214 00:21:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 642.11
[32m[20221214 00:21:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 435.97
[32m[20221214 00:21:11 @agent_ppo2.py:143][0m Total time:      23.67 min
[32m[20221214 00:21:11 @agent_ppo2.py:145][0m 2168832 total steps have happened
[32m[20221214 00:21:11 @agent_ppo2.py:121][0m #------------------------ Iteration 5059 --------------------------#
[32m[20221214 00:21:11 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:21:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:21:11 @agent_ppo2.py:185][0m |           0.0038 |          86.6406 |          -5.0982 |
[32m[20221214 00:21:11 @agent_ppo2.py:185][0m |          -0.0038 |          82.0952 |          -5.0311 |
[32m[20221214 00:21:11 @agent_ppo2.py:185][0m |          -0.0012 |          81.9152 |          -4.9180 |
[32m[20221214 00:21:11 @agent_ppo2.py:185][0m |          -0.0121 |          79.6300 |          -4.9394 |
[32m[20221214 00:21:12 @agent_ppo2.py:185][0m |          -0.0112 |          78.0184 |          -4.8079 |
[32m[20221214 00:21:12 @agent_ppo2.py:185][0m |          -0.0115 |          77.8125 |          -4.8009 |
[32m[20221214 00:21:12 @agent_ppo2.py:185][0m |          -0.0055 |          82.4618 |          -4.8071 |
[32m[20221214 00:21:12 @agent_ppo2.py:185][0m |          -0.0129 |          76.5980 |          -4.7091 |
[32m[20221214 00:21:12 @agent_ppo2.py:185][0m |          -0.0169 |          76.1628 |          -4.6088 |
[32m[20221214 00:21:12 @agent_ppo2.py:185][0m |          -0.0153 |          75.8718 |          -4.5599 |
[32m[20221214 00:21:12 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:21:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 540.14
[32m[20221214 00:21:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 611.79
[32m[20221214 00:21:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 551.82
[32m[20221214 00:21:12 @agent_ppo2.py:143][0m Total time:      23.70 min
[32m[20221214 00:21:12 @agent_ppo2.py:145][0m 2170880 total steps have happened
[32m[20221214 00:21:12 @agent_ppo2.py:121][0m #------------------------ Iteration 5060 --------------------------#
[32m[20221214 00:21:12 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:21:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:21:13 @agent_ppo2.py:185][0m |           0.0013 |          86.5400 |          -4.8974 |
[32m[20221214 00:21:13 @agent_ppo2.py:185][0m |           0.0021 |          77.5353 |          -4.9758 |
[32m[20221214 00:21:13 @agent_ppo2.py:185][0m |          -0.0080 |          74.3839 |          -4.9862 |
[32m[20221214 00:21:13 @agent_ppo2.py:185][0m |          -0.0067 |          72.2448 |          -4.9513 |
[32m[20221214 00:21:13 @agent_ppo2.py:185][0m |          -0.0119 |          70.7809 |          -5.0761 |
[32m[20221214 00:21:13 @agent_ppo2.py:185][0m |          -0.0043 |          74.8859 |          -4.9449 |
[32m[20221214 00:21:13 @agent_ppo2.py:185][0m |          -0.0154 |          67.6081 |          -4.9567 |
[32m[20221214 00:21:13 @agent_ppo2.py:185][0m |          -0.0167 |          67.1238 |          -5.0771 |
[32m[20221214 00:21:13 @agent_ppo2.py:185][0m |          -0.0191 |          66.2826 |          -5.0454 |
[32m[20221214 00:21:13 @agent_ppo2.py:185][0m |          -0.0161 |          65.8444 |          -5.0335 |
[32m[20221214 00:21:13 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:21:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 543.95
[32m[20221214 00:21:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 593.99
[32m[20221214 00:21:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 589.39
[32m[20221214 00:21:14 @agent_ppo2.py:143][0m Total time:      23.72 min
[32m[20221214 00:21:14 @agent_ppo2.py:145][0m 2172928 total steps have happened
[32m[20221214 00:21:14 @agent_ppo2.py:121][0m #------------------------ Iteration 5061 --------------------------#
[32m[20221214 00:21:14 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:21:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:21:14 @agent_ppo2.py:185][0m |           0.0005 |          95.4330 |          -4.5408 |
[32m[20221214 00:21:14 @agent_ppo2.py:185][0m |          -0.0038 |          84.9248 |          -4.6999 |
[32m[20221214 00:21:14 @agent_ppo2.py:185][0m |          -0.0089 |          81.0493 |          -4.7577 |
[32m[20221214 00:21:14 @agent_ppo2.py:185][0m |          -0.0098 |          78.6618 |          -4.8255 |
[32m[20221214 00:21:14 @agent_ppo2.py:185][0m |          -0.0125 |          76.3217 |          -4.7415 |
[32m[20221214 00:21:14 @agent_ppo2.py:185][0m |          -0.0136 |          75.0392 |          -4.8766 |
[32m[20221214 00:21:15 @agent_ppo2.py:185][0m |          -0.0145 |          73.7018 |          -4.8397 |
[32m[20221214 00:21:15 @agent_ppo2.py:185][0m |          -0.0213 |          73.0499 |          -4.8489 |
[32m[20221214 00:21:15 @agent_ppo2.py:185][0m |          -0.0170 |          72.2872 |          -4.9587 |
[32m[20221214 00:21:15 @agent_ppo2.py:185][0m |          -0.0230 |          71.7421 |          -4.9387 |
[32m[20221214 00:21:15 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:21:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 564.26
[32m[20221214 00:21:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 596.23
[32m[20221214 00:21:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 565.20
[32m[20221214 00:21:15 @agent_ppo2.py:143][0m Total time:      23.74 min
[32m[20221214 00:21:15 @agent_ppo2.py:145][0m 2174976 total steps have happened
[32m[20221214 00:21:15 @agent_ppo2.py:121][0m #------------------------ Iteration 5062 --------------------------#
[32m[20221214 00:21:15 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:21:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:21:15 @agent_ppo2.py:185][0m |          -0.0035 |          62.5485 |          -6.0837 |
[32m[20221214 00:21:15 @agent_ppo2.py:185][0m |          -0.0032 |          55.8963 |          -6.0647 |
[32m[20221214 00:21:15 @agent_ppo2.py:185][0m |          -0.0151 |          54.2194 |          -5.9701 |
[32m[20221214 00:21:16 @agent_ppo2.py:185][0m |          -0.0092 |          53.2234 |          -5.8663 |
[32m[20221214 00:21:16 @agent_ppo2.py:185][0m |          -0.0140 |          52.3744 |          -5.9461 |
[32m[20221214 00:21:16 @agent_ppo2.py:185][0m |          -0.0058 |          54.9042 |          -5.9438 |
[32m[20221214 00:21:16 @agent_ppo2.py:185][0m |          -0.0098 |          51.1406 |          -5.9151 |
[32m[20221214 00:21:16 @agent_ppo2.py:185][0m |          -0.0113 |          50.6247 |          -5.8183 |
[32m[20221214 00:21:16 @agent_ppo2.py:185][0m |          -0.0148 |          50.4243 |          -6.0331 |
[32m[20221214 00:21:16 @agent_ppo2.py:185][0m |          -0.0204 |          49.7498 |          -5.9213 |
[32m[20221214 00:21:16 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:21:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 521.23
[32m[20221214 00:21:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 561.70
[32m[20221214 00:21:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 626.23
[32m[20221214 00:21:16 @agent_ppo2.py:143][0m Total time:      23.77 min
[32m[20221214 00:21:16 @agent_ppo2.py:145][0m 2177024 total steps have happened
[32m[20221214 00:21:16 @agent_ppo2.py:121][0m #------------------------ Iteration 5063 --------------------------#
[32m[20221214 00:21:16 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:21:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:21:17 @agent_ppo2.py:185][0m |           0.0089 |          66.9909 |          -6.1992 |
[32m[20221214 00:21:17 @agent_ppo2.py:185][0m |           0.0016 |          58.3104 |          -6.1848 |
[32m[20221214 00:21:17 @agent_ppo2.py:185][0m |          -0.0083 |          54.7646 |          -6.0463 |
[32m[20221214 00:21:17 @agent_ppo2.py:185][0m |          -0.0030 |          55.1743 |          -6.0192 |
[32m[20221214 00:21:17 @agent_ppo2.py:185][0m |          -0.0117 |          51.4511 |          -5.9255 |
[32m[20221214 00:21:17 @agent_ppo2.py:185][0m |          -0.0127 |          50.2434 |          -5.8326 |
[32m[20221214 00:21:17 @agent_ppo2.py:185][0m |          -0.0167 |          50.1081 |          -5.7283 |
[32m[20221214 00:21:17 @agent_ppo2.py:185][0m |          -0.0196 |          49.1333 |          -5.7722 |
[32m[20221214 00:21:17 @agent_ppo2.py:185][0m |          -0.0207 |          48.3683 |          -5.6902 |
[32m[20221214 00:21:18 @agent_ppo2.py:185][0m |          -0.0147 |          47.7122 |          -5.6537 |
[32m[20221214 00:21:18 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:21:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 397.07
[32m[20221214 00:21:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 518.03
[32m[20221214 00:21:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 348.74
[32m[20221214 00:21:18 @agent_ppo2.py:143][0m Total time:      23.79 min
[32m[20221214 00:21:18 @agent_ppo2.py:145][0m 2179072 total steps have happened
[32m[20221214 00:21:18 @agent_ppo2.py:121][0m #------------------------ Iteration 5064 --------------------------#
[32m[20221214 00:21:18 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:21:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:21:18 @agent_ppo2.py:185][0m |          -0.0016 |          87.3031 |          -4.5875 |
[32m[20221214 00:21:18 @agent_ppo2.py:185][0m |          -0.0101 |          75.5467 |          -4.5846 |
[32m[20221214 00:21:18 @agent_ppo2.py:185][0m |          -0.0083 |          71.3203 |          -4.5588 |
[32m[20221214 00:21:18 @agent_ppo2.py:185][0m |          -0.0090 |          68.5851 |          -4.7526 |
[32m[20221214 00:21:18 @agent_ppo2.py:185][0m |          -0.0120 |          66.2864 |          -4.6610 |
[32m[20221214 00:21:19 @agent_ppo2.py:185][0m |          -0.0152 |          64.8719 |          -4.6563 |
[32m[20221214 00:21:19 @agent_ppo2.py:185][0m |          -0.0177 |          63.7684 |          -4.6838 |
[32m[20221214 00:21:19 @agent_ppo2.py:185][0m |          -0.0163 |          62.4868 |          -4.7291 |
[32m[20221214 00:21:19 @agent_ppo2.py:185][0m |          -0.0175 |          61.7476 |          -4.6225 |
[32m[20221214 00:21:19 @agent_ppo2.py:185][0m |          -0.0162 |          60.8939 |          -4.7565 |
[32m[20221214 00:21:19 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:21:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 457.86
[32m[20221214 00:21:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 621.84
[32m[20221214 00:21:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 472.26
[32m[20221214 00:21:19 @agent_ppo2.py:143][0m Total time:      23.81 min
[32m[20221214 00:21:19 @agent_ppo2.py:145][0m 2181120 total steps have happened
[32m[20221214 00:21:19 @agent_ppo2.py:121][0m #------------------------ Iteration 5065 --------------------------#
[32m[20221214 00:21:19 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:21:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:21:19 @agent_ppo2.py:185][0m |           0.0061 |          65.0274 |          -4.3673 |
[32m[20221214 00:21:19 @agent_ppo2.py:185][0m |          -0.0068 |          57.8376 |          -4.3878 |
[32m[20221214 00:21:20 @agent_ppo2.py:185][0m |          -0.0034 |          53.9777 |          -4.3681 |
[32m[20221214 00:21:20 @agent_ppo2.py:185][0m |          -0.0080 |          52.1590 |          -4.3119 |
[32m[20221214 00:21:20 @agent_ppo2.py:185][0m |          -0.0138 |          50.4495 |          -4.2871 |
[32m[20221214 00:21:20 @agent_ppo2.py:185][0m |          -0.0149 |          49.7033 |          -4.2414 |
[32m[20221214 00:21:20 @agent_ppo2.py:185][0m |          -0.0153 |          49.2296 |          -4.2921 |
[32m[20221214 00:21:20 @agent_ppo2.py:185][0m |          -0.0068 |          48.2057 |          -4.2686 |
[32m[20221214 00:21:20 @agent_ppo2.py:185][0m |          -0.0183 |          47.6127 |          -4.2794 |
[32m[20221214 00:21:20 @agent_ppo2.py:185][0m |          -0.0180 |          46.6807 |          -4.2841 |
[32m[20221214 00:21:20 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:21:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 411.25
[32m[20221214 00:21:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 555.54
[32m[20221214 00:21:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 565.89
[32m[20221214 00:21:20 @agent_ppo2.py:143][0m Total time:      23.83 min
[32m[20221214 00:21:20 @agent_ppo2.py:145][0m 2183168 total steps have happened
[32m[20221214 00:21:20 @agent_ppo2.py:121][0m #------------------------ Iteration 5066 --------------------------#
[32m[20221214 00:21:21 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:21:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:21:21 @agent_ppo2.py:185][0m |          -0.0014 |          83.7077 |          -5.2542 |
[32m[20221214 00:21:21 @agent_ppo2.py:185][0m |          -0.0055 |          76.5438 |          -5.3114 |
[32m[20221214 00:21:21 @agent_ppo2.py:185][0m |          -0.0044 |          73.7457 |          -5.2931 |
[32m[20221214 00:21:21 @agent_ppo2.py:185][0m |          -0.0080 |          72.2121 |          -5.1540 |
[32m[20221214 00:21:21 @agent_ppo2.py:185][0m |          -0.0114 |          69.6846 |          -5.1879 |
[32m[20221214 00:21:21 @agent_ppo2.py:185][0m |          -0.0167 |          69.1235 |          -5.1601 |
[32m[20221214 00:21:21 @agent_ppo2.py:185][0m |          -0.0165 |          67.0691 |          -5.1316 |
[32m[20221214 00:21:21 @agent_ppo2.py:185][0m |          -0.0115 |          66.8414 |          -5.1817 |
[32m[20221214 00:21:22 @agent_ppo2.py:185][0m |          -0.0094 |          67.6256 |          -5.1344 |
[32m[20221214 00:21:22 @agent_ppo2.py:185][0m |          -0.0166 |          65.8727 |          -5.0825 |
[32m[20221214 00:21:22 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:21:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 530.33
[32m[20221214 00:21:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 601.35
[32m[20221214 00:21:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 301.08
[32m[20221214 00:21:22 @agent_ppo2.py:143][0m Total time:      23.86 min
[32m[20221214 00:21:22 @agent_ppo2.py:145][0m 2185216 total steps have happened
[32m[20221214 00:21:22 @agent_ppo2.py:121][0m #------------------------ Iteration 5067 --------------------------#
[32m[20221214 00:21:22 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:21:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:21:22 @agent_ppo2.py:185][0m |          -0.0015 |          67.3711 |          -4.4663 |
[32m[20221214 00:21:22 @agent_ppo2.py:185][0m |          -0.0044 |          61.9607 |          -4.5071 |
[32m[20221214 00:21:22 @agent_ppo2.py:185][0m |          -0.0100 |          59.1989 |          -4.4958 |
[32m[20221214 00:21:22 @agent_ppo2.py:185][0m |          -0.0075 |          56.6790 |          -4.5361 |
[32m[20221214 00:21:23 @agent_ppo2.py:185][0m |          -0.0126 |          55.1177 |          -4.4750 |
[32m[20221214 00:21:23 @agent_ppo2.py:185][0m |          -0.0094 |          54.3899 |          -4.3548 |
[32m[20221214 00:21:23 @agent_ppo2.py:185][0m |          -0.0122 |          53.0920 |          -4.5227 |
[32m[20221214 00:21:23 @agent_ppo2.py:185][0m |          -0.0124 |          52.7923 |          -4.5216 |
[32m[20221214 00:21:23 @agent_ppo2.py:185][0m |          -0.0130 |          51.9158 |          -4.4570 |
[32m[20221214 00:21:23 @agent_ppo2.py:185][0m |          -0.0117 |          51.3152 |          -4.5713 |
[32m[20221214 00:21:23 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:21:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 510.03
[32m[20221214 00:21:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 567.41
[32m[20221214 00:21:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 523.72
[32m[20221214 00:21:23 @agent_ppo2.py:143][0m Total time:      23.88 min
[32m[20221214 00:21:23 @agent_ppo2.py:145][0m 2187264 total steps have happened
[32m[20221214 00:21:23 @agent_ppo2.py:121][0m #------------------------ Iteration 5068 --------------------------#
[32m[20221214 00:21:23 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:21:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:21:24 @agent_ppo2.py:185][0m |          -0.0005 |          52.6377 |          -4.9965 |
[32m[20221214 00:21:24 @agent_ppo2.py:185][0m |          -0.0009 |          42.3135 |          -4.9877 |
[32m[20221214 00:21:24 @agent_ppo2.py:185][0m |          -0.0057 |          39.9141 |          -4.8777 |
[32m[20221214 00:21:24 @agent_ppo2.py:185][0m |          -0.0098 |          38.3696 |          -5.1008 |
[32m[20221214 00:21:24 @agent_ppo2.py:185][0m |          -0.0061 |          39.7112 |          -5.1532 |
[32m[20221214 00:21:24 @agent_ppo2.py:185][0m |          -0.0062 |          37.8130 |          -4.9974 |
[32m[20221214 00:21:24 @agent_ppo2.py:185][0m |          -0.0152 |          36.5527 |          -5.1695 |
[32m[20221214 00:21:24 @agent_ppo2.py:185][0m |          -0.0198 |          36.5522 |          -5.1451 |
[32m[20221214 00:21:24 @agent_ppo2.py:185][0m |          -0.0197 |          35.8239 |          -5.3202 |
[32m[20221214 00:21:24 @agent_ppo2.py:185][0m |          -0.0177 |          35.6278 |          -5.3390 |
[32m[20221214 00:21:24 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:21:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 480.87
[32m[20221214 00:21:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 568.06
[32m[20221214 00:21:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 634.24
[32m[20221214 00:21:25 @agent_ppo2.py:143][0m Total time:      23.90 min
[32m[20221214 00:21:25 @agent_ppo2.py:145][0m 2189312 total steps have happened
[32m[20221214 00:21:25 @agent_ppo2.py:121][0m #------------------------ Iteration 5069 --------------------------#
[32m[20221214 00:21:25 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:21:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:21:25 @agent_ppo2.py:185][0m |           0.0016 |          64.8100 |          -6.9342 |
[32m[20221214 00:21:25 @agent_ppo2.py:185][0m |          -0.0038 |          60.4292 |          -6.8441 |
[32m[20221214 00:21:25 @agent_ppo2.py:185][0m |          -0.0030 |          57.0001 |          -6.8266 |
[32m[20221214 00:21:25 @agent_ppo2.py:185][0m |          -0.0053 |          55.0081 |          -6.7158 |
[32m[20221214 00:21:25 @agent_ppo2.py:185][0m |          -0.0055 |          55.1120 |          -6.8854 |
[32m[20221214 00:21:26 @agent_ppo2.py:185][0m |          -0.0111 |          53.3091 |          -6.7670 |
[32m[20221214 00:21:26 @agent_ppo2.py:185][0m |          -0.0040 |          52.7516 |          -6.7922 |
[32m[20221214 00:21:26 @agent_ppo2.py:185][0m |          -0.0129 |          52.0464 |          -6.8264 |
[32m[20221214 00:21:26 @agent_ppo2.py:185][0m |          -0.0131 |          50.6268 |          -6.8011 |
[32m[20221214 00:21:26 @agent_ppo2.py:185][0m |          -0.0150 |          50.1215 |          -6.7528 |
[32m[20221214 00:21:26 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221214 00:21:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 523.58
[32m[20221214 00:21:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 592.07
[32m[20221214 00:21:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 528.34
[32m[20221214 00:21:26 @agent_ppo2.py:143][0m Total time:      23.93 min
[32m[20221214 00:21:26 @agent_ppo2.py:145][0m 2191360 total steps have happened
[32m[20221214 00:21:26 @agent_ppo2.py:121][0m #------------------------ Iteration 5070 --------------------------#
[32m[20221214 00:21:26 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:21:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:21:26 @agent_ppo2.py:185][0m |          -0.0058 |          61.0385 |          -6.7532 |
[32m[20221214 00:21:27 @agent_ppo2.py:185][0m |           0.0133 |          65.1478 |          -6.6665 |
[32m[20221214 00:21:27 @agent_ppo2.py:185][0m |          -0.0077 |          56.3700 |          -6.6349 |
[32m[20221214 00:21:27 @agent_ppo2.py:185][0m |          -0.0084 |          54.9868 |          -6.5638 |
[32m[20221214 00:21:27 @agent_ppo2.py:185][0m |          -0.0141 |          54.6015 |          -6.6695 |
[32m[20221214 00:21:27 @agent_ppo2.py:185][0m |          -0.0116 |          53.8658 |          -6.6596 |
[32m[20221214 00:21:27 @agent_ppo2.py:185][0m |          -0.0083 |          53.1242 |          -6.6748 |
[32m[20221214 00:21:27 @agent_ppo2.py:185][0m |          -0.0203 |          52.7013 |          -6.8495 |
[32m[20221214 00:21:27 @agent_ppo2.py:185][0m |          -0.0123 |          52.3851 |          -6.8268 |
[32m[20221214 00:21:27 @agent_ppo2.py:185][0m |          -0.0110 |          52.4245 |          -6.8411 |
[32m[20221214 00:21:27 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:21:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 548.16
[32m[20221214 00:21:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 591.27
[32m[20221214 00:21:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 611.44
[32m[20221214 00:21:28 @agent_ppo2.py:143][0m Total time:      23.95 min
[32m[20221214 00:21:28 @agent_ppo2.py:145][0m 2193408 total steps have happened
[32m[20221214 00:21:28 @agent_ppo2.py:121][0m #------------------------ Iteration 5071 --------------------------#
[32m[20221214 00:21:28 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:21:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:21:28 @agent_ppo2.py:185][0m |           0.0118 |         124.9492 |          -6.2365 |
[32m[20221214 00:21:28 @agent_ppo2.py:185][0m |          -0.0045 |         117.5836 |          -6.1063 |
[32m[20221214 00:21:28 @agent_ppo2.py:185][0m |           0.0063 |         126.5972 |          -6.0149 |
[32m[20221214 00:21:28 @agent_ppo2.py:185][0m |           0.0071 |         124.0651 |          -5.9137 |
[32m[20221214 00:21:28 @agent_ppo2.py:185][0m |          -0.0105 |         114.3833 |          -5.9314 |
[32m[20221214 00:21:28 @agent_ppo2.py:185][0m |          -0.0092 |         113.0124 |          -5.8513 |
[32m[20221214 00:21:29 @agent_ppo2.py:185][0m |          -0.0080 |         112.4033 |          -5.8132 |
[32m[20221214 00:21:29 @agent_ppo2.py:185][0m |          -0.0111 |         112.2133 |          -5.8829 |
[32m[20221214 00:21:29 @agent_ppo2.py:185][0m |          -0.0030 |         113.9365 |          -5.9386 |
[32m[20221214 00:21:29 @agent_ppo2.py:185][0m |          -0.0082 |         111.3489 |          -5.9371 |
[32m[20221214 00:21:29 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221214 00:21:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 595.05
[32m[20221214 00:21:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 629.74
[32m[20221214 00:21:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 536.49
[32m[20221214 00:21:29 @agent_ppo2.py:143][0m Total time:      23.98 min
[32m[20221214 00:21:29 @agent_ppo2.py:145][0m 2195456 total steps have happened
[32m[20221214 00:21:29 @agent_ppo2.py:121][0m #------------------------ Iteration 5072 --------------------------#
[32m[20221214 00:21:29 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:21:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:21:29 @agent_ppo2.py:185][0m |           0.0009 |          67.5709 |          -6.6988 |
[32m[20221214 00:21:29 @agent_ppo2.py:185][0m |          -0.0086 |          57.5456 |          -6.5997 |
[32m[20221214 00:21:30 @agent_ppo2.py:185][0m |          -0.0117 |          53.9902 |          -6.6453 |
[32m[20221214 00:21:30 @agent_ppo2.py:185][0m |          -0.0139 |          51.9295 |          -6.5469 |
[32m[20221214 00:21:30 @agent_ppo2.py:185][0m |          -0.0159 |          50.7428 |          -6.5581 |
[32m[20221214 00:21:30 @agent_ppo2.py:185][0m |          -0.0150 |          49.9022 |          -6.5661 |
[32m[20221214 00:21:30 @agent_ppo2.py:185][0m |          -0.0148 |          48.2934 |          -6.5616 |
[32m[20221214 00:21:30 @agent_ppo2.py:185][0m |          -0.0178 |          47.7442 |          -6.4913 |
[32m[20221214 00:21:30 @agent_ppo2.py:185][0m |          -0.0231 |          46.6419 |          -6.5195 |
[32m[20221214 00:21:30 @agent_ppo2.py:185][0m |          -0.0197 |          46.0952 |          -6.4634 |
[32m[20221214 00:21:30 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 00:21:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 341.70
[32m[20221214 00:21:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 622.33
[32m[20221214 00:21:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 507.14
[32m[20221214 00:21:30 @agent_ppo2.py:143][0m Total time:      24.00 min
[32m[20221214 00:21:30 @agent_ppo2.py:145][0m 2197504 total steps have happened
[32m[20221214 00:21:30 @agent_ppo2.py:121][0m #------------------------ Iteration 5073 --------------------------#
[32m[20221214 00:21:31 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:21:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:21:31 @agent_ppo2.py:185][0m |          -0.0037 |          80.9341 |          -6.2073 |
[32m[20221214 00:21:31 @agent_ppo2.py:185][0m |          -0.0059 |          69.9892 |          -6.1341 |
[32m[20221214 00:21:31 @agent_ppo2.py:185][0m |          -0.0123 |          66.6024 |          -6.0929 |
[32m[20221214 00:21:31 @agent_ppo2.py:185][0m |          -0.0101 |          64.7636 |          -6.0313 |
[32m[20221214 00:21:31 @agent_ppo2.py:185][0m |          -0.0104 |          63.9582 |          -6.1531 |
[32m[20221214 00:21:31 @agent_ppo2.py:185][0m |          -0.0148 |          62.6113 |          -6.0830 |
[32m[20221214 00:21:31 @agent_ppo2.py:185][0m |          -0.0190 |          61.7958 |          -6.0848 |
[32m[20221214 00:21:32 @agent_ppo2.py:185][0m |          -0.0109 |          61.1671 |          -6.1223 |
[32m[20221214 00:21:32 @agent_ppo2.py:185][0m |          -0.0205 |          60.4611 |          -6.0813 |
[32m[20221214 00:21:32 @agent_ppo2.py:185][0m |          -0.0190 |          60.0505 |          -6.0847 |
[32m[20221214 00:21:32 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 00:21:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 403.53
[32m[20221214 00:21:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 555.07
[32m[20221214 00:21:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 521.19
[32m[20221214 00:21:32 @agent_ppo2.py:143][0m Total time:      24.03 min
[32m[20221214 00:21:32 @agent_ppo2.py:145][0m 2199552 total steps have happened
[32m[20221214 00:21:32 @agent_ppo2.py:121][0m #------------------------ Iteration 5074 --------------------------#
[32m[20221214 00:21:32 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:21:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:21:32 @agent_ppo2.py:185][0m |           0.0022 |          86.3209 |          -5.5769 |
[32m[20221214 00:21:32 @agent_ppo2.py:185][0m |          -0.0077 |          74.1775 |          -5.4475 |
[32m[20221214 00:21:33 @agent_ppo2.py:185][0m |          -0.0101 |          70.7394 |          -5.3439 |
[32m[20221214 00:21:33 @agent_ppo2.py:185][0m |          -0.0107 |          68.7613 |          -5.4297 |
[32m[20221214 00:21:33 @agent_ppo2.py:185][0m |          -0.0169 |          67.0146 |          -5.2882 |
[32m[20221214 00:21:33 @agent_ppo2.py:185][0m |          -0.0105 |          65.4455 |          -5.2859 |
[32m[20221214 00:21:33 @agent_ppo2.py:185][0m |          -0.0105 |          64.5492 |          -5.2074 |
[32m[20221214 00:21:33 @agent_ppo2.py:185][0m |          -0.0179 |          63.5657 |          -5.1223 |
[32m[20221214 00:21:33 @agent_ppo2.py:185][0m |          -0.0174 |          62.6568 |          -5.1844 |
[32m[20221214 00:21:33 @agent_ppo2.py:185][0m |          -0.0178 |          61.8741 |          -5.2217 |
[32m[20221214 00:21:33 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:21:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 550.27
[32m[20221214 00:21:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 622.80
[32m[20221214 00:21:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 492.59
[32m[20221214 00:21:33 @agent_ppo2.py:143][0m Total time:      24.05 min
[32m[20221214 00:21:33 @agent_ppo2.py:145][0m 2201600 total steps have happened
[32m[20221214 00:21:33 @agent_ppo2.py:121][0m #------------------------ Iteration 5075 --------------------------#
[32m[20221214 00:21:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:21:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:21:34 @agent_ppo2.py:185][0m |           0.0018 |          89.3025 |          -6.1894 |
[32m[20221214 00:21:34 @agent_ppo2.py:185][0m |          -0.0024 |          81.2015 |          -6.3174 |
[32m[20221214 00:21:34 @agent_ppo2.py:185][0m |           0.0117 |          89.1144 |          -6.3048 |
[32m[20221214 00:21:34 @agent_ppo2.py:185][0m |          -0.0004 |          77.9792 |          -6.3496 |
[32m[20221214 00:21:34 @agent_ppo2.py:185][0m |          -0.0037 |          77.6859 |          -6.3738 |
[32m[20221214 00:21:34 @agent_ppo2.py:185][0m |          -0.0054 |          76.7409 |          -6.4452 |
[32m[20221214 00:21:34 @agent_ppo2.py:185][0m |          -0.0104 |          76.2721 |          -6.4495 |
[32m[20221214 00:21:34 @agent_ppo2.py:185][0m |           0.0043 |          81.7514 |          -6.3523 |
[32m[20221214 00:21:35 @agent_ppo2.py:185][0m |          -0.0067 |          76.0170 |          -6.3698 |
[32m[20221214 00:21:35 @agent_ppo2.py:185][0m |          -0.0057 |          75.6964 |          -6.2368 |
[32m[20221214 00:21:35 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:21:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 591.25
[32m[20221214 00:21:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 643.40
[32m[20221214 00:21:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 570.19
[32m[20221214 00:21:35 @agent_ppo2.py:143][0m Total time:      24.07 min
[32m[20221214 00:21:35 @agent_ppo2.py:145][0m 2203648 total steps have happened
[32m[20221214 00:21:35 @agent_ppo2.py:121][0m #------------------------ Iteration 5076 --------------------------#
[32m[20221214 00:21:35 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:21:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:21:35 @agent_ppo2.py:185][0m |           0.0018 |          81.7597 |          -5.1694 |
[32m[20221214 00:21:35 @agent_ppo2.py:185][0m |          -0.0053 |          74.5141 |          -4.9872 |
[32m[20221214 00:21:35 @agent_ppo2.py:185][0m |          -0.0114 |          71.5220 |          -5.0049 |
[32m[20221214 00:21:35 @agent_ppo2.py:185][0m |          -0.0130 |          69.8054 |          -4.7357 |
[32m[20221214 00:21:36 @agent_ppo2.py:185][0m |          -0.0067 |          70.4792 |          -4.7052 |
[32m[20221214 00:21:36 @agent_ppo2.py:185][0m |          -0.0155 |          68.3169 |          -4.7267 |
[32m[20221214 00:21:36 @agent_ppo2.py:185][0m |          -0.0150 |          67.7833 |          -4.6258 |
[32m[20221214 00:21:36 @agent_ppo2.py:185][0m |          -0.0127 |          67.6950 |          -4.6056 |
[32m[20221214 00:21:36 @agent_ppo2.py:185][0m |          -0.0146 |          67.4788 |          -4.4565 |
[32m[20221214 00:21:36 @agent_ppo2.py:185][0m |          -0.0193 |          66.7543 |          -4.4137 |
[32m[20221214 00:21:36 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:21:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 506.79
[32m[20221214 00:21:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 613.32
[32m[20221214 00:21:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 547.92
[32m[20221214 00:21:36 @agent_ppo2.py:143][0m Total time:      24.10 min
[32m[20221214 00:21:36 @agent_ppo2.py:145][0m 2205696 total steps have happened
[32m[20221214 00:21:36 @agent_ppo2.py:121][0m #------------------------ Iteration 5077 --------------------------#
[32m[20221214 00:21:36 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:21:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:21:37 @agent_ppo2.py:185][0m |           0.0015 |          75.5868 |          -5.3605 |
[32m[20221214 00:21:37 @agent_ppo2.py:185][0m |          -0.0075 |          67.5534 |          -5.5253 |
[32m[20221214 00:21:37 @agent_ppo2.py:185][0m |          -0.0099 |          64.1922 |          -5.4610 |
[32m[20221214 00:21:37 @agent_ppo2.py:185][0m |          -0.0149 |          61.4873 |          -5.4453 |
[32m[20221214 00:21:37 @agent_ppo2.py:185][0m |          -0.0093 |          60.1766 |          -5.3947 |
[32m[20221214 00:21:37 @agent_ppo2.py:185][0m |          -0.0180 |          59.2551 |          -5.2580 |
[32m[20221214 00:21:37 @agent_ppo2.py:185][0m |          -0.0175 |          58.1570 |          -5.1375 |
[32m[20221214 00:21:37 @agent_ppo2.py:185][0m |          -0.0115 |          59.2964 |          -5.1015 |
[32m[20221214 00:21:37 @agent_ppo2.py:185][0m |          -0.0242 |          56.4368 |          -5.0762 |
[32m[20221214 00:21:37 @agent_ppo2.py:185][0m |          -0.0207 |          56.2162 |          -4.9710 |
[32m[20221214 00:21:37 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:21:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 529.08
[32m[20221214 00:21:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 660.46
[32m[20221214 00:21:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 513.07
[32m[20221214 00:21:38 @agent_ppo2.py:143][0m Total time:      24.12 min
[32m[20221214 00:21:38 @agent_ppo2.py:145][0m 2207744 total steps have happened
[32m[20221214 00:21:38 @agent_ppo2.py:121][0m #------------------------ Iteration 5078 --------------------------#
[32m[20221214 00:21:38 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:21:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:21:38 @agent_ppo2.py:185][0m |           0.0014 |          84.8768 |          -6.3843 |
[32m[20221214 00:21:38 @agent_ppo2.py:185][0m |          -0.0015 |          79.8526 |          -6.3285 |
[32m[20221214 00:21:38 @agent_ppo2.py:185][0m |          -0.0022 |          78.5723 |          -6.4979 |
[32m[20221214 00:21:38 @agent_ppo2.py:185][0m |          -0.0069 |          77.5962 |          -6.4658 |
[32m[20221214 00:21:38 @agent_ppo2.py:185][0m |          -0.0070 |          77.0492 |          -6.4767 |
[32m[20221214 00:21:38 @agent_ppo2.py:185][0m |          -0.0105 |          76.4824 |          -6.6033 |
[32m[20221214 00:21:39 @agent_ppo2.py:185][0m |          -0.0077 |          76.1187 |          -6.5926 |
[32m[20221214 00:21:39 @agent_ppo2.py:185][0m |          -0.0112 |          75.7729 |          -6.5079 |
[32m[20221214 00:21:39 @agent_ppo2.py:185][0m |          -0.0132 |          75.2736 |          -6.6427 |
[32m[20221214 00:21:39 @agent_ppo2.py:185][0m |          -0.0124 |          75.0896 |          -6.6890 |
[32m[20221214 00:21:39 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:21:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 551.75
[32m[20221214 00:21:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 584.98
[32m[20221214 00:21:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 542.89
[32m[20221214 00:21:39 @agent_ppo2.py:143][0m Total time:      24.14 min
[32m[20221214 00:21:39 @agent_ppo2.py:145][0m 2209792 total steps have happened
[32m[20221214 00:21:39 @agent_ppo2.py:121][0m #------------------------ Iteration 5079 --------------------------#
[32m[20221214 00:21:39 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:21:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:21:39 @agent_ppo2.py:185][0m |           0.0021 |          75.6690 |          -4.6480 |
[32m[20221214 00:21:39 @agent_ppo2.py:185][0m |          -0.0052 |          58.3360 |          -4.8023 |
[32m[20221214 00:21:40 @agent_ppo2.py:185][0m |          -0.0098 |          53.6300 |          -4.7036 |
[32m[20221214 00:21:40 @agent_ppo2.py:185][0m |          -0.0087 |          51.4183 |          -4.5915 |
[32m[20221214 00:21:40 @agent_ppo2.py:185][0m |          -0.0199 |          50.9926 |          -4.6104 |
[32m[20221214 00:21:40 @agent_ppo2.py:185][0m |          -0.0178 |          48.8343 |          -4.6476 |
[32m[20221214 00:21:40 @agent_ppo2.py:185][0m |          -0.0176 |          48.0721 |          -4.6092 |
[32m[20221214 00:21:40 @agent_ppo2.py:185][0m |          -0.0230 |          46.8900 |          -4.6597 |
[32m[20221214 00:21:40 @agent_ppo2.py:185][0m |          -0.0226 |          46.3717 |          -4.6551 |
[32m[20221214 00:21:40 @agent_ppo2.py:185][0m |          -0.0163 |          46.3361 |          -4.7159 |
[32m[20221214 00:21:40 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:21:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 471.04
[32m[20221214 00:21:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 594.15
[32m[20221214 00:21:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 597.30
[32m[20221214 00:21:40 @agent_ppo2.py:143][0m Total time:      24.17 min
[32m[20221214 00:21:40 @agent_ppo2.py:145][0m 2211840 total steps have happened
[32m[20221214 00:21:40 @agent_ppo2.py:121][0m #------------------------ Iteration 5080 --------------------------#
[32m[20221214 00:21:41 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:21:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:21:41 @agent_ppo2.py:185][0m |          -0.0029 |          88.5404 |          -5.6339 |
[32m[20221214 00:21:41 @agent_ppo2.py:185][0m |          -0.0051 |          77.7972 |          -5.5398 |
[32m[20221214 00:21:41 @agent_ppo2.py:185][0m |          -0.0043 |          74.7452 |          -5.5335 |
[32m[20221214 00:21:41 @agent_ppo2.py:185][0m |          -0.0062 |          72.6240 |          -5.4391 |
[32m[20221214 00:21:41 @agent_ppo2.py:185][0m |          -0.0085 |          72.1243 |          -5.5554 |
[32m[20221214 00:21:41 @agent_ppo2.py:185][0m |          -0.0108 |          70.4949 |          -5.5244 |
[32m[20221214 00:21:41 @agent_ppo2.py:185][0m |          -0.0126 |          69.1983 |          -5.5696 |
[32m[20221214 00:21:41 @agent_ppo2.py:185][0m |          -0.0115 |          68.8022 |          -5.4347 |
[32m[20221214 00:21:41 @agent_ppo2.py:185][0m |          -0.0125 |          68.2776 |          -5.5277 |
[32m[20221214 00:21:42 @agent_ppo2.py:185][0m |          -0.0097 |          68.4410 |          -5.4142 |
[32m[20221214 00:21:42 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:21:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 518.66
[32m[20221214 00:21:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 571.09
[32m[20221214 00:21:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 560.52
[32m[20221214 00:21:42 @agent_ppo2.py:143][0m Total time:      24.19 min
[32m[20221214 00:21:42 @agent_ppo2.py:145][0m 2213888 total steps have happened
[32m[20221214 00:21:42 @agent_ppo2.py:121][0m #------------------------ Iteration 5081 --------------------------#
[32m[20221214 00:21:42 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:21:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:21:42 @agent_ppo2.py:185][0m |           0.0097 |          50.7661 |          -4.9975 |
[32m[20221214 00:21:42 @agent_ppo2.py:185][0m |          -0.0055 |          40.4678 |          -5.0929 |
[32m[20221214 00:21:42 @agent_ppo2.py:185][0m |          -0.0115 |          37.2541 |          -5.1005 |
[32m[20221214 00:21:42 @agent_ppo2.py:185][0m |          -0.0136 |          35.9310 |          -5.0451 |
[32m[20221214 00:21:42 @agent_ppo2.py:185][0m |          -0.0094 |          34.4585 |          -5.0470 |
[32m[20221214 00:21:43 @agent_ppo2.py:185][0m |          -0.0188 |          33.5374 |          -5.0391 |
[32m[20221214 00:21:43 @agent_ppo2.py:185][0m |          -0.0157 |          32.8641 |          -5.0605 |
[32m[20221214 00:21:43 @agent_ppo2.py:185][0m |          -0.0166 |          31.8281 |          -4.9713 |
[32m[20221214 00:21:43 @agent_ppo2.py:185][0m |          -0.0131 |          31.0264 |          -5.0205 |
[32m[20221214 00:21:43 @agent_ppo2.py:185][0m |          -0.0175 |          30.4809 |          -5.0100 |
[32m[20221214 00:21:43 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:21:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 408.28
[32m[20221214 00:21:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 535.56
[32m[20221214 00:21:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 508.99
[32m[20221214 00:21:43 @agent_ppo2.py:143][0m Total time:      24.21 min
[32m[20221214 00:21:43 @agent_ppo2.py:145][0m 2215936 total steps have happened
[32m[20221214 00:21:43 @agent_ppo2.py:121][0m #------------------------ Iteration 5082 --------------------------#
[32m[20221214 00:21:43 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:21:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:21:43 @agent_ppo2.py:185][0m |           0.0077 |          80.2913 |          -6.2075 |
[32m[20221214 00:21:44 @agent_ppo2.py:185][0m |          -0.0039 |          73.7401 |          -6.0547 |
[32m[20221214 00:21:44 @agent_ppo2.py:185][0m |          -0.0082 |          71.0001 |          -5.9947 |
[32m[20221214 00:21:44 @agent_ppo2.py:185][0m |          -0.0104 |          69.7002 |          -5.8637 |
[32m[20221214 00:21:44 @agent_ppo2.py:185][0m |          -0.0074 |          69.9969 |          -5.8165 |
[32m[20221214 00:21:44 @agent_ppo2.py:185][0m |          -0.0140 |          67.2156 |          -5.7652 |
[32m[20221214 00:21:44 @agent_ppo2.py:185][0m |          -0.0163 |          67.4082 |          -5.7312 |
[32m[20221214 00:21:44 @agent_ppo2.py:185][0m |          -0.0095 |          66.7261 |          -5.6856 |
[32m[20221214 00:21:44 @agent_ppo2.py:185][0m |          -0.0131 |          65.9123 |          -5.7511 |
[32m[20221214 00:21:44 @agent_ppo2.py:185][0m |          -0.0178 |          65.2269 |          -5.6559 |
[32m[20221214 00:21:44 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:21:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 438.45
[32m[20221214 00:21:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 510.44
[32m[20221214 00:21:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 447.10
[32m[20221214 00:21:44 @agent_ppo2.py:143][0m Total time:      24.24 min
[32m[20221214 00:21:44 @agent_ppo2.py:145][0m 2217984 total steps have happened
[32m[20221214 00:21:44 @agent_ppo2.py:121][0m #------------------------ Iteration 5083 --------------------------#
[32m[20221214 00:21:45 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:21:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:21:45 @agent_ppo2.py:185][0m |           0.0049 |          91.1930 |          -5.3248 |
[32m[20221214 00:21:45 @agent_ppo2.py:185][0m |          -0.0098 |          85.1172 |          -5.0796 |
[32m[20221214 00:21:45 @agent_ppo2.py:185][0m |          -0.0159 |          82.3670 |          -5.0980 |
[32m[20221214 00:21:45 @agent_ppo2.py:185][0m |          -0.0165 |          80.5863 |          -5.0436 |
[32m[20221214 00:21:45 @agent_ppo2.py:185][0m |          -0.0115 |          79.4394 |          -4.9329 |
[32m[20221214 00:21:45 @agent_ppo2.py:185][0m |          -0.0227 |          78.5897 |          -4.8475 |
[32m[20221214 00:21:45 @agent_ppo2.py:185][0m |          -0.0183 |          77.5145 |          -4.8843 |
[32m[20221214 00:21:46 @agent_ppo2.py:185][0m |          -0.0194 |          77.0417 |          -4.7837 |
[32m[20221214 00:21:46 @agent_ppo2.py:185][0m |          -0.0193 |          76.3539 |          -4.8880 |
[32m[20221214 00:21:46 @agent_ppo2.py:185][0m |          -0.0215 |          76.0560 |          -4.7421 |
[32m[20221214 00:21:46 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:21:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 466.45
[32m[20221214 00:21:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 549.90
[32m[20221214 00:21:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 600.26
[32m[20221214 00:21:46 @agent_ppo2.py:143][0m Total time:      24.26 min
[32m[20221214 00:21:46 @agent_ppo2.py:145][0m 2220032 total steps have happened
[32m[20221214 00:21:46 @agent_ppo2.py:121][0m #------------------------ Iteration 5084 --------------------------#
[32m[20221214 00:21:46 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:21:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:21:46 @agent_ppo2.py:185][0m |           0.0069 |          95.0160 |          -3.9055 |
[32m[20221214 00:21:46 @agent_ppo2.py:185][0m |          -0.0050 |          87.1669 |          -3.8916 |
[32m[20221214 00:21:47 @agent_ppo2.py:185][0m |          -0.0057 |          85.1830 |          -3.7938 |
[32m[20221214 00:21:47 @agent_ppo2.py:185][0m |          -0.0052 |          87.2107 |          -3.7357 |
[32m[20221214 00:21:47 @agent_ppo2.py:185][0m |          -0.0058 |          84.0061 |          -3.7286 |
[32m[20221214 00:21:47 @agent_ppo2.py:185][0m |          -0.0120 |          81.2204 |          -3.5440 |
[32m[20221214 00:21:47 @agent_ppo2.py:185][0m |          -0.0115 |          80.7398 |          -3.6098 |
[32m[20221214 00:21:47 @agent_ppo2.py:185][0m |          -0.0162 |          79.9873 |          -3.5928 |
[32m[20221214 00:21:47 @agent_ppo2.py:185][0m |          -0.0179 |          79.5157 |          -3.4797 |
[32m[20221214 00:21:47 @agent_ppo2.py:185][0m |          -0.0153 |          78.7397 |          -3.4734 |
[32m[20221214 00:21:47 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:21:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 550.93
[32m[20221214 00:21:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 684.37
[32m[20221214 00:21:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 554.47
[32m[20221214 00:21:47 @agent_ppo2.py:143][0m Total time:      24.28 min
[32m[20221214 00:21:47 @agent_ppo2.py:145][0m 2222080 total steps have happened
[32m[20221214 00:21:47 @agent_ppo2.py:121][0m #------------------------ Iteration 5085 --------------------------#
[32m[20221214 00:21:48 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:21:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:21:48 @agent_ppo2.py:185][0m |           0.0038 |          86.5368 |          -4.9680 |
[32m[20221214 00:21:48 @agent_ppo2.py:185][0m |          -0.0044 |          80.0684 |          -5.0894 |
[32m[20221214 00:21:48 @agent_ppo2.py:185][0m |          -0.0050 |          77.6234 |          -4.9496 |
[32m[20221214 00:21:48 @agent_ppo2.py:185][0m |          -0.0134 |          76.0645 |          -4.9813 |
[32m[20221214 00:21:48 @agent_ppo2.py:185][0m |          -0.0119 |          75.2686 |          -5.0987 |
[32m[20221214 00:21:48 @agent_ppo2.py:185][0m |          -0.0114 |          74.4187 |          -5.1360 |
[32m[20221214 00:21:48 @agent_ppo2.py:185][0m |          -0.0138 |          74.8966 |          -5.0903 |
[32m[20221214 00:21:48 @agent_ppo2.py:185][0m |          -0.0215 |          73.4667 |          -5.1529 |
[32m[20221214 00:21:48 @agent_ppo2.py:185][0m |          -0.0199 |          72.5476 |          -5.0790 |
[32m[20221214 00:21:49 @agent_ppo2.py:185][0m |          -0.0173 |          72.3067 |          -5.1817 |
[32m[20221214 00:21:49 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:21:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 484.13
[32m[20221214 00:21:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 544.29
[32m[20221214 00:21:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 633.59
[32m[20221214 00:21:49 @agent_ppo2.py:143][0m Total time:      24.31 min
[32m[20221214 00:21:49 @agent_ppo2.py:145][0m 2224128 total steps have happened
[32m[20221214 00:21:49 @agent_ppo2.py:121][0m #------------------------ Iteration 5086 --------------------------#
[32m[20221214 00:21:49 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:21:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:21:49 @agent_ppo2.py:185][0m |           0.0018 |          98.0402 |          -5.1873 |
[32m[20221214 00:21:49 @agent_ppo2.py:185][0m |          -0.0019 |          92.8181 |          -5.2126 |
[32m[20221214 00:21:49 @agent_ppo2.py:185][0m |          -0.0114 |          90.5996 |          -5.1984 |
[32m[20221214 00:21:49 @agent_ppo2.py:185][0m |          -0.0084 |          90.7315 |          -5.1508 |
[32m[20221214 00:21:49 @agent_ppo2.py:185][0m |          -0.0148 |          87.9635 |          -5.1800 |
[32m[20221214 00:21:50 @agent_ppo2.py:185][0m |          -0.0172 |          87.6155 |          -5.1817 |
[32m[20221214 00:21:50 @agent_ppo2.py:185][0m |          -0.0147 |          86.5228 |          -5.0918 |
[32m[20221214 00:21:50 @agent_ppo2.py:185][0m |          -0.0158 |          86.7273 |          -5.0281 |
[32m[20221214 00:21:50 @agent_ppo2.py:185][0m |          -0.0180 |          85.3642 |          -5.1112 |
[32m[20221214 00:21:50 @agent_ppo2.py:185][0m |          -0.0195 |          85.2628 |          -4.9936 |
[32m[20221214 00:21:50 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:21:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 576.45
[32m[20221214 00:21:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 673.41
[32m[20221214 00:21:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 490.19
[32m[20221214 00:21:50 @agent_ppo2.py:143][0m Total time:      24.33 min
[32m[20221214 00:21:50 @agent_ppo2.py:145][0m 2226176 total steps have happened
[32m[20221214 00:21:50 @agent_ppo2.py:121][0m #------------------------ Iteration 5087 --------------------------#
[32m[20221214 00:21:50 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:21:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:21:50 @agent_ppo2.py:185][0m |           0.0006 |          97.5820 |          -4.3729 |
[32m[20221214 00:21:51 @agent_ppo2.py:185][0m |          -0.0047 |          86.9727 |          -4.3381 |
[32m[20221214 00:21:51 @agent_ppo2.py:185][0m |          -0.0074 |          83.5032 |          -4.3532 |
[32m[20221214 00:21:51 @agent_ppo2.py:185][0m |          -0.0097 |          81.7627 |          -4.3566 |
[32m[20221214 00:21:51 @agent_ppo2.py:185][0m |          -0.0100 |          79.9686 |          -4.3083 |
[32m[20221214 00:21:51 @agent_ppo2.py:185][0m |          -0.0042 |          79.8623 |          -4.2943 |
[32m[20221214 00:21:51 @agent_ppo2.py:185][0m |          -0.0123 |          77.9106 |          -4.2648 |
[32m[20221214 00:21:51 @agent_ppo2.py:185][0m |          -0.0103 |          77.5949 |          -4.4493 |
[32m[20221214 00:21:51 @agent_ppo2.py:185][0m |          -0.0115 |          76.7633 |          -4.3289 |
[32m[20221214 00:21:51 @agent_ppo2.py:185][0m |          -0.0186 |          75.7364 |          -4.3220 |
[32m[20221214 00:21:51 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:21:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 492.71
[32m[20221214 00:21:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 616.52
[32m[20221214 00:21:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 78.36
[32m[20221214 00:21:52 @agent_ppo2.py:143][0m Total time:      24.35 min
[32m[20221214 00:21:52 @agent_ppo2.py:145][0m 2228224 total steps have happened
[32m[20221214 00:21:52 @agent_ppo2.py:121][0m #------------------------ Iteration 5088 --------------------------#
[32m[20221214 00:21:52 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:21:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:21:52 @agent_ppo2.py:185][0m |           0.0065 |          93.6193 |          -4.3697 |
[32m[20221214 00:21:52 @agent_ppo2.py:185][0m |          -0.0055 |          85.1060 |          -4.3985 |
[32m[20221214 00:21:52 @agent_ppo2.py:185][0m |          -0.0074 |          82.4120 |          -4.4572 |
[32m[20221214 00:21:52 @agent_ppo2.py:185][0m |          -0.0096 |          80.7270 |          -4.4027 |
[32m[20221214 00:21:52 @agent_ppo2.py:185][0m |          -0.0090 |          79.4980 |          -4.4808 |
[32m[20221214 00:21:52 @agent_ppo2.py:185][0m |          -0.0118 |          78.6393 |          -4.4584 |
[32m[20221214 00:21:53 @agent_ppo2.py:185][0m |          -0.0110 |          77.7548 |          -4.5187 |
[32m[20221214 00:21:53 @agent_ppo2.py:185][0m |          -0.0123 |          77.2250 |          -4.5397 |
[32m[20221214 00:21:53 @agent_ppo2.py:185][0m |          -0.0124 |          76.7830 |          -4.5345 |
[32m[20221214 00:21:53 @agent_ppo2.py:185][0m |          -0.0127 |          76.1127 |          -4.5349 |
[32m[20221214 00:21:53 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:21:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 574.23
[32m[20221214 00:21:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 627.13
[32m[20221214 00:21:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 606.20
[32m[20221214 00:21:53 @agent_ppo2.py:143][0m Total time:      24.38 min
[32m[20221214 00:21:53 @agent_ppo2.py:145][0m 2230272 total steps have happened
[32m[20221214 00:21:53 @agent_ppo2.py:121][0m #------------------------ Iteration 5089 --------------------------#
[32m[20221214 00:21:53 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:21:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:21:53 @agent_ppo2.py:185][0m |           0.0019 |          82.9219 |          -5.2287 |
[32m[20221214 00:21:53 @agent_ppo2.py:185][0m |          -0.0078 |          74.6138 |          -5.2463 |
[32m[20221214 00:21:54 @agent_ppo2.py:185][0m |          -0.0130 |          71.1052 |          -5.1334 |
[32m[20221214 00:21:54 @agent_ppo2.py:185][0m |          -0.0076 |          69.9385 |          -5.1666 |
[32m[20221214 00:21:54 @agent_ppo2.py:185][0m |          -0.0090 |          67.7698 |          -5.0508 |
[32m[20221214 00:21:54 @agent_ppo2.py:185][0m |          -0.0197 |          66.7745 |          -5.0572 |
[32m[20221214 00:21:54 @agent_ppo2.py:185][0m |          -0.0114 |          65.1643 |          -5.0768 |
[32m[20221214 00:21:54 @agent_ppo2.py:185][0m |          -0.0163 |          62.9761 |          -4.9600 |
[32m[20221214 00:21:54 @agent_ppo2.py:185][0m |          -0.0194 |          61.3382 |          -5.0216 |
[32m[20221214 00:21:54 @agent_ppo2.py:185][0m |          -0.0152 |          60.1769 |          -4.9432 |
[32m[20221214 00:21:54 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221214 00:21:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 569.95
[32m[20221214 00:21:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 659.03
[32m[20221214 00:21:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 545.32
[32m[20221214 00:21:54 @agent_ppo2.py:143][0m Total time:      24.40 min
[32m[20221214 00:21:54 @agent_ppo2.py:145][0m 2232320 total steps have happened
[32m[20221214 00:21:54 @agent_ppo2.py:121][0m #------------------------ Iteration 5090 --------------------------#
[32m[20221214 00:21:55 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:21:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:21:55 @agent_ppo2.py:185][0m |           0.0013 |          97.9794 |          -4.1703 |
[32m[20221214 00:21:55 @agent_ppo2.py:185][0m |          -0.0037 |          88.9648 |          -4.0887 |
[32m[20221214 00:21:55 @agent_ppo2.py:185][0m |          -0.0060 |          85.5315 |          -3.9734 |
[32m[20221214 00:21:55 @agent_ppo2.py:185][0m |          -0.0095 |          83.3620 |          -3.8062 |
[32m[20221214 00:21:55 @agent_ppo2.py:185][0m |          -0.0101 |          82.9701 |          -3.7667 |
[32m[20221214 00:21:55 @agent_ppo2.py:185][0m |          -0.0126 |          81.6564 |          -3.6193 |
[32m[20221214 00:21:55 @agent_ppo2.py:185][0m |          -0.0135 |          80.4700 |          -3.4907 |
[32m[20221214 00:21:56 @agent_ppo2.py:185][0m |          -0.0126 |          79.6937 |          -3.5144 |
[32m[20221214 00:21:56 @agent_ppo2.py:185][0m |          -0.0136 |          78.7487 |          -3.4753 |
[32m[20221214 00:21:56 @agent_ppo2.py:185][0m |          -0.0152 |          79.3070 |          -3.2758 |
[32m[20221214 00:21:56 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:21:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 575.53
[32m[20221214 00:21:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 707.72
[32m[20221214 00:21:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 465.65
[32m[20221214 00:21:56 @agent_ppo2.py:143][0m Total time:      24.43 min
[32m[20221214 00:21:56 @agent_ppo2.py:145][0m 2234368 total steps have happened
[32m[20221214 00:21:56 @agent_ppo2.py:121][0m #------------------------ Iteration 5091 --------------------------#
[32m[20221214 00:21:56 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:21:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:21:56 @agent_ppo2.py:185][0m |           0.0006 |          93.0204 |          -2.5265 |
[32m[20221214 00:21:56 @agent_ppo2.py:185][0m |           0.0036 |          80.8065 |          -2.5458 |
[32m[20221214 00:21:56 @agent_ppo2.py:185][0m |          -0.0076 |          76.4404 |          -2.4654 |
[32m[20221214 00:21:57 @agent_ppo2.py:185][0m |          -0.0106 |          72.8892 |          -2.4879 |
[32m[20221214 00:21:57 @agent_ppo2.py:185][0m |          -0.0104 |          70.7483 |          -2.5210 |
[32m[20221214 00:21:57 @agent_ppo2.py:185][0m |          -0.0149 |          68.4867 |          -2.4797 |
[32m[20221214 00:21:57 @agent_ppo2.py:185][0m |          -0.0130 |          67.6794 |          -2.3709 |
[32m[20221214 00:21:57 @agent_ppo2.py:185][0m |          -0.0150 |          66.4851 |          -2.4204 |
[32m[20221214 00:21:57 @agent_ppo2.py:185][0m |          -0.0174 |          65.9084 |          -2.4343 |
[32m[20221214 00:21:57 @agent_ppo2.py:185][0m |          -0.0166 |          65.9732 |          -2.5269 |
[32m[20221214 00:21:57 @agent_ppo2.py:130][0m Policy update time: 1.30 s
[32m[20221214 00:21:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 513.25
[32m[20221214 00:21:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 601.68
[32m[20221214 00:21:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 515.50
[32m[20221214 00:21:58 @agent_ppo2.py:143][0m Total time:      24.45 min
[32m[20221214 00:21:58 @agent_ppo2.py:145][0m 2236416 total steps have happened
[32m[20221214 00:21:58 @agent_ppo2.py:121][0m #------------------------ Iteration 5092 --------------------------#
[32m[20221214 00:21:58 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:21:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:21:58 @agent_ppo2.py:185][0m |           0.0114 |          77.4225 |          -3.7184 |
[32m[20221214 00:21:58 @agent_ppo2.py:185][0m |          -0.0043 |          57.9111 |          -3.5563 |
[32m[20221214 00:21:58 @agent_ppo2.py:185][0m |          -0.0068 |          54.8036 |          -3.5913 |
[32m[20221214 00:21:58 @agent_ppo2.py:185][0m |          -0.0061 |          51.7789 |          -3.5577 |
[32m[20221214 00:21:58 @agent_ppo2.py:185][0m |          -0.0098 |          49.8923 |          -3.5149 |
[32m[20221214 00:21:58 @agent_ppo2.py:185][0m |          -0.0131 |          48.4434 |          -3.4576 |
[32m[20221214 00:21:58 @agent_ppo2.py:185][0m |          -0.0104 |          48.3909 |          -3.3096 |
[32m[20221214 00:21:59 @agent_ppo2.py:185][0m |          -0.0163 |          45.9919 |          -3.3853 |
[32m[20221214 00:21:59 @agent_ppo2.py:185][0m |          -0.0173 |          45.1886 |          -3.3147 |
[32m[20221214 00:21:59 @agent_ppo2.py:185][0m |          -0.0172 |          44.2517 |          -3.2883 |
[32m[20221214 00:21:59 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:21:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 349.73
[32m[20221214 00:21:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 466.09
[32m[20221214 00:21:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 465.23
[32m[20221214 00:21:59 @agent_ppo2.py:143][0m Total time:      24.48 min
[32m[20221214 00:21:59 @agent_ppo2.py:145][0m 2238464 total steps have happened
[32m[20221214 00:21:59 @agent_ppo2.py:121][0m #------------------------ Iteration 5093 --------------------------#
[32m[20221214 00:21:59 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:21:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:21:59 @agent_ppo2.py:185][0m |           0.0025 |          68.6442 |          -2.8987 |
[32m[20221214 00:21:59 @agent_ppo2.py:185][0m |          -0.0026 |          52.1121 |          -2.7146 |
[32m[20221214 00:22:00 @agent_ppo2.py:185][0m |          -0.0089 |          48.1132 |          -2.8392 |
[32m[20221214 00:22:00 @agent_ppo2.py:185][0m |          -0.0093 |          46.0849 |          -2.8235 |
[32m[20221214 00:22:00 @agent_ppo2.py:185][0m |          -0.0133 |          44.1837 |          -2.8838 |
[32m[20221214 00:22:00 @agent_ppo2.py:185][0m |          -0.0162 |          43.0909 |          -2.8737 |
[32m[20221214 00:22:00 @agent_ppo2.py:185][0m |          -0.0174 |          42.0135 |          -2.9837 |
[32m[20221214 00:22:00 @agent_ppo2.py:185][0m |          -0.0139 |          41.1664 |          -2.9249 |
[32m[20221214 00:22:00 @agent_ppo2.py:185][0m |          -0.0204 |          40.5539 |          -2.8754 |
[32m[20221214 00:22:00 @agent_ppo2.py:185][0m |          -0.0230 |          40.0786 |          -2.9623 |
[32m[20221214 00:22:00 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:22:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 387.75
[32m[20221214 00:22:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 493.53
[32m[20221214 00:22:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 266.76
[32m[20221214 00:22:00 @agent_ppo2.py:143][0m Total time:      24.50 min
[32m[20221214 00:22:00 @agent_ppo2.py:145][0m 2240512 total steps have happened
[32m[20221214 00:22:00 @agent_ppo2.py:121][0m #------------------------ Iteration 5094 --------------------------#
[32m[20221214 00:22:01 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:22:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:22:01 @agent_ppo2.py:185][0m |          -0.0010 |          73.0193 |          -2.7275 |
[32m[20221214 00:22:01 @agent_ppo2.py:185][0m |          -0.0056 |          60.8836 |          -2.6726 |
[32m[20221214 00:22:01 @agent_ppo2.py:185][0m |          -0.0098 |          54.5920 |          -2.7267 |
[32m[20221214 00:22:01 @agent_ppo2.py:185][0m |          -0.0171 |          51.5024 |          -2.6241 |
[32m[20221214 00:22:01 @agent_ppo2.py:185][0m |          -0.0030 |          51.4001 |          -2.6654 |
[32m[20221214 00:22:01 @agent_ppo2.py:185][0m |          -0.0145 |          47.1672 |          -2.6915 |
[32m[20221214 00:22:01 @agent_ppo2.py:185][0m |          -0.0134 |          45.9558 |          -2.6907 |
[32m[20221214 00:22:02 @agent_ppo2.py:185][0m |          -0.0182 |          44.7782 |          -2.6617 |
[32m[20221214 00:22:02 @agent_ppo2.py:185][0m |          -0.0180 |          43.1483 |          -2.5541 |
[32m[20221214 00:22:02 @agent_ppo2.py:185][0m |          -0.0178 |          42.5570 |          -2.5441 |
[32m[20221214 00:22:02 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 00:22:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 473.87
[32m[20221214 00:22:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 596.83
[32m[20221214 00:22:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 599.07
[32m[20221214 00:22:02 @agent_ppo2.py:143][0m Total time:      24.52 min
[32m[20221214 00:22:02 @agent_ppo2.py:145][0m 2242560 total steps have happened
[32m[20221214 00:22:02 @agent_ppo2.py:121][0m #------------------------ Iteration 5095 --------------------------#
[32m[20221214 00:22:02 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:22:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:22:02 @agent_ppo2.py:185][0m |          -0.0001 |          88.9173 |          -3.5266 |
[32m[20221214 00:22:02 @agent_ppo2.py:185][0m |          -0.0068 |          81.6568 |          -3.5696 |
[32m[20221214 00:22:02 @agent_ppo2.py:185][0m |          -0.0059 |          78.4994 |          -3.5388 |
[32m[20221214 00:22:03 @agent_ppo2.py:185][0m |          -0.0103 |          76.5557 |          -3.6966 |
[32m[20221214 00:22:03 @agent_ppo2.py:185][0m |          -0.0146 |          75.4928 |          -3.6749 |
[32m[20221214 00:22:03 @agent_ppo2.py:185][0m |          -0.0117 |          74.4161 |          -3.8459 |
[32m[20221214 00:22:03 @agent_ppo2.py:185][0m |          -0.0091 |          73.8455 |          -3.8164 |
[32m[20221214 00:22:03 @agent_ppo2.py:185][0m |          -0.0183 |          72.1409 |          -3.8276 |
[32m[20221214 00:22:03 @agent_ppo2.py:185][0m |          -0.0188 |          71.0648 |          -3.8084 |
[32m[20221214 00:22:03 @agent_ppo2.py:185][0m |          -0.0176 |          70.5728 |          -3.7405 |
[32m[20221214 00:22:03 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 00:22:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 470.01
[32m[20221214 00:22:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 563.54
[32m[20221214 00:22:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 577.21
[32m[20221214 00:22:03 @agent_ppo2.py:143][0m Total time:      24.55 min
[32m[20221214 00:22:03 @agent_ppo2.py:145][0m 2244608 total steps have happened
[32m[20221214 00:22:03 @agent_ppo2.py:121][0m #------------------------ Iteration 5096 --------------------------#
[32m[20221214 00:22:03 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:22:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:22:04 @agent_ppo2.py:185][0m |           0.0023 |          77.0055 |          -3.4762 |
[32m[20221214 00:22:04 @agent_ppo2.py:185][0m |          -0.0085 |          61.2473 |          -3.6375 |
[32m[20221214 00:22:04 @agent_ppo2.py:185][0m |           0.0055 |          63.9225 |          -3.3838 |
[32m[20221214 00:22:04 @agent_ppo2.py:185][0m |          -0.0099 |          57.8539 |          -3.3602 |
[32m[20221214 00:22:04 @agent_ppo2.py:185][0m |          -0.0115 |          55.1323 |          -3.3449 |
[32m[20221214 00:22:04 @agent_ppo2.py:185][0m |          -0.0044 |          54.0947 |          -3.2603 |
[32m[20221214 00:22:04 @agent_ppo2.py:185][0m |          -0.0123 |          53.4181 |          -3.2154 |
[32m[20221214 00:22:04 @agent_ppo2.py:185][0m |          -0.0199 |          52.8576 |          -3.2947 |
[32m[20221214 00:22:05 @agent_ppo2.py:185][0m |          -0.0122 |          52.1664 |          -3.1883 |
[32m[20221214 00:22:05 @agent_ppo2.py:185][0m |          -0.0166 |          51.5585 |          -3.1735 |
[32m[20221214 00:22:05 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:22:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 470.49
[32m[20221214 00:22:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 519.16
[32m[20221214 00:22:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 501.38
[32m[20221214 00:22:05 @agent_ppo2.py:143][0m Total time:      24.57 min
[32m[20221214 00:22:05 @agent_ppo2.py:145][0m 2246656 total steps have happened
[32m[20221214 00:22:05 @agent_ppo2.py:121][0m #------------------------ Iteration 5097 --------------------------#
[32m[20221214 00:22:05 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:22:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:22:05 @agent_ppo2.py:185][0m |           0.0045 |          88.1907 |          -3.4015 |
[32m[20221214 00:22:05 @agent_ppo2.py:185][0m |          -0.0052 |          82.7757 |          -3.4250 |
[32m[20221214 00:22:05 @agent_ppo2.py:185][0m |          -0.0064 |          81.0009 |          -3.4765 |
[32m[20221214 00:22:06 @agent_ppo2.py:185][0m |          -0.0089 |          80.2948 |          -3.5163 |
[32m[20221214 00:22:06 @agent_ppo2.py:185][0m |          -0.0147 |          79.0796 |          -3.5792 |
[32m[20221214 00:22:06 @agent_ppo2.py:185][0m |          -0.0161 |          78.7227 |          -3.6508 |
[32m[20221214 00:22:06 @agent_ppo2.py:185][0m |          -0.0168 |          77.6458 |          -3.6629 |
[32m[20221214 00:22:06 @agent_ppo2.py:185][0m |          -0.0101 |          77.5484 |          -3.6723 |
[32m[20221214 00:22:06 @agent_ppo2.py:185][0m |          -0.0156 |          76.8235 |          -3.7167 |
[32m[20221214 00:22:06 @agent_ppo2.py:185][0m |          -0.0156 |          76.0354 |          -3.6553 |
[32m[20221214 00:22:06 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:22:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 536.21
[32m[20221214 00:22:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 577.11
[32m[20221214 00:22:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 608.27
[32m[20221214 00:22:06 @agent_ppo2.py:143][0m Total time:      24.60 min
[32m[20221214 00:22:06 @agent_ppo2.py:145][0m 2248704 total steps have happened
[32m[20221214 00:22:06 @agent_ppo2.py:121][0m #------------------------ Iteration 5098 --------------------------#
[32m[20221214 00:22:06 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:22:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:22:07 @agent_ppo2.py:185][0m |          -0.0008 |          92.7816 |          -3.8315 |
[32m[20221214 00:22:07 @agent_ppo2.py:185][0m |          -0.0031 |          84.3706 |          -3.7514 |
[32m[20221214 00:22:07 @agent_ppo2.py:185][0m |          -0.0080 |          81.2824 |          -3.7760 |
[32m[20221214 00:22:07 @agent_ppo2.py:185][0m |          -0.0096 |          79.2724 |          -3.8337 |
[32m[20221214 00:22:07 @agent_ppo2.py:185][0m |          -0.0099 |          77.5835 |          -3.7368 |
[32m[20221214 00:22:07 @agent_ppo2.py:185][0m |          -0.0138 |          76.4441 |          -3.7344 |
[32m[20221214 00:22:07 @agent_ppo2.py:185][0m |          -0.0113 |          75.6843 |          -3.7340 |
[32m[20221214 00:22:07 @agent_ppo2.py:185][0m |          -0.0150 |          74.6341 |          -3.7585 |
[32m[20221214 00:22:07 @agent_ppo2.py:185][0m |          -0.0116 |          75.5036 |          -3.7114 |
[32m[20221214 00:22:08 @agent_ppo2.py:185][0m |          -0.0093 |          74.1586 |          -3.7225 |
[32m[20221214 00:22:08 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:22:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 523.38
[32m[20221214 00:22:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 593.84
[32m[20221214 00:22:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 484.49
[32m[20221214 00:22:08 @agent_ppo2.py:143][0m Total time:      24.62 min
[32m[20221214 00:22:08 @agent_ppo2.py:145][0m 2250752 total steps have happened
[32m[20221214 00:22:08 @agent_ppo2.py:121][0m #------------------------ Iteration 5099 --------------------------#
[32m[20221214 00:22:08 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:22:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:22:08 @agent_ppo2.py:185][0m |           0.0034 |          67.4893 |          -3.7169 |
[32m[20221214 00:22:08 @agent_ppo2.py:185][0m |          -0.0056 |          59.1621 |          -3.6668 |
[32m[20221214 00:22:08 @agent_ppo2.py:185][0m |          -0.0067 |          56.2848 |          -3.7636 |
[32m[20221214 00:22:08 @agent_ppo2.py:185][0m |          -0.0113 |          54.8624 |          -3.7200 |
[32m[20221214 00:22:08 @agent_ppo2.py:185][0m |          -0.0058 |          53.8430 |          -3.6512 |
[32m[20221214 00:22:09 @agent_ppo2.py:185][0m |          -0.0134 |          53.2295 |          -3.7495 |
[32m[20221214 00:22:09 @agent_ppo2.py:185][0m |          -0.0137 |          52.4827 |          -3.7051 |
[32m[20221214 00:22:09 @agent_ppo2.py:185][0m |          -0.0120 |          51.9783 |          -3.6969 |
[32m[20221214 00:22:09 @agent_ppo2.py:185][0m |          -0.0179 |          51.4330 |          -3.6582 |
[32m[20221214 00:22:09 @agent_ppo2.py:185][0m |          -0.0153 |          51.1704 |          -3.6562 |
[32m[20221214 00:22:09 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:22:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 490.57
[32m[20221214 00:22:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 511.37
[32m[20221214 00:22:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 631.53
[32m[20221214 00:22:09 @agent_ppo2.py:143][0m Total time:      24.64 min
[32m[20221214 00:22:09 @agent_ppo2.py:145][0m 2252800 total steps have happened
[32m[20221214 00:22:09 @agent_ppo2.py:121][0m #------------------------ Iteration 5100 --------------------------#
[32m[20221214 00:22:09 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:22:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:22:09 @agent_ppo2.py:185][0m |           0.0064 |          76.7454 |          -3.5428 |
[32m[20221214 00:22:10 @agent_ppo2.py:185][0m |          -0.0054 |          64.5925 |          -3.5863 |
[32m[20221214 00:22:10 @agent_ppo2.py:185][0m |          -0.0077 |          61.4370 |          -3.5549 |
[32m[20221214 00:22:10 @agent_ppo2.py:185][0m |          -0.0075 |          59.8105 |          -3.4835 |
[32m[20221214 00:22:10 @agent_ppo2.py:185][0m |          -0.0111 |          58.4095 |          -3.4264 |
[32m[20221214 00:22:10 @agent_ppo2.py:185][0m |          -0.0130 |          57.3589 |          -3.4724 |
[32m[20221214 00:22:10 @agent_ppo2.py:185][0m |          -0.0157 |          56.7363 |          -3.4588 |
[32m[20221214 00:22:10 @agent_ppo2.py:185][0m |          -0.0128 |          56.1295 |          -3.3867 |
[32m[20221214 00:22:10 @agent_ppo2.py:185][0m |          -0.0108 |          55.7421 |          -3.4220 |
[32m[20221214 00:22:10 @agent_ppo2.py:185][0m |          -0.0192 |          55.2621 |          -3.4857 |
[32m[20221214 00:22:10 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:22:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 542.73
[32m[20221214 00:22:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 626.54
[32m[20221214 00:22:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 678.82
[32m[20221214 00:22:10 @agent_ppo2.py:143][0m Total time:      24.67 min
[32m[20221214 00:22:10 @agent_ppo2.py:145][0m 2254848 total steps have happened
[32m[20221214 00:22:10 @agent_ppo2.py:121][0m #------------------------ Iteration 5101 --------------------------#
[32m[20221214 00:22:11 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:22:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:22:11 @agent_ppo2.py:185][0m |           0.0019 |          90.6593 |          -3.2278 |
[32m[20221214 00:22:11 @agent_ppo2.py:185][0m |          -0.0002 |          87.0749 |          -3.2145 |
[32m[20221214 00:22:11 @agent_ppo2.py:185][0m |          -0.0031 |          84.1967 |          -3.2230 |
[32m[20221214 00:22:11 @agent_ppo2.py:185][0m |          -0.0109 |          80.9802 |          -3.1970 |
[32m[20221214 00:22:11 @agent_ppo2.py:185][0m |          -0.0108 |          79.8057 |          -3.1135 |
[32m[20221214 00:22:11 @agent_ppo2.py:185][0m |          -0.0082 |          79.1768 |          -3.2032 |
[32m[20221214 00:22:11 @agent_ppo2.py:185][0m |          -0.0123 |          78.5610 |          -3.1815 |
[32m[20221214 00:22:12 @agent_ppo2.py:185][0m |          -0.0068 |          78.5269 |          -3.1768 |
[32m[20221214 00:22:12 @agent_ppo2.py:185][0m |          -0.0132 |          77.4767 |          -3.1653 |
[32m[20221214 00:22:12 @agent_ppo2.py:185][0m |          -0.0190 |          76.8899 |          -3.1593 |
[32m[20221214 00:22:12 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:22:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 554.29
[32m[20221214 00:22:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 591.22
[32m[20221214 00:22:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 670.18
[32m[20221214 00:22:12 @agent_ppo2.py:143][0m Total time:      24.69 min
[32m[20221214 00:22:12 @agent_ppo2.py:145][0m 2256896 total steps have happened
[32m[20221214 00:22:12 @agent_ppo2.py:121][0m #------------------------ Iteration 5102 --------------------------#
[32m[20221214 00:22:12 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:22:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:22:12 @agent_ppo2.py:185][0m |           0.0077 |          93.0637 |          -3.7161 |
[32m[20221214 00:22:12 @agent_ppo2.py:185][0m |          -0.0016 |          79.1337 |          -3.5625 |
[32m[20221214 00:22:12 @agent_ppo2.py:185][0m |          -0.0054 |          74.3169 |          -3.5774 |
[32m[20221214 00:22:13 @agent_ppo2.py:185][0m |          -0.0073 |          71.4877 |          -3.5408 |
[32m[20221214 00:22:13 @agent_ppo2.py:185][0m |          -0.0063 |          70.8104 |          -3.5527 |
[32m[20221214 00:22:13 @agent_ppo2.py:185][0m |          -0.0151 |          68.6773 |          -3.5522 |
[32m[20221214 00:22:13 @agent_ppo2.py:185][0m |          -0.0090 |          68.9507 |          -3.5768 |
[32m[20221214 00:22:13 @agent_ppo2.py:185][0m |          -0.0074 |          71.0790 |          -3.5394 |
[32m[20221214 00:22:13 @agent_ppo2.py:185][0m |          -0.0206 |          65.9931 |          -3.7045 |
[32m[20221214 00:22:13 @agent_ppo2.py:185][0m |          -0.0158 |          65.2907 |          -3.6068 |
[32m[20221214 00:22:13 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:22:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 498.56
[32m[20221214 00:22:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 578.75
[32m[20221214 00:22:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 305.04
[32m[20221214 00:22:13 @agent_ppo2.py:143][0m Total time:      24.72 min
[32m[20221214 00:22:13 @agent_ppo2.py:145][0m 2258944 total steps have happened
[32m[20221214 00:22:13 @agent_ppo2.py:121][0m #------------------------ Iteration 5103 --------------------------#
[32m[20221214 00:22:13 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:22:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:22:14 @agent_ppo2.py:185][0m |          -0.0048 |          88.9670 |          -3.7887 |
[32m[20221214 00:22:14 @agent_ppo2.py:185][0m |          -0.0064 |          72.3144 |          -3.8935 |
[32m[20221214 00:22:14 @agent_ppo2.py:185][0m |          -0.0115 |          67.1880 |          -3.9002 |
[32m[20221214 00:22:14 @agent_ppo2.py:185][0m |          -0.0096 |          64.3438 |          -4.0008 |
[32m[20221214 00:22:14 @agent_ppo2.py:185][0m |          -0.0103 |          62.6486 |          -4.0509 |
[32m[20221214 00:22:14 @agent_ppo2.py:185][0m |          -0.0169 |          61.3383 |          -4.0225 |
[32m[20221214 00:22:14 @agent_ppo2.py:185][0m |          -0.0060 |          69.4732 |          -4.0468 |
[32m[20221214 00:22:14 @agent_ppo2.py:185][0m |          -0.0154 |          60.3815 |          -4.0540 |
[32m[20221214 00:22:14 @agent_ppo2.py:185][0m |          -0.0169 |          58.3900 |          -4.1299 |
[32m[20221214 00:22:15 @agent_ppo2.py:185][0m |          -0.0181 |          57.6584 |          -4.2017 |
[32m[20221214 00:22:15 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 00:22:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 499.63
[32m[20221214 00:22:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 550.00
[32m[20221214 00:22:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 339.09
[32m[20221214 00:22:15 @agent_ppo2.py:143][0m Total time:      24.74 min
[32m[20221214 00:22:15 @agent_ppo2.py:145][0m 2260992 total steps have happened
[32m[20221214 00:22:15 @agent_ppo2.py:121][0m #------------------------ Iteration 5104 --------------------------#
[32m[20221214 00:22:15 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:22:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:22:15 @agent_ppo2.py:185][0m |           0.0009 |          67.8542 |          -4.5420 |
[32m[20221214 00:22:15 @agent_ppo2.py:185][0m |          -0.0073 |          55.6594 |          -4.5280 |
[32m[20221214 00:22:15 @agent_ppo2.py:185][0m |          -0.0085 |          51.2978 |          -4.5510 |
[32m[20221214 00:22:15 @agent_ppo2.py:185][0m |          -0.0146 |          49.2923 |          -4.4842 |
[32m[20221214 00:22:16 @agent_ppo2.py:185][0m |          -0.0103 |          47.5560 |          -4.5692 |
[32m[20221214 00:22:16 @agent_ppo2.py:185][0m |          -0.0147 |          46.0045 |          -4.4881 |
[32m[20221214 00:22:16 @agent_ppo2.py:185][0m |          -0.0147 |          45.1821 |          -4.6515 |
[32m[20221214 00:22:16 @agent_ppo2.py:185][0m |          -0.0150 |          44.3906 |          -4.5727 |
[32m[20221214 00:22:16 @agent_ppo2.py:185][0m |          -0.0210 |          43.9098 |          -4.6056 |
[32m[20221214 00:22:16 @agent_ppo2.py:185][0m |          -0.0210 |          43.2377 |          -4.6582 |
[32m[20221214 00:22:16 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:22:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 580.25
[32m[20221214 00:22:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 783.33
[32m[20221214 00:22:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 572.82
[32m[20221214 00:22:16 @agent_ppo2.py:143][0m Total time:      24.76 min
[32m[20221214 00:22:16 @agent_ppo2.py:145][0m 2263040 total steps have happened
[32m[20221214 00:22:16 @agent_ppo2.py:121][0m #------------------------ Iteration 5105 --------------------------#
[32m[20221214 00:22:16 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:22:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:22:17 @agent_ppo2.py:185][0m |           0.0001 |          71.7982 |          -4.2165 |
[32m[20221214 00:22:17 @agent_ppo2.py:185][0m |          -0.0108 |          59.1156 |          -4.1435 |
[32m[20221214 00:22:17 @agent_ppo2.py:185][0m |          -0.0063 |          60.4359 |          -4.2382 |
[32m[20221214 00:22:17 @agent_ppo2.py:185][0m |          -0.0089 |          54.7191 |          -4.1716 |
[32m[20221214 00:22:17 @agent_ppo2.py:185][0m |          -0.0105 |          54.0797 |          -4.0541 |
[32m[20221214 00:22:17 @agent_ppo2.py:185][0m |          -0.0072 |          51.6606 |          -3.9756 |
[32m[20221214 00:22:17 @agent_ppo2.py:185][0m |          -0.0179 |          50.6167 |          -3.9763 |
[32m[20221214 00:22:17 @agent_ppo2.py:185][0m |          -0.0212 |          51.4822 |          -3.9129 |
[32m[20221214 00:22:17 @agent_ppo2.py:185][0m |          -0.0176 |          49.9419 |          -3.8716 |
[32m[20221214 00:22:17 @agent_ppo2.py:185][0m |          -0.0138 |          50.2853 |          -3.9042 |
[32m[20221214 00:22:17 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:22:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 424.04
[32m[20221214 00:22:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 550.28
[32m[20221214 00:22:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 428.26
[32m[20221214 00:22:18 @agent_ppo2.py:143][0m Total time:      24.79 min
[32m[20221214 00:22:18 @agent_ppo2.py:145][0m 2265088 total steps have happened
[32m[20221214 00:22:18 @agent_ppo2.py:121][0m #------------------------ Iteration 5106 --------------------------#
[32m[20221214 00:22:18 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:22:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:22:18 @agent_ppo2.py:185][0m |          -0.0007 |          71.5331 |          -3.6181 |
[32m[20221214 00:22:18 @agent_ppo2.py:185][0m |          -0.0059 |          62.2415 |          -3.5975 |
[32m[20221214 00:22:18 @agent_ppo2.py:185][0m |          -0.0013 |          64.2984 |          -3.3890 |
[32m[20221214 00:22:18 @agent_ppo2.py:185][0m |          -0.0139 |          58.1583 |          -3.5129 |
[32m[20221214 00:22:18 @agent_ppo2.py:185][0m |          -0.0105 |          54.8362 |          -3.4659 |
[32m[20221214 00:22:18 @agent_ppo2.py:185][0m |          -0.0121 |          53.3811 |          -3.4598 |
[32m[20221214 00:22:19 @agent_ppo2.py:185][0m |          -0.0201 |          52.1605 |          -3.4422 |
[32m[20221214 00:22:19 @agent_ppo2.py:185][0m |          -0.0167 |          51.4532 |          -3.4374 |
[32m[20221214 00:22:19 @agent_ppo2.py:185][0m |          -0.0065 |          60.0852 |          -3.3901 |
[32m[20221214 00:22:19 @agent_ppo2.py:185][0m |          -0.0225 |          48.7974 |          -3.4185 |
[32m[20221214 00:22:19 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:22:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 457.50
[32m[20221214 00:22:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 642.07
[32m[20221214 00:22:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 569.20
[32m[20221214 00:22:19 @agent_ppo2.py:143][0m Total time:      24.81 min
[32m[20221214 00:22:19 @agent_ppo2.py:145][0m 2267136 total steps have happened
[32m[20221214 00:22:19 @agent_ppo2.py:121][0m #------------------------ Iteration 5107 --------------------------#
[32m[20221214 00:22:19 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:22:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:22:19 @agent_ppo2.py:185][0m |           0.0052 |          79.2258 |          -5.1018 |
[32m[20221214 00:22:20 @agent_ppo2.py:185][0m |          -0.0103 |          67.0139 |          -5.0177 |
[32m[20221214 00:22:20 @agent_ppo2.py:185][0m |          -0.0128 |          63.6133 |          -5.0395 |
[32m[20221214 00:22:20 @agent_ppo2.py:185][0m |          -0.0087 |          62.3570 |          -4.9911 |
[32m[20221214 00:22:20 @agent_ppo2.py:185][0m |          -0.0192 |          61.3119 |          -4.9402 |
[32m[20221214 00:22:20 @agent_ppo2.py:185][0m |          -0.0178 |          60.4681 |          -4.8581 |
[32m[20221214 00:22:20 @agent_ppo2.py:185][0m |          -0.0154 |          59.4061 |          -4.8690 |
[32m[20221214 00:22:20 @agent_ppo2.py:185][0m |          -0.0195 |          58.7319 |          -4.7812 |
[32m[20221214 00:22:20 @agent_ppo2.py:185][0m |          -0.0203 |          58.1097 |          -4.8063 |
[32m[20221214 00:22:20 @agent_ppo2.py:185][0m |          -0.0170 |          57.2719 |          -4.6961 |
[32m[20221214 00:22:20 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:22:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 472.85
[32m[20221214 00:22:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 549.47
[32m[20221214 00:22:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 560.54
[32m[20221214 00:22:21 @agent_ppo2.py:143][0m Total time:      24.84 min
[32m[20221214 00:22:21 @agent_ppo2.py:145][0m 2269184 total steps have happened
[32m[20221214 00:22:21 @agent_ppo2.py:121][0m #------------------------ Iteration 5108 --------------------------#
[32m[20221214 00:22:21 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:22:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:22:21 @agent_ppo2.py:185][0m |           0.0059 |          69.6910 |          -4.4697 |
[32m[20221214 00:22:21 @agent_ppo2.py:185][0m |          -0.0127 |          60.6090 |          -4.3923 |
[32m[20221214 00:22:21 @agent_ppo2.py:185][0m |          -0.0093 |          58.8246 |          -4.3292 |
[32m[20221214 00:22:21 @agent_ppo2.py:185][0m |          -0.0073 |          55.5555 |          -4.5121 |
[32m[20221214 00:22:21 @agent_ppo2.py:185][0m |          -0.0127 |          54.5390 |          -4.2958 |
[32m[20221214 00:22:21 @agent_ppo2.py:185][0m |          -0.0127 |          53.0209 |          -4.3657 |
[32m[20221214 00:22:21 @agent_ppo2.py:185][0m |          -0.0166 |          50.8575 |          -4.3241 |
[32m[20221214 00:22:22 @agent_ppo2.py:185][0m |          -0.0227 |          50.2247 |          -4.3317 |
[32m[20221214 00:22:22 @agent_ppo2.py:185][0m |          -0.0207 |          49.5548 |          -4.2258 |
[32m[20221214 00:22:22 @agent_ppo2.py:185][0m |          -0.0236 |          49.2628 |          -4.2601 |
[32m[20221214 00:22:22 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:22:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 433.11
[32m[20221214 00:22:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 573.13
[32m[20221214 00:22:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 719.32
[32m[20221214 00:22:22 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 719.32
[32m[20221214 00:22:22 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 719.32
[32m[20221214 00:22:22 @agent_ppo2.py:143][0m Total time:      24.86 min
[32m[20221214 00:22:22 @agent_ppo2.py:145][0m 2271232 total steps have happened
[32m[20221214 00:22:22 @agent_ppo2.py:121][0m #------------------------ Iteration 5109 --------------------------#
[32m[20221214 00:22:22 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:22:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:22:22 @agent_ppo2.py:185][0m |           0.0021 |          80.4862 |          -4.5661 |
[32m[20221214 00:22:22 @agent_ppo2.py:185][0m |          -0.0062 |          73.0421 |          -4.4833 |
[32m[20221214 00:22:22 @agent_ppo2.py:185][0m |           0.0028 |          72.1514 |          -4.3169 |
[32m[20221214 00:22:23 @agent_ppo2.py:185][0m |          -0.0026 |          68.7493 |          -4.4807 |
[32m[20221214 00:22:23 @agent_ppo2.py:185][0m |          -0.0119 |          67.1485 |          -4.3610 |
[32m[20221214 00:22:23 @agent_ppo2.py:185][0m |          -0.0111 |          65.4423 |          -4.3851 |
[32m[20221214 00:22:23 @agent_ppo2.py:185][0m |          -0.0172 |          65.0322 |          -4.3466 |
[32m[20221214 00:22:23 @agent_ppo2.py:185][0m |          -0.0150 |          64.3964 |          -4.4154 |
[32m[20221214 00:22:23 @agent_ppo2.py:185][0m |          -0.0124 |          66.4866 |          -4.3705 |
[32m[20221214 00:22:23 @agent_ppo2.py:185][0m |          -0.0147 |          62.8319 |          -4.3987 |
[32m[20221214 00:22:23 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:22:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 390.91
[32m[20221214 00:22:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 507.41
[32m[20221214 00:22:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 573.74
[32m[20221214 00:22:23 @agent_ppo2.py:143][0m Total time:      24.88 min
[32m[20221214 00:22:23 @agent_ppo2.py:145][0m 2273280 total steps have happened
[32m[20221214 00:22:23 @agent_ppo2.py:121][0m #------------------------ Iteration 5110 --------------------------#
[32m[20221214 00:22:23 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:22:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:22:24 @agent_ppo2.py:185][0m |          -0.0059 |          64.1908 |          -3.4662 |
[32m[20221214 00:22:24 @agent_ppo2.py:185][0m |           0.0000 |          60.0228 |          -3.7334 |
[32m[20221214 00:22:24 @agent_ppo2.py:185][0m |          -0.0098 |          54.7389 |          -3.6829 |
[32m[20221214 00:22:24 @agent_ppo2.py:185][0m |          -0.0170 |          53.2178 |          -3.6385 |
[32m[20221214 00:22:24 @agent_ppo2.py:185][0m |          -0.0149 |          52.4519 |          -3.5520 |
[32m[20221214 00:22:24 @agent_ppo2.py:185][0m |          -0.0154 |          51.4439 |          -3.6280 |
[32m[20221214 00:22:24 @agent_ppo2.py:185][0m |          -0.0164 |          50.9957 |          -3.6619 |
[32m[20221214 00:22:24 @agent_ppo2.py:185][0m |          -0.0177 |          50.5190 |          -3.7005 |
[32m[20221214 00:22:24 @agent_ppo2.py:185][0m |          -0.0161 |          50.0748 |          -3.7063 |
[32m[20221214 00:22:25 @agent_ppo2.py:185][0m |          -0.0215 |          49.4473 |          -3.6733 |
[32m[20221214 00:22:25 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:22:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 552.66
[32m[20221214 00:22:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 632.52
[32m[20221214 00:22:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 667.67
[32m[20221214 00:22:25 @agent_ppo2.py:143][0m Total time:      24.91 min
[32m[20221214 00:22:25 @agent_ppo2.py:145][0m 2275328 total steps have happened
[32m[20221214 00:22:25 @agent_ppo2.py:121][0m #------------------------ Iteration 5111 --------------------------#
[32m[20221214 00:22:25 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:22:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:22:25 @agent_ppo2.py:185][0m |           0.0117 |          76.1913 |          -4.3976 |
[32m[20221214 00:22:25 @agent_ppo2.py:185][0m |          -0.0005 |          61.9362 |          -4.3644 |
[32m[20221214 00:22:25 @agent_ppo2.py:185][0m |          -0.0094 |          58.7386 |          -4.2901 |
[32m[20221214 00:22:25 @agent_ppo2.py:185][0m |          -0.0154 |          57.3103 |          -4.3489 |
[32m[20221214 00:22:25 @agent_ppo2.py:185][0m |          -0.0151 |          56.1174 |          -4.3809 |
[32m[20221214 00:22:26 @agent_ppo2.py:185][0m |          -0.0105 |          56.3069 |          -4.2225 |
[32m[20221214 00:22:26 @agent_ppo2.py:185][0m |          -0.0119 |          54.4203 |          -4.2607 |
[32m[20221214 00:22:26 @agent_ppo2.py:185][0m |          -0.0183 |          53.6184 |          -4.3146 |
[32m[20221214 00:22:26 @agent_ppo2.py:185][0m |          -0.0154 |          53.4206 |          -4.4185 |
[32m[20221214 00:22:26 @agent_ppo2.py:185][0m |          -0.0229 |          52.8511 |          -4.3681 |
[32m[20221214 00:22:26 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:22:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 469.62
[32m[20221214 00:22:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 582.05
[32m[20221214 00:22:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 534.67
[32m[20221214 00:22:26 @agent_ppo2.py:143][0m Total time:      24.93 min
[32m[20221214 00:22:26 @agent_ppo2.py:145][0m 2277376 total steps have happened
[32m[20221214 00:22:26 @agent_ppo2.py:121][0m #------------------------ Iteration 5112 --------------------------#
[32m[20221214 00:22:26 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:22:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:22:26 @agent_ppo2.py:185][0m |           0.0116 |          67.9519 |          -5.0349 |
[32m[20221214 00:22:27 @agent_ppo2.py:185][0m |          -0.0104 |          55.2711 |          -4.9537 |
[32m[20221214 00:22:27 @agent_ppo2.py:185][0m |          -0.0129 |          51.9914 |          -5.0045 |
[32m[20221214 00:22:27 @agent_ppo2.py:185][0m |          -0.0074 |          50.9729 |          -4.9082 |
[32m[20221214 00:22:27 @agent_ppo2.py:185][0m |          -0.0153 |          49.1122 |          -4.9661 |
[32m[20221214 00:22:27 @agent_ppo2.py:185][0m |          -0.0183 |          48.6058 |          -5.0085 |
[32m[20221214 00:22:27 @agent_ppo2.py:185][0m |          -0.0169 |          47.0742 |          -5.0254 |
[32m[20221214 00:22:27 @agent_ppo2.py:185][0m |          -0.0201 |          46.4398 |          -5.0234 |
[32m[20221214 00:22:27 @agent_ppo2.py:185][0m |          -0.0173 |          45.7914 |          -4.9911 |
[32m[20221214 00:22:27 @agent_ppo2.py:185][0m |          -0.0182 |          45.1015 |          -4.9776 |
[32m[20221214 00:22:27 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:22:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 417.59
[32m[20221214 00:22:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 468.17
[32m[20221214 00:22:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 675.70
[32m[20221214 00:22:27 @agent_ppo2.py:143][0m Total time:      24.95 min
[32m[20221214 00:22:27 @agent_ppo2.py:145][0m 2279424 total steps have happened
[32m[20221214 00:22:27 @agent_ppo2.py:121][0m #------------------------ Iteration 5113 --------------------------#
[32m[20221214 00:22:28 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:22:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:22:28 @agent_ppo2.py:185][0m |           0.0004 |          61.3180 |          -4.3490 |
[32m[20221214 00:22:28 @agent_ppo2.py:185][0m |          -0.0106 |          53.8665 |          -4.1188 |
[32m[20221214 00:22:28 @agent_ppo2.py:185][0m |          -0.0093 |          50.1871 |          -4.2701 |
[32m[20221214 00:22:28 @agent_ppo2.py:185][0m |          -0.0155 |          48.1235 |          -4.1592 |
[32m[20221214 00:22:28 @agent_ppo2.py:185][0m |          -0.0139 |          46.8722 |          -4.1572 |
[32m[20221214 00:22:28 @agent_ppo2.py:185][0m |          -0.0162 |          45.7930 |          -4.0804 |
[32m[20221214 00:22:28 @agent_ppo2.py:185][0m |          -0.0149 |          44.8219 |          -4.0326 |
[32m[20221214 00:22:28 @agent_ppo2.py:185][0m |          -0.0162 |          43.6731 |          -4.0542 |
[32m[20221214 00:22:29 @agent_ppo2.py:185][0m |          -0.0202 |          42.9752 |          -4.0439 |
[32m[20221214 00:22:29 @agent_ppo2.py:185][0m |          -0.0212 |          42.6017 |          -4.0878 |
[32m[20221214 00:22:29 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:22:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 473.36
[32m[20221214 00:22:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 504.31
[32m[20221214 00:22:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 519.74
[32m[20221214 00:22:29 @agent_ppo2.py:143][0m Total time:      24.98 min
[32m[20221214 00:22:29 @agent_ppo2.py:145][0m 2281472 total steps have happened
[32m[20221214 00:22:29 @agent_ppo2.py:121][0m #------------------------ Iteration 5114 --------------------------#
[32m[20221214 00:22:29 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:22:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:22:29 @agent_ppo2.py:185][0m |           0.0072 |          88.0784 |          -5.4361 |
[32m[20221214 00:22:29 @agent_ppo2.py:185][0m |           0.0011 |          76.5485 |          -5.3787 |
[32m[20221214 00:22:29 @agent_ppo2.py:185][0m |          -0.0111 |          70.5758 |          -5.5388 |
[32m[20221214 00:22:30 @agent_ppo2.py:185][0m |          -0.0132 |          68.2180 |          -5.5090 |
[32m[20221214 00:22:30 @agent_ppo2.py:185][0m |          -0.0178 |          66.4013 |          -5.5090 |
[32m[20221214 00:22:30 @agent_ppo2.py:185][0m |          -0.0178 |          65.5409 |          -5.5267 |
[32m[20221214 00:22:30 @agent_ppo2.py:185][0m |          -0.0148 |          64.1404 |          -5.6570 |
[32m[20221214 00:22:30 @agent_ppo2.py:185][0m |          -0.0044 |          69.5407 |          -5.6410 |
[32m[20221214 00:22:30 @agent_ppo2.py:185][0m |          -0.0178 |          62.3501 |          -5.5532 |
[32m[20221214 00:22:30 @agent_ppo2.py:185][0m |          -0.0245 |          60.5915 |          -5.6396 |
[32m[20221214 00:22:30 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:22:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 498.05
[32m[20221214 00:22:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 601.10
[32m[20221214 00:22:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 669.11
[32m[20221214 00:22:30 @agent_ppo2.py:143][0m Total time:      25.00 min
[32m[20221214 00:22:30 @agent_ppo2.py:145][0m 2283520 total steps have happened
[32m[20221214 00:22:30 @agent_ppo2.py:121][0m #------------------------ Iteration 5115 --------------------------#
[32m[20221214 00:22:31 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 00:22:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:22:31 @agent_ppo2.py:185][0m |          -0.0008 |          69.9810 |          -5.8715 |
[32m[20221214 00:22:31 @agent_ppo2.py:185][0m |          -0.0061 |          60.4032 |          -5.9063 |
[32m[20221214 00:22:31 @agent_ppo2.py:185][0m |          -0.0122 |          58.7044 |          -5.8446 |
[32m[20221214 00:22:31 @agent_ppo2.py:185][0m |          -0.0113 |          56.8153 |          -5.8284 |
[32m[20221214 00:22:31 @agent_ppo2.py:185][0m |          -0.0163 |          55.8039 |          -5.7233 |
[32m[20221214 00:22:31 @agent_ppo2.py:185][0m |          -0.0123 |          54.8868 |          -5.7847 |
[32m[20221214 00:22:31 @agent_ppo2.py:185][0m |          -0.0194 |          54.2723 |          -5.7699 |
[32m[20221214 00:22:31 @agent_ppo2.py:185][0m |          -0.0164 |          53.2290 |          -5.7725 |
[32m[20221214 00:22:32 @agent_ppo2.py:185][0m |          -0.0184 |          53.0459 |          -5.7345 |
[32m[20221214 00:22:32 @agent_ppo2.py:185][0m |          -0.0154 |          52.0450 |          -5.7043 |
[32m[20221214 00:22:32 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:22:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 480.32
[32m[20221214 00:22:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 556.07
[32m[20221214 00:22:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 481.21
[32m[20221214 00:22:32 @agent_ppo2.py:143][0m Total time:      25.02 min
[32m[20221214 00:22:32 @agent_ppo2.py:145][0m 2285568 total steps have happened
[32m[20221214 00:22:32 @agent_ppo2.py:121][0m #------------------------ Iteration 5116 --------------------------#
[32m[20221214 00:22:32 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:22:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:22:32 @agent_ppo2.py:185][0m |           0.0046 |          82.2350 |          -5.4637 |
[32m[20221214 00:22:32 @agent_ppo2.py:185][0m |          -0.0047 |          75.0642 |          -5.4662 |
[32m[20221214 00:22:32 @agent_ppo2.py:185][0m |          -0.0063 |          73.0541 |          -5.5203 |
[32m[20221214 00:22:32 @agent_ppo2.py:185][0m |          -0.0086 |          71.5282 |          -5.4447 |
[32m[20221214 00:22:33 @agent_ppo2.py:185][0m |           0.0042 |          74.3268 |          -5.4998 |
[32m[20221214 00:22:33 @agent_ppo2.py:185][0m |          -0.0077 |          69.6385 |          -5.3604 |
[32m[20221214 00:22:33 @agent_ppo2.py:185][0m |          -0.0055 |          73.3069 |          -5.3897 |
[32m[20221214 00:22:33 @agent_ppo2.py:185][0m |          -0.0057 |          69.1492 |          -5.3233 |
[32m[20221214 00:22:33 @agent_ppo2.py:185][0m |           0.0013 |          72.3710 |          -5.4645 |
[32m[20221214 00:22:33 @agent_ppo2.py:185][0m |          -0.0024 |          71.0729 |          -5.4454 |
[32m[20221214 00:22:33 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:22:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 616.64
[32m[20221214 00:22:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 757.28
[32m[20221214 00:22:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 604.81
[32m[20221214 00:22:33 @agent_ppo2.py:143][0m Total time:      25.05 min
[32m[20221214 00:22:33 @agent_ppo2.py:145][0m 2287616 total steps have happened
[32m[20221214 00:22:33 @agent_ppo2.py:121][0m #------------------------ Iteration 5117 --------------------------#
[32m[20221214 00:22:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:22:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:22:34 @agent_ppo2.py:185][0m |          -0.0010 |          78.2863 |          -5.2472 |
[32m[20221214 00:22:34 @agent_ppo2.py:185][0m |          -0.0057 |          66.1171 |          -5.1850 |
[32m[20221214 00:22:34 @agent_ppo2.py:185][0m |          -0.0132 |          62.4380 |          -5.2474 |
[32m[20221214 00:22:34 @agent_ppo2.py:185][0m |          -0.0103 |          60.6398 |          -5.2883 |
[32m[20221214 00:22:34 @agent_ppo2.py:185][0m |          -0.0161 |          59.0202 |          -5.3151 |
[32m[20221214 00:22:34 @agent_ppo2.py:185][0m |          -0.0151 |          57.8895 |          -5.4005 |
[32m[20221214 00:22:34 @agent_ppo2.py:185][0m |          -0.0099 |          60.4196 |          -5.4486 |
[32m[20221214 00:22:34 @agent_ppo2.py:185][0m |          -0.0146 |          56.3252 |          -5.4659 |
[32m[20221214 00:22:34 @agent_ppo2.py:185][0m |          -0.0183 |          55.1614 |          -5.4416 |
[32m[20221214 00:22:34 @agent_ppo2.py:185][0m |          -0.0192 |          54.5343 |          -5.3946 |
[32m[20221214 00:22:34 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:22:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 424.82
[32m[20221214 00:22:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 528.09
[32m[20221214 00:22:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 446.62
[32m[20221214 00:22:35 @agent_ppo2.py:143][0m Total time:      25.07 min
[32m[20221214 00:22:35 @agent_ppo2.py:145][0m 2289664 total steps have happened
[32m[20221214 00:22:35 @agent_ppo2.py:121][0m #------------------------ Iteration 5118 --------------------------#
[32m[20221214 00:22:35 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:22:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:22:35 @agent_ppo2.py:185][0m |           0.0027 |          54.8710 |          -6.5591 |
[32m[20221214 00:22:35 @agent_ppo2.py:185][0m |          -0.0043 |          46.1649 |          -6.4686 |
[32m[20221214 00:22:35 @agent_ppo2.py:185][0m |          -0.0160 |          43.2129 |          -6.2857 |
[32m[20221214 00:22:35 @agent_ppo2.py:185][0m |          -0.0127 |          41.5404 |          -6.4253 |
[32m[20221214 00:22:35 @agent_ppo2.py:185][0m |          -0.0147 |          39.7910 |          -6.4952 |
[32m[20221214 00:22:35 @agent_ppo2.py:185][0m |          -0.0114 |          38.6854 |          -6.4109 |
[32m[20221214 00:22:36 @agent_ppo2.py:185][0m |          -0.0151 |          37.7560 |          -6.5518 |
[32m[20221214 00:22:36 @agent_ppo2.py:185][0m |          -0.0170 |          37.4791 |          -6.3856 |
[32m[20221214 00:22:36 @agent_ppo2.py:185][0m |          -0.0166 |          36.9290 |          -6.6365 |
[32m[20221214 00:22:36 @agent_ppo2.py:185][0m |          -0.0125 |          37.4030 |          -6.5554 |
[32m[20221214 00:22:36 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:22:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 559.52
[32m[20221214 00:22:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 604.66
[32m[20221214 00:22:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 629.15
[32m[20221214 00:22:36 @agent_ppo2.py:143][0m Total time:      25.09 min
[32m[20221214 00:22:36 @agent_ppo2.py:145][0m 2291712 total steps have happened
[32m[20221214 00:22:36 @agent_ppo2.py:121][0m #------------------------ Iteration 5119 --------------------------#
[32m[20221214 00:22:36 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:22:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:22:36 @agent_ppo2.py:185][0m |           0.0056 |          49.4426 |          -5.3810 |
[32m[20221214 00:22:36 @agent_ppo2.py:185][0m |          -0.0055 |          40.5211 |          -5.4629 |
[32m[20221214 00:22:37 @agent_ppo2.py:185][0m |          -0.0051 |          38.0089 |          -5.4881 |
[32m[20221214 00:22:37 @agent_ppo2.py:185][0m |          -0.0082 |          36.4778 |          -5.5232 |
[32m[20221214 00:22:37 @agent_ppo2.py:185][0m |          -0.0186 |          35.7339 |          -5.4755 |
[32m[20221214 00:22:37 @agent_ppo2.py:185][0m |          -0.0152 |          34.4917 |          -5.5536 |
[32m[20221214 00:22:37 @agent_ppo2.py:185][0m |          -0.0179 |          33.8667 |          -5.6551 |
[32m[20221214 00:22:37 @agent_ppo2.py:185][0m |          -0.0171 |          33.2033 |          -5.5484 |
[32m[20221214 00:22:37 @agent_ppo2.py:185][0m |          -0.0219 |          32.5241 |          -5.6659 |
[32m[20221214 00:22:37 @agent_ppo2.py:185][0m |          -0.0207 |          32.2353 |          -5.6405 |
[32m[20221214 00:22:37 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 00:22:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 395.98
[32m[20221214 00:22:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 550.08
[32m[20221214 00:22:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 600.88
[32m[20221214 00:22:37 @agent_ppo2.py:143][0m Total time:      25.12 min
[32m[20221214 00:22:37 @agent_ppo2.py:145][0m 2293760 total steps have happened
[32m[20221214 00:22:37 @agent_ppo2.py:121][0m #------------------------ Iteration 5120 --------------------------#
[32m[20221214 00:22:38 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:22:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:22:38 @agent_ppo2.py:185][0m |          -0.0018 |          76.4008 |          -7.1370 |
[32m[20221214 00:22:38 @agent_ppo2.py:185][0m |           0.0056 |          73.3258 |          -7.0941 |
[32m[20221214 00:22:38 @agent_ppo2.py:185][0m |          -0.0091 |          63.4490 |          -7.0052 |
[32m[20221214 00:22:38 @agent_ppo2.py:185][0m |          -0.0116 |          61.2132 |          -7.0611 |
[32m[20221214 00:22:38 @agent_ppo2.py:185][0m |          -0.0142 |          59.7119 |          -6.9983 |
[32m[20221214 00:22:38 @agent_ppo2.py:185][0m |          -0.0150 |          58.5238 |          -6.9365 |
[32m[20221214 00:22:38 @agent_ppo2.py:185][0m |          -0.0154 |          57.8288 |          -6.9233 |
[32m[20221214 00:22:39 @agent_ppo2.py:185][0m |          -0.0182 |          57.2970 |          -6.9559 |
[32m[20221214 00:22:39 @agent_ppo2.py:185][0m |          -0.0166 |          56.6526 |          -6.9034 |
[32m[20221214 00:22:39 @agent_ppo2.py:185][0m |          -0.0160 |          56.3084 |          -6.8714 |
[32m[20221214 00:22:39 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:22:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 593.65
[32m[20221214 00:22:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 641.99
[32m[20221214 00:22:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 558.66
[32m[20221214 00:22:39 @agent_ppo2.py:143][0m Total time:      25.14 min
[32m[20221214 00:22:39 @agent_ppo2.py:145][0m 2295808 total steps have happened
[32m[20221214 00:22:39 @agent_ppo2.py:121][0m #------------------------ Iteration 5121 --------------------------#
[32m[20221214 00:22:39 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:22:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:22:39 @agent_ppo2.py:185][0m |           0.0037 |          98.0554 |          -6.6878 |
[32m[20221214 00:22:39 @agent_ppo2.py:185][0m |          -0.0015 |          84.0048 |          -6.7086 |
[32m[20221214 00:22:39 @agent_ppo2.py:185][0m |          -0.0098 |          80.8283 |          -6.6583 |
[32m[20221214 00:22:40 @agent_ppo2.py:185][0m |          -0.0108 |          78.3606 |          -6.8460 |
[32m[20221214 00:22:40 @agent_ppo2.py:185][0m |          -0.0162 |          77.4153 |          -6.7521 |
[32m[20221214 00:22:40 @agent_ppo2.py:185][0m |          -0.0199 |          76.3688 |          -6.7924 |
[32m[20221214 00:22:40 @agent_ppo2.py:185][0m |          -0.0192 |          76.3281 |          -6.8392 |
[32m[20221214 00:22:40 @agent_ppo2.py:185][0m |          -0.0168 |          75.4205 |          -6.8644 |
[32m[20221214 00:22:40 @agent_ppo2.py:185][0m |          -0.0209 |          75.0052 |          -6.9545 |
[32m[20221214 00:22:40 @agent_ppo2.py:185][0m |          -0.0176 |          74.4025 |          -6.9608 |
[32m[20221214 00:22:40 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:22:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 468.25
[32m[20221214 00:22:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 527.31
[32m[20221214 00:22:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 600.52
[32m[20221214 00:22:40 @agent_ppo2.py:143][0m Total time:      25.17 min
[32m[20221214 00:22:40 @agent_ppo2.py:145][0m 2297856 total steps have happened
[32m[20221214 00:22:40 @agent_ppo2.py:121][0m #------------------------ Iteration 5122 --------------------------#
[32m[20221214 00:22:40 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:22:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:22:41 @agent_ppo2.py:185][0m |           0.0091 |          69.6234 |          -6.7818 |
[32m[20221214 00:22:41 @agent_ppo2.py:185][0m |          -0.0073 |          58.9401 |          -6.7061 |
[32m[20221214 00:22:41 @agent_ppo2.py:185][0m |          -0.0106 |          55.9256 |          -6.6954 |
[32m[20221214 00:22:41 @agent_ppo2.py:185][0m |          -0.0147 |          53.9854 |          -6.6912 |
[32m[20221214 00:22:41 @agent_ppo2.py:185][0m |          -0.0179 |          52.9065 |          -6.7359 |
[32m[20221214 00:22:41 @agent_ppo2.py:185][0m |          -0.0012 |          58.2247 |          -6.7715 |
[32m[20221214 00:22:41 @agent_ppo2.py:185][0m |          -0.0173 |          50.9943 |          -6.6734 |
[32m[20221214 00:22:41 @agent_ppo2.py:185][0m |          -0.0163 |          49.8669 |          -6.6829 |
[32m[20221214 00:22:41 @agent_ppo2.py:185][0m |          -0.0154 |          49.7641 |          -6.5943 |
[32m[20221214 00:22:42 @agent_ppo2.py:185][0m |          -0.0171 |          49.2225 |          -6.6751 |
[32m[20221214 00:22:42 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:22:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 464.58
[32m[20221214 00:22:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 517.89
[32m[20221214 00:22:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 513.90
[32m[20221214 00:22:42 @agent_ppo2.py:143][0m Total time:      25.19 min
[32m[20221214 00:22:42 @agent_ppo2.py:145][0m 2299904 total steps have happened
[32m[20221214 00:22:42 @agent_ppo2.py:121][0m #------------------------ Iteration 5123 --------------------------#
[32m[20221214 00:22:42 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:22:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:22:42 @agent_ppo2.py:185][0m |           0.0017 |          66.3675 |          -8.2076 |
[32m[20221214 00:22:42 @agent_ppo2.py:185][0m |           0.0092 |          61.5637 |          -8.2726 |
[32m[20221214 00:22:42 @agent_ppo2.py:185][0m |          -0.0071 |          56.0477 |          -8.1429 |
[32m[20221214 00:22:42 @agent_ppo2.py:185][0m |          -0.0122 |          53.2901 |          -8.1747 |
[32m[20221214 00:22:42 @agent_ppo2.py:185][0m |          -0.0061 |          53.4427 |          -8.2615 |
[32m[20221214 00:22:43 @agent_ppo2.py:185][0m |          -0.0095 |          51.9669 |          -8.3019 |
[32m[20221214 00:22:43 @agent_ppo2.py:185][0m |          -0.0090 |          50.6240 |          -8.3317 |
[32m[20221214 00:22:43 @agent_ppo2.py:185][0m |          -0.0051 |          51.3060 |          -8.3117 |
[32m[20221214 00:22:43 @agent_ppo2.py:185][0m |          -0.0151 |          49.3153 |          -8.4511 |
[32m[20221214 00:22:43 @agent_ppo2.py:185][0m |          -0.0170 |          48.8451 |          -8.4161 |
[32m[20221214 00:22:43 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:22:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 557.59
[32m[20221214 00:22:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 616.26
[32m[20221214 00:22:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 590.45
[32m[20221214 00:22:43 @agent_ppo2.py:143][0m Total time:      25.21 min
[32m[20221214 00:22:43 @agent_ppo2.py:145][0m 2301952 total steps have happened
[32m[20221214 00:22:43 @agent_ppo2.py:121][0m #------------------------ Iteration 5124 --------------------------#
[32m[20221214 00:22:43 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:22:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:22:44 @agent_ppo2.py:185][0m |           0.0002 |          88.4302 |          -7.6044 |
[32m[20221214 00:22:44 @agent_ppo2.py:185][0m |          -0.0087 |          82.4255 |          -7.6626 |
[32m[20221214 00:22:44 @agent_ppo2.py:185][0m |          -0.0092 |          79.9090 |          -7.7427 |
[32m[20221214 00:22:44 @agent_ppo2.py:185][0m |          -0.0022 |          80.7554 |          -7.5469 |
[32m[20221214 00:22:44 @agent_ppo2.py:185][0m |          -0.0111 |          77.3281 |          -7.5834 |
[32m[20221214 00:22:44 @agent_ppo2.py:185][0m |          -0.0138 |          77.4399 |          -7.7321 |
[32m[20221214 00:22:44 @agent_ppo2.py:185][0m |          -0.0138 |          76.7943 |          -7.6089 |
[32m[20221214 00:22:44 @agent_ppo2.py:185][0m |          -0.0138 |          76.5433 |          -7.6659 |
[32m[20221214 00:22:44 @agent_ppo2.py:185][0m |          -0.0170 |          75.5798 |          -7.6500 |
[32m[20221214 00:22:44 @agent_ppo2.py:185][0m |          -0.0087 |          86.8830 |          -7.6591 |
[32m[20221214 00:22:44 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:22:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 477.23
[32m[20221214 00:22:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 578.18
[32m[20221214 00:22:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 562.94
[32m[20221214 00:22:45 @agent_ppo2.py:143][0m Total time:      25.24 min
[32m[20221214 00:22:45 @agent_ppo2.py:145][0m 2304000 total steps have happened
[32m[20221214 00:22:45 @agent_ppo2.py:121][0m #------------------------ Iteration 5125 --------------------------#
[32m[20221214 00:22:45 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:22:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:22:45 @agent_ppo2.py:185][0m |           0.0003 |          79.2621 |          -7.9083 |
[32m[20221214 00:22:45 @agent_ppo2.py:185][0m |          -0.0080 |          74.2487 |          -7.7330 |
[32m[20221214 00:22:45 @agent_ppo2.py:185][0m |          -0.0044 |          72.9625 |          -7.7207 |
[32m[20221214 00:22:45 @agent_ppo2.py:185][0m |          -0.0139 |          70.3777 |          -7.6238 |
[32m[20221214 00:22:45 @agent_ppo2.py:185][0m |          -0.0160 |          69.7375 |          -7.5593 |
[32m[20221214 00:22:45 @agent_ppo2.py:185][0m |          -0.0018 |          72.3367 |          -7.4355 |
[32m[20221214 00:22:46 @agent_ppo2.py:185][0m |          -0.0186 |          67.5982 |          -7.4378 |
[32m[20221214 00:22:46 @agent_ppo2.py:185][0m |          -0.0149 |          67.4834 |          -7.3357 |
[32m[20221214 00:22:46 @agent_ppo2.py:185][0m |          -0.0180 |          66.1674 |          -7.2176 |
[32m[20221214 00:22:46 @agent_ppo2.py:185][0m |          -0.0180 |          65.5928 |          -7.2330 |
[32m[20221214 00:22:46 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:22:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 435.67
[32m[20221214 00:22:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 540.70
[32m[20221214 00:22:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 540.74
[32m[20221214 00:22:46 @agent_ppo2.py:143][0m Total time:      25.26 min
[32m[20221214 00:22:46 @agent_ppo2.py:145][0m 2306048 total steps have happened
[32m[20221214 00:22:46 @agent_ppo2.py:121][0m #------------------------ Iteration 5126 --------------------------#
[32m[20221214 00:22:46 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:22:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:22:46 @agent_ppo2.py:185][0m |          -0.0008 |          81.2958 |          -5.7214 |
[32m[20221214 00:22:46 @agent_ppo2.py:185][0m |          -0.0086 |          71.9824 |          -5.5911 |
[32m[20221214 00:22:47 @agent_ppo2.py:185][0m |          -0.0136 |          68.8735 |          -5.6015 |
[32m[20221214 00:22:47 @agent_ppo2.py:185][0m |          -0.0109 |          66.6323 |          -5.4870 |
[32m[20221214 00:22:47 @agent_ppo2.py:185][0m |          -0.0143 |          65.3199 |          -5.5183 |
[32m[20221214 00:22:47 @agent_ppo2.py:185][0m |          -0.0174 |          63.9122 |          -5.3450 |
[32m[20221214 00:22:47 @agent_ppo2.py:185][0m |          -0.0088 |          64.2827 |          -5.3358 |
[32m[20221214 00:22:47 @agent_ppo2.py:185][0m |          -0.0166 |          62.3369 |          -5.3597 |
[32m[20221214 00:22:47 @agent_ppo2.py:185][0m |          -0.0186 |          60.9465 |          -5.5064 |
[32m[20221214 00:22:47 @agent_ppo2.py:185][0m |          -0.0202 |          60.1538 |          -5.3975 |
[32m[20221214 00:22:47 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:22:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 557.23
[32m[20221214 00:22:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 631.90
[32m[20221214 00:22:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 517.88
[32m[20221214 00:22:47 @agent_ppo2.py:143][0m Total time:      25.28 min
[32m[20221214 00:22:47 @agent_ppo2.py:145][0m 2308096 total steps have happened
[32m[20221214 00:22:47 @agent_ppo2.py:121][0m #------------------------ Iteration 5127 --------------------------#
[32m[20221214 00:22:48 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:22:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:22:48 @agent_ppo2.py:185][0m |          -0.0037 |         102.9470 |          -5.1980 |
[32m[20221214 00:22:48 @agent_ppo2.py:185][0m |          -0.0070 |          94.8802 |          -5.1630 |
[32m[20221214 00:22:48 @agent_ppo2.py:185][0m |          -0.0062 |          91.9967 |          -5.2264 |
[32m[20221214 00:22:48 @agent_ppo2.py:185][0m |          -0.0128 |          90.5253 |          -5.1414 |
[32m[20221214 00:22:48 @agent_ppo2.py:185][0m |          -0.0165 |          88.0762 |          -5.3608 |
[32m[20221214 00:22:48 @agent_ppo2.py:185][0m |          -0.0154 |          87.4729 |          -5.2939 |
[32m[20221214 00:22:48 @agent_ppo2.py:185][0m |          -0.0154 |          86.7454 |          -5.3690 |
[32m[20221214 00:22:48 @agent_ppo2.py:185][0m |          -0.0173 |          86.2425 |          -5.3444 |
[32m[20221214 00:22:49 @agent_ppo2.py:185][0m |          -0.0197 |          85.9526 |          -5.4586 |
[32m[20221214 00:22:49 @agent_ppo2.py:185][0m |          -0.0177 |          85.2736 |          -5.4261 |
[32m[20221214 00:22:49 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:22:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 540.33
[32m[20221214 00:22:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 599.16
[32m[20221214 00:22:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 510.09
[32m[20221214 00:22:49 @agent_ppo2.py:143][0m Total time:      25.31 min
[32m[20221214 00:22:49 @agent_ppo2.py:145][0m 2310144 total steps have happened
[32m[20221214 00:22:49 @agent_ppo2.py:121][0m #------------------------ Iteration 5128 --------------------------#
[32m[20221214 00:22:49 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:22:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:22:49 @agent_ppo2.py:185][0m |           0.0028 |          86.0570 |          -6.4407 |
[32m[20221214 00:22:49 @agent_ppo2.py:185][0m |          -0.0066 |          81.2562 |          -6.5391 |
[32m[20221214 00:22:49 @agent_ppo2.py:185][0m |          -0.0057 |          78.9929 |          -6.7503 |
[32m[20221214 00:22:49 @agent_ppo2.py:185][0m |          -0.0073 |          77.8809 |          -6.6654 |
[32m[20221214 00:22:50 @agent_ppo2.py:185][0m |          -0.0094 |          76.9706 |          -6.8801 |
[32m[20221214 00:22:50 @agent_ppo2.py:185][0m |          -0.0141 |          76.4129 |          -6.9872 |
[32m[20221214 00:22:50 @agent_ppo2.py:185][0m |          -0.0128 |          75.4476 |          -6.8802 |
[32m[20221214 00:22:50 @agent_ppo2.py:185][0m |          -0.0141 |          75.5087 |          -6.8346 |
[32m[20221214 00:22:50 @agent_ppo2.py:185][0m |          -0.0171 |          74.7835 |          -7.0738 |
[32m[20221214 00:22:50 @agent_ppo2.py:185][0m |          -0.0032 |          82.7534 |          -7.2029 |
[32m[20221214 00:22:50 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:22:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 529.00
[32m[20221214 00:22:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 593.23
[32m[20221214 00:22:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 625.07
[32m[20221214 00:22:50 @agent_ppo2.py:143][0m Total time:      25.33 min
[32m[20221214 00:22:50 @agent_ppo2.py:145][0m 2312192 total steps have happened
[32m[20221214 00:22:50 @agent_ppo2.py:121][0m #------------------------ Iteration 5129 --------------------------#
[32m[20221214 00:22:50 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:22:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:22:51 @agent_ppo2.py:185][0m |           0.0054 |          91.5450 |          -8.9653 |
[32m[20221214 00:22:51 @agent_ppo2.py:185][0m |          -0.0013 |          81.9452 |          -8.8845 |
[32m[20221214 00:22:51 @agent_ppo2.py:185][0m |          -0.0081 |          78.1312 |          -8.7985 |
[32m[20221214 00:22:51 @agent_ppo2.py:185][0m |          -0.0110 |          75.9271 |          -8.6816 |
[32m[20221214 00:22:51 @agent_ppo2.py:185][0m |          -0.0167 |          74.4972 |          -8.7881 |
[32m[20221214 00:22:51 @agent_ppo2.py:185][0m |          -0.0118 |          73.7692 |          -8.7080 |
[32m[20221214 00:22:51 @agent_ppo2.py:185][0m |          -0.0060 |          74.5878 |          -8.5621 |
[32m[20221214 00:22:51 @agent_ppo2.py:185][0m |          -0.0184 |          72.5286 |          -8.6897 |
[32m[20221214 00:22:51 @agent_ppo2.py:185][0m |          -0.0135 |          73.3340 |          -8.5638 |
[32m[20221214 00:22:51 @agent_ppo2.py:185][0m |          -0.0189 |          72.1703 |          -8.5920 |
[32m[20221214 00:22:51 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:22:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 601.92
[32m[20221214 00:22:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 687.53
[32m[20221214 00:22:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 510.77
[32m[20221214 00:22:52 @agent_ppo2.py:143][0m Total time:      25.35 min
[32m[20221214 00:22:52 @agent_ppo2.py:145][0m 2314240 total steps have happened
[32m[20221214 00:22:52 @agent_ppo2.py:121][0m #------------------------ Iteration 5130 --------------------------#
[32m[20221214 00:22:52 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:22:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:22:52 @agent_ppo2.py:185][0m |          -0.0022 |          76.0796 |          -8.7575 |
[32m[20221214 00:22:52 @agent_ppo2.py:185][0m |          -0.0076 |          69.4683 |          -8.7608 |
[32m[20221214 00:22:52 @agent_ppo2.py:185][0m |          -0.0156 |          65.8792 |          -8.6298 |
[32m[20221214 00:22:52 @agent_ppo2.py:185][0m |          -0.0204 |          63.7877 |          -8.6506 |
[32m[20221214 00:22:52 @agent_ppo2.py:185][0m |          -0.0179 |          62.9821 |          -8.6515 |
[32m[20221214 00:22:52 @agent_ppo2.py:185][0m |          -0.0172 |          62.1459 |          -8.6517 |
[32m[20221214 00:22:53 @agent_ppo2.py:185][0m |          -0.0164 |          61.6809 |          -8.6249 |
[32m[20221214 00:22:53 @agent_ppo2.py:185][0m |          -0.0211 |          60.7663 |          -8.6320 |
[32m[20221214 00:22:53 @agent_ppo2.py:185][0m |          -0.0268 |          60.5213 |          -8.5517 |
[32m[20221214 00:22:53 @agent_ppo2.py:185][0m |          -0.0192 |          60.3247 |          -8.4755 |
[32m[20221214 00:22:53 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:22:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 517.59
[32m[20221214 00:22:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 626.33
[32m[20221214 00:22:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 516.41
[32m[20221214 00:22:53 @agent_ppo2.py:143][0m Total time:      25.38 min
[32m[20221214 00:22:53 @agent_ppo2.py:145][0m 2316288 total steps have happened
[32m[20221214 00:22:53 @agent_ppo2.py:121][0m #------------------------ Iteration 5131 --------------------------#
[32m[20221214 00:22:53 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:22:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:22:53 @agent_ppo2.py:185][0m |           0.0192 |          97.8866 |          -6.7810 |
[32m[20221214 00:22:53 @agent_ppo2.py:185][0m |          -0.0072 |          83.7306 |          -6.4270 |
[32m[20221214 00:22:54 @agent_ppo2.py:185][0m |          -0.0066 |          81.7732 |          -6.6424 |
[32m[20221214 00:22:54 @agent_ppo2.py:185][0m |          -0.0106 |          79.9751 |          -6.7337 |
[32m[20221214 00:22:54 @agent_ppo2.py:185][0m |          -0.0097 |          79.0372 |          -6.6814 |
[32m[20221214 00:22:54 @agent_ppo2.py:185][0m |          -0.0120 |          78.1426 |          -6.4493 |
[32m[20221214 00:22:54 @agent_ppo2.py:185][0m |          -0.0113 |          77.4273 |          -6.5614 |
[32m[20221214 00:22:54 @agent_ppo2.py:185][0m |          -0.0082 |          79.2151 |          -6.5123 |
[32m[20221214 00:22:54 @agent_ppo2.py:185][0m |          -0.0081 |          77.0235 |          -6.5362 |
[32m[20221214 00:22:54 @agent_ppo2.py:185][0m |          -0.0159 |          75.9945 |          -6.5799 |
[32m[20221214 00:22:54 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:22:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 578.44
[32m[20221214 00:22:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 630.75
[32m[20221214 00:22:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 618.63
[32m[20221214 00:22:54 @agent_ppo2.py:143][0m Total time:      25.40 min
[32m[20221214 00:22:54 @agent_ppo2.py:145][0m 2318336 total steps have happened
[32m[20221214 00:22:54 @agent_ppo2.py:121][0m #------------------------ Iteration 5132 --------------------------#
[32m[20221214 00:22:55 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:22:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:22:55 @agent_ppo2.py:185][0m |          -0.0004 |          51.8604 |          -6.5693 |
[32m[20221214 00:22:55 @agent_ppo2.py:185][0m |          -0.0052 |          46.0932 |          -6.6345 |
[32m[20221214 00:22:55 @agent_ppo2.py:185][0m |          -0.0048 |          44.6856 |          -6.5947 |
[32m[20221214 00:22:55 @agent_ppo2.py:185][0m |          -0.0135 |          43.1843 |          -6.6397 |
[32m[20221214 00:22:55 @agent_ppo2.py:185][0m |          -0.0069 |          43.0225 |          -6.5878 |
[32m[20221214 00:22:55 @agent_ppo2.py:185][0m |          -0.0091 |          42.2356 |          -6.4646 |
[32m[20221214 00:22:55 @agent_ppo2.py:185][0m |          -0.0103 |          41.6259 |          -6.4023 |
[32m[20221214 00:22:55 @agent_ppo2.py:185][0m |          -0.0173 |          41.2867 |          -6.4021 |
[32m[20221214 00:22:56 @agent_ppo2.py:185][0m |          -0.0151 |          41.0502 |          -6.5552 |
[32m[20221214 00:22:56 @agent_ppo2.py:185][0m |          -0.0176 |          40.7655 |          -6.3679 |
[32m[20221214 00:22:56 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:22:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 539.67
[32m[20221214 00:22:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 603.59
[32m[20221214 00:22:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 564.44
[32m[20221214 00:22:56 @agent_ppo2.py:143][0m Total time:      25.42 min
[32m[20221214 00:22:56 @agent_ppo2.py:145][0m 2320384 total steps have happened
[32m[20221214 00:22:56 @agent_ppo2.py:121][0m #------------------------ Iteration 5133 --------------------------#
[32m[20221214 00:22:56 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:22:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:22:56 @agent_ppo2.py:185][0m |          -0.0001 |          64.2009 |          -7.1593 |
[32m[20221214 00:22:56 @agent_ppo2.py:185][0m |          -0.0044 |          59.5447 |          -7.0532 |
[32m[20221214 00:22:56 @agent_ppo2.py:185][0m |          -0.0119 |          58.1672 |          -7.0429 |
[32m[20221214 00:22:56 @agent_ppo2.py:185][0m |          -0.0112 |          57.2300 |          -7.1310 |
[32m[20221214 00:22:57 @agent_ppo2.py:185][0m |          -0.0153 |          56.2735 |          -7.0428 |
[32m[20221214 00:22:57 @agent_ppo2.py:185][0m |          -0.0155 |          55.8517 |          -7.0698 |
[32m[20221214 00:22:57 @agent_ppo2.py:185][0m |          -0.0174 |          55.5252 |          -6.9934 |
[32m[20221214 00:22:57 @agent_ppo2.py:185][0m |          -0.0188 |          54.9821 |          -6.9304 |
[32m[20221214 00:22:57 @agent_ppo2.py:185][0m |          -0.0134 |          54.4816 |          -6.8859 |
[32m[20221214 00:22:57 @agent_ppo2.py:185][0m |          -0.0107 |          56.7152 |          -6.9423 |
[32m[20221214 00:22:57 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:22:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 512.36
[32m[20221214 00:22:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 558.33
[32m[20221214 00:22:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 567.18
[32m[20221214 00:22:57 @agent_ppo2.py:143][0m Total time:      25.45 min
[32m[20221214 00:22:57 @agent_ppo2.py:145][0m 2322432 total steps have happened
[32m[20221214 00:22:57 @agent_ppo2.py:121][0m #------------------------ Iteration 5134 --------------------------#
[32m[20221214 00:22:57 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:22:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:22:58 @agent_ppo2.py:185][0m |           0.0027 |          71.8748 |          -7.8541 |
[32m[20221214 00:22:58 @agent_ppo2.py:185][0m |          -0.0098 |          60.7744 |          -7.8097 |
[32m[20221214 00:22:58 @agent_ppo2.py:185][0m |          -0.0125 |          57.9269 |          -7.7415 |
[32m[20221214 00:22:58 @agent_ppo2.py:185][0m |          -0.0096 |          55.1220 |          -7.8136 |
[32m[20221214 00:22:58 @agent_ppo2.py:185][0m |          -0.0163 |          53.2208 |          -7.6766 |
[32m[20221214 00:22:58 @agent_ppo2.py:185][0m |          -0.0176 |          51.9322 |          -7.7143 |
[32m[20221214 00:22:58 @agent_ppo2.py:185][0m |          -0.0189 |          51.1234 |          -7.6466 |
[32m[20221214 00:22:58 @agent_ppo2.py:185][0m |          -0.0207 |          49.9979 |          -7.5360 |
[32m[20221214 00:22:58 @agent_ppo2.py:185][0m |          -0.0099 |          54.9484 |          -7.4491 |
[32m[20221214 00:22:58 @agent_ppo2.py:185][0m |          -0.0251 |          48.6957 |          -7.4309 |
[32m[20221214 00:22:58 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:22:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 481.88
[32m[20221214 00:22:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 604.72
[32m[20221214 00:22:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 575.28
[32m[20221214 00:22:59 @agent_ppo2.py:143][0m Total time:      25.47 min
[32m[20221214 00:22:59 @agent_ppo2.py:145][0m 2324480 total steps have happened
[32m[20221214 00:22:59 @agent_ppo2.py:121][0m #------------------------ Iteration 5135 --------------------------#
[32m[20221214 00:22:59 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:22:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:22:59 @agent_ppo2.py:185][0m |          -0.0063 |          19.2435 |          -6.6350 |
[32m[20221214 00:22:59 @agent_ppo2.py:185][0m |          -0.0009 |          17.2488 |          -6.4561 |
[32m[20221214 00:22:59 @agent_ppo2.py:185][0m |          -0.0076 |          16.7872 |          -6.6490 |
[32m[20221214 00:22:59 @agent_ppo2.py:185][0m |          -0.0081 |          16.6178 |          -6.9016 |
[32m[20221214 00:22:59 @agent_ppo2.py:185][0m |          -0.0028 |          16.6984 |          -6.9077 |
[32m[20221214 00:22:59 @agent_ppo2.py:185][0m |          -0.0069 |          16.5140 |          -6.7778 |
[32m[20221214 00:23:00 @agent_ppo2.py:185][0m |          -0.0056 |          16.4550 |          -6.8662 |
[32m[20221214 00:23:00 @agent_ppo2.py:185][0m |          -0.0089 |          16.4151 |          -6.9197 |
[32m[20221214 00:23:00 @agent_ppo2.py:185][0m |          -0.0085 |          16.3943 |          -6.9476 |
[32m[20221214 00:23:00 @agent_ppo2.py:185][0m |           0.0059 |          18.4819 |          -6.9108 |
[32m[20221214 00:23:00 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:23:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221214 00:23:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221214 00:23:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 476.96
[32m[20221214 00:23:00 @agent_ppo2.py:143][0m Total time:      25.49 min
[32m[20221214 00:23:00 @agent_ppo2.py:145][0m 2326528 total steps have happened
[32m[20221214 00:23:00 @agent_ppo2.py:121][0m #------------------------ Iteration 5136 --------------------------#
[32m[20221214 00:23:00 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:23:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:23:00 @agent_ppo2.py:185][0m |           0.0062 |          67.0707 |          -7.9448 |
[32m[20221214 00:23:00 @agent_ppo2.py:185][0m |          -0.0096 |          57.7435 |          -7.8380 |
[32m[20221214 00:23:01 @agent_ppo2.py:185][0m |          -0.0135 |          56.0691 |          -7.8596 |
[32m[20221214 00:23:01 @agent_ppo2.py:185][0m |          -0.0129 |          54.7068 |          -7.8838 |
[32m[20221214 00:23:01 @agent_ppo2.py:185][0m |          -0.0117 |          56.0294 |          -7.7635 |
[32m[20221214 00:23:01 @agent_ppo2.py:185][0m |          -0.0164 |          53.4607 |          -7.9403 |
[32m[20221214 00:23:01 @agent_ppo2.py:185][0m |          -0.0189 |          53.4291 |          -7.8082 |
[32m[20221214 00:23:01 @agent_ppo2.py:185][0m |          -0.0221 |          52.7279 |          -7.8531 |
[32m[20221214 00:23:01 @agent_ppo2.py:185][0m |          -0.0154 |          52.7567 |          -7.8321 |
[32m[20221214 00:23:01 @agent_ppo2.py:185][0m |          -0.0198 |          52.0339 |          -7.8289 |
[32m[20221214 00:23:01 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:23:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 522.71
[32m[20221214 00:23:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 562.20
[32m[20221214 00:23:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 504.15
[32m[20221214 00:23:01 @agent_ppo2.py:143][0m Total time:      25.52 min
[32m[20221214 00:23:01 @agent_ppo2.py:145][0m 2328576 total steps have happened
[32m[20221214 00:23:01 @agent_ppo2.py:121][0m #------------------------ Iteration 5137 --------------------------#
[32m[20221214 00:23:02 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:23:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:23:02 @agent_ppo2.py:185][0m |          -0.0001 |          94.4857 |          -7.2617 |
[32m[20221214 00:23:02 @agent_ppo2.py:185][0m |          -0.0013 |          83.9381 |          -7.1380 |
[32m[20221214 00:23:02 @agent_ppo2.py:185][0m |          -0.0057 |          80.2617 |          -7.2763 |
[32m[20221214 00:23:02 @agent_ppo2.py:185][0m |          -0.0100 |          77.9195 |          -6.9852 |
[32m[20221214 00:23:02 @agent_ppo2.py:185][0m |          -0.0026 |          76.4406 |          -7.0848 |
[32m[20221214 00:23:02 @agent_ppo2.py:185][0m |          -0.0085 |          74.7410 |          -6.9532 |
[32m[20221214 00:23:02 @agent_ppo2.py:185][0m |          -0.0127 |          74.0822 |          -7.0364 |
[32m[20221214 00:23:02 @agent_ppo2.py:185][0m |          -0.0041 |          75.0095 |          -6.9985 |
[32m[20221214 00:23:03 @agent_ppo2.py:185][0m |          -0.0143 |          72.3785 |          -6.9308 |
[32m[20221214 00:23:03 @agent_ppo2.py:185][0m |          -0.0085 |          71.8791 |          -7.1197 |
[32m[20221214 00:23:03 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:23:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 535.99
[32m[20221214 00:23:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 573.08
[32m[20221214 00:23:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 522.50
[32m[20221214 00:23:03 @agent_ppo2.py:143][0m Total time:      25.54 min
[32m[20221214 00:23:03 @agent_ppo2.py:145][0m 2330624 total steps have happened
[32m[20221214 00:23:03 @agent_ppo2.py:121][0m #------------------------ Iteration 5138 --------------------------#
[32m[20221214 00:23:03 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:23:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:23:03 @agent_ppo2.py:185][0m |           0.0017 |          78.4519 |          -6.1076 |
[32m[20221214 00:23:03 @agent_ppo2.py:185][0m |          -0.0115 |          70.4786 |          -6.0657 |
[32m[20221214 00:23:03 @agent_ppo2.py:185][0m |          -0.0039 |          69.8265 |          -5.9518 |
[32m[20221214 00:23:03 @agent_ppo2.py:185][0m |          -0.0087 |          65.5914 |          -6.0982 |
[32m[20221214 00:23:04 @agent_ppo2.py:185][0m |          -0.0112 |          62.9330 |          -6.1450 |
[32m[20221214 00:23:04 @agent_ppo2.py:185][0m |          -0.0077 |          63.1737 |          -6.1978 |
[32m[20221214 00:23:04 @agent_ppo2.py:185][0m |          -0.0119 |          61.1957 |          -6.2287 |
[32m[20221214 00:23:04 @agent_ppo2.py:185][0m |          -0.0127 |          60.6787 |          -6.2213 |
[32m[20221214 00:23:04 @agent_ppo2.py:185][0m |          -0.0105 |          60.8887 |          -6.2315 |
[32m[20221214 00:23:04 @agent_ppo2.py:185][0m |          -0.0169 |          58.4341 |          -6.2051 |
[32m[20221214 00:23:04 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:23:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 494.27
[32m[20221214 00:23:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 566.48
[32m[20221214 00:23:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 573.17
[32m[20221214 00:23:04 @agent_ppo2.py:143][0m Total time:      25.56 min
[32m[20221214 00:23:04 @agent_ppo2.py:145][0m 2332672 total steps have happened
[32m[20221214 00:23:04 @agent_ppo2.py:121][0m #------------------------ Iteration 5139 --------------------------#
[32m[20221214 00:23:04 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:23:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:23:05 @agent_ppo2.py:185][0m |          -0.0016 |          60.1201 |          -7.9998 |
[32m[20221214 00:23:05 @agent_ppo2.py:185][0m |          -0.0054 |          50.8975 |          -7.7625 |
[32m[20221214 00:23:05 @agent_ppo2.py:185][0m |          -0.0069 |          48.6790 |          -7.7495 |
[32m[20221214 00:23:05 @agent_ppo2.py:185][0m |          -0.0140 |          47.3361 |          -7.6431 |
[32m[20221214 00:23:05 @agent_ppo2.py:185][0m |          -0.0090 |          46.3278 |          -7.7006 |
[32m[20221214 00:23:05 @agent_ppo2.py:185][0m |          -0.0104 |          48.7081 |          -7.6032 |
[32m[20221214 00:23:05 @agent_ppo2.py:185][0m |          -0.0125 |          45.4035 |          -7.6047 |
[32m[20221214 00:23:05 @agent_ppo2.py:185][0m |          -0.0204 |          44.8302 |          -7.6433 |
[32m[20221214 00:23:05 @agent_ppo2.py:185][0m |          -0.0140 |          45.0948 |          -7.6063 |
[32m[20221214 00:23:05 @agent_ppo2.py:185][0m |          -0.0143 |          43.9967 |          -7.5697 |
[32m[20221214 00:23:05 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:23:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 475.23
[32m[20221214 00:23:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 513.90
[32m[20221214 00:23:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 616.54
[32m[20221214 00:23:06 @agent_ppo2.py:143][0m Total time:      25.59 min
[32m[20221214 00:23:06 @agent_ppo2.py:145][0m 2334720 total steps have happened
[32m[20221214 00:23:06 @agent_ppo2.py:121][0m #------------------------ Iteration 5140 --------------------------#
[32m[20221214 00:23:06 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:23:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:23:06 @agent_ppo2.py:185][0m |           0.0021 |          86.7837 |          -6.9713 |
[32m[20221214 00:23:06 @agent_ppo2.py:185][0m |          -0.0073 |          78.9700 |          -7.1394 |
[32m[20221214 00:23:06 @agent_ppo2.py:185][0m |          -0.0103 |          76.3507 |          -7.0688 |
[32m[20221214 00:23:06 @agent_ppo2.py:185][0m |          -0.0124 |          74.8877 |          -7.0416 |
[32m[20221214 00:23:06 @agent_ppo2.py:185][0m |          -0.0121 |          73.5400 |          -7.1479 |
[32m[20221214 00:23:06 @agent_ppo2.py:185][0m |          -0.0151 |          72.4006 |          -7.1569 |
[32m[20221214 00:23:06 @agent_ppo2.py:185][0m |          -0.0193 |          71.5600 |          -7.0866 |
[32m[20221214 00:23:07 @agent_ppo2.py:185][0m |          -0.0133 |          70.5889 |          -7.2307 |
[32m[20221214 00:23:07 @agent_ppo2.py:185][0m |          -0.0154 |          69.3613 |          -7.1018 |
[32m[20221214 00:23:07 @agent_ppo2.py:185][0m |          -0.0194 |          69.1692 |          -7.0974 |
[32m[20221214 00:23:07 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:23:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 512.91
[32m[20221214 00:23:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 589.84
[32m[20221214 00:23:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 502.62
[32m[20221214 00:23:07 @agent_ppo2.py:143][0m Total time:      25.61 min
[32m[20221214 00:23:07 @agent_ppo2.py:145][0m 2336768 total steps have happened
[32m[20221214 00:23:07 @agent_ppo2.py:121][0m #------------------------ Iteration 5141 --------------------------#
[32m[20221214 00:23:07 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:23:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:23:07 @agent_ppo2.py:185][0m |           0.0066 |          76.1444 |          -7.5354 |
[32m[20221214 00:23:07 @agent_ppo2.py:185][0m |          -0.0058 |          65.0807 |          -7.7553 |
[32m[20221214 00:23:07 @agent_ppo2.py:185][0m |          -0.0069 |          60.1918 |          -7.4846 |
[32m[20221214 00:23:08 @agent_ppo2.py:185][0m |          -0.0060 |          57.7836 |          -7.5854 |
[32m[20221214 00:23:08 @agent_ppo2.py:185][0m |          -0.0105 |          55.0076 |          -7.4875 |
[32m[20221214 00:23:08 @agent_ppo2.py:185][0m |          -0.0128 |          54.0624 |          -7.5664 |
[32m[20221214 00:23:08 @agent_ppo2.py:185][0m |          -0.0117 |          51.6719 |          -7.3653 |
[32m[20221214 00:23:08 @agent_ppo2.py:185][0m |          -0.0126 |          50.5613 |          -7.4325 |
[32m[20221214 00:23:08 @agent_ppo2.py:185][0m |          -0.0137 |          49.1444 |          -7.4694 |
[32m[20221214 00:23:08 @agent_ppo2.py:185][0m |          -0.0163 |          48.4912 |          -7.3756 |
[32m[20221214 00:23:08 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:23:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 469.61
[32m[20221214 00:23:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 524.21
[32m[20221214 00:23:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 584.38
[32m[20221214 00:23:08 @agent_ppo2.py:143][0m Total time:      25.63 min
[32m[20221214 00:23:08 @agent_ppo2.py:145][0m 2338816 total steps have happened
[32m[20221214 00:23:08 @agent_ppo2.py:121][0m #------------------------ Iteration 5142 --------------------------#
[32m[20221214 00:23:08 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:23:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:23:09 @agent_ppo2.py:185][0m |          -0.0043 |          76.2691 |          -7.4869 |
[32m[20221214 00:23:09 @agent_ppo2.py:185][0m |          -0.0073 |          70.2290 |          -7.3879 |
[32m[20221214 00:23:09 @agent_ppo2.py:185][0m |          -0.0082 |          67.1361 |          -7.4246 |
[32m[20221214 00:23:09 @agent_ppo2.py:185][0m |          -0.0106 |          65.6927 |          -7.3233 |
[32m[20221214 00:23:09 @agent_ppo2.py:185][0m |          -0.0132 |          64.5654 |          -7.3066 |
[32m[20221214 00:23:09 @agent_ppo2.py:185][0m |          -0.0128 |          63.8281 |          -7.3340 |
[32m[20221214 00:23:09 @agent_ppo2.py:185][0m |          -0.0110 |          63.0364 |          -7.4042 |
[32m[20221214 00:23:09 @agent_ppo2.py:185][0m |          -0.0147 |          62.3395 |          -7.3842 |
[32m[20221214 00:23:09 @agent_ppo2.py:185][0m |          -0.0191 |          61.6423 |          -7.4378 |
[32m[20221214 00:23:09 @agent_ppo2.py:185][0m |          -0.0154 |          61.0271 |          -7.4362 |
[32m[20221214 00:23:09 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:23:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 518.19
[32m[20221214 00:23:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 563.27
[32m[20221214 00:23:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 616.13
[32m[20221214 00:23:10 @agent_ppo2.py:143][0m Total time:      25.65 min
[32m[20221214 00:23:10 @agent_ppo2.py:145][0m 2340864 total steps have happened
[32m[20221214 00:23:10 @agent_ppo2.py:121][0m #------------------------ Iteration 5143 --------------------------#
[32m[20221214 00:23:10 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:23:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:23:10 @agent_ppo2.py:185][0m |           0.0026 |          59.8561 |          -7.8234 |
[32m[20221214 00:23:10 @agent_ppo2.py:185][0m |          -0.0050 |          54.5275 |          -7.6709 |
[32m[20221214 00:23:10 @agent_ppo2.py:185][0m |          -0.0035 |          52.6230 |          -7.7133 |
[32m[20221214 00:23:10 @agent_ppo2.py:185][0m |          -0.0095 |          51.3198 |          -7.7702 |
[32m[20221214 00:23:10 @agent_ppo2.py:185][0m |          -0.0082 |          50.7625 |          -7.7751 |
[32m[20221214 00:23:10 @agent_ppo2.py:185][0m |          -0.0157 |          49.9823 |          -7.6465 |
[32m[20221214 00:23:11 @agent_ppo2.py:185][0m |          -0.0078 |          50.3396 |          -7.7356 |
[32m[20221214 00:23:11 @agent_ppo2.py:185][0m |          -0.0153 |          48.9933 |          -7.6069 |
[32m[20221214 00:23:11 @agent_ppo2.py:185][0m |          -0.0163 |          48.8221 |          -7.5644 |
[32m[20221214 00:23:11 @agent_ppo2.py:185][0m |          -0.0185 |          48.1818 |          -7.6165 |
[32m[20221214 00:23:11 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:23:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 530.04
[32m[20221214 00:23:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 560.29
[32m[20221214 00:23:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 563.68
[32m[20221214 00:23:11 @agent_ppo2.py:143][0m Total time:      25.68 min
[32m[20221214 00:23:11 @agent_ppo2.py:145][0m 2342912 total steps have happened
[32m[20221214 00:23:11 @agent_ppo2.py:121][0m #------------------------ Iteration 5144 --------------------------#
[32m[20221214 00:23:11 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:23:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:23:11 @agent_ppo2.py:185][0m |           0.0045 |          64.0671 |          -8.0550 |
[32m[20221214 00:23:11 @agent_ppo2.py:185][0m |          -0.0102 |          56.7462 |          -8.0349 |
[32m[20221214 00:23:12 @agent_ppo2.py:185][0m |          -0.0114 |          54.4164 |          -8.1493 |
[32m[20221214 00:23:12 @agent_ppo2.py:185][0m |          -0.0143 |          53.1295 |          -8.0766 |
[32m[20221214 00:23:12 @agent_ppo2.py:185][0m |          -0.0138 |          52.0165 |          -8.0287 |
[32m[20221214 00:23:12 @agent_ppo2.py:185][0m |          -0.0126 |          51.2109 |          -8.0571 |
[32m[20221214 00:23:12 @agent_ppo2.py:185][0m |          -0.0123 |          50.3965 |          -8.0308 |
[32m[20221214 00:23:12 @agent_ppo2.py:185][0m |          -0.0168 |          50.0350 |          -8.1462 |
[32m[20221214 00:23:12 @agent_ppo2.py:185][0m |          -0.0179 |          49.5849 |          -8.1498 |
[32m[20221214 00:23:12 @agent_ppo2.py:185][0m |          -0.0155 |          49.5547 |          -8.1172 |
[32m[20221214 00:23:12 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:23:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 440.96
[32m[20221214 00:23:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 502.87
[32m[20221214 00:23:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 505.82
[32m[20221214 00:23:12 @agent_ppo2.py:143][0m Total time:      25.70 min
[32m[20221214 00:23:12 @agent_ppo2.py:145][0m 2344960 total steps have happened
[32m[20221214 00:23:12 @agent_ppo2.py:121][0m #------------------------ Iteration 5145 --------------------------#
[32m[20221214 00:23:13 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:23:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:23:13 @agent_ppo2.py:185][0m |           0.0048 |          67.9852 |          -8.2324 |
[32m[20221214 00:23:13 @agent_ppo2.py:185][0m |          -0.0042 |          61.7274 |          -8.3204 |
[32m[20221214 00:23:13 @agent_ppo2.py:185][0m |          -0.0096 |          59.5094 |          -8.3173 |
[32m[20221214 00:23:13 @agent_ppo2.py:185][0m |          -0.0111 |          58.6244 |          -8.2463 |
[32m[20221214 00:23:13 @agent_ppo2.py:185][0m |          -0.0143 |          57.5877 |          -8.0998 |
[32m[20221214 00:23:13 @agent_ppo2.py:185][0m |          -0.0175 |          57.1549 |          -8.2846 |
[32m[20221214 00:23:13 @agent_ppo2.py:185][0m |          -0.0163 |          56.7178 |          -8.1571 |
[32m[20221214 00:23:13 @agent_ppo2.py:185][0m |          -0.0042 |          64.0705 |          -8.3353 |
[32m[20221214 00:23:13 @agent_ppo2.py:185][0m |          -0.0166 |          55.7421 |          -8.2829 |
[32m[20221214 00:23:14 @agent_ppo2.py:185][0m |          -0.0214 |          55.8119 |          -8.3109 |
[32m[20221214 00:23:14 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:23:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 440.09
[32m[20221214 00:23:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 496.95
[32m[20221214 00:23:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 538.70
[32m[20221214 00:23:14 @agent_ppo2.py:143][0m Total time:      25.72 min
[32m[20221214 00:23:14 @agent_ppo2.py:145][0m 2347008 total steps have happened
[32m[20221214 00:23:14 @agent_ppo2.py:121][0m #------------------------ Iteration 5146 --------------------------#
[32m[20221214 00:23:14 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:23:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:23:14 @agent_ppo2.py:185][0m |           0.0053 |          67.5101 |          -7.4476 |
[32m[20221214 00:23:14 @agent_ppo2.py:185][0m |          -0.0050 |          62.7599 |          -7.5504 |
[32m[20221214 00:23:14 @agent_ppo2.py:185][0m |          -0.0095 |          60.9869 |          -7.6228 |
[32m[20221214 00:23:14 @agent_ppo2.py:185][0m |          -0.0023 |          61.0333 |          -7.5338 |
[32m[20221214 00:23:14 @agent_ppo2.py:185][0m |          -0.0169 |          59.6488 |          -7.6694 |
[32m[20221214 00:23:15 @agent_ppo2.py:185][0m |          -0.0149 |          58.9646 |          -7.6260 |
[32m[20221214 00:23:15 @agent_ppo2.py:185][0m |          -0.0190 |          58.1229 |          -7.6357 |
[32m[20221214 00:23:15 @agent_ppo2.py:185][0m |          -0.0146 |          57.6424 |          -7.6244 |
[32m[20221214 00:23:15 @agent_ppo2.py:185][0m |          -0.0182 |          57.1500 |          -7.5450 |
[32m[20221214 00:23:15 @agent_ppo2.py:185][0m |          -0.0173 |          56.5974 |          -7.5525 |
[32m[20221214 00:23:15 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:23:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 455.42
[32m[20221214 00:23:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 513.36
[32m[20221214 00:23:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 644.52
[32m[20221214 00:23:15 @agent_ppo2.py:143][0m Total time:      25.75 min
[32m[20221214 00:23:15 @agent_ppo2.py:145][0m 2349056 total steps have happened
[32m[20221214 00:23:15 @agent_ppo2.py:121][0m #------------------------ Iteration 5147 --------------------------#
[32m[20221214 00:23:15 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:23:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:23:15 @agent_ppo2.py:185][0m |           0.0067 |          66.1233 |          -7.5936 |
[32m[20221214 00:23:16 @agent_ppo2.py:185][0m |          -0.0024 |          58.2044 |          -7.7304 |
[32m[20221214 00:23:16 @agent_ppo2.py:185][0m |          -0.0057 |          56.3784 |          -7.9547 |
[32m[20221214 00:23:16 @agent_ppo2.py:185][0m |          -0.0051 |          55.3663 |          -7.8962 |
[32m[20221214 00:23:16 @agent_ppo2.py:185][0m |          -0.0057 |          53.5353 |          -7.8807 |
[32m[20221214 00:23:16 @agent_ppo2.py:185][0m |          -0.0057 |          52.5539 |          -7.7529 |
[32m[20221214 00:23:16 @agent_ppo2.py:185][0m |          -0.0109 |          52.1776 |          -7.8519 |
[32m[20221214 00:23:16 @agent_ppo2.py:185][0m |          -0.0104 |          51.4230 |          -7.8595 |
[32m[20221214 00:23:16 @agent_ppo2.py:185][0m |          -0.0137 |          50.9504 |          -7.9968 |
[32m[20221214 00:23:16 @agent_ppo2.py:185][0m |          -0.0119 |          50.5544 |          -8.0204 |
[32m[20221214 00:23:16 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:23:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 571.11
[32m[20221214 00:23:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 661.85
[32m[20221214 00:23:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 522.63
[32m[20221214 00:23:16 @agent_ppo2.py:143][0m Total time:      25.77 min
[32m[20221214 00:23:16 @agent_ppo2.py:145][0m 2351104 total steps have happened
[32m[20221214 00:23:16 @agent_ppo2.py:121][0m #------------------------ Iteration 5148 --------------------------#
[32m[20221214 00:23:17 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:23:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:23:17 @agent_ppo2.py:185][0m |           0.0015 |          63.7105 |          -8.4737 |
[32m[20221214 00:23:17 @agent_ppo2.py:185][0m |          -0.0040 |          60.6960 |          -8.3895 |
[32m[20221214 00:23:17 @agent_ppo2.py:185][0m |          -0.0040 |          59.8041 |          -8.3161 |
[32m[20221214 00:23:17 @agent_ppo2.py:185][0m |          -0.0047 |          59.1668 |          -8.2960 |
[32m[20221214 00:23:17 @agent_ppo2.py:185][0m |          -0.0108 |          58.5966 |          -8.1917 |
[32m[20221214 00:23:17 @agent_ppo2.py:185][0m |          -0.0087 |          58.2796 |          -8.2211 |
[32m[20221214 00:23:17 @agent_ppo2.py:185][0m |          -0.0117 |          57.8918 |          -8.1170 |
[32m[20221214 00:23:17 @agent_ppo2.py:185][0m |          -0.0031 |          61.1425 |          -8.0833 |
[32m[20221214 00:23:18 @agent_ppo2.py:185][0m |          -0.0135 |          57.5418 |          -7.9818 |
[32m[20221214 00:23:18 @agent_ppo2.py:185][0m |          -0.0123 |          57.3001 |          -8.0028 |
[32m[20221214 00:23:18 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:23:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 534.35
[32m[20221214 00:23:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 562.15
[32m[20221214 00:23:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 599.22
[32m[20221214 00:23:18 @agent_ppo2.py:143][0m Total time:      25.79 min
[32m[20221214 00:23:18 @agent_ppo2.py:145][0m 2353152 total steps have happened
[32m[20221214 00:23:18 @agent_ppo2.py:121][0m #------------------------ Iteration 5149 --------------------------#
[32m[20221214 00:23:18 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:23:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:23:18 @agent_ppo2.py:185][0m |           0.0026 |          47.3554 |          -8.5009 |
[32m[20221214 00:23:18 @agent_ppo2.py:185][0m |          -0.0071 |          40.1437 |          -8.4418 |
[32m[20221214 00:23:18 @agent_ppo2.py:185][0m |          -0.0073 |          38.6869 |          -8.4584 |
[32m[20221214 00:23:18 @agent_ppo2.py:185][0m |          -0.0126 |          37.7709 |          -8.4798 |
[32m[20221214 00:23:19 @agent_ppo2.py:185][0m |          -0.0119 |          37.1168 |          -8.5101 |
[32m[20221214 00:23:19 @agent_ppo2.py:185][0m |          -0.0155 |          36.4199 |          -8.4702 |
[32m[20221214 00:23:19 @agent_ppo2.py:185][0m |          -0.0194 |          36.3584 |          -8.5773 |
[32m[20221214 00:23:19 @agent_ppo2.py:185][0m |          -0.0140 |          35.7234 |          -8.5296 |
[32m[20221214 00:23:19 @agent_ppo2.py:185][0m |          -0.0192 |          35.3013 |          -8.5099 |
[32m[20221214 00:23:19 @agent_ppo2.py:185][0m |          -0.0200 |          34.9423 |          -8.6060 |
[32m[20221214 00:23:19 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:23:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 491.84
[32m[20221214 00:23:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 608.43
[32m[20221214 00:23:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 547.96
[32m[20221214 00:23:19 @agent_ppo2.py:143][0m Total time:      25.81 min
[32m[20221214 00:23:19 @agent_ppo2.py:145][0m 2355200 total steps have happened
[32m[20221214 00:23:19 @agent_ppo2.py:121][0m #------------------------ Iteration 5150 --------------------------#
[32m[20221214 00:23:19 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:23:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:23:20 @agent_ppo2.py:185][0m |          -0.0014 |          93.6758 |          -8.3687 |
[32m[20221214 00:23:20 @agent_ppo2.py:185][0m |          -0.0065 |          78.4077 |          -8.2649 |
[32m[20221214 00:23:20 @agent_ppo2.py:185][0m |          -0.0052 |          71.2858 |          -8.2828 |
[32m[20221214 00:23:20 @agent_ppo2.py:185][0m |          -0.0119 |          67.5799 |          -8.2544 |
[32m[20221214 00:23:20 @agent_ppo2.py:185][0m |          -0.0073 |          65.5318 |          -8.3344 |
[32m[20221214 00:23:20 @agent_ppo2.py:185][0m |          -0.0086 |          63.1801 |          -8.5714 |
[32m[20221214 00:23:20 @agent_ppo2.py:185][0m |          -0.0132 |          62.2635 |          -8.5774 |
[32m[20221214 00:23:20 @agent_ppo2.py:185][0m |          -0.0181 |          60.5202 |          -8.6111 |
[32m[20221214 00:23:20 @agent_ppo2.py:185][0m |          -0.0077 |          60.6630 |          -8.7271 |
[32m[20221214 00:23:20 @agent_ppo2.py:185][0m |          -0.0192 |          58.7468 |          -8.7050 |
[32m[20221214 00:23:20 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:23:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 533.72
[32m[20221214 00:23:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 610.39
[32m[20221214 00:23:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 331.71
[32m[20221214 00:23:21 @agent_ppo2.py:143][0m Total time:      25.84 min
[32m[20221214 00:23:21 @agent_ppo2.py:145][0m 2357248 total steps have happened
[32m[20221214 00:23:21 @agent_ppo2.py:121][0m #------------------------ Iteration 5151 --------------------------#
[32m[20221214 00:23:21 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:23:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:23:21 @agent_ppo2.py:185][0m |           0.0000 |          73.5636 |          -8.5242 |
[32m[20221214 00:23:21 @agent_ppo2.py:185][0m |          -0.0008 |          60.3528 |          -8.5843 |
[32m[20221214 00:23:21 @agent_ppo2.py:185][0m |          -0.0131 |          55.5004 |          -8.6677 |
[32m[20221214 00:23:21 @agent_ppo2.py:185][0m |          -0.0192 |          52.9527 |          -8.5842 |
[32m[20221214 00:23:21 @agent_ppo2.py:185][0m |          -0.0030 |          53.7454 |          -8.6299 |
[32m[20221214 00:23:21 @agent_ppo2.py:185][0m |          -0.0138 |          48.8161 |          -8.6587 |
[32m[20221214 00:23:21 @agent_ppo2.py:185][0m |          -0.0167 |          47.0564 |          -8.7630 |
[32m[20221214 00:23:22 @agent_ppo2.py:185][0m |          -0.0216 |          45.8479 |          -8.7893 |
[32m[20221214 00:23:22 @agent_ppo2.py:185][0m |          -0.0261 |          44.6611 |          -8.8149 |
[32m[20221214 00:23:22 @agent_ppo2.py:185][0m |          -0.0171 |          42.7236 |          -8.8272 |
[32m[20221214 00:23:22 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:23:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 529.51
[32m[20221214 00:23:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 697.69
[32m[20221214 00:23:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 529.95
[32m[20221214 00:23:22 @agent_ppo2.py:143][0m Total time:      25.86 min
[32m[20221214 00:23:22 @agent_ppo2.py:145][0m 2359296 total steps have happened
[32m[20221214 00:23:22 @agent_ppo2.py:121][0m #------------------------ Iteration 5152 --------------------------#
[32m[20221214 00:23:22 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:23:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:23:22 @agent_ppo2.py:185][0m |           0.0009 |          79.9717 |         -10.0820 |
[32m[20221214 00:23:22 @agent_ppo2.py:185][0m |          -0.0105 |          75.4533 |         -10.2442 |
[32m[20221214 00:23:22 @agent_ppo2.py:185][0m |          -0.0082 |          72.3794 |         -10.3020 |
[32m[20221214 00:23:23 @agent_ppo2.py:185][0m |          -0.0125 |          70.6908 |         -10.1844 |
[32m[20221214 00:23:23 @agent_ppo2.py:185][0m |          -0.0137 |          69.2011 |         -10.1998 |
[32m[20221214 00:23:23 @agent_ppo2.py:185][0m |          -0.0172 |          68.0623 |         -10.2570 |
[32m[20221214 00:23:23 @agent_ppo2.py:185][0m |          -0.0175 |          67.1084 |         -10.3171 |
[32m[20221214 00:23:23 @agent_ppo2.py:185][0m |          -0.0203 |          66.5228 |         -10.1714 |
[32m[20221214 00:23:23 @agent_ppo2.py:185][0m |          -0.0192 |          65.5494 |         -10.2280 |
[32m[20221214 00:23:23 @agent_ppo2.py:185][0m |          -0.0196 |          64.7929 |         -10.0730 |
[32m[20221214 00:23:23 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:23:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 551.76
[32m[20221214 00:23:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 644.65
[32m[20221214 00:23:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 664.29
[32m[20221214 00:23:23 @agent_ppo2.py:143][0m Total time:      25.88 min
[32m[20221214 00:23:23 @agent_ppo2.py:145][0m 2361344 total steps have happened
[32m[20221214 00:23:23 @agent_ppo2.py:121][0m #------------------------ Iteration 5153 --------------------------#
[32m[20221214 00:23:23 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:23:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:23:24 @agent_ppo2.py:185][0m |           0.0060 |          82.4016 |         -10.2149 |
[32m[20221214 00:23:24 @agent_ppo2.py:185][0m |          -0.0076 |          75.9830 |         -10.2150 |
[32m[20221214 00:23:24 @agent_ppo2.py:185][0m |          -0.0055 |          73.1127 |         -10.3111 |
[32m[20221214 00:23:24 @agent_ppo2.py:185][0m |          -0.0093 |          71.2660 |         -10.3108 |
[32m[20221214 00:23:24 @agent_ppo2.py:185][0m |          -0.0112 |          70.4069 |         -10.3980 |
[32m[20221214 00:23:24 @agent_ppo2.py:185][0m |          -0.0105 |          69.1672 |         -10.2311 |
[32m[20221214 00:23:24 @agent_ppo2.py:185][0m |          -0.0110 |          68.2933 |         -10.3238 |
[32m[20221214 00:23:24 @agent_ppo2.py:185][0m |          -0.0149 |          67.9078 |         -10.3517 |
[32m[20221214 00:23:24 @agent_ppo2.py:185][0m |          -0.0080 |          72.7660 |         -10.3509 |
[32m[20221214 00:23:24 @agent_ppo2.py:185][0m |          -0.0149 |          66.7395 |         -10.3918 |
[32m[20221214 00:23:24 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:23:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 579.41
[32m[20221214 00:23:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 622.77
[32m[20221214 00:23:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 581.19
[32m[20221214 00:23:25 @agent_ppo2.py:143][0m Total time:      25.90 min
[32m[20221214 00:23:25 @agent_ppo2.py:145][0m 2363392 total steps have happened
[32m[20221214 00:23:25 @agent_ppo2.py:121][0m #------------------------ Iteration 5154 --------------------------#
[32m[20221214 00:23:25 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:23:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:23:25 @agent_ppo2.py:185][0m |          -0.0015 |          68.3155 |         -10.2831 |
[32m[20221214 00:23:25 @agent_ppo2.py:185][0m |          -0.0034 |          56.6006 |         -10.0993 |
[32m[20221214 00:23:25 @agent_ppo2.py:185][0m |          -0.0127 |          53.6670 |          -9.9445 |
[32m[20221214 00:23:25 @agent_ppo2.py:185][0m |          -0.0111 |          51.6490 |          -9.8651 |
[32m[20221214 00:23:25 @agent_ppo2.py:185][0m |          -0.0125 |          51.2341 |          -9.6906 |
[32m[20221214 00:23:25 @agent_ppo2.py:185][0m |          -0.0108 |          54.4067 |          -9.6616 |
[32m[20221214 00:23:26 @agent_ppo2.py:185][0m |          -0.0203 |          49.3064 |          -9.5015 |
[32m[20221214 00:23:26 @agent_ppo2.py:185][0m |          -0.0201 |          48.4383 |          -9.4632 |
[32m[20221214 00:23:26 @agent_ppo2.py:185][0m |          -0.0116 |          48.5450 |          -9.3654 |
[32m[20221214 00:23:26 @agent_ppo2.py:185][0m |          -0.0196 |          47.4385 |          -9.3495 |
[32m[20221214 00:23:26 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:23:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 493.14
[32m[20221214 00:23:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 568.77
[32m[20221214 00:23:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 566.31
[32m[20221214 00:23:26 @agent_ppo2.py:143][0m Total time:      25.93 min
[32m[20221214 00:23:26 @agent_ppo2.py:145][0m 2365440 total steps have happened
[32m[20221214 00:23:26 @agent_ppo2.py:121][0m #------------------------ Iteration 5155 --------------------------#
[32m[20221214 00:23:26 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:23:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:23:26 @agent_ppo2.py:185][0m |           0.0011 |          50.0765 |          -9.2301 |
[32m[20221214 00:23:26 @agent_ppo2.py:185][0m |          -0.0030 |          42.5324 |          -9.1181 |
[32m[20221214 00:23:27 @agent_ppo2.py:185][0m |          -0.0065 |          41.5873 |          -8.9697 |
[32m[20221214 00:23:27 @agent_ppo2.py:185][0m |          -0.0092 |          38.9680 |          -9.0036 |
[32m[20221214 00:23:27 @agent_ppo2.py:185][0m |          -0.0137 |          37.7751 |          -8.9499 |
[32m[20221214 00:23:27 @agent_ppo2.py:185][0m |          -0.0091 |          37.3673 |          -8.9101 |
[32m[20221214 00:23:27 @agent_ppo2.py:185][0m |          -0.0151 |          36.6325 |          -8.7758 |
[32m[20221214 00:23:27 @agent_ppo2.py:185][0m |          -0.0139 |          35.7220 |          -8.8229 |
[32m[20221214 00:23:27 @agent_ppo2.py:185][0m |          -0.0090 |          35.4134 |          -8.6974 |
[32m[20221214 00:23:27 @agent_ppo2.py:185][0m |          -0.0163 |          35.1549 |          -8.6354 |
[32m[20221214 00:23:27 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:23:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 453.03
[32m[20221214 00:23:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 558.92
[32m[20221214 00:23:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 438.81
[32m[20221214 00:23:27 @agent_ppo2.py:143][0m Total time:      25.95 min
[32m[20221214 00:23:27 @agent_ppo2.py:145][0m 2367488 total steps have happened
[32m[20221214 00:23:27 @agent_ppo2.py:121][0m #------------------------ Iteration 5156 --------------------------#
[32m[20221214 00:23:28 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:23:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:23:28 @agent_ppo2.py:185][0m |           0.0006 |          91.7502 |         -10.0120 |
[32m[20221214 00:23:28 @agent_ppo2.py:185][0m |          -0.0064 |          84.2698 |         -10.0907 |
[32m[20221214 00:23:28 @agent_ppo2.py:185][0m |          -0.0098 |          81.4493 |          -9.9479 |
[32m[20221214 00:23:28 @agent_ppo2.py:185][0m |          -0.0075 |          79.5960 |          -9.7942 |
[32m[20221214 00:23:28 @agent_ppo2.py:185][0m |          -0.0114 |          78.3832 |          -9.5952 |
[32m[20221214 00:23:28 @agent_ppo2.py:185][0m |          -0.0142 |          77.5617 |          -9.6381 |
[32m[20221214 00:23:28 @agent_ppo2.py:185][0m |          -0.0126 |          77.5578 |          -9.6492 |
[32m[20221214 00:23:28 @agent_ppo2.py:185][0m |          -0.0139 |          76.4065 |          -9.5903 |
[32m[20221214 00:23:29 @agent_ppo2.py:185][0m |          -0.0196 |          76.4814 |          -9.5146 |
[32m[20221214 00:23:29 @agent_ppo2.py:185][0m |          -0.0189 |          76.0563 |          -9.4493 |
[32m[20221214 00:23:29 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:23:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 569.23
[32m[20221214 00:23:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 586.55
[32m[20221214 00:23:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 564.02
[32m[20221214 00:23:29 @agent_ppo2.py:143][0m Total time:      25.97 min
[32m[20221214 00:23:29 @agent_ppo2.py:145][0m 2369536 total steps have happened
[32m[20221214 00:23:29 @agent_ppo2.py:121][0m #------------------------ Iteration 5157 --------------------------#
[32m[20221214 00:23:29 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:23:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:23:29 @agent_ppo2.py:185][0m |           0.0014 |         103.4522 |          -7.4603 |
[32m[20221214 00:23:29 @agent_ppo2.py:185][0m |          -0.0048 |          98.8392 |          -7.6246 |
[32m[20221214 00:23:29 @agent_ppo2.py:185][0m |           0.0000 |          96.2139 |          -7.6981 |
[32m[20221214 00:23:29 @agent_ppo2.py:185][0m |          -0.0097 |          94.2183 |          -7.7688 |
[32m[20221214 00:23:30 @agent_ppo2.py:185][0m |          -0.0125 |          93.2108 |          -7.7754 |
[32m[20221214 00:23:30 @agent_ppo2.py:185][0m |          -0.0116 |          91.5871 |          -7.8868 |
[32m[20221214 00:23:30 @agent_ppo2.py:185][0m |          -0.0117 |          90.4536 |          -7.9403 |
[32m[20221214 00:23:30 @agent_ppo2.py:185][0m |          -0.0155 |          89.6544 |          -8.0676 |
[32m[20221214 00:23:30 @agent_ppo2.py:185][0m |          -0.0175 |          88.8731 |          -8.0860 |
[32m[20221214 00:23:30 @agent_ppo2.py:185][0m |          -0.0156 |          88.5977 |          -8.1671 |
[32m[20221214 00:23:30 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221214 00:23:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 563.63
[32m[20221214 00:23:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 592.36
[32m[20221214 00:23:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 589.53
[32m[20221214 00:23:30 @agent_ppo2.py:143][0m Total time:      26.00 min
[32m[20221214 00:23:30 @agent_ppo2.py:145][0m 2371584 total steps have happened
[32m[20221214 00:23:30 @agent_ppo2.py:121][0m #------------------------ Iteration 5158 --------------------------#
[32m[20221214 00:23:30 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:23:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:23:31 @agent_ppo2.py:185][0m |          -0.0056 |          60.8042 |          -8.6870 |
[32m[20221214 00:23:31 @agent_ppo2.py:185][0m |          -0.0033 |          54.1233 |          -8.5536 |
[32m[20221214 00:23:31 @agent_ppo2.py:185][0m |           0.0000 |          54.1688 |          -8.4862 |
[32m[20221214 00:23:31 @agent_ppo2.py:185][0m |          -0.0134 |          52.2222 |          -8.5234 |
[32m[20221214 00:23:31 @agent_ppo2.py:185][0m |          -0.0001 |          51.4197 |          -8.3969 |
[32m[20221214 00:23:31 @agent_ppo2.py:185][0m |          -0.0054 |          56.1571 |          -8.3358 |
[32m[20221214 00:23:31 @agent_ppo2.py:185][0m |          -0.0111 |          50.5584 |          -8.2962 |
[32m[20221214 00:23:31 @agent_ppo2.py:185][0m |          -0.0161 |          49.6007 |          -8.2469 |
[32m[20221214 00:23:31 @agent_ppo2.py:185][0m |          -0.0186 |          49.4213 |          -8.2246 |
[32m[20221214 00:23:31 @agent_ppo2.py:185][0m |          -0.0142 |          49.0256 |          -8.0724 |
[32m[20221214 00:23:31 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:23:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 493.54
[32m[20221214 00:23:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 592.50
[32m[20221214 00:23:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 666.96
[32m[20221214 00:23:32 @agent_ppo2.py:143][0m Total time:      26.02 min
[32m[20221214 00:23:32 @agent_ppo2.py:145][0m 2373632 total steps have happened
[32m[20221214 00:23:32 @agent_ppo2.py:121][0m #------------------------ Iteration 5159 --------------------------#
[32m[20221214 00:23:32 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:23:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:23:32 @agent_ppo2.py:185][0m |           0.0025 |          70.7936 |          -8.4475 |
[32m[20221214 00:23:32 @agent_ppo2.py:185][0m |          -0.0036 |          60.9100 |          -8.5023 |
[32m[20221214 00:23:32 @agent_ppo2.py:185][0m |          -0.0101 |          57.1233 |          -8.5203 |
[32m[20221214 00:23:32 @agent_ppo2.py:185][0m |          -0.0128 |          54.7933 |          -8.4672 |
[32m[20221214 00:23:32 @agent_ppo2.py:185][0m |          -0.0106 |          53.5889 |          -8.3678 |
[32m[20221214 00:23:32 @agent_ppo2.py:185][0m |          -0.0178 |          51.5923 |          -8.4149 |
[32m[20221214 00:23:33 @agent_ppo2.py:185][0m |          -0.0177 |          51.4034 |          -8.3934 |
[32m[20221214 00:23:33 @agent_ppo2.py:185][0m |          -0.0190 |          49.3475 |          -8.4228 |
[32m[20221214 00:23:33 @agent_ppo2.py:185][0m |          -0.0215 |          48.5314 |          -8.4028 |
[32m[20221214 00:23:33 @agent_ppo2.py:185][0m |          -0.0275 |          47.9993 |          -8.3764 |
[32m[20221214 00:23:33 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 00:23:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 399.44
[32m[20221214 00:23:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 549.87
[32m[20221214 00:23:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 580.42
[32m[20221214 00:23:33 @agent_ppo2.py:143][0m Total time:      26.04 min
[32m[20221214 00:23:33 @agent_ppo2.py:145][0m 2375680 total steps have happened
[32m[20221214 00:23:33 @agent_ppo2.py:121][0m #------------------------ Iteration 5160 --------------------------#
[32m[20221214 00:23:33 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:23:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:23:33 @agent_ppo2.py:185][0m |           0.0067 |          41.2500 |          -8.2055 |
[32m[20221214 00:23:34 @agent_ppo2.py:185][0m |           0.0065 |          33.1446 |          -8.0378 |
[32m[20221214 00:23:34 @agent_ppo2.py:185][0m |          -0.0011 |          30.5233 |          -8.1041 |
[32m[20221214 00:23:34 @agent_ppo2.py:185][0m |           0.0053 |          29.1870 |          -8.0735 |
[32m[20221214 00:23:34 @agent_ppo2.py:185][0m |          -0.0062 |          28.4678 |          -8.2080 |
[32m[20221214 00:23:34 @agent_ppo2.py:185][0m |          -0.0097 |          27.5396 |          -8.1295 |
[32m[20221214 00:23:34 @agent_ppo2.py:185][0m |          -0.0105 |          26.4183 |          -8.3112 |
[32m[20221214 00:23:34 @agent_ppo2.py:185][0m |          -0.0100 |          26.1839 |          -8.2207 |
[32m[20221214 00:23:34 @agent_ppo2.py:185][0m |          -0.0095 |          25.6472 |          -8.1727 |
[32m[20221214 00:23:34 @agent_ppo2.py:185][0m |          -0.0162 |          25.2885 |          -8.1175 |
[32m[20221214 00:23:34 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:23:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 504.93
[32m[20221214 00:23:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 627.81
[32m[20221214 00:23:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 777.65
[32m[20221214 00:23:34 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 777.65
[32m[20221214 00:23:34 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 777.65
[32m[20221214 00:23:34 @agent_ppo2.py:143][0m Total time:      26.07 min
[32m[20221214 00:23:34 @agent_ppo2.py:145][0m 2377728 total steps have happened
[32m[20221214 00:23:34 @agent_ppo2.py:121][0m #------------------------ Iteration 5161 --------------------------#
[32m[20221214 00:23:35 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:23:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:23:35 @agent_ppo2.py:185][0m |           0.0011 |          98.6850 |          -7.8369 |
[32m[20221214 00:23:35 @agent_ppo2.py:185][0m |          -0.0042 |          92.3889 |          -7.7612 |
[32m[20221214 00:23:35 @agent_ppo2.py:185][0m |          -0.0041 |          90.4664 |          -7.7309 |
[32m[20221214 00:23:35 @agent_ppo2.py:185][0m |          -0.0087 |          89.2574 |          -7.7194 |
[32m[20221214 00:23:35 @agent_ppo2.py:185][0m |          -0.0097 |          88.1169 |          -7.8163 |
[32m[20221214 00:23:35 @agent_ppo2.py:185][0m |          -0.0164 |          87.2415 |          -7.9027 |
[32m[20221214 00:23:35 @agent_ppo2.py:185][0m |          -0.0129 |          86.0633 |          -7.7173 |
[32m[20221214 00:23:36 @agent_ppo2.py:185][0m |           0.0034 |          98.6415 |          -7.8922 |
[32m[20221214 00:23:36 @agent_ppo2.py:185][0m |          -0.0144 |          85.8106 |          -7.8315 |
[32m[20221214 00:23:36 @agent_ppo2.py:185][0m |          -0.0162 |          84.7726 |          -7.8268 |
[32m[20221214 00:23:36 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:23:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 554.52
[32m[20221214 00:23:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 589.06
[32m[20221214 00:23:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 499.78
[32m[20221214 00:23:36 @agent_ppo2.py:143][0m Total time:      26.09 min
[32m[20221214 00:23:36 @agent_ppo2.py:145][0m 2379776 total steps have happened
[32m[20221214 00:23:36 @agent_ppo2.py:121][0m #------------------------ Iteration 5162 --------------------------#
[32m[20221214 00:23:36 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:23:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:23:36 @agent_ppo2.py:185][0m |          -0.0003 |          67.6907 |          -9.6051 |
[32m[20221214 00:23:36 @agent_ppo2.py:185][0m |          -0.0025 |          64.2914 |          -9.5467 |
[32m[20221214 00:23:36 @agent_ppo2.py:185][0m |          -0.0141 |          55.6598 |          -9.5309 |
[32m[20221214 00:23:37 @agent_ppo2.py:185][0m |          -0.0105 |          55.1003 |          -9.7555 |
[32m[20221214 00:23:37 @agent_ppo2.py:185][0m |          -0.0200 |          52.2470 |          -9.5806 |
[32m[20221214 00:23:37 @agent_ppo2.py:185][0m |          -0.0145 |          51.0684 |          -9.6594 |
[32m[20221214 00:23:37 @agent_ppo2.py:185][0m |          -0.0135 |          53.3333 |          -9.6895 |
[32m[20221214 00:23:37 @agent_ppo2.py:185][0m |          -0.0204 |          48.9805 |          -9.7161 |
[32m[20221214 00:23:37 @agent_ppo2.py:185][0m |          -0.0203 |          48.6302 |          -9.7884 |
[32m[20221214 00:23:37 @agent_ppo2.py:185][0m |          -0.0244 |          47.8037 |          -9.8701 |
[32m[20221214 00:23:37 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:23:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 562.85
[32m[20221214 00:23:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 698.59
[32m[20221214 00:23:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 641.86
[32m[20221214 00:23:37 @agent_ppo2.py:143][0m Total time:      26.12 min
[32m[20221214 00:23:37 @agent_ppo2.py:145][0m 2381824 total steps have happened
[32m[20221214 00:23:37 @agent_ppo2.py:121][0m #------------------------ Iteration 5163 --------------------------#
[32m[20221214 00:23:37 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:23:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:23:38 @agent_ppo2.py:185][0m |           0.0018 |          63.9947 |          -9.2374 |
[32m[20221214 00:23:38 @agent_ppo2.py:185][0m |          -0.0103 |          53.0174 |          -9.0314 |
[32m[20221214 00:23:38 @agent_ppo2.py:185][0m |          -0.0086 |          48.3708 |          -9.0275 |
[32m[20221214 00:23:38 @agent_ppo2.py:185][0m |          -0.0198 |          45.9565 |          -9.1120 |
[32m[20221214 00:23:38 @agent_ppo2.py:185][0m |          -0.0165 |          44.0193 |          -9.0742 |
[32m[20221214 00:23:38 @agent_ppo2.py:185][0m |          -0.0132 |          44.5744 |          -9.0916 |
[32m[20221214 00:23:38 @agent_ppo2.py:185][0m |          -0.0209 |          41.6409 |          -9.0585 |
[32m[20221214 00:23:38 @agent_ppo2.py:185][0m |          -0.0165 |          41.0287 |          -9.0781 |
[32m[20221214 00:23:38 @agent_ppo2.py:185][0m |          -0.0248 |          40.2707 |          -9.0412 |
[32m[20221214 00:23:39 @agent_ppo2.py:185][0m |          -0.0133 |          39.9289 |          -8.9450 |
[32m[20221214 00:23:39 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:23:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 381.21
[32m[20221214 00:23:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 553.99
[32m[20221214 00:23:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 833.45
[32m[20221214 00:23:39 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 833.45
[32m[20221214 00:23:39 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 833.45
[32m[20221214 00:23:39 @agent_ppo2.py:143][0m Total time:      26.14 min
[32m[20221214 00:23:39 @agent_ppo2.py:145][0m 2383872 total steps have happened
[32m[20221214 00:23:39 @agent_ppo2.py:121][0m #------------------------ Iteration 5164 --------------------------#
[32m[20221214 00:23:39 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:23:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:23:39 @agent_ppo2.py:185][0m |           0.0043 |          16.7969 |          -9.8163 |
[32m[20221214 00:23:39 @agent_ppo2.py:185][0m |          -0.0017 |          11.3040 |          -9.8396 |
[32m[20221214 00:23:39 @agent_ppo2.py:185][0m |          -0.0045 |          11.0193 |          -9.8335 |
[32m[20221214 00:23:39 @agent_ppo2.py:185][0m |           0.0079 |          11.2388 |          -9.7858 |
[32m[20221214 00:23:39 @agent_ppo2.py:185][0m |          -0.0055 |          10.7754 |          -9.8715 |
[32m[20221214 00:23:40 @agent_ppo2.py:185][0m |           0.0006 |          11.2813 |          -9.7255 |
[32m[20221214 00:23:40 @agent_ppo2.py:185][0m |          -0.0065 |          10.6849 |          -9.8470 |
[32m[20221214 00:23:40 @agent_ppo2.py:185][0m |          -0.0084 |          10.6189 |          -9.8673 |
[32m[20221214 00:23:40 @agent_ppo2.py:185][0m |          -0.0071 |          10.6082 |          -9.8544 |
[32m[20221214 00:23:40 @agent_ppo2.py:185][0m |          -0.0066 |          10.5899 |          -9.8581 |
[32m[20221214 00:23:40 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:23:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221214 00:23:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221214 00:23:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 712.51
[32m[20221214 00:23:40 @agent_ppo2.py:143][0m Total time:      26.16 min
[32m[20221214 00:23:40 @agent_ppo2.py:145][0m 2385920 total steps have happened
[32m[20221214 00:23:40 @agent_ppo2.py:121][0m #------------------------ Iteration 5165 --------------------------#
[32m[20221214 00:23:40 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:23:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:23:40 @agent_ppo2.py:185][0m |           0.0047 |          62.9795 |          -8.8468 |
[32m[20221214 00:23:40 @agent_ppo2.py:185][0m |           0.0004 |          51.0943 |          -9.0036 |
[32m[20221214 00:23:41 @agent_ppo2.py:185][0m |          -0.0105 |          47.4414 |          -9.0063 |
[32m[20221214 00:23:41 @agent_ppo2.py:185][0m |          -0.0127 |          45.4208 |          -9.2031 |
[32m[20221214 00:23:41 @agent_ppo2.py:185][0m |          -0.0110 |          44.3655 |          -9.1234 |
[32m[20221214 00:23:41 @agent_ppo2.py:185][0m |          -0.0142 |          42.9630 |          -9.3182 |
[32m[20221214 00:23:41 @agent_ppo2.py:185][0m |          -0.0136 |          42.4572 |          -9.2505 |
[32m[20221214 00:23:41 @agent_ppo2.py:185][0m |          -0.0211 |          41.5476 |          -9.3382 |
[32m[20221214 00:23:41 @agent_ppo2.py:185][0m |          -0.0205 |          40.9131 |          -9.3489 |
[32m[20221214 00:23:41 @agent_ppo2.py:185][0m |          -0.0168 |          40.3579 |          -9.4652 |
[32m[20221214 00:23:41 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:23:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 405.31
[32m[20221214 00:23:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 549.38
[32m[20221214 00:23:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 436.64
[32m[20221214 00:23:41 @agent_ppo2.py:143][0m Total time:      26.18 min
[32m[20221214 00:23:41 @agent_ppo2.py:145][0m 2387968 total steps have happened
[32m[20221214 00:23:41 @agent_ppo2.py:121][0m #------------------------ Iteration 5166 --------------------------#
[32m[20221214 00:23:42 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:23:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:23:42 @agent_ppo2.py:185][0m |           0.0057 |          67.9907 |          -9.7686 |
[32m[20221214 00:23:42 @agent_ppo2.py:185][0m |          -0.0012 |          60.8280 |          -9.6660 |
[32m[20221214 00:23:42 @agent_ppo2.py:185][0m |          -0.0052 |          58.5032 |          -9.6247 |
[32m[20221214 00:23:42 @agent_ppo2.py:185][0m |          -0.0057 |          56.4807 |          -9.6309 |
[32m[20221214 00:23:42 @agent_ppo2.py:185][0m |          -0.0190 |          54.2656 |          -9.6698 |
[32m[20221214 00:23:42 @agent_ppo2.py:185][0m |          -0.0088 |          53.8829 |          -9.6227 |
[32m[20221214 00:23:42 @agent_ppo2.py:185][0m |          -0.0065 |          54.3492 |          -9.5657 |
[32m[20221214 00:23:42 @agent_ppo2.py:185][0m |          -0.0161 |          51.1506 |          -9.5838 |
[32m[20221214 00:23:43 @agent_ppo2.py:185][0m |          -0.0160 |          50.8583 |          -9.6425 |
[32m[20221214 00:23:43 @agent_ppo2.py:185][0m |          -0.0068 |          50.8931 |          -9.5874 |
[32m[20221214 00:23:43 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:23:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 409.02
[32m[20221214 00:23:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 572.23
[32m[20221214 00:23:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 556.58
[32m[20221214 00:23:43 @agent_ppo2.py:143][0m Total time:      26.21 min
[32m[20221214 00:23:43 @agent_ppo2.py:145][0m 2390016 total steps have happened
[32m[20221214 00:23:43 @agent_ppo2.py:121][0m #------------------------ Iteration 5167 --------------------------#
[32m[20221214 00:23:43 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:23:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:23:43 @agent_ppo2.py:185][0m |           0.0038 |          46.4275 |         -10.6723 |
[32m[20221214 00:23:43 @agent_ppo2.py:185][0m |          -0.0076 |          39.1775 |         -10.6919 |
[32m[20221214 00:23:43 @agent_ppo2.py:185][0m |          -0.0033 |          36.7793 |         -10.6970 |
[32m[20221214 00:23:43 @agent_ppo2.py:185][0m |          -0.0110 |          35.0951 |         -10.6067 |
[32m[20221214 00:23:44 @agent_ppo2.py:185][0m |          -0.0150 |          34.3070 |         -10.5659 |
[32m[20221214 00:23:44 @agent_ppo2.py:185][0m |          -0.0147 |          33.5069 |         -10.7625 |
[32m[20221214 00:23:44 @agent_ppo2.py:185][0m |          -0.0146 |          32.8924 |         -10.6745 |
[32m[20221214 00:23:44 @agent_ppo2.py:185][0m |          -0.0135 |          31.6143 |         -10.7229 |
[32m[20221214 00:23:44 @agent_ppo2.py:185][0m |          -0.0134 |          31.3497 |         -10.7986 |
[32m[20221214 00:23:44 @agent_ppo2.py:185][0m |          -0.0149 |          32.2751 |         -10.7155 |
[32m[20221214 00:23:44 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:23:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 460.50
[32m[20221214 00:23:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 509.88
[32m[20221214 00:23:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 465.31
[32m[20221214 00:23:44 @agent_ppo2.py:143][0m Total time:      26.23 min
[32m[20221214 00:23:44 @agent_ppo2.py:145][0m 2392064 total steps have happened
[32m[20221214 00:23:44 @agent_ppo2.py:121][0m #------------------------ Iteration 5168 --------------------------#
[32m[20221214 00:23:44 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:23:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:23:44 @agent_ppo2.py:185][0m |           0.0028 |          77.3433 |         -10.7281 |
[32m[20221214 00:23:45 @agent_ppo2.py:185][0m |          -0.0058 |          61.5411 |         -10.7883 |
[32m[20221214 00:23:45 @agent_ppo2.py:185][0m |          -0.0107 |          54.9617 |         -10.8587 |
[32m[20221214 00:23:45 @agent_ppo2.py:185][0m |          -0.0110 |          51.6127 |         -10.8715 |
[32m[20221214 00:23:45 @agent_ppo2.py:185][0m |          -0.0099 |          49.4084 |         -11.0731 |
[32m[20221214 00:23:45 @agent_ppo2.py:185][0m |          -0.0094 |          47.7082 |         -10.9835 |
[32m[20221214 00:23:45 @agent_ppo2.py:185][0m |          -0.0163 |          46.3010 |         -11.0221 |
[32m[20221214 00:23:45 @agent_ppo2.py:185][0m |          -0.0150 |          45.3825 |         -11.1851 |
[32m[20221214 00:23:45 @agent_ppo2.py:185][0m |          -0.0184 |          44.3377 |         -11.1655 |
[32m[20221214 00:23:45 @agent_ppo2.py:185][0m |          -0.0127 |          44.5633 |         -11.3196 |
[32m[20221214 00:23:45 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:23:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 531.53
[32m[20221214 00:23:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 664.89
[32m[20221214 00:23:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 631.59
[32m[20221214 00:23:46 @agent_ppo2.py:143][0m Total time:      26.25 min
[32m[20221214 00:23:46 @agent_ppo2.py:145][0m 2394112 total steps have happened
[32m[20221214 00:23:46 @agent_ppo2.py:121][0m #------------------------ Iteration 5169 --------------------------#
[32m[20221214 00:23:46 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:23:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:23:46 @agent_ppo2.py:185][0m |           0.0042 |          91.0769 |         -10.6196 |
[32m[20221214 00:23:46 @agent_ppo2.py:185][0m |           0.0011 |          75.7331 |         -10.6511 |
[32m[20221214 00:23:46 @agent_ppo2.py:185][0m |          -0.0051 |          71.0491 |         -10.7476 |
[32m[20221214 00:23:46 @agent_ppo2.py:185][0m |          -0.0109 |          67.1579 |         -10.7759 |
[32m[20221214 00:23:46 @agent_ppo2.py:185][0m |          -0.0051 |          66.0900 |         -10.7259 |
[32m[20221214 00:23:46 @agent_ppo2.py:185][0m |          -0.0115 |          63.4585 |         -10.7272 |
[32m[20221214 00:23:46 @agent_ppo2.py:185][0m |          -0.0093 |          63.3278 |         -10.8774 |
[32m[20221214 00:23:47 @agent_ppo2.py:185][0m |          -0.0176 |          61.3678 |         -10.7931 |
[32m[20221214 00:23:47 @agent_ppo2.py:185][0m |          -0.0134 |          59.7411 |         -10.9063 |
[32m[20221214 00:23:47 @agent_ppo2.py:185][0m |          -0.0117 |          58.9415 |         -10.9351 |
[32m[20221214 00:23:47 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:23:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 600.31
[32m[20221214 00:23:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 788.21
[32m[20221214 00:23:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 765.88
[32m[20221214 00:23:47 @agent_ppo2.py:143][0m Total time:      26.27 min
[32m[20221214 00:23:47 @agent_ppo2.py:145][0m 2396160 total steps have happened
[32m[20221214 00:23:47 @agent_ppo2.py:121][0m #------------------------ Iteration 5170 --------------------------#
[32m[20221214 00:23:47 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:23:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:23:47 @agent_ppo2.py:185][0m |           0.0047 |          95.0283 |         -11.6417 |
[32m[20221214 00:23:47 @agent_ppo2.py:185][0m |          -0.0091 |          83.2987 |         -11.5397 |
[32m[20221214 00:23:47 @agent_ppo2.py:185][0m |          -0.0087 |          79.3231 |         -11.5246 |
[32m[20221214 00:23:48 @agent_ppo2.py:185][0m |          -0.0124 |          77.1798 |         -11.5191 |
[32m[20221214 00:23:48 @agent_ppo2.py:185][0m |          -0.0137 |          75.6477 |         -11.4430 |
[32m[20221214 00:23:48 @agent_ppo2.py:185][0m |          -0.0127 |          74.2248 |         -11.4652 |
[32m[20221214 00:23:48 @agent_ppo2.py:185][0m |          -0.0136 |          73.3633 |         -11.4220 |
[32m[20221214 00:23:48 @agent_ppo2.py:185][0m |          -0.0161 |          72.2195 |         -11.3368 |
[32m[20221214 00:23:48 @agent_ppo2.py:185][0m |          -0.0167 |          71.1664 |         -11.2161 |
[32m[20221214 00:23:48 @agent_ppo2.py:185][0m |          -0.0118 |          70.4315 |         -11.1816 |
[32m[20221214 00:23:48 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:23:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 545.17
[32m[20221214 00:23:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 645.97
[32m[20221214 00:23:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 539.26
[32m[20221214 00:23:48 @agent_ppo2.py:143][0m Total time:      26.30 min
[32m[20221214 00:23:48 @agent_ppo2.py:145][0m 2398208 total steps have happened
[32m[20221214 00:23:48 @agent_ppo2.py:121][0m #------------------------ Iteration 5171 --------------------------#
[32m[20221214 00:23:48 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:23:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:23:49 @agent_ppo2.py:185][0m |          -0.0015 |          81.0254 |         -11.5793 |
[32m[20221214 00:23:49 @agent_ppo2.py:185][0m |          -0.0000 |          72.0013 |         -11.3880 |
[32m[20221214 00:23:49 @agent_ppo2.py:185][0m |          -0.0039 |          70.1778 |         -11.3378 |
[32m[20221214 00:23:49 @agent_ppo2.py:185][0m |           0.0016 |          73.6429 |         -11.4566 |
[32m[20221214 00:23:49 @agent_ppo2.py:185][0m |          -0.0067 |          68.4539 |         -11.4481 |
[32m[20221214 00:23:49 @agent_ppo2.py:185][0m |          -0.0053 |          67.2136 |         -11.3469 |
[32m[20221214 00:23:49 @agent_ppo2.py:185][0m |          -0.0079 |          66.9581 |         -11.5410 |
[32m[20221214 00:23:49 @agent_ppo2.py:185][0m |          -0.0061 |          66.8133 |         -11.3318 |
[32m[20221214 00:23:49 @agent_ppo2.py:185][0m |          -0.0073 |          66.0633 |         -11.7171 |
[32m[20221214 00:23:50 @agent_ppo2.py:185][0m |          -0.0075 |          66.4371 |         -11.6465 |
[32m[20221214 00:23:50 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:23:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 532.67
[32m[20221214 00:23:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 656.09
[32m[20221214 00:23:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 681.12
[32m[20221214 00:23:50 @agent_ppo2.py:143][0m Total time:      26.32 min
[32m[20221214 00:23:50 @agent_ppo2.py:145][0m 2400256 total steps have happened
[32m[20221214 00:23:50 @agent_ppo2.py:121][0m #------------------------ Iteration 5172 --------------------------#
[32m[20221214 00:23:50 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:23:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:23:50 @agent_ppo2.py:185][0m |          -0.0013 |          39.6791 |         -11.3051 |
[32m[20221214 00:23:50 @agent_ppo2.py:185][0m |          -0.0044 |          32.5879 |         -11.3462 |
[32m[20221214 00:23:50 @agent_ppo2.py:185][0m |          -0.0063 |          31.6417 |         -11.3722 |
[32m[20221214 00:23:50 @agent_ppo2.py:185][0m |          -0.0156 |          30.6506 |         -11.3143 |
[32m[20221214 00:23:50 @agent_ppo2.py:185][0m |          -0.0095 |          30.0930 |         -11.4342 |
[32m[20221214 00:23:51 @agent_ppo2.py:185][0m |          -0.0100 |          30.2575 |         -11.3268 |
[32m[20221214 00:23:51 @agent_ppo2.py:185][0m |          -0.0100 |          29.5495 |         -11.2781 |
[32m[20221214 00:23:51 @agent_ppo2.py:185][0m |          -0.0159 |          28.5888 |         -11.3817 |
[32m[20221214 00:23:51 @agent_ppo2.py:185][0m |          -0.0102 |          28.3231 |         -11.3959 |
[32m[20221214 00:23:51 @agent_ppo2.py:185][0m |          -0.0221 |          28.0939 |         -11.3379 |
[32m[20221214 00:23:51 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:23:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 478.12
[32m[20221214 00:23:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 587.38
[32m[20221214 00:23:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 653.32
[32m[20221214 00:23:51 @agent_ppo2.py:143][0m Total time:      26.34 min
[32m[20221214 00:23:51 @agent_ppo2.py:145][0m 2402304 total steps have happened
[32m[20221214 00:23:51 @agent_ppo2.py:121][0m #------------------------ Iteration 5173 --------------------------#
[32m[20221214 00:23:51 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:23:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:23:51 @agent_ppo2.py:185][0m |           0.0012 |         101.7052 |         -12.1593 |
[32m[20221214 00:23:52 @agent_ppo2.py:185][0m |          -0.0031 |          92.9498 |         -11.9688 |
[32m[20221214 00:23:52 @agent_ppo2.py:185][0m |           0.0046 |         103.2162 |         -11.9346 |
[32m[20221214 00:23:52 @agent_ppo2.py:185][0m |          -0.0043 |          87.9730 |         -11.9226 |
[32m[20221214 00:23:52 @agent_ppo2.py:185][0m |          -0.0130 |          86.7363 |         -12.0472 |
[32m[20221214 00:23:52 @agent_ppo2.py:185][0m |          -0.0079 |          85.8387 |         -12.0302 |
[32m[20221214 00:23:52 @agent_ppo2.py:185][0m |          -0.0166 |          84.7551 |         -11.9704 |
[32m[20221214 00:23:52 @agent_ppo2.py:185][0m |          -0.0152 |          83.9936 |         -11.8519 |
[32m[20221214 00:23:52 @agent_ppo2.py:185][0m |          -0.0180 |          84.2885 |         -11.9693 |
[32m[20221214 00:23:52 @agent_ppo2.py:185][0m |          -0.0138 |          83.1698 |         -11.8600 |
[32m[20221214 00:23:52 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:23:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 591.34
[32m[20221214 00:23:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 698.69
[32m[20221214 00:23:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 660.85
[32m[20221214 00:23:52 @agent_ppo2.py:143][0m Total time:      26.37 min
[32m[20221214 00:23:52 @agent_ppo2.py:145][0m 2404352 total steps have happened
[32m[20221214 00:23:52 @agent_ppo2.py:121][0m #------------------------ Iteration 5174 --------------------------#
[32m[20221214 00:23:53 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:23:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:23:53 @agent_ppo2.py:185][0m |           0.0042 |         105.9479 |         -12.8036 |
[32m[20221214 00:23:53 @agent_ppo2.py:185][0m |          -0.0074 |          98.9452 |         -12.7518 |
[32m[20221214 00:23:53 @agent_ppo2.py:185][0m |          -0.0053 |          98.1859 |         -12.7615 |
[32m[20221214 00:23:53 @agent_ppo2.py:185][0m |          -0.0121 |          94.8264 |         -12.4938 |
[32m[20221214 00:23:53 @agent_ppo2.py:185][0m |          -0.0067 |          94.6850 |         -12.5356 |
[32m[20221214 00:23:53 @agent_ppo2.py:185][0m |          -0.0150 |          93.2528 |         -12.4423 |
[32m[20221214 00:23:53 @agent_ppo2.py:185][0m |          -0.0144 |          92.9511 |         -12.4291 |
[32m[20221214 00:23:54 @agent_ppo2.py:185][0m |          -0.0157 |          91.5127 |         -12.5649 |
[32m[20221214 00:23:54 @agent_ppo2.py:185][0m |          -0.0147 |          90.9497 |         -12.6404 |
[32m[20221214 00:23:54 @agent_ppo2.py:185][0m |          -0.0102 |          91.1410 |         -12.6255 |
[32m[20221214 00:23:54 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:23:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 557.81
[32m[20221214 00:23:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 688.80
[32m[20221214 00:23:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 593.69
[32m[20221214 00:23:54 @agent_ppo2.py:143][0m Total time:      26.39 min
[32m[20221214 00:23:54 @agent_ppo2.py:145][0m 2406400 total steps have happened
[32m[20221214 00:23:54 @agent_ppo2.py:121][0m #------------------------ Iteration 5175 --------------------------#
[32m[20221214 00:23:54 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:23:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:23:54 @agent_ppo2.py:185][0m |          -0.0018 |          71.3322 |         -12.5358 |
[32m[20221214 00:23:54 @agent_ppo2.py:185][0m |          -0.0117 |          65.3561 |         -12.3612 |
[32m[20221214 00:23:54 @agent_ppo2.py:185][0m |          -0.0073 |          63.0203 |         -12.3530 |
[32m[20221214 00:23:54 @agent_ppo2.py:185][0m |          -0.0119 |          60.7796 |         -12.2716 |
[32m[20221214 00:23:55 @agent_ppo2.py:185][0m |          -0.0148 |          59.6806 |         -12.2142 |
[32m[20221214 00:23:55 @agent_ppo2.py:185][0m |          -0.0129 |          58.8234 |         -12.3189 |
[32m[20221214 00:23:55 @agent_ppo2.py:185][0m |          -0.0117 |          58.5939 |         -12.2554 |
[32m[20221214 00:23:55 @agent_ppo2.py:185][0m |          -0.0209 |          57.2466 |         -12.1328 |
[32m[20221214 00:23:55 @agent_ppo2.py:185][0m |          -0.0152 |          56.8044 |         -12.2062 |
[32m[20221214 00:23:55 @agent_ppo2.py:185][0m |          -0.0110 |          57.3688 |         -12.1820 |
[32m[20221214 00:23:55 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:23:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 462.46
[32m[20221214 00:23:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 555.32
[32m[20221214 00:23:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 603.57
[32m[20221214 00:23:55 @agent_ppo2.py:143][0m Total time:      26.41 min
[32m[20221214 00:23:55 @agent_ppo2.py:145][0m 2408448 total steps have happened
[32m[20221214 00:23:55 @agent_ppo2.py:121][0m #------------------------ Iteration 5176 --------------------------#
[32m[20221214 00:23:55 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:23:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:23:56 @agent_ppo2.py:185][0m |          -0.0016 |          80.6989 |         -13.8368 |
[32m[20221214 00:23:56 @agent_ppo2.py:185][0m |          -0.0075 |          73.7595 |         -13.8679 |
[32m[20221214 00:23:56 @agent_ppo2.py:185][0m |           0.0007 |          71.8964 |         -13.5732 |
[32m[20221214 00:23:56 @agent_ppo2.py:185][0m |          -0.0107 |          70.7506 |         -13.7286 |
[32m[20221214 00:23:56 @agent_ppo2.py:185][0m |          -0.0126 |          69.1530 |         -13.7334 |
[32m[20221214 00:23:56 @agent_ppo2.py:185][0m |          -0.0060 |          68.5608 |         -13.7119 |
[32m[20221214 00:23:56 @agent_ppo2.py:185][0m |          -0.0089 |          68.4254 |         -13.6292 |
[32m[20221214 00:23:56 @agent_ppo2.py:185][0m |          -0.0130 |          67.0555 |         -13.5394 |
[32m[20221214 00:23:56 @agent_ppo2.py:185][0m |          -0.0005 |          71.8346 |         -13.5718 |
[32m[20221214 00:23:56 @agent_ppo2.py:185][0m |          -0.0079 |          67.5455 |         -13.4307 |
[32m[20221214 00:23:56 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:23:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 443.85
[32m[20221214 00:23:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 496.88
[32m[20221214 00:23:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 719.98
[32m[20221214 00:23:57 @agent_ppo2.py:143][0m Total time:      26.44 min
[32m[20221214 00:23:57 @agent_ppo2.py:145][0m 2410496 total steps have happened
[32m[20221214 00:23:57 @agent_ppo2.py:121][0m #------------------------ Iteration 5177 --------------------------#
[32m[20221214 00:23:57 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:23:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:23:57 @agent_ppo2.py:185][0m |          -0.0021 |          90.9497 |         -11.8126 |
[32m[20221214 00:23:57 @agent_ppo2.py:185][0m |          -0.0052 |          87.0737 |         -11.8237 |
[32m[20221214 00:23:57 @agent_ppo2.py:185][0m |          -0.0094 |          80.0085 |         -11.6555 |
[32m[20221214 00:23:57 @agent_ppo2.py:185][0m |          -0.0084 |          78.5240 |         -11.6830 |
[32m[20221214 00:23:57 @agent_ppo2.py:185][0m |          -0.0152 |          76.9838 |         -11.6085 |
[32m[20221214 00:23:57 @agent_ppo2.py:185][0m |          -0.0181 |          75.4269 |         -11.6466 |
[32m[20221214 00:23:58 @agent_ppo2.py:185][0m |          -0.0218 |          74.8329 |         -11.5943 |
[32m[20221214 00:23:58 @agent_ppo2.py:185][0m |          -0.0129 |          74.1109 |         -11.7400 |
[32m[20221214 00:23:58 @agent_ppo2.py:185][0m |          -0.0180 |          74.9096 |         -11.7729 |
[32m[20221214 00:23:58 @agent_ppo2.py:185][0m |          -0.0164 |          72.7014 |         -11.6921 |
[32m[20221214 00:23:58 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:23:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 460.28
[32m[20221214 00:23:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 560.78
[32m[20221214 00:23:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 604.85
[32m[20221214 00:23:58 @agent_ppo2.py:143][0m Total time:      26.46 min
[32m[20221214 00:23:58 @agent_ppo2.py:145][0m 2412544 total steps have happened
[32m[20221214 00:23:58 @agent_ppo2.py:121][0m #------------------------ Iteration 5178 --------------------------#
[32m[20221214 00:23:58 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:23:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:23:58 @agent_ppo2.py:185][0m |          -0.0031 |          84.5005 |         -12.6236 |
[32m[20221214 00:23:58 @agent_ppo2.py:185][0m |          -0.0044 |          66.8574 |         -12.7046 |
[32m[20221214 00:23:59 @agent_ppo2.py:185][0m |          -0.0088 |          63.0278 |         -12.4872 |
[32m[20221214 00:23:59 @agent_ppo2.py:185][0m |          -0.0071 |          66.4112 |         -12.5880 |
[32m[20221214 00:23:59 @agent_ppo2.py:185][0m |          -0.0103 |          60.0363 |         -12.4700 |
[32m[20221214 00:23:59 @agent_ppo2.py:185][0m |          -0.0113 |          58.5352 |         -12.5988 |
[32m[20221214 00:23:59 @agent_ppo2.py:185][0m |          -0.0079 |          57.2023 |         -12.5761 |
[32m[20221214 00:23:59 @agent_ppo2.py:185][0m |          -0.0197 |          55.4435 |         -12.6103 |
[32m[20221214 00:23:59 @agent_ppo2.py:185][0m |          -0.0168 |          54.7770 |         -12.5724 |
[32m[20221214 00:23:59 @agent_ppo2.py:185][0m |          -0.0183 |          53.8317 |         -12.6300 |
[32m[20221214 00:23:59 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:23:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 516.08
[32m[20221214 00:23:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 599.17
[32m[20221214 00:23:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 526.96
[32m[20221214 00:23:59 @agent_ppo2.py:143][0m Total time:      26.48 min
[32m[20221214 00:23:59 @agent_ppo2.py:145][0m 2414592 total steps have happened
[32m[20221214 00:23:59 @agent_ppo2.py:121][0m #------------------------ Iteration 5179 --------------------------#
[32m[20221214 00:23:59 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:24:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:24:00 @agent_ppo2.py:185][0m |           0.0027 |          67.9024 |         -12.3822 |
[32m[20221214 00:24:00 @agent_ppo2.py:185][0m |          -0.0089 |          61.1136 |         -12.3289 |
[32m[20221214 00:24:00 @agent_ppo2.py:185][0m |           0.0014 |          66.5650 |         -12.3840 |
[32m[20221214 00:24:00 @agent_ppo2.py:185][0m |          -0.0095 |          57.7756 |         -12.4611 |
[32m[20221214 00:24:00 @agent_ppo2.py:185][0m |          -0.0175 |          57.2953 |         -12.4237 |
[32m[20221214 00:24:00 @agent_ppo2.py:185][0m |          -0.0148 |          56.4370 |         -12.3184 |
[32m[20221214 00:24:00 @agent_ppo2.py:185][0m |          -0.0134 |          55.8771 |         -12.3419 |
[32m[20221214 00:24:00 @agent_ppo2.py:185][0m |          -0.0165 |          55.3258 |         -12.4084 |
[32m[20221214 00:24:00 @agent_ppo2.py:185][0m |          -0.0098 |          55.2135 |         -12.3112 |
[32m[20221214 00:24:01 @agent_ppo2.py:185][0m |          -0.0007 |          65.5705 |         -12.3793 |
[32m[20221214 00:24:01 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:24:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 576.50
[32m[20221214 00:24:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 635.43
[32m[20221214 00:24:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 573.89
[32m[20221214 00:24:01 @agent_ppo2.py:143][0m Total time:      26.51 min
[32m[20221214 00:24:01 @agent_ppo2.py:145][0m 2416640 total steps have happened
[32m[20221214 00:24:01 @agent_ppo2.py:121][0m #------------------------ Iteration 5180 --------------------------#
[32m[20221214 00:24:01 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:24:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:24:01 @agent_ppo2.py:185][0m |          -0.0028 |          82.8232 |         -12.8124 |
[32m[20221214 00:24:01 @agent_ppo2.py:185][0m |          -0.0110 |          73.5385 |         -12.5559 |
[32m[20221214 00:24:01 @agent_ppo2.py:185][0m |          -0.0098 |          71.5410 |         -12.7918 |
[32m[20221214 00:24:01 @agent_ppo2.py:185][0m |          -0.0089 |          68.1553 |         -12.6044 |
[32m[20221214 00:24:01 @agent_ppo2.py:185][0m |          -0.0179 |          66.3853 |         -12.6650 |
[32m[20221214 00:24:02 @agent_ppo2.py:185][0m |          -0.0193 |          65.0025 |         -12.5230 |
[32m[20221214 00:24:02 @agent_ppo2.py:185][0m |          -0.0196 |          63.8339 |         -12.6673 |
[32m[20221214 00:24:02 @agent_ppo2.py:185][0m |          -0.0200 |          63.7127 |         -12.5286 |
[32m[20221214 00:24:02 @agent_ppo2.py:185][0m |          -0.0166 |          62.3758 |         -12.4205 |
[32m[20221214 00:24:02 @agent_ppo2.py:185][0m |          -0.0235 |          61.4494 |         -12.4467 |
[32m[20221214 00:24:02 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:24:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 549.28
[32m[20221214 00:24:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 633.36
[32m[20221214 00:24:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 721.79
[32m[20221214 00:24:02 @agent_ppo2.py:143][0m Total time:      26.53 min
[32m[20221214 00:24:02 @agent_ppo2.py:145][0m 2418688 total steps have happened
[32m[20221214 00:24:02 @agent_ppo2.py:121][0m #------------------------ Iteration 5181 --------------------------#
[32m[20221214 00:24:02 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:24:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:24:02 @agent_ppo2.py:185][0m |           0.0022 |          99.8655 |         -12.6093 |
[32m[20221214 00:24:02 @agent_ppo2.py:185][0m |          -0.0025 |          82.9928 |         -12.4497 |
[32m[20221214 00:24:03 @agent_ppo2.py:185][0m |          -0.0034 |          79.4011 |         -12.2769 |
[32m[20221214 00:24:03 @agent_ppo2.py:185][0m |          -0.0090 |          77.6944 |         -12.3150 |
[32m[20221214 00:24:03 @agent_ppo2.py:185][0m |          -0.0028 |          76.6282 |         -12.2524 |
[32m[20221214 00:24:03 @agent_ppo2.py:185][0m |          -0.0146 |          75.7346 |         -12.1936 |
[32m[20221214 00:24:03 @agent_ppo2.py:185][0m |          -0.0054 |          75.1949 |         -12.1249 |
[32m[20221214 00:24:03 @agent_ppo2.py:185][0m |          -0.0130 |          74.5067 |         -12.1428 |
[32m[20221214 00:24:03 @agent_ppo2.py:185][0m |          -0.0160 |          73.9847 |         -12.0150 |
[32m[20221214 00:24:03 @agent_ppo2.py:185][0m |          -0.0077 |          73.5135 |         -11.9649 |
[32m[20221214 00:24:03 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:24:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 585.51
[32m[20221214 00:24:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 640.58
[32m[20221214 00:24:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 534.35
[32m[20221214 00:24:03 @agent_ppo2.py:143][0m Total time:      26.55 min
[32m[20221214 00:24:03 @agent_ppo2.py:145][0m 2420736 total steps have happened
[32m[20221214 00:24:03 @agent_ppo2.py:121][0m #------------------------ Iteration 5182 --------------------------#
[32m[20221214 00:24:04 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:24:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:24:04 @agent_ppo2.py:185][0m |          -0.0029 |          92.5893 |         -12.9402 |
[32m[20221214 00:24:04 @agent_ppo2.py:185][0m |          -0.0043 |          81.0359 |         -12.9840 |
[32m[20221214 00:24:04 @agent_ppo2.py:185][0m |          -0.0043 |          76.4341 |         -13.0457 |
[32m[20221214 00:24:04 @agent_ppo2.py:185][0m |          -0.0065 |          73.5549 |         -13.0672 |
[32m[20221214 00:24:04 @agent_ppo2.py:185][0m |          -0.0117 |          71.5289 |         -12.9759 |
[32m[20221214 00:24:04 @agent_ppo2.py:185][0m |          -0.0078 |          70.3471 |         -12.8815 |
[32m[20221214 00:24:04 @agent_ppo2.py:185][0m |          -0.0104 |          69.0450 |         -13.0691 |
[32m[20221214 00:24:04 @agent_ppo2.py:185][0m |          -0.0037 |          72.5458 |         -13.1648 |
[32m[20221214 00:24:05 @agent_ppo2.py:185][0m |          -0.0129 |          67.4568 |         -13.1454 |
[32m[20221214 00:24:05 @agent_ppo2.py:185][0m |          -0.0138 |          66.3808 |         -13.1576 |
[32m[20221214 00:24:05 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:24:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 600.50
[32m[20221214 00:24:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 648.74
[32m[20221214 00:24:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 603.97
[32m[20221214 00:24:05 @agent_ppo2.py:143][0m Total time:      26.57 min
[32m[20221214 00:24:05 @agent_ppo2.py:145][0m 2422784 total steps have happened
[32m[20221214 00:24:05 @agent_ppo2.py:121][0m #------------------------ Iteration 5183 --------------------------#
[32m[20221214 00:24:05 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:24:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:24:05 @agent_ppo2.py:185][0m |          -0.0011 |         122.7637 |         -12.4062 |
[32m[20221214 00:24:05 @agent_ppo2.py:185][0m |          -0.0033 |         112.0547 |         -12.5233 |
[32m[20221214 00:24:05 @agent_ppo2.py:185][0m |          -0.0107 |         106.7528 |         -12.4922 |
[32m[20221214 00:24:05 @agent_ppo2.py:185][0m |          -0.0083 |         104.0037 |         -12.3787 |
[32m[20221214 00:24:06 @agent_ppo2.py:185][0m |          -0.0122 |         101.3721 |         -12.5229 |
[32m[20221214 00:24:06 @agent_ppo2.py:185][0m |          -0.0179 |          99.5271 |         -12.4774 |
[32m[20221214 00:24:06 @agent_ppo2.py:185][0m |          -0.0095 |          98.3869 |         -12.5045 |
[32m[20221214 00:24:06 @agent_ppo2.py:185][0m |          -0.0142 |          97.4973 |         -12.4345 |
[32m[20221214 00:24:06 @agent_ppo2.py:185][0m |          -0.0143 |          97.0139 |         -12.4867 |
[32m[20221214 00:24:06 @agent_ppo2.py:185][0m |          -0.0167 |          95.3502 |         -12.4100 |
[32m[20221214 00:24:06 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:24:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 478.03
[32m[20221214 00:24:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 618.64
[32m[20221214 00:24:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 662.73
[32m[20221214 00:24:06 @agent_ppo2.py:143][0m Total time:      26.60 min
[32m[20221214 00:24:06 @agent_ppo2.py:145][0m 2424832 total steps have happened
[32m[20221214 00:24:06 @agent_ppo2.py:121][0m #------------------------ Iteration 5184 --------------------------#
[32m[20221214 00:24:06 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:24:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:24:07 @agent_ppo2.py:185][0m |          -0.0009 |          70.2963 |         -13.5873 |
[32m[20221214 00:24:07 @agent_ppo2.py:185][0m |          -0.0022 |          62.7582 |         -13.5035 |
[32m[20221214 00:24:07 @agent_ppo2.py:185][0m |          -0.0042 |          60.5582 |         -13.4999 |
[32m[20221214 00:24:07 @agent_ppo2.py:185][0m |          -0.0030 |          59.5633 |         -13.5793 |
[32m[20221214 00:24:07 @agent_ppo2.py:185][0m |          -0.0139 |          58.4277 |         -13.5128 |
[32m[20221214 00:24:07 @agent_ppo2.py:185][0m |          -0.0083 |          57.1846 |         -13.4757 |
[32m[20221214 00:24:07 @agent_ppo2.py:185][0m |          -0.0133 |          57.0542 |         -13.5518 |
[32m[20221214 00:24:07 @agent_ppo2.py:185][0m |          -0.0086 |          56.4298 |         -13.5828 |
[32m[20221214 00:24:07 @agent_ppo2.py:185][0m |          -0.0141 |          56.1281 |         -13.7217 |
[32m[20221214 00:24:07 @agent_ppo2.py:185][0m |          -0.0188 |          55.6604 |         -13.6456 |
[32m[20221214 00:24:07 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:24:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 529.22
[32m[20221214 00:24:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 610.61
[32m[20221214 00:24:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 622.71
[32m[20221214 00:24:08 @agent_ppo2.py:143][0m Total time:      26.62 min
[32m[20221214 00:24:08 @agent_ppo2.py:145][0m 2426880 total steps have happened
[32m[20221214 00:24:08 @agent_ppo2.py:121][0m #------------------------ Iteration 5185 --------------------------#
[32m[20221214 00:24:08 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:24:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:24:08 @agent_ppo2.py:185][0m |           0.0066 |          33.5412 |         -13.3383 |
[32m[20221214 00:24:08 @agent_ppo2.py:185][0m |          -0.0072 |          22.6201 |         -13.1377 |
[32m[20221214 00:24:08 @agent_ppo2.py:185][0m |          -0.0146 |          20.3191 |         -13.1941 |
[32m[20221214 00:24:08 @agent_ppo2.py:185][0m |          -0.0070 |          19.7631 |         -13.1556 |
[32m[20221214 00:24:08 @agent_ppo2.py:185][0m |          -0.0152 |          18.6008 |         -13.1269 |
[32m[20221214 00:24:08 @agent_ppo2.py:185][0m |          -0.0161 |          17.9983 |         -12.9365 |
[32m[20221214 00:24:09 @agent_ppo2.py:185][0m |          -0.0195 |          17.7929 |         -13.0487 |
[32m[20221214 00:24:09 @agent_ppo2.py:185][0m |          -0.0166 |          17.3307 |         -12.8599 |
[32m[20221214 00:24:09 @agent_ppo2.py:185][0m |          -0.0132 |          17.1993 |         -12.9216 |
[32m[20221214 00:24:09 @agent_ppo2.py:185][0m |          -0.0212 |          16.5086 |         -12.8109 |
[32m[20221214 00:24:09 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:24:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 524.16
[32m[20221214 00:24:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 631.12
[32m[20221214 00:24:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 551.73
[32m[20221214 00:24:09 @agent_ppo2.py:143][0m Total time:      26.64 min
[32m[20221214 00:24:09 @agent_ppo2.py:145][0m 2428928 total steps have happened
[32m[20221214 00:24:09 @agent_ppo2.py:121][0m #------------------------ Iteration 5186 --------------------------#
[32m[20221214 00:24:09 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:24:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:24:09 @agent_ppo2.py:185][0m |           0.0110 |          70.5445 |         -12.7994 |
[32m[20221214 00:24:09 @agent_ppo2.py:185][0m |          -0.0077 |          55.6026 |         -12.7921 |
[32m[20221214 00:24:09 @agent_ppo2.py:185][0m |          -0.0002 |          57.0840 |         -12.7818 |
[32m[20221214 00:24:10 @agent_ppo2.py:185][0m |           0.0022 |          57.0381 |         -12.7729 |
[32m[20221214 00:24:10 @agent_ppo2.py:185][0m |          -0.0125 |          52.3027 |         -12.7867 |
[32m[20221214 00:24:10 @agent_ppo2.py:185][0m |          -0.0135 |          50.9881 |         -12.7991 |
[32m[20221214 00:24:10 @agent_ppo2.py:185][0m |          -0.0228 |          50.7369 |         -12.8109 |
[32m[20221214 00:24:10 @agent_ppo2.py:185][0m |          -0.0158 |          50.8572 |         -12.6110 |
[32m[20221214 00:24:10 @agent_ppo2.py:185][0m |          -0.0193 |          49.9029 |         -12.7983 |
[32m[20221214 00:24:10 @agent_ppo2.py:185][0m |          -0.0177 |          50.1736 |         -12.5871 |
[32m[20221214 00:24:10 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:24:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 455.84
[32m[20221214 00:24:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 586.91
[32m[20221214 00:24:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 86.87
[32m[20221214 00:24:10 @agent_ppo2.py:143][0m Total time:      26.67 min
[32m[20221214 00:24:10 @agent_ppo2.py:145][0m 2430976 total steps have happened
[32m[20221214 00:24:10 @agent_ppo2.py:121][0m #------------------------ Iteration 5187 --------------------------#
[32m[20221214 00:24:10 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:24:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:24:11 @agent_ppo2.py:185][0m |          -0.0015 |          88.7405 |         -13.1206 |
[32m[20221214 00:24:11 @agent_ppo2.py:185][0m |          -0.0054 |          81.0993 |         -13.1282 |
[32m[20221214 00:24:11 @agent_ppo2.py:185][0m |          -0.0009 |          79.1722 |         -12.9562 |
[32m[20221214 00:24:11 @agent_ppo2.py:185][0m |           0.0117 |          85.8048 |         -12.9110 |
[32m[20221214 00:24:11 @agent_ppo2.py:185][0m |          -0.0126 |          75.6123 |         -12.7730 |
[32m[20221214 00:24:11 @agent_ppo2.py:185][0m |          -0.0146 |          75.0873 |         -13.0121 |
[32m[20221214 00:24:11 @agent_ppo2.py:185][0m |          -0.0158 |          73.9557 |         -12.8210 |
[32m[20221214 00:24:11 @agent_ppo2.py:185][0m |          -0.0145 |          73.2802 |         -12.8043 |
[32m[20221214 00:24:11 @agent_ppo2.py:185][0m |          -0.0173 |          73.2243 |         -12.6889 |
[32m[20221214 00:24:12 @agent_ppo2.py:185][0m |          -0.0143 |          73.1809 |         -12.6561 |
[32m[20221214 00:24:12 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:24:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 588.36
[32m[20221214 00:24:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 704.76
[32m[20221214 00:24:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 541.61
[32m[20221214 00:24:12 @agent_ppo2.py:143][0m Total time:      26.69 min
[32m[20221214 00:24:12 @agent_ppo2.py:145][0m 2433024 total steps have happened
[32m[20221214 00:24:12 @agent_ppo2.py:121][0m #------------------------ Iteration 5188 --------------------------#
[32m[20221214 00:24:12 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:24:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:24:12 @agent_ppo2.py:185][0m |          -0.0026 |          52.5230 |         -10.9256 |
[32m[20221214 00:24:12 @agent_ppo2.py:185][0m |          -0.0079 |          45.4133 |         -10.7735 |
[32m[20221214 00:24:12 @agent_ppo2.py:185][0m |          -0.0055 |          43.1250 |         -10.9391 |
[32m[20221214 00:24:12 @agent_ppo2.py:185][0m |          -0.0043 |          43.6013 |         -10.9159 |
[32m[20221214 00:24:12 @agent_ppo2.py:185][0m |          -0.0079 |          41.6965 |         -10.8500 |
[32m[20221214 00:24:12 @agent_ppo2.py:185][0m |          -0.0027 |          48.8964 |         -10.8526 |
[32m[20221214 00:24:13 @agent_ppo2.py:185][0m |          -0.0132 |          40.1481 |         -10.8376 |
[32m[20221214 00:24:13 @agent_ppo2.py:185][0m |          -0.0179 |          39.3576 |         -10.8143 |
[32m[20221214 00:24:13 @agent_ppo2.py:185][0m |          -0.0215 |          39.5570 |         -10.8384 |
[32m[20221214 00:24:13 @agent_ppo2.py:185][0m |          -0.0155 |          38.9546 |         -10.8695 |
[32m[20221214 00:24:13 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:24:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 483.08
[32m[20221214 00:24:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 566.70
[32m[20221214 00:24:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 753.09
[32m[20221214 00:24:13 @agent_ppo2.py:143][0m Total time:      26.71 min
[32m[20221214 00:24:13 @agent_ppo2.py:145][0m 2435072 total steps have happened
[32m[20221214 00:24:13 @agent_ppo2.py:121][0m #------------------------ Iteration 5189 --------------------------#
[32m[20221214 00:24:13 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:24:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:24:13 @agent_ppo2.py:185][0m |           0.0044 |          63.2652 |         -12.1947 |
[32m[20221214 00:24:13 @agent_ppo2.py:185][0m |          -0.0034 |          57.5479 |         -12.1397 |
[32m[20221214 00:24:14 @agent_ppo2.py:185][0m |          -0.0065 |          55.4624 |         -12.2309 |
[32m[20221214 00:24:14 @agent_ppo2.py:185][0m |          -0.0026 |          58.7964 |         -12.1249 |
[32m[20221214 00:24:14 @agent_ppo2.py:185][0m |          -0.0105 |          53.5337 |         -12.1380 |
[32m[20221214 00:24:14 @agent_ppo2.py:185][0m |          -0.0147 |          52.7786 |         -12.0904 |
[32m[20221214 00:24:14 @agent_ppo2.py:185][0m |          -0.0125 |          52.5617 |         -12.1653 |
[32m[20221214 00:24:14 @agent_ppo2.py:185][0m |          -0.0081 |          51.9048 |         -12.1920 |
[32m[20221214 00:24:14 @agent_ppo2.py:185][0m |          -0.0184 |          51.3348 |         -12.1433 |
[32m[20221214 00:24:14 @agent_ppo2.py:185][0m |          -0.0165 |          50.9984 |         -12.1587 |
[32m[20221214 00:24:14 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:24:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 502.11
[32m[20221214 00:24:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 589.81
[32m[20221214 00:24:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 455.47
[32m[20221214 00:24:14 @agent_ppo2.py:143][0m Total time:      26.73 min
[32m[20221214 00:24:14 @agent_ppo2.py:145][0m 2437120 total steps have happened
[32m[20221214 00:24:14 @agent_ppo2.py:121][0m #------------------------ Iteration 5190 --------------------------#
[32m[20221214 00:24:15 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:24:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:24:15 @agent_ppo2.py:185][0m |          -0.0031 |          82.7197 |         -13.0086 |
[32m[20221214 00:24:15 @agent_ppo2.py:185][0m |           0.0045 |          76.5547 |         -13.0756 |
[32m[20221214 00:24:15 @agent_ppo2.py:185][0m |          -0.0079 |          70.7228 |         -12.9857 |
[32m[20221214 00:24:15 @agent_ppo2.py:185][0m |          -0.0154 |          69.0647 |         -13.0869 |
[32m[20221214 00:24:15 @agent_ppo2.py:185][0m |          -0.0161 |          67.2085 |         -13.1325 |
[32m[20221214 00:24:15 @agent_ppo2.py:185][0m |          -0.0181 |          66.1436 |         -13.1512 |
[32m[20221214 00:24:15 @agent_ppo2.py:185][0m |          -0.0185 |          65.6614 |         -13.0699 |
[32m[20221214 00:24:15 @agent_ppo2.py:185][0m |          -0.0171 |          64.5814 |         -13.0719 |
[32m[20221214 00:24:16 @agent_ppo2.py:185][0m |          -0.0163 |          63.7461 |         -13.0115 |
[32m[20221214 00:24:16 @agent_ppo2.py:185][0m |          -0.0215 |          62.5725 |         -13.0359 |
[32m[20221214 00:24:16 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:24:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 617.14
[32m[20221214 00:24:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 707.02
[32m[20221214 00:24:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 574.90
[32m[20221214 00:24:16 @agent_ppo2.py:143][0m Total time:      26.76 min
[32m[20221214 00:24:16 @agent_ppo2.py:145][0m 2439168 total steps have happened
[32m[20221214 00:24:16 @agent_ppo2.py:121][0m #------------------------ Iteration 5191 --------------------------#
[32m[20221214 00:24:16 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:24:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:24:16 @agent_ppo2.py:185][0m |          -0.0011 |          57.7244 |         -12.8918 |
[32m[20221214 00:24:16 @agent_ppo2.py:185][0m |          -0.0084 |          48.9887 |         -12.9093 |
[32m[20221214 00:24:16 @agent_ppo2.py:185][0m |          -0.0138 |          46.8383 |         -12.6656 |
[32m[20221214 00:24:16 @agent_ppo2.py:185][0m |          -0.0178 |          45.8537 |         -12.6844 |
[32m[20221214 00:24:16 @agent_ppo2.py:185][0m |          -0.0155 |          44.9610 |         -12.7783 |
[32m[20221214 00:24:17 @agent_ppo2.py:185][0m |          -0.0238 |          44.6495 |         -12.7792 |
[32m[20221214 00:24:17 @agent_ppo2.py:185][0m |          -0.0092 |          48.1076 |         -12.7814 |
[32m[20221214 00:24:17 @agent_ppo2.py:185][0m |          -0.0177 |          43.5064 |         -12.8326 |
[32m[20221214 00:24:17 @agent_ppo2.py:185][0m |          -0.0185 |          43.2236 |         -12.9383 |
[32m[20221214 00:24:17 @agent_ppo2.py:185][0m |          -0.0260 |          42.9413 |         -12.9125 |
[32m[20221214 00:24:17 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:24:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 467.41
[32m[20221214 00:24:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 516.86
[32m[20221214 00:24:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 366.12
[32m[20221214 00:24:17 @agent_ppo2.py:143][0m Total time:      26.78 min
[32m[20221214 00:24:17 @agent_ppo2.py:145][0m 2441216 total steps have happened
[32m[20221214 00:24:17 @agent_ppo2.py:121][0m #------------------------ Iteration 5192 --------------------------#
[32m[20221214 00:24:17 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:24:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:24:17 @agent_ppo2.py:185][0m |           0.0155 |          89.0217 |         -13.9858 |
[32m[20221214 00:24:18 @agent_ppo2.py:185][0m |           0.0079 |          79.3537 |         -14.0166 |
[32m[20221214 00:24:18 @agent_ppo2.py:185][0m |          -0.0069 |          76.3016 |         -14.1024 |
[32m[20221214 00:24:18 @agent_ppo2.py:185][0m |          -0.0130 |          73.7165 |         -13.9811 |
[32m[20221214 00:24:18 @agent_ppo2.py:185][0m |          -0.0043 |          71.9657 |         -14.0788 |
[32m[20221214 00:24:18 @agent_ppo2.py:185][0m |          -0.0090 |          70.6950 |         -14.0308 |
[32m[20221214 00:24:18 @agent_ppo2.py:185][0m |          -0.0116 |          70.2468 |         -14.2017 |
[32m[20221214 00:24:18 @agent_ppo2.py:185][0m |          -0.0125 |          69.2186 |         -14.1008 |
[32m[20221214 00:24:18 @agent_ppo2.py:185][0m |          -0.0043 |          69.4354 |         -14.1259 |
[32m[20221214 00:24:18 @agent_ppo2.py:185][0m |          -0.0082 |          70.1363 |         -13.9233 |
[32m[20221214 00:24:18 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:24:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 484.36
[32m[20221214 00:24:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 573.36
[32m[20221214 00:24:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 829.05
[32m[20221214 00:24:18 @agent_ppo2.py:143][0m Total time:      26.80 min
[32m[20221214 00:24:18 @agent_ppo2.py:145][0m 2443264 total steps have happened
[32m[20221214 00:24:18 @agent_ppo2.py:121][0m #------------------------ Iteration 5193 --------------------------#
[32m[20221214 00:24:19 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:24:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:24:19 @agent_ppo2.py:185][0m |           0.0016 |          80.2143 |         -14.1640 |
[32m[20221214 00:24:19 @agent_ppo2.py:185][0m |           0.0018 |          71.4323 |         -14.3240 |
[32m[20221214 00:24:19 @agent_ppo2.py:185][0m |          -0.0043 |          67.5005 |         -14.2385 |
[32m[20221214 00:24:19 @agent_ppo2.py:185][0m |          -0.0039 |          65.9475 |         -14.2489 |
[32m[20221214 00:24:19 @agent_ppo2.py:185][0m |          -0.0050 |          64.4733 |         -14.1974 |
[32m[20221214 00:24:19 @agent_ppo2.py:185][0m |          -0.0154 |          63.6815 |         -14.2492 |
[32m[20221214 00:24:19 @agent_ppo2.py:185][0m |          -0.0114 |          62.9380 |         -14.4108 |
[32m[20221214 00:24:19 @agent_ppo2.py:185][0m |          -0.0171 |          62.1862 |         -14.2211 |
[32m[20221214 00:24:20 @agent_ppo2.py:185][0m |          -0.0095 |          62.3203 |         -14.2749 |
[32m[20221214 00:24:20 @agent_ppo2.py:185][0m |          -0.0133 |          61.8215 |         -14.4655 |
[32m[20221214 00:24:20 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:24:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 508.78
[32m[20221214 00:24:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 605.19
[32m[20221214 00:24:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 548.40
[32m[20221214 00:24:20 @agent_ppo2.py:143][0m Total time:      26.82 min
[32m[20221214 00:24:20 @agent_ppo2.py:145][0m 2445312 total steps have happened
[32m[20221214 00:24:20 @agent_ppo2.py:121][0m #------------------------ Iteration 5194 --------------------------#
[32m[20221214 00:24:20 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:24:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:24:20 @agent_ppo2.py:185][0m |          -0.0001 |          94.5095 |         -14.0089 |
[32m[20221214 00:24:20 @agent_ppo2.py:185][0m |           0.0010 |          91.3420 |         -13.8807 |
[32m[20221214 00:24:20 @agent_ppo2.py:185][0m |          -0.0073 |          84.4583 |         -13.8699 |
[32m[20221214 00:24:20 @agent_ppo2.py:185][0m |          -0.0072 |          82.8815 |         -13.9279 |
[32m[20221214 00:24:21 @agent_ppo2.py:185][0m |          -0.0104 |          82.9117 |         -13.9622 |
[32m[20221214 00:24:21 @agent_ppo2.py:185][0m |          -0.0080 |          82.6545 |         -14.0138 |
[32m[20221214 00:24:21 @agent_ppo2.py:185][0m |          -0.0127 |          81.5836 |         -14.1085 |
[32m[20221214 00:24:21 @agent_ppo2.py:185][0m |          -0.0144 |          79.5425 |         -14.1618 |
[32m[20221214 00:24:21 @agent_ppo2.py:185][0m |          -0.0153 |          79.2832 |         -13.9468 |
[32m[20221214 00:24:21 @agent_ppo2.py:185][0m |          -0.0156 |          79.2875 |         -14.1330 |
[32m[20221214 00:24:21 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:24:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 541.46
[32m[20221214 00:24:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 606.74
[32m[20221214 00:24:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 753.55
[32m[20221214 00:24:21 @agent_ppo2.py:143][0m Total time:      26.85 min
[32m[20221214 00:24:21 @agent_ppo2.py:145][0m 2447360 total steps have happened
[32m[20221214 00:24:21 @agent_ppo2.py:121][0m #------------------------ Iteration 5195 --------------------------#
[32m[20221214 00:24:21 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:24:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:24:22 @agent_ppo2.py:185][0m |           0.0003 |          87.9586 |         -15.5379 |
[32m[20221214 00:24:22 @agent_ppo2.py:185][0m |           0.0004 |          83.6383 |         -15.4561 |
[32m[20221214 00:24:22 @agent_ppo2.py:185][0m |          -0.0061 |          80.8645 |         -15.3598 |
[32m[20221214 00:24:22 @agent_ppo2.py:185][0m |          -0.0056 |          79.6259 |         -15.4055 |
[32m[20221214 00:24:22 @agent_ppo2.py:185][0m |          -0.0108 |          78.7763 |         -15.5864 |
[32m[20221214 00:24:22 @agent_ppo2.py:185][0m |          -0.0065 |          78.0512 |         -15.7096 |
[32m[20221214 00:24:22 @agent_ppo2.py:185][0m |          -0.0100 |          77.2857 |         -15.6241 |
[32m[20221214 00:24:22 @agent_ppo2.py:185][0m |          -0.0100 |          77.3729 |         -15.6978 |
[32m[20221214 00:24:22 @agent_ppo2.py:185][0m |          -0.0048 |          80.7946 |         -15.8889 |
[32m[20221214 00:24:22 @agent_ppo2.py:185][0m |          -0.0112 |          75.9656 |         -15.8728 |
[32m[20221214 00:24:22 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:24:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 547.82
[32m[20221214 00:24:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 603.80
[32m[20221214 00:24:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 547.46
[32m[20221214 00:24:23 @agent_ppo2.py:143][0m Total time:      26.87 min
[32m[20221214 00:24:23 @agent_ppo2.py:145][0m 2449408 total steps have happened
[32m[20221214 00:24:23 @agent_ppo2.py:121][0m #------------------------ Iteration 5196 --------------------------#
[32m[20221214 00:24:23 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:24:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:24:23 @agent_ppo2.py:185][0m |           0.0041 |         109.1664 |         -15.7524 |
[32m[20221214 00:24:23 @agent_ppo2.py:185][0m |          -0.0057 |          98.3321 |         -15.8702 |
[32m[20221214 00:24:23 @agent_ppo2.py:185][0m |          -0.0110 |          93.5674 |         -16.0046 |
[32m[20221214 00:24:23 @agent_ppo2.py:185][0m |          -0.0110 |          91.3939 |         -15.9816 |
[32m[20221214 00:24:23 @agent_ppo2.py:185][0m |          -0.0137 |          89.2737 |         -16.0220 |
[32m[20221214 00:24:23 @agent_ppo2.py:185][0m |          -0.0132 |          88.9909 |         -16.1323 |
[32m[20221214 00:24:23 @agent_ppo2.py:185][0m |          -0.0136 |          87.0028 |         -16.2244 |
[32m[20221214 00:24:24 @agent_ppo2.py:185][0m |          -0.0043 |          91.2235 |         -16.1102 |
[32m[20221214 00:24:24 @agent_ppo2.py:185][0m |          -0.0154 |          85.7421 |         -16.2822 |
[32m[20221214 00:24:24 @agent_ppo2.py:185][0m |          -0.0057 |          89.8995 |         -16.2647 |
[32m[20221214 00:24:24 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:24:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 551.59
[32m[20221214 00:24:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 595.81
[32m[20221214 00:24:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 826.17
[32m[20221214 00:24:24 @agent_ppo2.py:143][0m Total time:      26.89 min
[32m[20221214 00:24:24 @agent_ppo2.py:145][0m 2451456 total steps have happened
[32m[20221214 00:24:24 @agent_ppo2.py:121][0m #------------------------ Iteration 5197 --------------------------#
[32m[20221214 00:24:24 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:24:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:24:24 @agent_ppo2.py:185][0m |          -0.0000 |         110.2025 |         -16.0746 |
[32m[20221214 00:24:24 @agent_ppo2.py:185][0m |          -0.0110 |          96.4982 |         -16.0948 |
[32m[20221214 00:24:24 @agent_ppo2.py:185][0m |          -0.0004 |         100.5112 |         -16.0981 |
[32m[20221214 00:24:25 @agent_ppo2.py:185][0m |          -0.0152 |          92.1020 |         -15.8761 |
[32m[20221214 00:24:25 @agent_ppo2.py:185][0m |          -0.0168 |          91.2964 |         -15.7763 |
[32m[20221214 00:24:25 @agent_ppo2.py:185][0m |          -0.0153 |          90.3474 |         -15.9702 |
[32m[20221214 00:24:25 @agent_ppo2.py:185][0m |          -0.0169 |          90.0261 |         -15.9321 |
[32m[20221214 00:24:25 @agent_ppo2.py:185][0m |          -0.0161 |          89.7634 |         -15.9047 |
[32m[20221214 00:24:25 @agent_ppo2.py:185][0m |          -0.0035 |          98.8206 |         -15.8839 |
[32m[20221214 00:24:25 @agent_ppo2.py:185][0m |          -0.0216 |          91.2941 |         -15.9288 |
[32m[20221214 00:24:25 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:24:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 489.31
[32m[20221214 00:24:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 528.25
[32m[20221214 00:24:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 789.40
[32m[20221214 00:24:25 @agent_ppo2.py:143][0m Total time:      26.92 min
[32m[20221214 00:24:25 @agent_ppo2.py:145][0m 2453504 total steps have happened
[32m[20221214 00:24:25 @agent_ppo2.py:121][0m #------------------------ Iteration 5198 --------------------------#
[32m[20221214 00:24:25 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:24:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:24:26 @agent_ppo2.py:185][0m |           0.0009 |          78.1389 |         -17.9389 |
[32m[20221214 00:24:26 @agent_ppo2.py:185][0m |          -0.0084 |          68.5530 |         -17.9213 |
[32m[20221214 00:24:26 @agent_ppo2.py:185][0m |          -0.0065 |          65.5715 |         -18.0649 |
[32m[20221214 00:24:26 @agent_ppo2.py:185][0m |          -0.0093 |          63.5588 |         -17.9458 |
[32m[20221214 00:24:26 @agent_ppo2.py:185][0m |          -0.0185 |          62.5850 |         -17.9911 |
[32m[20221214 00:24:26 @agent_ppo2.py:185][0m |          -0.0154 |          61.5085 |         -18.1825 |
[32m[20221214 00:24:26 @agent_ppo2.py:185][0m |          -0.0196 |          60.7751 |         -18.4085 |
[32m[20221214 00:24:26 @agent_ppo2.py:185][0m |          -0.0116 |          60.3007 |         -18.1760 |
[32m[20221214 00:24:26 @agent_ppo2.py:185][0m |          -0.0211 |          59.6512 |         -18.1827 |
[32m[20221214 00:24:27 @agent_ppo2.py:185][0m |          -0.0198 |          59.1079 |         -18.3861 |
[32m[20221214 00:24:27 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:24:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 446.49
[32m[20221214 00:24:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 554.83
[32m[20221214 00:24:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 675.71
[32m[20221214 00:24:27 @agent_ppo2.py:143][0m Total time:      26.94 min
[32m[20221214 00:24:27 @agent_ppo2.py:145][0m 2455552 total steps have happened
[32m[20221214 00:24:27 @agent_ppo2.py:121][0m #------------------------ Iteration 5199 --------------------------#
[32m[20221214 00:24:27 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:24:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:24:27 @agent_ppo2.py:185][0m |          -0.0006 |          71.7002 |         -17.3602 |
[32m[20221214 00:24:27 @agent_ppo2.py:185][0m |          -0.0123 |          64.6475 |         -17.3106 |
[32m[20221214 00:24:27 @agent_ppo2.py:185][0m |          -0.0158 |          61.9643 |         -17.3926 |
[32m[20221214 00:24:27 @agent_ppo2.py:185][0m |          -0.0135 |          61.2222 |         -17.2150 |
[32m[20221214 00:24:27 @agent_ppo2.py:185][0m |          -0.0166 |          60.4458 |         -17.1901 |
[32m[20221214 00:24:27 @agent_ppo2.py:185][0m |          -0.0211 |          59.6744 |         -17.0688 |
[32m[20221214 00:24:28 @agent_ppo2.py:185][0m |          -0.0161 |          60.9480 |         -17.1624 |
[32m[20221214 00:24:28 @agent_ppo2.py:185][0m |          -0.0196 |          60.6474 |         -17.1218 |
[32m[20221214 00:24:28 @agent_ppo2.py:185][0m |          -0.0150 |          59.4838 |         -17.1510 |
[32m[20221214 00:24:28 @agent_ppo2.py:185][0m |          -0.0227 |          57.9775 |         -17.0852 |
[32m[20221214 00:24:28 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:24:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 520.18
[32m[20221214 00:24:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 646.30
[32m[20221214 00:24:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 706.68
[32m[20221214 00:24:28 @agent_ppo2.py:143][0m Total time:      26.96 min
[32m[20221214 00:24:28 @agent_ppo2.py:145][0m 2457600 total steps have happened
[32m[20221214 00:24:28 @agent_ppo2.py:121][0m #------------------------ Iteration 5200 --------------------------#
[32m[20221214 00:24:28 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:24:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:24:28 @agent_ppo2.py:185][0m |           0.0026 |         118.3003 |         -17.2639 |
[32m[20221214 00:24:28 @agent_ppo2.py:185][0m |          -0.0059 |         101.4275 |         -17.3086 |
[32m[20221214 00:24:29 @agent_ppo2.py:185][0m |          -0.0060 |          95.3531 |         -17.1639 |
[32m[20221214 00:24:29 @agent_ppo2.py:185][0m |          -0.0086 |          92.3049 |         -17.2801 |
[32m[20221214 00:24:29 @agent_ppo2.py:185][0m |          -0.0103 |          88.9734 |         -17.3728 |
[32m[20221214 00:24:29 @agent_ppo2.py:185][0m |          -0.0130 |          87.3441 |         -17.4036 |
[32m[20221214 00:24:29 @agent_ppo2.py:185][0m |          -0.0147 |          85.1585 |         -17.2135 |
[32m[20221214 00:24:29 @agent_ppo2.py:185][0m |          -0.0138 |          83.5071 |         -17.6324 |
[32m[20221214 00:24:29 @agent_ppo2.py:185][0m |          -0.0154 |          82.2998 |         -17.6995 |
[32m[20221214 00:24:29 @agent_ppo2.py:185][0m |          -0.0176 |          81.2303 |         -17.6985 |
[32m[20221214 00:24:29 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:24:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 632.81
[32m[20221214 00:24:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 734.77
[32m[20221214 00:24:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 439.36
[32m[20221214 00:24:29 @agent_ppo2.py:143][0m Total time:      26.98 min
[32m[20221214 00:24:29 @agent_ppo2.py:145][0m 2459648 total steps have happened
[32m[20221214 00:24:29 @agent_ppo2.py:121][0m #------------------------ Iteration 5201 --------------------------#
[32m[20221214 00:24:30 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:24:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:24:30 @agent_ppo2.py:185][0m |           0.0062 |         100.8117 |         -18.1924 |
[32m[20221214 00:24:30 @agent_ppo2.py:185][0m |          -0.0010 |          89.6547 |         -18.2272 |
[32m[20221214 00:24:30 @agent_ppo2.py:185][0m |          -0.0072 |          82.8792 |         -18.3418 |
[32m[20221214 00:24:30 @agent_ppo2.py:185][0m |          -0.0086 |          80.2364 |         -18.3152 |
[32m[20221214 00:24:30 @agent_ppo2.py:185][0m |          -0.0017 |          84.7373 |         -18.3709 |
[32m[20221214 00:24:30 @agent_ppo2.py:185][0m |          -0.0100 |          76.5058 |         -18.4552 |
[32m[20221214 00:24:30 @agent_ppo2.py:185][0m |          -0.0128 |          75.2659 |         -18.2872 |
[32m[20221214 00:24:30 @agent_ppo2.py:185][0m |          -0.0143 |          74.3177 |         -18.3922 |
[32m[20221214 00:24:30 @agent_ppo2.py:185][0m |          -0.0057 |          74.0179 |         -18.3672 |
[32m[20221214 00:24:31 @agent_ppo2.py:185][0m |          -0.0120 |          72.4480 |         -18.5673 |
[32m[20221214 00:24:31 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:24:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 633.52
[32m[20221214 00:24:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 685.24
[32m[20221214 00:24:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 553.98
[32m[20221214 00:24:31 @agent_ppo2.py:143][0m Total time:      27.01 min
[32m[20221214 00:24:31 @agent_ppo2.py:145][0m 2461696 total steps have happened
[32m[20221214 00:24:31 @agent_ppo2.py:121][0m #------------------------ Iteration 5202 --------------------------#
[32m[20221214 00:24:31 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:24:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:24:31 @agent_ppo2.py:185][0m |           0.0184 |          85.6702 |         -17.7844 |
[32m[20221214 00:24:31 @agent_ppo2.py:185][0m |          -0.0051 |          63.1599 |         -17.5492 |
[32m[20221214 00:24:31 @agent_ppo2.py:185][0m |          -0.0098 |          60.2394 |         -17.3602 |
[32m[20221214 00:24:31 @agent_ppo2.py:185][0m |          -0.0022 |          59.2154 |         -17.3398 |
[32m[20221214 00:24:31 @agent_ppo2.py:185][0m |          -0.0019 |          60.0940 |         -17.4311 |
[32m[20221214 00:24:32 @agent_ppo2.py:185][0m |          -0.0119 |          55.8316 |         -17.2372 |
[32m[20221214 00:24:32 @agent_ppo2.py:185][0m |          -0.0092 |          55.8656 |         -17.2631 |
[32m[20221214 00:24:32 @agent_ppo2.py:185][0m |          -0.0147 |          54.3913 |         -17.1586 |
[32m[20221214 00:24:32 @agent_ppo2.py:185][0m |          -0.0124 |          54.1221 |         -17.0138 |
[32m[20221214 00:24:32 @agent_ppo2.py:185][0m |          -0.0176 |          53.4479 |         -16.9382 |
[32m[20221214 00:24:32 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:24:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 484.55
[32m[20221214 00:24:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 559.38
[32m[20221214 00:24:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 814.92
[32m[20221214 00:24:32 @agent_ppo2.py:143][0m Total time:      27.03 min
[32m[20221214 00:24:32 @agent_ppo2.py:145][0m 2463744 total steps have happened
[32m[20221214 00:24:32 @agent_ppo2.py:121][0m #------------------------ Iteration 5203 --------------------------#
[32m[20221214 00:24:32 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:24:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:24:32 @agent_ppo2.py:185][0m |          -0.0010 |          70.1705 |         -15.5192 |
[32m[20221214 00:24:33 @agent_ppo2.py:185][0m |          -0.0086 |          60.2702 |         -15.3656 |
[32m[20221214 00:24:33 @agent_ppo2.py:185][0m |          -0.0108 |          57.7110 |         -15.3222 |
[32m[20221214 00:24:33 @agent_ppo2.py:185][0m |          -0.0084 |          55.6152 |         -15.3532 |
[32m[20221214 00:24:33 @agent_ppo2.py:185][0m |          -0.0121 |          53.9768 |         -15.0932 |
[32m[20221214 00:24:33 @agent_ppo2.py:185][0m |          -0.0109 |          53.0544 |         -15.2566 |
[32m[20221214 00:24:33 @agent_ppo2.py:185][0m |          -0.0163 |          52.1452 |         -15.1733 |
[32m[20221214 00:24:33 @agent_ppo2.py:185][0m |          -0.0106 |          51.5656 |         -15.0304 |
[32m[20221214 00:24:33 @agent_ppo2.py:185][0m |          -0.0137 |          50.7706 |         -14.9746 |
[32m[20221214 00:24:33 @agent_ppo2.py:185][0m |          -0.0171 |          50.6718 |         -14.9599 |
[32m[20221214 00:24:33 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:24:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 619.38
[32m[20221214 00:24:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 797.50
[32m[20221214 00:24:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 592.18
[32m[20221214 00:24:33 @agent_ppo2.py:143][0m Total time:      27.05 min
[32m[20221214 00:24:33 @agent_ppo2.py:145][0m 2465792 total steps have happened
[32m[20221214 00:24:33 @agent_ppo2.py:121][0m #------------------------ Iteration 5204 --------------------------#
[32m[20221214 00:24:34 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:24:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:24:34 @agent_ppo2.py:185][0m |           0.0135 |         126.0549 |         -16.2503 |
[32m[20221214 00:24:34 @agent_ppo2.py:185][0m |          -0.0027 |         112.3923 |         -16.3415 |
[32m[20221214 00:24:34 @agent_ppo2.py:185][0m |          -0.0057 |         109.3457 |         -16.5645 |
[32m[20221214 00:24:34 @agent_ppo2.py:185][0m |           0.0003 |         113.5109 |         -16.4455 |
[32m[20221214 00:24:34 @agent_ppo2.py:185][0m |          -0.0119 |         105.6306 |         -16.3065 |
[32m[20221214 00:24:34 @agent_ppo2.py:185][0m |          -0.0108 |         104.6542 |         -16.5941 |
[32m[20221214 00:24:34 @agent_ppo2.py:185][0m |          -0.0111 |         103.8740 |         -16.6332 |
[32m[20221214 00:24:34 @agent_ppo2.py:185][0m |          -0.0118 |         103.3760 |         -16.5167 |
[32m[20221214 00:24:35 @agent_ppo2.py:185][0m |          -0.0147 |         102.7497 |         -16.5948 |
[32m[20221214 00:24:35 @agent_ppo2.py:185][0m |          -0.0093 |         102.8335 |         -16.6309 |
[32m[20221214 00:24:35 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:24:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 665.22
[32m[20221214 00:24:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 820.99
[32m[20221214 00:24:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 723.78
[32m[20221214 00:24:35 @agent_ppo2.py:143][0m Total time:      27.07 min
[32m[20221214 00:24:35 @agent_ppo2.py:145][0m 2467840 total steps have happened
[32m[20221214 00:24:35 @agent_ppo2.py:121][0m #------------------------ Iteration 5205 --------------------------#
[32m[20221214 00:24:35 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:24:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:24:35 @agent_ppo2.py:185][0m |           0.0071 |          56.4497 |         -17.5225 |
[32m[20221214 00:24:35 @agent_ppo2.py:185][0m |          -0.0044 |          46.0748 |         -17.2834 |
[32m[20221214 00:24:35 @agent_ppo2.py:185][0m |          -0.0081 |          43.4940 |         -17.1446 |
[32m[20221214 00:24:36 @agent_ppo2.py:185][0m |          -0.0120 |          41.8030 |         -17.1248 |
[32m[20221214 00:24:36 @agent_ppo2.py:185][0m |          -0.0087 |          40.9749 |         -17.0342 |
[32m[20221214 00:24:36 @agent_ppo2.py:185][0m |          -0.0166 |          40.1245 |         -17.0408 |
[32m[20221214 00:24:36 @agent_ppo2.py:185][0m |          -0.0137 |          39.3198 |         -16.9530 |
[32m[20221214 00:24:36 @agent_ppo2.py:185][0m |          -0.0125 |          38.8036 |         -16.8210 |
[32m[20221214 00:24:36 @agent_ppo2.py:185][0m |          -0.0149 |          38.4032 |         -16.8104 |
[32m[20221214 00:24:36 @agent_ppo2.py:185][0m |          -0.0190 |          38.0628 |         -16.7857 |
[32m[20221214 00:24:36 @agent_ppo2.py:130][0m Policy update time: 1.36 s
[32m[20221214 00:24:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 518.76
[32m[20221214 00:24:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 645.93
[32m[20221214 00:24:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 581.60
[32m[20221214 00:24:37 @agent_ppo2.py:143][0m Total time:      27.10 min
[32m[20221214 00:24:37 @agent_ppo2.py:145][0m 2469888 total steps have happened
[32m[20221214 00:24:37 @agent_ppo2.py:121][0m #------------------------ Iteration 5206 --------------------------#
[32m[20221214 00:24:37 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:24:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:24:37 @agent_ppo2.py:185][0m |           0.0024 |          59.7036 |         -17.1713 |
[32m[20221214 00:24:37 @agent_ppo2.py:185][0m |          -0.0045 |          53.6918 |         -17.1231 |
[32m[20221214 00:24:37 @agent_ppo2.py:185][0m |          -0.0058 |          51.2840 |         -17.1878 |
[32m[20221214 00:24:37 @agent_ppo2.py:185][0m |          -0.0030 |          49.8445 |         -17.1758 |
[32m[20221214 00:24:37 @agent_ppo2.py:185][0m |          -0.0076 |          49.1074 |         -17.3035 |
[32m[20221214 00:24:37 @agent_ppo2.py:185][0m |          -0.0091 |          48.7866 |         -17.3666 |
[32m[20221214 00:24:37 @agent_ppo2.py:185][0m |          -0.0120 |          48.0419 |         -17.2535 |
[32m[20221214 00:24:38 @agent_ppo2.py:185][0m |          -0.0142 |          47.3852 |         -17.5118 |
[32m[20221214 00:24:38 @agent_ppo2.py:185][0m |          -0.0112 |          47.0316 |         -17.3764 |
[32m[20221214 00:24:38 @agent_ppo2.py:185][0m |          -0.0085 |          46.6897 |         -17.3805 |
[32m[20221214 00:24:38 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:24:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 444.90
[32m[20221214 00:24:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 650.42
[32m[20221214 00:24:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 713.49
[32m[20221214 00:24:38 @agent_ppo2.py:143][0m Total time:      27.13 min
[32m[20221214 00:24:38 @agent_ppo2.py:145][0m 2471936 total steps have happened
[32m[20221214 00:24:38 @agent_ppo2.py:121][0m #------------------------ Iteration 5207 --------------------------#
[32m[20221214 00:24:38 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:24:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:24:38 @agent_ppo2.py:185][0m |           0.0013 |          51.6876 |         -17.7190 |
[32m[20221214 00:24:38 @agent_ppo2.py:185][0m |          -0.0071 |          46.5877 |         -17.5193 |
[32m[20221214 00:24:38 @agent_ppo2.py:185][0m |          -0.0059 |          44.5685 |         -17.6159 |
[32m[20221214 00:24:39 @agent_ppo2.py:185][0m |          -0.0092 |          42.6997 |         -17.4292 |
[32m[20221214 00:24:39 @agent_ppo2.py:185][0m |          -0.0126 |          41.6637 |         -17.4704 |
[32m[20221214 00:24:39 @agent_ppo2.py:185][0m |          -0.0060 |          41.4498 |         -17.4658 |
[32m[20221214 00:24:39 @agent_ppo2.py:185][0m |          -0.0112 |          39.7272 |         -17.4860 |
[32m[20221214 00:24:39 @agent_ppo2.py:185][0m |          -0.0110 |          39.2171 |         -17.4457 |
[32m[20221214 00:24:39 @agent_ppo2.py:185][0m |          -0.0123 |          38.4834 |         -17.5600 |
[32m[20221214 00:24:39 @agent_ppo2.py:185][0m |          -0.0206 |          37.8415 |         -17.4790 |
[32m[20221214 00:24:39 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:24:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 515.47
[32m[20221214 00:24:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 659.78
[32m[20221214 00:24:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 745.42
[32m[20221214 00:24:39 @agent_ppo2.py:143][0m Total time:      27.15 min
[32m[20221214 00:24:39 @agent_ppo2.py:145][0m 2473984 total steps have happened
[32m[20221214 00:24:39 @agent_ppo2.py:121][0m #------------------------ Iteration 5208 --------------------------#
[32m[20221214 00:24:40 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:24:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:24:40 @agent_ppo2.py:185][0m |          -0.0011 |          82.1424 |         -18.9874 |
[32m[20221214 00:24:40 @agent_ppo2.py:185][0m |           0.0070 |          76.7748 |         -19.0401 |
[32m[20221214 00:24:40 @agent_ppo2.py:185][0m |          -0.0012 |          74.2518 |         -18.9192 |
[32m[20221214 00:24:40 @agent_ppo2.py:185][0m |          -0.0102 |          73.0458 |         -19.0804 |
[32m[20221214 00:24:40 @agent_ppo2.py:185][0m |          -0.0085 |          72.1946 |         -19.0688 |
[32m[20221214 00:24:40 @agent_ppo2.py:185][0m |          -0.0088 |          71.5340 |         -19.1698 |
[32m[20221214 00:24:40 @agent_ppo2.py:185][0m |          -0.0097 |          70.7528 |         -19.1472 |
[32m[20221214 00:24:40 @agent_ppo2.py:185][0m |          -0.0129 |          70.3834 |         -19.3483 |
[32m[20221214 00:24:41 @agent_ppo2.py:185][0m |          -0.0147 |          69.8446 |         -19.1741 |
[32m[20221214 00:24:41 @agent_ppo2.py:185][0m |          -0.0182 |          69.6175 |         -19.2174 |
[32m[20221214 00:24:41 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:24:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 496.07
[32m[20221214 00:24:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 627.51
[32m[20221214 00:24:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 766.36
[32m[20221214 00:24:41 @agent_ppo2.py:143][0m Total time:      27.17 min
[32m[20221214 00:24:41 @agent_ppo2.py:145][0m 2476032 total steps have happened
[32m[20221214 00:24:41 @agent_ppo2.py:121][0m #------------------------ Iteration 5209 --------------------------#
[32m[20221214 00:24:41 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:24:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:24:41 @agent_ppo2.py:185][0m |           0.0062 |          86.4862 |         -19.3326 |
[32m[20221214 00:24:41 @agent_ppo2.py:185][0m |          -0.0034 |          80.3048 |         -18.9233 |
[32m[20221214 00:24:41 @agent_ppo2.py:185][0m |          -0.0062 |          79.3070 |         -19.1694 |
[32m[20221214 00:24:41 @agent_ppo2.py:185][0m |          -0.0054 |          77.4232 |         -19.0322 |
[32m[20221214 00:24:42 @agent_ppo2.py:185][0m |          -0.0119 |          76.6734 |         -19.0430 |
[32m[20221214 00:24:42 @agent_ppo2.py:185][0m |          -0.0119 |          76.4377 |         -19.0885 |
[32m[20221214 00:24:42 @agent_ppo2.py:185][0m |          -0.0118 |          75.3276 |         -18.9791 |
[32m[20221214 00:24:42 @agent_ppo2.py:185][0m |          -0.0134 |          74.6213 |         -19.2117 |
[32m[20221214 00:24:42 @agent_ppo2.py:185][0m |          -0.0128 |          74.2958 |         -19.0742 |
[32m[20221214 00:24:42 @agent_ppo2.py:185][0m |          -0.0087 |          73.5712 |         -18.9815 |
[32m[20221214 00:24:42 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:24:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 598.77
[32m[20221214 00:24:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 654.44
[32m[20221214 00:24:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 603.15
[32m[20221214 00:24:42 @agent_ppo2.py:143][0m Total time:      27.20 min
[32m[20221214 00:24:42 @agent_ppo2.py:145][0m 2478080 total steps have happened
[32m[20221214 00:24:42 @agent_ppo2.py:121][0m #------------------------ Iteration 5210 --------------------------#
[32m[20221214 00:24:42 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:24:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:24:43 @agent_ppo2.py:185][0m |          -0.0005 |          80.8889 |         -18.7798 |
[32m[20221214 00:24:43 @agent_ppo2.py:185][0m |          -0.0076 |          76.8861 |         -18.7966 |
[32m[20221214 00:24:43 @agent_ppo2.py:185][0m |          -0.0061 |          75.1870 |         -18.9634 |
[32m[20221214 00:24:43 @agent_ppo2.py:185][0m |          -0.0081 |          74.2969 |         -18.8277 |
[32m[20221214 00:24:43 @agent_ppo2.py:185][0m |          -0.0096 |          73.6562 |         -19.0686 |
[32m[20221214 00:24:43 @agent_ppo2.py:185][0m |          -0.0093 |          72.9901 |         -18.6615 |
[32m[20221214 00:24:43 @agent_ppo2.py:185][0m |          -0.0117 |          72.3909 |         -18.9826 |
[32m[20221214 00:24:43 @agent_ppo2.py:185][0m |          -0.0118 |          71.9589 |         -18.6935 |
[32m[20221214 00:24:43 @agent_ppo2.py:185][0m |          -0.0142 |          71.4616 |         -19.0583 |
[32m[20221214 00:24:44 @agent_ppo2.py:185][0m |          -0.0079 |          71.2004 |         -19.0179 |
[32m[20221214 00:24:44 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221214 00:24:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 570.34
[32m[20221214 00:24:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 634.58
[32m[20221214 00:24:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 580.18
[32m[20221214 00:24:44 @agent_ppo2.py:143][0m Total time:      27.22 min
[32m[20221214 00:24:44 @agent_ppo2.py:145][0m 2480128 total steps have happened
[32m[20221214 00:24:44 @agent_ppo2.py:121][0m #------------------------ Iteration 5211 --------------------------#
[32m[20221214 00:24:44 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:24:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:24:44 @agent_ppo2.py:185][0m |           0.0002 |         116.6120 |         -18.0146 |
[32m[20221214 00:24:44 @agent_ppo2.py:185][0m |          -0.0045 |         109.0053 |         -17.9663 |
[32m[20221214 00:24:44 @agent_ppo2.py:185][0m |          -0.0061 |         106.5033 |         -18.0742 |
[32m[20221214 00:24:44 @agent_ppo2.py:185][0m |          -0.0041 |         104.0243 |         -17.9086 |
[32m[20221214 00:24:44 @agent_ppo2.py:185][0m |          -0.0093 |         102.8213 |         -18.0183 |
[32m[20221214 00:24:45 @agent_ppo2.py:185][0m |          -0.0103 |         101.6561 |         -18.0702 |
[32m[20221214 00:24:45 @agent_ppo2.py:185][0m |          -0.0050 |         107.5949 |         -18.1266 |
[32m[20221214 00:24:45 @agent_ppo2.py:185][0m |          -0.0108 |         100.4752 |         -18.0613 |
[32m[20221214 00:24:45 @agent_ppo2.py:185][0m |          -0.0125 |          98.7660 |         -18.1094 |
[32m[20221214 00:24:45 @agent_ppo2.py:185][0m |          -0.0123 |          97.7686 |         -18.1244 |
[32m[20221214 00:24:45 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:24:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 592.89
[32m[20221214 00:24:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 648.74
[32m[20221214 00:24:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 743.23
[32m[20221214 00:24:45 @agent_ppo2.py:143][0m Total time:      27.25 min
[32m[20221214 00:24:45 @agent_ppo2.py:145][0m 2482176 total steps have happened
[32m[20221214 00:24:45 @agent_ppo2.py:121][0m #------------------------ Iteration 5212 --------------------------#
[32m[20221214 00:24:45 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:24:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:24:45 @agent_ppo2.py:185][0m |           0.0038 |         105.7849 |         -19.7208 |
[32m[20221214 00:24:46 @agent_ppo2.py:185][0m |          -0.0088 |          98.7870 |         -19.8353 |
[32m[20221214 00:24:46 @agent_ppo2.py:185][0m |          -0.0071 |          96.4632 |         -19.7719 |
[32m[20221214 00:24:46 @agent_ppo2.py:185][0m |          -0.0096 |          94.9943 |         -19.7841 |
[32m[20221214 00:24:46 @agent_ppo2.py:185][0m |          -0.0132 |          93.7208 |         -19.5411 |
[32m[20221214 00:24:46 @agent_ppo2.py:185][0m |          -0.0146 |          92.8499 |         -19.6934 |
[32m[20221214 00:24:46 @agent_ppo2.py:185][0m |          -0.0149 |          92.0468 |         -19.5355 |
[32m[20221214 00:24:46 @agent_ppo2.py:185][0m |          -0.0128 |          91.7584 |         -19.4391 |
[32m[20221214 00:24:46 @agent_ppo2.py:185][0m |          -0.0138 |          90.7567 |         -19.6081 |
[32m[20221214 00:24:46 @agent_ppo2.py:185][0m |          -0.0148 |          90.6241 |         -19.4445 |
[32m[20221214 00:24:46 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:24:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 442.35
[32m[20221214 00:24:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 563.87
[32m[20221214 00:24:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 458.98
[32m[20221214 00:24:47 @agent_ppo2.py:143][0m Total time:      27.27 min
[32m[20221214 00:24:47 @agent_ppo2.py:145][0m 2484224 total steps have happened
[32m[20221214 00:24:47 @agent_ppo2.py:121][0m #------------------------ Iteration 5213 --------------------------#
[32m[20221214 00:24:47 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:24:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:24:47 @agent_ppo2.py:185][0m |           0.0016 |          89.0864 |         -19.6618 |
[32m[20221214 00:24:47 @agent_ppo2.py:185][0m |           0.0133 |          97.6463 |         -19.5488 |
[32m[20221214 00:24:47 @agent_ppo2.py:185][0m |          -0.0038 |          80.4751 |         -19.2900 |
[32m[20221214 00:24:47 @agent_ppo2.py:185][0m |          -0.0125 |          77.9691 |         -19.8788 |
[32m[20221214 00:24:47 @agent_ppo2.py:185][0m |          -0.0105 |          76.6855 |         -19.9953 |
[32m[20221214 00:24:47 @agent_ppo2.py:185][0m |          -0.0139 |          75.7086 |         -19.6931 |
[32m[20221214 00:24:47 @agent_ppo2.py:185][0m |          -0.0121 |          76.2438 |         -20.0426 |
[32m[20221214 00:24:48 @agent_ppo2.py:185][0m |          -0.0117 |          74.1139 |         -20.1205 |
[32m[20221214 00:24:48 @agent_ppo2.py:185][0m |          -0.0159 |          73.4767 |         -20.1761 |
[32m[20221214 00:24:48 @agent_ppo2.py:185][0m |          -0.0164 |          72.9521 |         -20.1156 |
[32m[20221214 00:24:48 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:24:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 526.56
[32m[20221214 00:24:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 594.36
[32m[20221214 00:24:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 660.90
[32m[20221214 00:24:48 @agent_ppo2.py:143][0m Total time:      27.29 min
[32m[20221214 00:24:48 @agent_ppo2.py:145][0m 2486272 total steps have happened
[32m[20221214 00:24:48 @agent_ppo2.py:121][0m #------------------------ Iteration 5214 --------------------------#
[32m[20221214 00:24:48 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:24:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:24:48 @agent_ppo2.py:185][0m |           0.0054 |          86.3540 |         -20.1442 |
[32m[20221214 00:24:48 @agent_ppo2.py:185][0m |          -0.0037 |          70.2582 |         -20.0271 |
[32m[20221214 00:24:48 @agent_ppo2.py:185][0m |          -0.0044 |          66.3961 |         -20.1027 |
[32m[20221214 00:24:49 @agent_ppo2.py:185][0m |          -0.0103 |          64.5909 |         -20.0934 |
[32m[20221214 00:24:49 @agent_ppo2.py:185][0m |          -0.0136 |          63.2366 |         -19.8914 |
[32m[20221214 00:24:49 @agent_ppo2.py:185][0m |           0.0084 |          78.2099 |         -20.0475 |
[32m[20221214 00:24:49 @agent_ppo2.py:185][0m |          -0.0089 |          64.3159 |         -19.9592 |
[32m[20221214 00:24:49 @agent_ppo2.py:185][0m |          -0.0097 |          61.0392 |         -19.8972 |
[32m[20221214 00:24:49 @agent_ppo2.py:185][0m |          -0.0166 |          60.4587 |         -19.9405 |
[32m[20221214 00:24:49 @agent_ppo2.py:185][0m |          -0.0166 |          59.6895 |         -19.9242 |
[32m[20221214 00:24:49 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:24:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 535.64
[32m[20221214 00:24:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 685.29
[32m[20221214 00:24:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 612.15
[32m[20221214 00:24:49 @agent_ppo2.py:143][0m Total time:      27.32 min
[32m[20221214 00:24:49 @agent_ppo2.py:145][0m 2488320 total steps have happened
[32m[20221214 00:24:49 @agent_ppo2.py:121][0m #------------------------ Iteration 5215 --------------------------#
[32m[20221214 00:24:50 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:24:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:24:50 @agent_ppo2.py:185][0m |           0.0009 |         133.1334 |         -22.4871 |
[32m[20221214 00:24:50 @agent_ppo2.py:185][0m |          -0.0044 |         122.7068 |         -22.3085 |
[32m[20221214 00:24:50 @agent_ppo2.py:185][0m |          -0.0063 |         118.7734 |         -22.1240 |
[32m[20221214 00:24:50 @agent_ppo2.py:185][0m |          -0.0018 |         118.2773 |         -22.1471 |
[32m[20221214 00:24:50 @agent_ppo2.py:185][0m |          -0.0092 |         115.0160 |         -22.0143 |
[32m[20221214 00:24:50 @agent_ppo2.py:185][0m |          -0.0114 |         112.9725 |         -22.0346 |
[32m[20221214 00:24:50 @agent_ppo2.py:185][0m |          -0.0058 |         112.3851 |         -22.1539 |
[32m[20221214 00:24:50 @agent_ppo2.py:185][0m |           0.0023 |         120.0420 |         -22.0356 |
[32m[20221214 00:24:51 @agent_ppo2.py:185][0m |           0.0006 |         123.2674 |         -22.0738 |
[32m[20221214 00:24:51 @agent_ppo2.py:185][0m |          -0.0053 |         114.9336 |         -21.9992 |
[32m[20221214 00:24:51 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 00:24:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 575.19
[32m[20221214 00:24:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 741.41
[32m[20221214 00:24:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 522.16
[32m[20221214 00:24:51 @agent_ppo2.py:143][0m Total time:      27.34 min
[32m[20221214 00:24:51 @agent_ppo2.py:145][0m 2490368 total steps have happened
[32m[20221214 00:24:51 @agent_ppo2.py:121][0m #------------------------ Iteration 5216 --------------------------#
[32m[20221214 00:24:51 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:24:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:24:51 @agent_ppo2.py:185][0m |           0.0035 |         104.5245 |         -17.8841 |
[32m[20221214 00:24:51 @agent_ppo2.py:185][0m |           0.0026 |          99.7558 |         -18.0046 |
[32m[20221214 00:24:51 @agent_ppo2.py:185][0m |           0.0013 |          97.2150 |         -17.9967 |
[32m[20221214 00:24:51 @agent_ppo2.py:185][0m |          -0.0001 |          95.5055 |         -17.8606 |
[32m[20221214 00:24:52 @agent_ppo2.py:185][0m |          -0.0010 |          93.7368 |         -17.9343 |
[32m[20221214 00:24:52 @agent_ppo2.py:185][0m |          -0.0064 |          92.1323 |         -17.7940 |
[32m[20221214 00:24:52 @agent_ppo2.py:185][0m |          -0.0054 |          90.7741 |         -18.0421 |
[32m[20221214 00:24:52 @agent_ppo2.py:185][0m |          -0.0030 |          91.1766 |         -18.0077 |
[32m[20221214 00:24:52 @agent_ppo2.py:185][0m |          -0.0053 |          88.8292 |         -18.0566 |
[32m[20221214 00:24:52 @agent_ppo2.py:185][0m |          -0.0051 |          87.6429 |         -18.1191 |
[32m[20221214 00:24:52 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:24:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 660.96
[32m[20221214 00:24:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 770.93
[32m[20221214 00:24:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 730.27
[32m[20221214 00:24:52 @agent_ppo2.py:143][0m Total time:      27.36 min
[32m[20221214 00:24:52 @agent_ppo2.py:145][0m 2492416 total steps have happened
[32m[20221214 00:24:52 @agent_ppo2.py:121][0m #------------------------ Iteration 5217 --------------------------#
[32m[20221214 00:24:52 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:24:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:24:53 @agent_ppo2.py:185][0m |           0.0010 |         133.3456 |         -20.7429 |
[32m[20221214 00:24:53 @agent_ppo2.py:185][0m |          -0.0066 |         126.0042 |         -20.9615 |
[32m[20221214 00:24:53 @agent_ppo2.py:185][0m |          -0.0025 |         123.4640 |         -20.6652 |
[32m[20221214 00:24:53 @agent_ppo2.py:185][0m |           0.0060 |         133.6598 |         -20.7004 |
[32m[20221214 00:24:53 @agent_ppo2.py:185][0m |          -0.0101 |         119.8784 |         -20.5618 |
[32m[20221214 00:24:53 @agent_ppo2.py:185][0m |          -0.0109 |         118.2891 |         -20.8759 |
[32m[20221214 00:24:53 @agent_ppo2.py:185][0m |          -0.0091 |         117.4995 |         -20.5066 |
[32m[20221214 00:24:53 @agent_ppo2.py:185][0m |          -0.0035 |         118.1370 |         -20.4694 |
[32m[20221214 00:24:53 @agent_ppo2.py:185][0m |          -0.0089 |         116.8013 |         -20.3783 |
[32m[20221214 00:24:53 @agent_ppo2.py:185][0m |          -0.0105 |         115.6229 |         -20.5159 |
[32m[20221214 00:24:53 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:24:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 586.32
[32m[20221214 00:24:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 696.53
[32m[20221214 00:24:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 764.58
[32m[20221214 00:24:54 @agent_ppo2.py:143][0m Total time:      27.39 min
[32m[20221214 00:24:54 @agent_ppo2.py:145][0m 2494464 total steps have happened
[32m[20221214 00:24:54 @agent_ppo2.py:121][0m #------------------------ Iteration 5218 --------------------------#
[32m[20221214 00:24:54 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:24:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:24:54 @agent_ppo2.py:185][0m |           0.0003 |          77.8320 |         -19.8280 |
[32m[20221214 00:24:54 @agent_ppo2.py:185][0m |          -0.0063 |          62.9879 |         -19.7196 |
[32m[20221214 00:24:54 @agent_ppo2.py:185][0m |           0.0047 |          63.7281 |         -19.6265 |
[32m[20221214 00:24:54 @agent_ppo2.py:185][0m |          -0.0051 |          62.0657 |         -19.7220 |
[32m[20221214 00:24:54 @agent_ppo2.py:185][0m |          -0.0078 |          58.9256 |         -19.5915 |
[32m[20221214 00:24:54 @agent_ppo2.py:185][0m |          -0.0052 |          57.9236 |         -19.6698 |
[32m[20221214 00:24:55 @agent_ppo2.py:185][0m |          -0.0100 |          57.4686 |         -19.5708 |
[32m[20221214 00:24:55 @agent_ppo2.py:185][0m |          -0.0086 |          56.4192 |         -19.5250 |
[32m[20221214 00:24:55 @agent_ppo2.py:185][0m |          -0.0120 |          56.7569 |         -19.7252 |
[32m[20221214 00:24:55 @agent_ppo2.py:185][0m |          -0.0146 |          56.2722 |         -19.5110 |
[32m[20221214 00:24:55 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:24:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 492.47
[32m[20221214 00:24:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 709.10
[32m[20221214 00:24:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 790.13
[32m[20221214 00:24:55 @agent_ppo2.py:143][0m Total time:      27.41 min
[32m[20221214 00:24:55 @agent_ppo2.py:145][0m 2496512 total steps have happened
[32m[20221214 00:24:55 @agent_ppo2.py:121][0m #------------------------ Iteration 5219 --------------------------#
[32m[20221214 00:24:55 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:24:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:24:55 @agent_ppo2.py:185][0m |          -0.0024 |          57.8417 |         -20.1597 |
[32m[20221214 00:24:55 @agent_ppo2.py:185][0m |          -0.0019 |          53.8684 |         -20.1347 |
[32m[20221214 00:24:55 @agent_ppo2.py:185][0m |          -0.0053 |          51.7650 |         -20.1535 |
[32m[20221214 00:24:56 @agent_ppo2.py:185][0m |          -0.0071 |          50.5467 |         -20.1042 |
[32m[20221214 00:24:56 @agent_ppo2.py:185][0m |          -0.0114 |          50.1080 |         -20.2295 |
[32m[20221214 00:24:56 @agent_ppo2.py:185][0m |          -0.0138 |          49.8271 |         -20.0492 |
[32m[20221214 00:24:56 @agent_ppo2.py:185][0m |          -0.0092 |          49.1072 |         -20.2141 |
[32m[20221214 00:24:56 @agent_ppo2.py:185][0m |          -0.0096 |          48.7163 |         -20.0969 |
[32m[20221214 00:24:56 @agent_ppo2.py:185][0m |          -0.0144 |          48.1703 |         -20.1739 |
[32m[20221214 00:24:56 @agent_ppo2.py:185][0m |          -0.0077 |          48.9080 |         -20.0704 |
[32m[20221214 00:24:56 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:24:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 628.69
[32m[20221214 00:24:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 693.99
[32m[20221214 00:24:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 678.47
[32m[20221214 00:24:56 @agent_ppo2.py:143][0m Total time:      27.43 min
[32m[20221214 00:24:56 @agent_ppo2.py:145][0m 2498560 total steps have happened
[32m[20221214 00:24:56 @agent_ppo2.py:121][0m #------------------------ Iteration 5220 --------------------------#
[32m[20221214 00:24:57 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:24:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:24:57 @agent_ppo2.py:185][0m |           0.0038 |          89.5877 |         -19.0469 |
[32m[20221214 00:24:57 @agent_ppo2.py:185][0m |          -0.0059 |          83.6608 |         -18.8684 |
[32m[20221214 00:24:57 @agent_ppo2.py:185][0m |          -0.0072 |          81.3048 |         -18.8179 |
[32m[20221214 00:24:57 @agent_ppo2.py:185][0m |          -0.0079 |          80.0087 |         -18.8710 |
[32m[20221214 00:24:57 @agent_ppo2.py:185][0m |          -0.0010 |          85.4109 |         -18.6766 |
[32m[20221214 00:24:57 @agent_ppo2.py:185][0m |          -0.0085 |          81.4482 |         -18.7385 |
[32m[20221214 00:24:57 @agent_ppo2.py:185][0m |          -0.0028 |          77.6294 |         -18.6244 |
[32m[20221214 00:24:57 @agent_ppo2.py:185][0m |          -0.0169 |          77.1175 |         -18.3895 |
[32m[20221214 00:24:57 @agent_ppo2.py:185][0m |          -0.0149 |          76.0242 |         -18.5293 |
[32m[20221214 00:24:58 @agent_ppo2.py:185][0m |          -0.0154 |          76.4852 |         -18.5882 |
[32m[20221214 00:24:58 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:24:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 620.38
[32m[20221214 00:24:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 723.99
[32m[20221214 00:24:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 580.17
[32m[20221214 00:24:58 @agent_ppo2.py:143][0m Total time:      27.46 min
[32m[20221214 00:24:58 @agent_ppo2.py:145][0m 2500608 total steps have happened
[32m[20221214 00:24:58 @agent_ppo2.py:121][0m #------------------------ Iteration 5221 --------------------------#
[32m[20221214 00:24:58 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:24:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:24:58 @agent_ppo2.py:185][0m |           0.0030 |         102.2527 |         -20.2926 |
[32m[20221214 00:24:58 @agent_ppo2.py:185][0m |          -0.0068 |          93.6232 |         -20.2794 |
[32m[20221214 00:24:58 @agent_ppo2.py:185][0m |          -0.0076 |          91.0382 |         -20.1417 |
[32m[20221214 00:24:58 @agent_ppo2.py:185][0m |          -0.0098 |          88.9798 |         -20.0923 |
[32m[20221214 00:24:58 @agent_ppo2.py:185][0m |          -0.0146 |          87.7734 |         -19.8734 |
[32m[20221214 00:24:59 @agent_ppo2.py:185][0m |          -0.0155 |          86.4338 |         -20.0012 |
[32m[20221214 00:24:59 @agent_ppo2.py:185][0m |          -0.0148 |          85.5175 |         -19.8605 |
[32m[20221214 00:24:59 @agent_ppo2.py:185][0m |          -0.0159 |          84.4182 |         -19.7102 |
[32m[20221214 00:24:59 @agent_ppo2.py:185][0m |          -0.0203 |          83.9303 |         -19.7048 |
[32m[20221214 00:24:59 @agent_ppo2.py:185][0m |          -0.0113 |          83.0207 |         -19.6613 |
[32m[20221214 00:24:59 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:24:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 539.81
[32m[20221214 00:24:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 599.00
[32m[20221214 00:24:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 496.34
[32m[20221214 00:24:59 @agent_ppo2.py:143][0m Total time:      27.48 min
[32m[20221214 00:24:59 @agent_ppo2.py:145][0m 2502656 total steps have happened
[32m[20221214 00:24:59 @agent_ppo2.py:121][0m #------------------------ Iteration 5222 --------------------------#
[32m[20221214 00:24:59 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:24:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:24:59 @agent_ppo2.py:185][0m |           0.0011 |          97.9103 |         -19.1345 |
[32m[20221214 00:25:00 @agent_ppo2.py:185][0m |          -0.0063 |          89.9009 |         -19.1165 |
[32m[20221214 00:25:00 @agent_ppo2.py:185][0m |          -0.0084 |          86.8859 |         -19.1359 |
[32m[20221214 00:25:00 @agent_ppo2.py:185][0m |           0.0131 |         101.7595 |         -18.9287 |
[32m[20221214 00:25:00 @agent_ppo2.py:185][0m |          -0.0142 |          84.2885 |         -18.8797 |
[32m[20221214 00:25:00 @agent_ppo2.py:185][0m |          -0.0027 |          92.7733 |         -18.9805 |
[32m[20221214 00:25:00 @agent_ppo2.py:185][0m |          -0.0157 |          81.7158 |         -18.8354 |
[32m[20221214 00:25:00 @agent_ppo2.py:185][0m |          -0.0192 |          80.9689 |         -19.1194 |
[32m[20221214 00:25:00 @agent_ppo2.py:185][0m |          -0.0177 |          80.1990 |         -19.1660 |
[32m[20221214 00:25:00 @agent_ppo2.py:185][0m |          -0.0187 |          79.7265 |         -19.2422 |
[32m[20221214 00:25:00 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:25:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 380.44
[32m[20221214 00:25:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 594.97
[32m[20221214 00:25:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 540.91
[32m[20221214 00:25:00 @agent_ppo2.py:143][0m Total time:      27.50 min
[32m[20221214 00:25:00 @agent_ppo2.py:145][0m 2504704 total steps have happened
[32m[20221214 00:25:00 @agent_ppo2.py:121][0m #------------------------ Iteration 5223 --------------------------#
[32m[20221214 00:25:01 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:25:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:25:01 @agent_ppo2.py:185][0m |           0.0153 |          96.6762 |         -20.0189 |
[32m[20221214 00:25:01 @agent_ppo2.py:185][0m |          -0.0031 |          79.0577 |         -20.2863 |
[32m[20221214 00:25:01 @agent_ppo2.py:185][0m |          -0.0016 |          76.8031 |         -20.4472 |
[32m[20221214 00:25:01 @agent_ppo2.py:185][0m |          -0.0084 |          75.2074 |         -20.4182 |
[32m[20221214 00:25:01 @agent_ppo2.py:185][0m |          -0.0119 |          71.5889 |         -20.6746 |
[32m[20221214 00:25:01 @agent_ppo2.py:185][0m |          -0.0101 |          70.2925 |         -20.9675 |
[32m[20221214 00:25:01 @agent_ppo2.py:185][0m |          -0.0120 |          69.4658 |         -20.9247 |
[32m[20221214 00:25:01 @agent_ppo2.py:185][0m |          -0.0128 |          69.6309 |         -20.7286 |
[32m[20221214 00:25:02 @agent_ppo2.py:185][0m |          -0.0163 |          67.4336 |         -20.8285 |
[32m[20221214 00:25:02 @agent_ppo2.py:185][0m |          -0.0162 |          67.1472 |         -21.2403 |
[32m[20221214 00:25:02 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:25:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 462.99
[32m[20221214 00:25:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 568.94
[32m[20221214 00:25:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 701.53
[32m[20221214 00:25:02 @agent_ppo2.py:143][0m Total time:      27.52 min
[32m[20221214 00:25:02 @agent_ppo2.py:145][0m 2506752 total steps have happened
[32m[20221214 00:25:02 @agent_ppo2.py:121][0m #------------------------ Iteration 5224 --------------------------#
[32m[20221214 00:25:02 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:25:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:25:02 @agent_ppo2.py:185][0m |           0.0012 |          78.6277 |         -19.5305 |
[32m[20221214 00:25:02 @agent_ppo2.py:185][0m |          -0.0009 |          71.7574 |         -19.2286 |
[32m[20221214 00:25:02 @agent_ppo2.py:185][0m |          -0.0083 |          68.0159 |         -19.3153 |
[32m[20221214 00:25:02 @agent_ppo2.py:185][0m |          -0.0110 |          65.3342 |         -19.3621 |
[32m[20221214 00:25:03 @agent_ppo2.py:185][0m |          -0.0079 |          64.1343 |         -19.2016 |
[32m[20221214 00:25:03 @agent_ppo2.py:185][0m |          -0.0142 |          62.7690 |         -19.2835 |
[32m[20221214 00:25:03 @agent_ppo2.py:185][0m |          -0.0133 |          62.7769 |         -19.2440 |
[32m[20221214 00:25:03 @agent_ppo2.py:185][0m |          -0.0157 |          63.0346 |         -19.2251 |
[32m[20221214 00:25:03 @agent_ppo2.py:185][0m |          -0.0152 |          61.2380 |         -19.1750 |
[32m[20221214 00:25:03 @agent_ppo2.py:185][0m |          -0.0170 |          60.2801 |         -19.0230 |
[32m[20221214 00:25:03 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:25:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 535.02
[32m[20221214 00:25:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 610.43
[32m[20221214 00:25:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 475.43
[32m[20221214 00:25:03 @agent_ppo2.py:143][0m Total time:      27.55 min
[32m[20221214 00:25:03 @agent_ppo2.py:145][0m 2508800 total steps have happened
[32m[20221214 00:25:03 @agent_ppo2.py:121][0m #------------------------ Iteration 5225 --------------------------#
[32m[20221214 00:25:03 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:25:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:25:03 @agent_ppo2.py:185][0m |          -0.0018 |         108.2287 |         -20.0933 |
[32m[20221214 00:25:04 @agent_ppo2.py:185][0m |          -0.0051 |          92.8079 |         -19.7539 |
[32m[20221214 00:25:04 @agent_ppo2.py:185][0m |          -0.0043 |          89.7127 |         -19.8347 |
[32m[20221214 00:25:04 @agent_ppo2.py:185][0m |          -0.0055 |          87.2411 |         -19.6959 |
[32m[20221214 00:25:04 @agent_ppo2.py:185][0m |          -0.0061 |          85.7023 |         -19.7204 |
[32m[20221214 00:25:04 @agent_ppo2.py:185][0m |          -0.0137 |          85.0395 |         -19.6611 |
[32m[20221214 00:25:04 @agent_ppo2.py:185][0m |          -0.0051 |          83.8294 |         -19.7106 |
[32m[20221214 00:25:04 @agent_ppo2.py:185][0m |          -0.0112 |          83.0280 |         -19.5715 |
[32m[20221214 00:25:04 @agent_ppo2.py:185][0m |          -0.0130 |          82.4808 |         -19.5634 |
[32m[20221214 00:25:04 @agent_ppo2.py:185][0m |          -0.0121 |          81.8979 |         -19.3554 |
[32m[20221214 00:25:04 @agent_ppo2.py:130][0m Policy update time: 1.17 s
[32m[20221214 00:25:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 611.16
[32m[20221214 00:25:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 662.91
[32m[20221214 00:25:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 622.17
[32m[20221214 00:25:05 @agent_ppo2.py:143][0m Total time:      27.57 min
[32m[20221214 00:25:05 @agent_ppo2.py:145][0m 2510848 total steps have happened
[32m[20221214 00:25:05 @agent_ppo2.py:121][0m #------------------------ Iteration 5226 --------------------------#
[32m[20221214 00:25:05 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:25:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:25:05 @agent_ppo2.py:185][0m |           0.0043 |          58.5194 |         -20.8384 |
[32m[20221214 00:25:05 @agent_ppo2.py:185][0m |          -0.0106 |          43.5075 |         -20.9821 |
[32m[20221214 00:25:05 @agent_ppo2.py:185][0m |          -0.0117 |          39.4757 |         -20.8547 |
[32m[20221214 00:25:05 @agent_ppo2.py:185][0m |          -0.0253 |          36.7299 |         -21.0560 |
[32m[20221214 00:25:05 @agent_ppo2.py:185][0m |          -0.0179 |          35.1757 |         -20.9058 |
[32m[20221214 00:25:06 @agent_ppo2.py:185][0m |          -0.0206 |          33.5819 |         -20.8667 |
[32m[20221214 00:25:06 @agent_ppo2.py:185][0m |          -0.0216 |          32.9371 |         -20.9702 |
[32m[20221214 00:25:06 @agent_ppo2.py:185][0m |          -0.0282 |          32.8288 |         -20.8141 |
[32m[20221214 00:25:06 @agent_ppo2.py:185][0m |          -0.0254 |          31.0041 |         -21.0101 |
[32m[20221214 00:25:06 @agent_ppo2.py:185][0m |          -0.0233 |          30.8491 |         -20.9221 |
[32m[20221214 00:25:06 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221214 00:25:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 404.85
[32m[20221214 00:25:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 666.22
[32m[20221214 00:25:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 711.84
[32m[20221214 00:25:06 @agent_ppo2.py:143][0m Total time:      27.60 min
[32m[20221214 00:25:06 @agent_ppo2.py:145][0m 2512896 total steps have happened
[32m[20221214 00:25:06 @agent_ppo2.py:121][0m #------------------------ Iteration 5227 --------------------------#
[32m[20221214 00:25:06 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221214 00:25:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:25:07 @agent_ppo2.py:185][0m |           0.0018 |          93.4938 |         -19.1892 |
[32m[20221214 00:25:07 @agent_ppo2.py:185][0m |          -0.0090 |          83.6573 |         -19.0401 |
[32m[20221214 00:25:07 @agent_ppo2.py:185][0m |           0.0012 |          81.6224 |         -18.9330 |
[32m[20221214 00:25:07 @agent_ppo2.py:185][0m |          -0.0116 |          81.2366 |         -18.8057 |
[32m[20221214 00:25:07 @agent_ppo2.py:185][0m |          -0.0135 |          77.8112 |         -18.8468 |
[32m[20221214 00:25:07 @agent_ppo2.py:185][0m |          -0.0108 |          76.8061 |         -18.6429 |
[32m[20221214 00:25:07 @agent_ppo2.py:185][0m |          -0.0186 |          75.8305 |         -18.7046 |
[32m[20221214 00:25:07 @agent_ppo2.py:185][0m |          -0.0153 |          76.0339 |         -18.5469 |
[32m[20221214 00:25:07 @agent_ppo2.py:185][0m |          -0.0068 |          84.0199 |         -18.6275 |
[32m[20221214 00:25:07 @agent_ppo2.py:185][0m |          -0.0204 |          75.8205 |         -18.3344 |
[32m[20221214 00:25:07 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221214 00:25:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 490.34
[32m[20221214 00:25:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 743.53
[32m[20221214 00:25:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 700.81
[32m[20221214 00:25:08 @agent_ppo2.py:143][0m Total time:      27.62 min
[32m[20221214 00:25:08 @agent_ppo2.py:145][0m 2514944 total steps have happened
[32m[20221214 00:25:08 @agent_ppo2.py:121][0m #------------------------ Iteration 5228 --------------------------#
[32m[20221214 00:25:08 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:25:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:25:08 @agent_ppo2.py:185][0m |           0.0044 |          92.2980 |         -20.1901 |
[32m[20221214 00:25:08 @agent_ppo2.py:185][0m |          -0.0045 |          84.7034 |         -20.0511 |
[32m[20221214 00:25:08 @agent_ppo2.py:185][0m |          -0.0045 |          81.2733 |         -19.9487 |
[32m[20221214 00:25:08 @agent_ppo2.py:185][0m |          -0.0009 |          85.7309 |         -20.1066 |
[32m[20221214 00:25:08 @agent_ppo2.py:185][0m |          -0.0089 |          78.0169 |         -20.0757 |
[32m[20221214 00:25:08 @agent_ppo2.py:185][0m |          -0.0089 |          77.2283 |         -20.1134 |
[32m[20221214 00:25:09 @agent_ppo2.py:185][0m |          -0.0047 |          76.6402 |         -20.3700 |
[32m[20221214 00:25:09 @agent_ppo2.py:185][0m |          -0.0005 |          81.7615 |         -20.2400 |
[32m[20221214 00:25:09 @agent_ppo2.py:185][0m |          -0.0117 |          74.2184 |         -20.2819 |
[32m[20221214 00:25:09 @agent_ppo2.py:185][0m |          -0.0128 |          74.7307 |         -20.2551 |
[32m[20221214 00:25:09 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:25:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 540.32
[32m[20221214 00:25:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 695.61
[32m[20221214 00:25:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 512.62
[32m[20221214 00:25:09 @agent_ppo2.py:143][0m Total time:      27.64 min
[32m[20221214 00:25:09 @agent_ppo2.py:145][0m 2516992 total steps have happened
[32m[20221214 00:25:09 @agent_ppo2.py:121][0m #------------------------ Iteration 5229 --------------------------#
[32m[20221214 00:25:09 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:25:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:25:09 @agent_ppo2.py:185][0m |          -0.0020 |          94.8229 |         -20.0148 |
[32m[20221214 00:25:09 @agent_ppo2.py:185][0m |          -0.0069 |          87.4678 |         -19.9763 |
[32m[20221214 00:25:10 @agent_ppo2.py:185][0m |          -0.0068 |          85.0782 |         -20.2933 |
[32m[20221214 00:25:10 @agent_ppo2.py:185][0m |          -0.0086 |          83.5763 |         -19.9963 |
[32m[20221214 00:25:10 @agent_ppo2.py:185][0m |          -0.0078 |          82.7032 |         -19.9830 |
[32m[20221214 00:25:10 @agent_ppo2.py:185][0m |          -0.0058 |          82.3133 |         -20.0400 |
[32m[20221214 00:25:10 @agent_ppo2.py:185][0m |          -0.0091 |          81.1680 |         -20.0084 |
[32m[20221214 00:25:10 @agent_ppo2.py:185][0m |          -0.0039 |          84.5647 |         -20.0359 |
[32m[20221214 00:25:10 @agent_ppo2.py:185][0m |          -0.0099 |          80.4227 |         -20.0457 |
[32m[20221214 00:25:10 @agent_ppo2.py:185][0m |          -0.0110 |          80.6430 |         -20.2437 |
[32m[20221214 00:25:10 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:25:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 655.46
[32m[20221214 00:25:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 725.21
[32m[20221214 00:25:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 532.26
[32m[20221214 00:25:10 @agent_ppo2.py:143][0m Total time:      27.67 min
[32m[20221214 00:25:10 @agent_ppo2.py:145][0m 2519040 total steps have happened
[32m[20221214 00:25:10 @agent_ppo2.py:121][0m #------------------------ Iteration 5230 --------------------------#
[32m[20221214 00:25:11 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:25:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:25:11 @agent_ppo2.py:185][0m |           0.0076 |         118.1874 |         -21.0486 |
[32m[20221214 00:25:11 @agent_ppo2.py:185][0m |          -0.0043 |         107.8100 |         -21.0004 |
[32m[20221214 00:25:11 @agent_ppo2.py:185][0m |           0.0045 |         104.7074 |         -20.8614 |
[32m[20221214 00:25:11 @agent_ppo2.py:185][0m |          -0.0088 |          99.5804 |         -20.9572 |
[32m[20221214 00:25:11 @agent_ppo2.py:185][0m |          -0.0110 |          97.6917 |         -20.9972 |
[32m[20221214 00:25:11 @agent_ppo2.py:185][0m |          -0.0083 |          96.2363 |         -20.8459 |
[32m[20221214 00:25:11 @agent_ppo2.py:185][0m |          -0.0046 |          96.6180 |         -20.7057 |
[32m[20221214 00:25:11 @agent_ppo2.py:185][0m |          -0.0110 |          95.1292 |         -20.9177 |
[32m[20221214 00:25:11 @agent_ppo2.py:185][0m |          -0.0112 |          93.7526 |         -20.8583 |
[32m[20221214 00:25:12 @agent_ppo2.py:185][0m |          -0.0125 |          93.0073 |         -20.8315 |
[32m[20221214 00:25:12 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:25:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 648.86
[32m[20221214 00:25:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 705.67
[32m[20221214 00:25:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 621.12
[32m[20221214 00:25:12 @agent_ppo2.py:143][0m Total time:      27.69 min
[32m[20221214 00:25:12 @agent_ppo2.py:145][0m 2521088 total steps have happened
[32m[20221214 00:25:12 @agent_ppo2.py:121][0m #------------------------ Iteration 5231 --------------------------#
[32m[20221214 00:25:12 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:25:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:25:12 @agent_ppo2.py:185][0m |           0.0032 |          55.8695 |         -20.3374 |
[32m[20221214 00:25:12 @agent_ppo2.py:185][0m |          -0.0042 |          45.1924 |         -20.4343 |
[32m[20221214 00:25:12 @agent_ppo2.py:185][0m |          -0.0103 |          43.3569 |         -20.2537 |
[32m[20221214 00:25:12 @agent_ppo2.py:185][0m |          -0.0072 |          43.3329 |         -20.3207 |
[32m[20221214 00:25:12 @agent_ppo2.py:185][0m |          -0.0045 |          41.3659 |         -20.2861 |
[32m[20221214 00:25:13 @agent_ppo2.py:185][0m |          -0.0108 |          41.5120 |         -20.5484 |
[32m[20221214 00:25:13 @agent_ppo2.py:185][0m |          -0.0132 |          40.2987 |         -20.3549 |
[32m[20221214 00:25:13 @agent_ppo2.py:185][0m |          -0.0045 |          40.2666 |         -20.2749 |
[32m[20221214 00:25:13 @agent_ppo2.py:185][0m |          -0.0098 |          40.3794 |         -20.6386 |
[32m[20221214 00:25:13 @agent_ppo2.py:185][0m |          -0.0117 |          40.7699 |         -20.5689 |
[32m[20221214 00:25:13 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:25:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 594.51
[32m[20221214 00:25:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 758.37
[32m[20221214 00:25:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 622.99
[32m[20221214 00:25:13 @agent_ppo2.py:143][0m Total time:      27.71 min
[32m[20221214 00:25:13 @agent_ppo2.py:145][0m 2523136 total steps have happened
[32m[20221214 00:25:13 @agent_ppo2.py:121][0m #------------------------ Iteration 5232 --------------------------#
[32m[20221214 00:25:13 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:25:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:25:13 @agent_ppo2.py:185][0m |           0.0003 |         106.8266 |         -22.6117 |
[32m[20221214 00:25:14 @agent_ppo2.py:185][0m |          -0.0059 |          99.9475 |         -22.5149 |
[32m[20221214 00:25:14 @agent_ppo2.py:185][0m |          -0.0061 |          97.3167 |         -22.2411 |
[32m[20221214 00:25:14 @agent_ppo2.py:185][0m |          -0.0127 |          95.5150 |         -22.5827 |
[32m[20221214 00:25:14 @agent_ppo2.py:185][0m |          -0.0095 |          94.3982 |         -22.5679 |
[32m[20221214 00:25:14 @agent_ppo2.py:185][0m |          -0.0139 |          93.0281 |         -22.3476 |
[32m[20221214 00:25:14 @agent_ppo2.py:185][0m |          -0.0099 |          91.9284 |         -22.3421 |
[32m[20221214 00:25:14 @agent_ppo2.py:185][0m |          -0.0050 |          95.4875 |         -22.2882 |
[32m[20221214 00:25:14 @agent_ppo2.py:185][0m |          -0.0100 |          90.4929 |         -22.2830 |
[32m[20221214 00:25:14 @agent_ppo2.py:185][0m |          -0.0102 |          89.8974 |         -22.3077 |
[32m[20221214 00:25:14 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:25:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 589.13
[32m[20221214 00:25:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 631.59
[32m[20221214 00:25:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 594.82
[32m[20221214 00:25:14 @agent_ppo2.py:143][0m Total time:      27.74 min
[32m[20221214 00:25:14 @agent_ppo2.py:145][0m 2525184 total steps have happened
[32m[20221214 00:25:14 @agent_ppo2.py:121][0m #------------------------ Iteration 5233 --------------------------#
[32m[20221214 00:25:15 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:25:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:25:15 @agent_ppo2.py:185][0m |           0.0035 |          25.9656 |         -21.5527 |
[32m[20221214 00:25:15 @agent_ppo2.py:185][0m |          -0.0040 |          18.4108 |         -21.3755 |
[32m[20221214 00:25:15 @agent_ppo2.py:185][0m |          -0.0087 |          16.9318 |         -21.4898 |
[32m[20221214 00:25:15 @agent_ppo2.py:185][0m |          -0.0151 |          15.8800 |         -21.2752 |
[32m[20221214 00:25:15 @agent_ppo2.py:185][0m |          -0.0147 |          15.0647 |         -21.1691 |
[32m[20221214 00:25:15 @agent_ppo2.py:185][0m |          -0.0234 |          14.5715 |         -21.5490 |
[32m[20221214 00:25:15 @agent_ppo2.py:185][0m |          -0.0168 |          14.1821 |         -21.4690 |
[32m[20221214 00:25:15 @agent_ppo2.py:185][0m |          -0.0150 |          13.7829 |         -21.4907 |
[32m[20221214 00:25:16 @agent_ppo2.py:185][0m |          -0.0156 |          13.4940 |         -21.5286 |
[32m[20221214 00:25:16 @agent_ppo2.py:185][0m |          -0.0231 |          13.1552 |         -21.3354 |
[32m[20221214 00:25:16 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:25:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 423.70
[32m[20221214 00:25:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 523.51
[32m[20221214 00:25:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 783.79
[32m[20221214 00:25:16 @agent_ppo2.py:143][0m Total time:      27.76 min
[32m[20221214 00:25:16 @agent_ppo2.py:145][0m 2527232 total steps have happened
[32m[20221214 00:25:16 @agent_ppo2.py:121][0m #------------------------ Iteration 5234 --------------------------#
[32m[20221214 00:25:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:25:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:25:16 @agent_ppo2.py:185][0m |           0.0013 |         109.9418 |         -22.4608 |
[32m[20221214 00:25:16 @agent_ppo2.py:185][0m |           0.0120 |         108.2509 |         -22.1817 |
[32m[20221214 00:25:16 @agent_ppo2.py:185][0m |          -0.0070 |          99.4190 |         -22.3450 |
[32m[20221214 00:25:16 @agent_ppo2.py:185][0m |          -0.0097 |          95.6693 |         -22.3953 |
[32m[20221214 00:25:17 @agent_ppo2.py:185][0m |          -0.0089 |          94.0312 |         -22.3335 |
[32m[20221214 00:25:17 @agent_ppo2.py:185][0m |          -0.0132 |          92.2172 |         -22.6272 |
[32m[20221214 00:25:17 @agent_ppo2.py:185][0m |          -0.0077 |          92.3243 |         -22.4106 |
[32m[20221214 00:25:17 @agent_ppo2.py:185][0m |          -0.0041 |          95.2499 |         -22.2614 |
[32m[20221214 00:25:17 @agent_ppo2.py:185][0m |          -0.0160 |          90.5549 |         -22.4606 |
[32m[20221214 00:25:17 @agent_ppo2.py:185][0m |          -0.0115 |          89.6030 |         -22.1990 |
[32m[20221214 00:25:17 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:25:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 584.45
[32m[20221214 00:25:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 660.71
[32m[20221214 00:25:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 782.49
[32m[20221214 00:25:17 @agent_ppo2.py:143][0m Total time:      27.78 min
[32m[20221214 00:25:17 @agent_ppo2.py:145][0m 2529280 total steps have happened
[32m[20221214 00:25:17 @agent_ppo2.py:121][0m #------------------------ Iteration 5235 --------------------------#
[32m[20221214 00:25:17 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:25:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:25:18 @agent_ppo2.py:185][0m |           0.0033 |         110.3697 |         -21.9556 |
[32m[20221214 00:25:18 @agent_ppo2.py:185][0m |          -0.0040 |          99.0772 |         -21.8429 |
[32m[20221214 00:25:18 @agent_ppo2.py:185][0m |          -0.0093 |          95.4449 |         -21.8637 |
[32m[20221214 00:25:18 @agent_ppo2.py:185][0m |          -0.0130 |          92.0881 |         -21.7845 |
[32m[20221214 00:25:18 @agent_ppo2.py:185][0m |          -0.0132 |          90.1210 |         -21.9902 |
[32m[20221214 00:25:18 @agent_ppo2.py:185][0m |          -0.0155 |          88.5584 |         -21.9728 |
[32m[20221214 00:25:18 @agent_ppo2.py:185][0m |          -0.0156 |          87.5734 |         -21.9046 |
[32m[20221214 00:25:18 @agent_ppo2.py:185][0m |          -0.0062 |          90.1980 |         -21.8407 |
[32m[20221214 00:25:18 @agent_ppo2.py:185][0m |          -0.0211 |          85.3705 |         -21.9842 |
[32m[20221214 00:25:18 @agent_ppo2.py:185][0m |          -0.0057 |          94.8713 |         -21.9404 |
[32m[20221214 00:25:18 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:25:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 462.12
[32m[20221214 00:25:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 608.06
[32m[20221214 00:25:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 750.16
[32m[20221214 00:25:19 @agent_ppo2.py:143][0m Total time:      27.80 min
[32m[20221214 00:25:19 @agent_ppo2.py:145][0m 2531328 total steps have happened
[32m[20221214 00:25:19 @agent_ppo2.py:121][0m #------------------------ Iteration 5236 --------------------------#
[32m[20221214 00:25:19 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:25:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:25:19 @agent_ppo2.py:185][0m |           0.0155 |         111.7922 |         -21.5997 |
[32m[20221214 00:25:19 @agent_ppo2.py:185][0m |          -0.0020 |          91.7742 |         -21.3433 |
[32m[20221214 00:25:19 @agent_ppo2.py:185][0m |          -0.0119 |          86.1966 |         -21.6750 |
[32m[20221214 00:25:19 @agent_ppo2.py:185][0m |          -0.0053 |          83.9667 |         -21.4512 |
[32m[20221214 00:25:19 @agent_ppo2.py:185][0m |          -0.0091 |          82.9165 |         -21.6011 |
[32m[20221214 00:25:19 @agent_ppo2.py:185][0m |          -0.0122 |          81.6569 |         -21.6903 |
[32m[20221214 00:25:19 @agent_ppo2.py:185][0m |          -0.0141 |          80.4839 |         -21.8678 |
[32m[20221214 00:25:20 @agent_ppo2.py:185][0m |          -0.0129 |          80.5869 |         -21.8357 |
[32m[20221214 00:25:20 @agent_ppo2.py:185][0m |          -0.0148 |          79.5849 |         -21.9155 |
[32m[20221214 00:25:20 @agent_ppo2.py:185][0m |          -0.0098 |          80.7955 |         -22.0200 |
[32m[20221214 00:25:20 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:25:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 485.29
[32m[20221214 00:25:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 571.56
[32m[20221214 00:25:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 561.59
[32m[20221214 00:25:20 @agent_ppo2.py:143][0m Total time:      27.83 min
[32m[20221214 00:25:20 @agent_ppo2.py:145][0m 2533376 total steps have happened
[32m[20221214 00:25:20 @agent_ppo2.py:121][0m #------------------------ Iteration 5237 --------------------------#
[32m[20221214 00:25:20 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:25:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:25:20 @agent_ppo2.py:185][0m |           0.0005 |          30.5332 |         -21.7541 |
[32m[20221214 00:25:20 @agent_ppo2.py:185][0m |          -0.0070 |          23.1321 |         -21.7661 |
[32m[20221214 00:25:20 @agent_ppo2.py:185][0m |          -0.0088 |          21.2866 |         -21.6705 |
[32m[20221214 00:25:21 @agent_ppo2.py:185][0m |          -0.0111 |          19.8741 |         -21.6636 |
[32m[20221214 00:25:21 @agent_ppo2.py:185][0m |          -0.0176 |          19.0651 |         -21.6831 |
[32m[20221214 00:25:21 @agent_ppo2.py:185][0m |          -0.0164 |          18.3327 |         -21.7876 |
[32m[20221214 00:25:21 @agent_ppo2.py:185][0m |          -0.0199 |          17.7415 |         -21.5792 |
[32m[20221214 00:25:21 @agent_ppo2.py:185][0m |          -0.0266 |          17.4310 |         -21.5516 |
[32m[20221214 00:25:21 @agent_ppo2.py:185][0m |          -0.0232 |          16.8369 |         -21.7469 |
[32m[20221214 00:25:21 @agent_ppo2.py:185][0m |          -0.0200 |          16.5191 |         -21.5985 |
[32m[20221214 00:25:21 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:25:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 359.40
[32m[20221214 00:25:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 517.68
[32m[20221214 00:25:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 793.99
[32m[20221214 00:25:21 @agent_ppo2.py:143][0m Total time:      27.85 min
[32m[20221214 00:25:21 @agent_ppo2.py:145][0m 2535424 total steps have happened
[32m[20221214 00:25:21 @agent_ppo2.py:121][0m #------------------------ Iteration 5238 --------------------------#
[32m[20221214 00:25:21 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:25:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:25:22 @agent_ppo2.py:185][0m |          -0.0002 |         112.8081 |         -22.0471 |
[32m[20221214 00:25:22 @agent_ppo2.py:185][0m |          -0.0096 |         104.9090 |         -22.1457 |
[32m[20221214 00:25:22 @agent_ppo2.py:185][0m |          -0.0115 |         101.7590 |         -22.2724 |
[32m[20221214 00:25:22 @agent_ppo2.py:185][0m |          -0.0107 |          98.8503 |         -22.1072 |
[32m[20221214 00:25:22 @agent_ppo2.py:185][0m |          -0.0122 |          97.3945 |         -22.0032 |
[32m[20221214 00:25:22 @agent_ppo2.py:185][0m |          -0.0110 |          97.0211 |         -21.7828 |
[32m[20221214 00:25:22 @agent_ppo2.py:185][0m |          -0.0112 |          95.0709 |         -21.8452 |
[32m[20221214 00:25:22 @agent_ppo2.py:185][0m |          -0.0140 |          94.0141 |         -21.4544 |
[32m[20221214 00:25:22 @agent_ppo2.py:185][0m |          -0.0163 |          93.8672 |         -21.4964 |
[32m[20221214 00:25:22 @agent_ppo2.py:185][0m |          -0.0173 |          93.4638 |         -21.3061 |
[32m[20221214 00:25:22 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:25:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 602.02
[32m[20221214 00:25:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 707.60
[32m[20221214 00:25:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 786.99
[32m[20221214 00:25:23 @agent_ppo2.py:143][0m Total time:      27.87 min
[32m[20221214 00:25:23 @agent_ppo2.py:145][0m 2537472 total steps have happened
[32m[20221214 00:25:23 @agent_ppo2.py:121][0m #------------------------ Iteration 5239 --------------------------#
[32m[20221214 00:25:23 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:25:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:25:23 @agent_ppo2.py:185][0m |           0.0039 |          34.4697 |         -22.7040 |
[32m[20221214 00:25:23 @agent_ppo2.py:185][0m |          -0.0166 |          29.5827 |         -22.8404 |
[32m[20221214 00:25:23 @agent_ppo2.py:185][0m |          -0.0150 |          26.8797 |         -22.8375 |
[32m[20221214 00:25:23 @agent_ppo2.py:185][0m |          -0.0111 |          25.1068 |         -22.7517 |
[32m[20221214 00:25:23 @agent_ppo2.py:185][0m |          -0.0201 |          24.7642 |         -22.7035 |
[32m[20221214 00:25:23 @agent_ppo2.py:185][0m |          -0.0212 |          23.8125 |         -22.7211 |
[32m[20221214 00:25:23 @agent_ppo2.py:185][0m |          -0.0189 |          23.4098 |         -22.5944 |
[32m[20221214 00:25:24 @agent_ppo2.py:185][0m |          -0.0258 |          22.7098 |         -22.6609 |
[32m[20221214 00:25:24 @agent_ppo2.py:185][0m |          -0.0236 |          22.0970 |         -22.6419 |
[32m[20221214 00:25:24 @agent_ppo2.py:185][0m |          -0.0239 |          21.9956 |         -22.6896 |
[32m[20221214 00:25:24 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:25:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 433.75
[32m[20221214 00:25:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 480.90
[32m[20221214 00:25:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 494.48
[32m[20221214 00:25:24 @agent_ppo2.py:143][0m Total time:      27.89 min
[32m[20221214 00:25:24 @agent_ppo2.py:145][0m 2539520 total steps have happened
[32m[20221214 00:25:24 @agent_ppo2.py:121][0m #------------------------ Iteration 5240 --------------------------#
[32m[20221214 00:25:24 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:25:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:25:24 @agent_ppo2.py:185][0m |          -0.0045 |          41.4127 |         -20.3622 |
[32m[20221214 00:25:24 @agent_ppo2.py:185][0m |          -0.0142 |          34.8755 |         -20.4445 |
[32m[20221214 00:25:24 @agent_ppo2.py:185][0m |          -0.0158 |          32.3228 |         -20.2341 |
[32m[20221214 00:25:25 @agent_ppo2.py:185][0m |          -0.0000 |          34.2240 |         -20.4743 |
[32m[20221214 00:25:25 @agent_ppo2.py:185][0m |          -0.0207 |          31.1275 |         -20.3303 |
[32m[20221214 00:25:25 @agent_ppo2.py:185][0m |          -0.0137 |          29.6539 |         -20.5452 |
[32m[20221214 00:25:25 @agent_ppo2.py:185][0m |          -0.0235 |          28.8237 |         -20.7340 |
[32m[20221214 00:25:25 @agent_ppo2.py:185][0m |          -0.0241 |          28.5513 |         -20.6822 |
[32m[20221214 00:25:25 @agent_ppo2.py:185][0m |          -0.0239 |          28.0185 |         -20.6202 |
[32m[20221214 00:25:25 @agent_ppo2.py:185][0m |          -0.0224 |          28.3647 |         -20.8712 |
[32m[20221214 00:25:25 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:25:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 475.89
[32m[20221214 00:25:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 635.66
[32m[20221214 00:25:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 554.41
[32m[20221214 00:25:25 @agent_ppo2.py:143][0m Total time:      27.91 min
[32m[20221214 00:25:25 @agent_ppo2.py:145][0m 2541568 total steps have happened
[32m[20221214 00:25:25 @agent_ppo2.py:121][0m #------------------------ Iteration 5241 --------------------------#
[32m[20221214 00:25:25 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:25:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:25:26 @agent_ppo2.py:185][0m |           0.0017 |          30.5867 |         -21.5145 |
[32m[20221214 00:25:26 @agent_ppo2.py:185][0m |          -0.0055 |          24.4969 |         -21.3749 |
[32m[20221214 00:25:26 @agent_ppo2.py:185][0m |          -0.0066 |          22.8339 |         -21.4571 |
[32m[20221214 00:25:26 @agent_ppo2.py:185][0m |          -0.0121 |          21.5952 |         -21.3385 |
[32m[20221214 00:25:26 @agent_ppo2.py:185][0m |          -0.0194 |          20.9351 |         -21.3504 |
[32m[20221214 00:25:26 @agent_ppo2.py:185][0m |          -0.0213 |          20.1640 |         -21.3452 |
[32m[20221214 00:25:26 @agent_ppo2.py:185][0m |          -0.0244 |          19.4109 |         -21.4397 |
[32m[20221214 00:25:26 @agent_ppo2.py:185][0m |          -0.0308 |          19.4456 |         -21.4385 |
[32m[20221214 00:25:26 @agent_ppo2.py:185][0m |          -0.0255 |          18.8932 |         -21.3669 |
[32m[20221214 00:25:27 @agent_ppo2.py:185][0m |          -0.0303 |          18.1878 |         -21.1452 |
[32m[20221214 00:25:27 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:25:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 476.36
[32m[20221214 00:25:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 515.78
[32m[20221214 00:25:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 758.26
[32m[20221214 00:25:27 @agent_ppo2.py:143][0m Total time:      27.94 min
[32m[20221214 00:25:27 @agent_ppo2.py:145][0m 2543616 total steps have happened
[32m[20221214 00:25:27 @agent_ppo2.py:121][0m #------------------------ Iteration 5242 --------------------------#
[32m[20221214 00:25:27 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:25:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:25:27 @agent_ppo2.py:185][0m |           0.0046 |          47.8472 |         -22.6504 |
[32m[20221214 00:25:27 @agent_ppo2.py:185][0m |          -0.0024 |          44.9905 |         -22.2133 |
[32m[20221214 00:25:27 @agent_ppo2.py:185][0m |          -0.0105 |          42.8075 |         -22.4542 |
[32m[20221214 00:25:27 @agent_ppo2.py:185][0m |          -0.0140 |          42.6241 |         -22.3450 |
[32m[20221214 00:25:27 @agent_ppo2.py:185][0m |          -0.0114 |          40.5738 |         -22.4344 |
[32m[20221214 00:25:27 @agent_ppo2.py:185][0m |          -0.0138 |          39.2277 |         -22.1583 |
[32m[20221214 00:25:28 @agent_ppo2.py:185][0m |          -0.0200 |          38.4425 |         -22.4295 |
[32m[20221214 00:25:28 @agent_ppo2.py:185][0m |          -0.0157 |          38.1107 |         -22.5605 |
[32m[20221214 00:25:28 @agent_ppo2.py:185][0m |          -0.0208 |          37.9416 |         -22.4087 |
[32m[20221214 00:25:28 @agent_ppo2.py:185][0m |          -0.0186 |          37.5676 |         -22.3879 |
[32m[20221214 00:25:28 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:25:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 506.35
[32m[20221214 00:25:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 613.05
[32m[20221214 00:25:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 725.46
[32m[20221214 00:25:28 @agent_ppo2.py:143][0m Total time:      27.96 min
[32m[20221214 00:25:28 @agent_ppo2.py:145][0m 2545664 total steps have happened
[32m[20221214 00:25:28 @agent_ppo2.py:121][0m #------------------------ Iteration 5243 --------------------------#
[32m[20221214 00:25:28 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:25:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:25:28 @agent_ppo2.py:185][0m |           0.0029 |          86.4080 |         -22.6609 |
[32m[20221214 00:25:28 @agent_ppo2.py:185][0m |          -0.0063 |          77.3117 |         -22.6330 |
[32m[20221214 00:25:29 @agent_ppo2.py:185][0m |          -0.0051 |          73.2957 |         -22.7876 |
[32m[20221214 00:25:29 @agent_ppo2.py:185][0m |          -0.0058 |          73.0159 |         -22.5826 |
[32m[20221214 00:25:29 @agent_ppo2.py:185][0m |           0.0245 |          83.4849 |         -22.6805 |
[32m[20221214 00:25:29 @agent_ppo2.py:185][0m |          -0.0114 |          69.0822 |         -22.4839 |
[32m[20221214 00:25:29 @agent_ppo2.py:185][0m |          -0.0139 |          66.7212 |         -22.4996 |
[32m[20221214 00:25:29 @agent_ppo2.py:185][0m |          -0.0170 |          66.0846 |         -22.6680 |
[32m[20221214 00:25:29 @agent_ppo2.py:185][0m |          -0.0145 |          65.2058 |         -22.6278 |
[32m[20221214 00:25:29 @agent_ppo2.py:185][0m |          -0.0172 |          64.6925 |         -22.6583 |
[32m[20221214 00:25:29 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:25:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 525.42
[32m[20221214 00:25:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 660.52
[32m[20221214 00:25:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 582.96
[32m[20221214 00:25:29 @agent_ppo2.py:143][0m Total time:      27.98 min
[32m[20221214 00:25:29 @agent_ppo2.py:145][0m 2547712 total steps have happened
[32m[20221214 00:25:29 @agent_ppo2.py:121][0m #------------------------ Iteration 5244 --------------------------#
[32m[20221214 00:25:30 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:25:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:25:30 @agent_ppo2.py:185][0m |           0.0013 |         112.9626 |         -22.9120 |
[32m[20221214 00:25:30 @agent_ppo2.py:185][0m |          -0.0036 |         107.2487 |         -23.0854 |
[32m[20221214 00:25:30 @agent_ppo2.py:185][0m |          -0.0028 |         104.5530 |         -23.1394 |
[32m[20221214 00:25:30 @agent_ppo2.py:185][0m |          -0.0043 |         102.3619 |         -22.9126 |
[32m[20221214 00:25:30 @agent_ppo2.py:185][0m |          -0.0042 |         101.5082 |         -23.1606 |
[32m[20221214 00:25:30 @agent_ppo2.py:185][0m |          -0.0049 |         100.9137 |         -23.0762 |
[32m[20221214 00:25:30 @agent_ppo2.py:185][0m |          -0.0031 |         104.1798 |         -23.4729 |
[32m[20221214 00:25:30 @agent_ppo2.py:185][0m |          -0.0058 |          99.1338 |         -23.4722 |
[32m[20221214 00:25:31 @agent_ppo2.py:185][0m |          -0.0024 |          99.4500 |         -23.5076 |
[32m[20221214 00:25:31 @agent_ppo2.py:185][0m |          -0.0084 |          97.5566 |         -23.2467 |
[32m[20221214 00:25:31 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:25:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 618.84
[32m[20221214 00:25:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 681.49
[32m[20221214 00:25:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 684.03
[32m[20221214 00:25:31 @agent_ppo2.py:143][0m Total time:      28.01 min
[32m[20221214 00:25:31 @agent_ppo2.py:145][0m 2549760 total steps have happened
[32m[20221214 00:25:31 @agent_ppo2.py:121][0m #------------------------ Iteration 5245 --------------------------#
[32m[20221214 00:25:31 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:25:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:25:31 @agent_ppo2.py:185][0m |           0.0062 |          48.0684 |         -24.3208 |
[32m[20221214 00:25:31 @agent_ppo2.py:185][0m |          -0.0072 |          41.6192 |         -24.3291 |
[32m[20221214 00:25:31 @agent_ppo2.py:185][0m |          -0.0040 |          40.5685 |         -24.6346 |
[32m[20221214 00:25:31 @agent_ppo2.py:185][0m |          -0.0170 |          38.8387 |         -24.4168 |
[32m[20221214 00:25:32 @agent_ppo2.py:185][0m |          -0.0150 |          37.5146 |         -24.7102 |
[32m[20221214 00:25:32 @agent_ppo2.py:185][0m |          -0.0165 |          36.7520 |         -24.7686 |
[32m[20221214 00:25:32 @agent_ppo2.py:185][0m |          -0.0141 |          36.9293 |         -24.9998 |
[32m[20221214 00:25:32 @agent_ppo2.py:185][0m |          -0.0145 |          35.2771 |         -24.8841 |
[32m[20221214 00:25:32 @agent_ppo2.py:185][0m |          -0.0152 |          36.1027 |         -24.9874 |
[32m[20221214 00:25:32 @agent_ppo2.py:185][0m |          -0.0151 |          35.2950 |         -25.0920 |
[32m[20221214 00:25:32 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:25:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 451.47
[32m[20221214 00:25:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 661.56
[32m[20221214 00:25:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 598.91
[32m[20221214 00:25:32 @agent_ppo2.py:143][0m Total time:      28.03 min
[32m[20221214 00:25:32 @agent_ppo2.py:145][0m 2551808 total steps have happened
[32m[20221214 00:25:32 @agent_ppo2.py:121][0m #------------------------ Iteration 5246 --------------------------#
[32m[20221214 00:25:32 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:25:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:25:33 @agent_ppo2.py:185][0m |           0.0043 |         102.6260 |         -25.7306 |
[32m[20221214 00:25:33 @agent_ppo2.py:185][0m |          -0.0064 |          97.0688 |         -25.5337 |
[32m[20221214 00:25:33 @agent_ppo2.py:185][0m |          -0.0060 |          95.3120 |         -25.5438 |
[32m[20221214 00:25:33 @agent_ppo2.py:185][0m |          -0.0080 |          94.2428 |         -25.5075 |
[32m[20221214 00:25:33 @agent_ppo2.py:185][0m |          -0.0103 |          96.6585 |         -25.3679 |
[32m[20221214 00:25:33 @agent_ppo2.py:185][0m |          -0.0064 |          96.7881 |         -25.1624 |
[32m[20221214 00:25:33 @agent_ppo2.py:185][0m |          -0.0138 |          92.2337 |         -25.0622 |
[32m[20221214 00:25:33 @agent_ppo2.py:185][0m |          -0.0166 |          91.6530 |         -25.0748 |
[32m[20221214 00:25:33 @agent_ppo2.py:185][0m |          -0.0177 |          91.5229 |         -25.0564 |
[32m[20221214 00:25:33 @agent_ppo2.py:185][0m |          -0.0145 |          91.0082 |         -24.8416 |
[32m[20221214 00:25:33 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:25:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 568.94
[32m[20221214 00:25:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 686.35
[32m[20221214 00:25:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 674.53
[32m[20221214 00:25:34 @agent_ppo2.py:143][0m Total time:      28.05 min
[32m[20221214 00:25:34 @agent_ppo2.py:145][0m 2553856 total steps have happened
[32m[20221214 00:25:34 @agent_ppo2.py:121][0m #------------------------ Iteration 5247 --------------------------#
[32m[20221214 00:25:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:25:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:25:34 @agent_ppo2.py:185][0m |          -0.0003 |         137.0457 |         -22.6618 |
[32m[20221214 00:25:34 @agent_ppo2.py:185][0m |          -0.0074 |         130.5765 |         -22.7227 |
[32m[20221214 00:25:34 @agent_ppo2.py:185][0m |          -0.0087 |         128.2576 |         -22.6363 |
[32m[20221214 00:25:34 @agent_ppo2.py:185][0m |          -0.0125 |         126.5374 |         -22.4898 |
[32m[20221214 00:25:34 @agent_ppo2.py:185][0m |          -0.0087 |         125.7559 |         -22.6379 |
[32m[20221214 00:25:34 @agent_ppo2.py:185][0m |          -0.0163 |         125.1866 |         -22.7375 |
[32m[20221214 00:25:35 @agent_ppo2.py:185][0m |          -0.0132 |         124.3131 |         -22.3760 |
[32m[20221214 00:25:35 @agent_ppo2.py:185][0m |          -0.0168 |         124.0869 |         -22.6137 |
[32m[20221214 00:25:35 @agent_ppo2.py:185][0m |          -0.0172 |         123.5207 |         -22.4175 |
[32m[20221214 00:25:35 @agent_ppo2.py:185][0m |          -0.0124 |         124.7213 |         -22.3526 |
[32m[20221214 00:25:35 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:25:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 543.98
[32m[20221214 00:25:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 608.57
[32m[20221214 00:25:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 719.33
[32m[20221214 00:25:35 @agent_ppo2.py:143][0m Total time:      28.08 min
[32m[20221214 00:25:35 @agent_ppo2.py:145][0m 2555904 total steps have happened
[32m[20221214 00:25:35 @agent_ppo2.py:121][0m #------------------------ Iteration 5248 --------------------------#
[32m[20221214 00:25:35 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:25:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:25:35 @agent_ppo2.py:185][0m |          -0.0006 |         134.1481 |         -24.5299 |
[32m[20221214 00:25:35 @agent_ppo2.py:185][0m |          -0.0052 |         127.4036 |         -24.4254 |
[32m[20221214 00:25:35 @agent_ppo2.py:185][0m |          -0.0086 |         125.0679 |         -24.3705 |
[32m[20221214 00:25:36 @agent_ppo2.py:185][0m |          -0.0090 |         123.3166 |         -24.5009 |
[32m[20221214 00:25:36 @agent_ppo2.py:185][0m |          -0.0081 |         122.0572 |         -24.2380 |
[32m[20221214 00:25:36 @agent_ppo2.py:185][0m |          -0.0093 |         121.3359 |         -24.3285 |
[32m[20221214 00:25:36 @agent_ppo2.py:185][0m |          -0.0080 |         120.3824 |         -24.3689 |
[32m[20221214 00:25:36 @agent_ppo2.py:185][0m |          -0.0100 |         120.6059 |         -24.1169 |
[32m[20221214 00:25:36 @agent_ppo2.py:185][0m |          -0.0138 |         119.7023 |         -24.1960 |
[32m[20221214 00:25:36 @agent_ppo2.py:185][0m |          -0.0135 |         119.5347 |         -24.1625 |
[32m[20221214 00:25:36 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:25:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 566.09
[32m[20221214 00:25:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 732.36
[32m[20221214 00:25:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 694.01
[32m[20221214 00:25:36 @agent_ppo2.py:143][0m Total time:      28.10 min
[32m[20221214 00:25:36 @agent_ppo2.py:145][0m 2557952 total steps have happened
[32m[20221214 00:25:36 @agent_ppo2.py:121][0m #------------------------ Iteration 5249 --------------------------#
[32m[20221214 00:25:36 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:25:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:25:37 @agent_ppo2.py:185][0m |           0.0011 |         106.8491 |         -23.8195 |
[32m[20221214 00:25:37 @agent_ppo2.py:185][0m |          -0.0040 |          99.1128 |         -23.5440 |
[32m[20221214 00:25:37 @agent_ppo2.py:185][0m |          -0.0024 |          95.5549 |         -23.9755 |
[32m[20221214 00:25:37 @agent_ppo2.py:185][0m |          -0.0075 |          93.6429 |         -23.9128 |
[32m[20221214 00:25:37 @agent_ppo2.py:185][0m |          -0.0068 |          92.4484 |         -23.9565 |
[32m[20221214 00:25:37 @agent_ppo2.py:185][0m |          -0.0089 |          91.1559 |         -23.9029 |
[32m[20221214 00:25:37 @agent_ppo2.py:185][0m |          -0.0058 |          90.3700 |         -23.7634 |
[32m[20221214 00:25:37 @agent_ppo2.py:185][0m |          -0.0096 |          90.1474 |         -24.0201 |
[32m[20221214 00:25:37 @agent_ppo2.py:185][0m |          -0.0095 |          89.7369 |         -23.9986 |
[32m[20221214 00:25:37 @agent_ppo2.py:185][0m |          -0.0072 |          89.4239 |         -23.8790 |
[32m[20221214 00:25:37 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:25:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 569.50
[32m[20221214 00:25:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 747.62
[32m[20221214 00:25:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 560.43
[32m[20221214 00:25:38 @agent_ppo2.py:143][0m Total time:      28.12 min
[32m[20221214 00:25:38 @agent_ppo2.py:145][0m 2560000 total steps have happened
[32m[20221214 00:25:38 @agent_ppo2.py:121][0m #------------------------ Iteration 5250 --------------------------#
[32m[20221214 00:25:38 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:25:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:25:38 @agent_ppo2.py:185][0m |          -0.0020 |         135.1894 |         -24.8674 |
[32m[20221214 00:25:38 @agent_ppo2.py:185][0m |          -0.0060 |         128.5117 |         -24.9554 |
[32m[20221214 00:25:38 @agent_ppo2.py:185][0m |          -0.0077 |         126.1750 |         -24.7048 |
[32m[20221214 00:25:38 @agent_ppo2.py:185][0m |          -0.0100 |         124.0726 |         -24.8175 |
[32m[20221214 00:25:38 @agent_ppo2.py:185][0m |          -0.0089 |         122.7586 |         -24.7315 |
[32m[20221214 00:25:38 @agent_ppo2.py:185][0m |          -0.0082 |         121.9176 |         -24.5720 |
[32m[20221214 00:25:39 @agent_ppo2.py:185][0m |          -0.0121 |         121.8697 |         -24.6599 |
[32m[20221214 00:25:39 @agent_ppo2.py:185][0m |          -0.0102 |         120.4009 |         -24.5424 |
[32m[20221214 00:25:39 @agent_ppo2.py:185][0m |          -0.0116 |         119.8611 |         -24.5384 |
[32m[20221214 00:25:39 @agent_ppo2.py:185][0m |          -0.0111 |         120.0629 |         -24.3944 |
[32m[20221214 00:25:39 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:25:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 620.66
[32m[20221214 00:25:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 684.08
[32m[20221214 00:25:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 687.73
[32m[20221214 00:25:39 @agent_ppo2.py:143][0m Total time:      28.14 min
[32m[20221214 00:25:39 @agent_ppo2.py:145][0m 2562048 total steps have happened
[32m[20221214 00:25:39 @agent_ppo2.py:121][0m #------------------------ Iteration 5251 --------------------------#
[32m[20221214 00:25:39 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:25:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:25:39 @agent_ppo2.py:185][0m |           0.0008 |          66.0107 |         -23.3932 |
[32m[20221214 00:25:39 @agent_ppo2.py:185][0m |          -0.0048 |          60.0399 |         -23.5475 |
[32m[20221214 00:25:40 @agent_ppo2.py:185][0m |          -0.0022 |          58.0178 |         -23.4260 |
[32m[20221214 00:25:40 @agent_ppo2.py:185][0m |          -0.0027 |          56.4336 |         -23.5526 |
[32m[20221214 00:25:40 @agent_ppo2.py:185][0m |          -0.0116 |          55.5099 |         -23.7149 |
[32m[20221214 00:25:40 @agent_ppo2.py:185][0m |          -0.0103 |          54.4333 |         -23.6821 |
[32m[20221214 00:25:40 @agent_ppo2.py:185][0m |          -0.0136 |          53.7589 |         -23.5705 |
[32m[20221214 00:25:40 @agent_ppo2.py:185][0m |          -0.0134 |          53.3028 |         -23.7572 |
[32m[20221214 00:25:40 @agent_ppo2.py:185][0m |          -0.0096 |          53.0411 |         -23.7504 |
[32m[20221214 00:25:40 @agent_ppo2.py:185][0m |          -0.0100 |          53.2612 |         -23.6593 |
[32m[20221214 00:25:40 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:25:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 600.00
[32m[20221214 00:25:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 786.10
[32m[20221214 00:25:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 764.86
[32m[20221214 00:25:40 @agent_ppo2.py:143][0m Total time:      28.17 min
[32m[20221214 00:25:40 @agent_ppo2.py:145][0m 2564096 total steps have happened
[32m[20221214 00:25:40 @agent_ppo2.py:121][0m #------------------------ Iteration 5252 --------------------------#
[32m[20221214 00:25:41 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:25:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:25:41 @agent_ppo2.py:185][0m |           0.0020 |          96.0264 |         -23.6226 |
[32m[20221214 00:25:41 @agent_ppo2.py:185][0m |          -0.0089 |          80.9449 |         -23.7557 |
[32m[20221214 00:25:41 @agent_ppo2.py:185][0m |          -0.0074 |          75.1443 |         -23.6924 |
[32m[20221214 00:25:41 @agent_ppo2.py:185][0m |          -0.0077 |          71.7058 |         -23.5616 |
[32m[20221214 00:25:41 @agent_ppo2.py:185][0m |          -0.0107 |          69.3754 |         -23.4926 |
[32m[20221214 00:25:41 @agent_ppo2.py:185][0m |          -0.0039 |          72.1211 |         -23.5828 |
[32m[20221214 00:25:41 @agent_ppo2.py:185][0m |          -0.0113 |          70.6038 |         -23.3388 |
[32m[20221214 00:25:41 @agent_ppo2.py:185][0m |          -0.0154 |          64.9112 |         -23.4114 |
[32m[20221214 00:25:41 @agent_ppo2.py:185][0m |           0.0004 |          66.9727 |         -23.4636 |
[32m[20221214 00:25:42 @agent_ppo2.py:185][0m |          -0.0128 |          62.8612 |         -23.3086 |
[32m[20221214 00:25:42 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:25:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 609.75
[32m[20221214 00:25:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 782.51
[32m[20221214 00:25:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 434.20
[32m[20221214 00:25:42 @agent_ppo2.py:143][0m Total time:      28.19 min
[32m[20221214 00:25:42 @agent_ppo2.py:145][0m 2566144 total steps have happened
[32m[20221214 00:25:42 @agent_ppo2.py:121][0m #------------------------ Iteration 5253 --------------------------#
[32m[20221214 00:25:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:25:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:25:42 @agent_ppo2.py:185][0m |           0.0024 |         113.7753 |         -24.2910 |
[32m[20221214 00:25:42 @agent_ppo2.py:185][0m |           0.0045 |         108.2443 |         -24.1965 |
[32m[20221214 00:25:42 @agent_ppo2.py:185][0m |          -0.0024 |          94.3579 |         -24.0234 |
[32m[20221214 00:25:42 @agent_ppo2.py:185][0m |          -0.0064 |          88.6403 |         -24.0579 |
[32m[20221214 00:25:42 @agent_ppo2.py:185][0m |          -0.0096 |          88.1374 |         -24.1014 |
[32m[20221214 00:25:43 @agent_ppo2.py:185][0m |          -0.0134 |          82.5222 |         -24.0438 |
[32m[20221214 00:25:43 @agent_ppo2.py:185][0m |          -0.0121 |          81.5446 |         -24.0525 |
[32m[20221214 00:25:43 @agent_ppo2.py:185][0m |          -0.0146 |          79.8017 |         -23.9506 |
[32m[20221214 00:25:43 @agent_ppo2.py:185][0m |          -0.0107 |          81.7904 |         -24.0815 |
[32m[20221214 00:25:43 @agent_ppo2.py:185][0m |          -0.0048 |          78.4213 |         -23.9523 |
[32m[20221214 00:25:43 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:25:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 580.81
[32m[20221214 00:25:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 654.05
[32m[20221214 00:25:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 548.80
[32m[20221214 00:25:43 @agent_ppo2.py:143][0m Total time:      28.21 min
[32m[20221214 00:25:43 @agent_ppo2.py:145][0m 2568192 total steps have happened
[32m[20221214 00:25:43 @agent_ppo2.py:121][0m #------------------------ Iteration 5254 --------------------------#
[32m[20221214 00:25:43 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:25:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:25:43 @agent_ppo2.py:185][0m |           0.0007 |         119.1189 |         -22.3864 |
[32m[20221214 00:25:44 @agent_ppo2.py:185][0m |          -0.0067 |         108.3709 |         -22.7821 |
[32m[20221214 00:25:44 @agent_ppo2.py:185][0m |          -0.0066 |         107.4313 |         -22.8703 |
[32m[20221214 00:25:44 @agent_ppo2.py:185][0m |          -0.0126 |         102.9958 |         -22.8665 |
[32m[20221214 00:25:44 @agent_ppo2.py:185][0m |          -0.0128 |         104.9325 |         -22.9335 |
[32m[20221214 00:25:44 @agent_ppo2.py:185][0m |          -0.0150 |         102.7614 |         -23.2200 |
[32m[20221214 00:25:44 @agent_ppo2.py:185][0m |          -0.0173 |         102.7678 |         -22.9964 |
[32m[20221214 00:25:44 @agent_ppo2.py:185][0m |          -0.0089 |         108.6030 |         -23.3624 |
[32m[20221214 00:25:44 @agent_ppo2.py:185][0m |          -0.0133 |         101.6883 |         -23.3199 |
[32m[20221214 00:25:44 @agent_ppo2.py:185][0m |          -0.0151 |         101.0163 |         -23.1584 |
[32m[20221214 00:25:44 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:25:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 546.58
[32m[20221214 00:25:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 698.57
[32m[20221214 00:25:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 718.57
[32m[20221214 00:25:44 @agent_ppo2.py:143][0m Total time:      28.23 min
[32m[20221214 00:25:44 @agent_ppo2.py:145][0m 2570240 total steps have happened
[32m[20221214 00:25:44 @agent_ppo2.py:121][0m #------------------------ Iteration 5255 --------------------------#
[32m[20221214 00:25:45 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:25:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:25:45 @agent_ppo2.py:185][0m |           0.0053 |         164.7297 |         -23.0227 |
[32m[20221214 00:25:45 @agent_ppo2.py:185][0m |          -0.0013 |         158.2786 |         -22.6626 |
[32m[20221214 00:25:45 @agent_ppo2.py:185][0m |           0.0177 |         177.2745 |         -22.7982 |
[32m[20221214 00:25:45 @agent_ppo2.py:185][0m |           0.0051 |         164.5207 |         -22.6977 |
[32m[20221214 00:25:45 @agent_ppo2.py:185][0m |          -0.0029 |         155.6723 |         -22.6325 |
[32m[20221214 00:25:45 @agent_ppo2.py:185][0m |          -0.0037 |         156.5715 |         -22.7177 |
[32m[20221214 00:25:45 @agent_ppo2.py:185][0m |          -0.0041 |         156.4581 |         -22.7754 |
[32m[20221214 00:25:45 @agent_ppo2.py:185][0m |           0.0035 |         158.9738 |         -22.7535 |
[32m[20221214 00:25:46 @agent_ppo2.py:185][0m |          -0.0024 |         153.5127 |         -22.8140 |
[32m[20221214 00:25:46 @agent_ppo2.py:185][0m |          -0.0060 |         154.4626 |         -22.7414 |
[32m[20221214 00:25:46 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:25:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 724.91
[32m[20221214 00:25:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 781.39
[32m[20221214 00:25:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 736.20
[32m[20221214 00:25:46 @agent_ppo2.py:143][0m Total time:      28.26 min
[32m[20221214 00:25:46 @agent_ppo2.py:145][0m 2572288 total steps have happened
[32m[20221214 00:25:46 @agent_ppo2.py:121][0m #------------------------ Iteration 5256 --------------------------#
[32m[20221214 00:25:46 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:25:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:25:46 @agent_ppo2.py:185][0m |           0.0148 |         194.0430 |         -25.0439 |
[32m[20221214 00:25:46 @agent_ppo2.py:185][0m |          -0.0016 |         171.5006 |         -24.9196 |
[32m[20221214 00:25:46 @agent_ppo2.py:185][0m |          -0.0048 |         168.6654 |         -24.7232 |
[32m[20221214 00:25:46 @agent_ppo2.py:185][0m |          -0.0013 |         169.1982 |         -25.0386 |
[32m[20221214 00:25:47 @agent_ppo2.py:185][0m |          -0.0082 |         166.1300 |         -24.9449 |
[32m[20221214 00:25:47 @agent_ppo2.py:185][0m |           0.0006 |         167.2783 |         -25.0023 |
[32m[20221214 00:25:47 @agent_ppo2.py:185][0m |          -0.0069 |         169.4578 |         -25.0933 |
[32m[20221214 00:25:47 @agent_ppo2.py:185][0m |          -0.0008 |         169.1514 |         -25.1258 |
[32m[20221214 00:25:47 @agent_ppo2.py:185][0m |          -0.0109 |         161.4966 |         -24.9976 |
[32m[20221214 00:25:47 @agent_ppo2.py:185][0m |          -0.0101 |         160.7662 |         -25.0235 |
[32m[20221214 00:25:47 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:25:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 611.62
[32m[20221214 00:25:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 752.20
[32m[20221214 00:25:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 743.28
[32m[20221214 00:25:47 @agent_ppo2.py:143][0m Total time:      28.28 min
[32m[20221214 00:25:47 @agent_ppo2.py:145][0m 2574336 total steps have happened
[32m[20221214 00:25:47 @agent_ppo2.py:121][0m #------------------------ Iteration 5257 --------------------------#
[32m[20221214 00:25:47 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:25:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:25:48 @agent_ppo2.py:185][0m |          -0.0005 |         146.8607 |         -25.4117 |
[32m[20221214 00:25:48 @agent_ppo2.py:185][0m |          -0.0044 |         138.5140 |         -25.3889 |
[32m[20221214 00:25:48 @agent_ppo2.py:185][0m |          -0.0052 |         134.4503 |         -25.3106 |
[32m[20221214 00:25:48 @agent_ppo2.py:185][0m |          -0.0068 |         133.0346 |         -25.0894 |
[32m[20221214 00:25:48 @agent_ppo2.py:185][0m |           0.0007 |         137.0074 |         -25.0883 |
[32m[20221214 00:25:48 @agent_ppo2.py:185][0m |          -0.0099 |         130.5914 |         -25.1997 |
[32m[20221214 00:25:48 @agent_ppo2.py:185][0m |          -0.0113 |         129.5376 |         -25.1620 |
[32m[20221214 00:25:48 @agent_ppo2.py:185][0m |          -0.0010 |         130.4640 |         -25.1239 |
[32m[20221214 00:25:48 @agent_ppo2.py:185][0m |          -0.0108 |         128.5568 |         -25.1003 |
[32m[20221214 00:25:48 @agent_ppo2.py:185][0m |           0.0018 |         137.6515 |         -25.0668 |
[32m[20221214 00:25:48 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:25:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 548.65
[32m[20221214 00:25:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 627.57
[32m[20221214 00:25:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 799.27
[32m[20221214 00:25:49 @agent_ppo2.py:143][0m Total time:      28.30 min
[32m[20221214 00:25:49 @agent_ppo2.py:145][0m 2576384 total steps have happened
[32m[20221214 00:25:49 @agent_ppo2.py:121][0m #------------------------ Iteration 5258 --------------------------#
[32m[20221214 00:25:49 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:25:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:25:49 @agent_ppo2.py:185][0m |          -0.0003 |         130.2313 |         -25.0055 |
[32m[20221214 00:25:49 @agent_ppo2.py:185][0m |          -0.0058 |         120.6074 |         -24.9557 |
[32m[20221214 00:25:49 @agent_ppo2.py:185][0m |          -0.0100 |         116.9751 |         -24.8794 |
[32m[20221214 00:25:49 @agent_ppo2.py:185][0m |          -0.0095 |         114.5837 |         -24.9162 |
[32m[20221214 00:25:49 @agent_ppo2.py:185][0m |          -0.0087 |         112.8746 |         -24.8348 |
[32m[20221214 00:25:49 @agent_ppo2.py:185][0m |          -0.0127 |         111.6279 |         -24.7614 |
[32m[20221214 00:25:49 @agent_ppo2.py:185][0m |          -0.0014 |         124.4247 |         -24.7509 |
[32m[20221214 00:25:50 @agent_ppo2.py:185][0m |          -0.0121 |         110.0534 |         -24.7421 |
[32m[20221214 00:25:50 @agent_ppo2.py:185][0m |          -0.0173 |         108.5080 |         -24.3770 |
[32m[20221214 00:25:50 @agent_ppo2.py:185][0m |          -0.0159 |         108.1776 |         -24.5637 |
[32m[20221214 00:25:50 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:25:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 636.92
[32m[20221214 00:25:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 722.81
[32m[20221214 00:25:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 764.82
[32m[20221214 00:25:50 @agent_ppo2.py:143][0m Total time:      28.33 min
[32m[20221214 00:25:50 @agent_ppo2.py:145][0m 2578432 total steps have happened
[32m[20221214 00:25:50 @agent_ppo2.py:121][0m #------------------------ Iteration 5259 --------------------------#
[32m[20221214 00:25:50 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:25:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:25:50 @agent_ppo2.py:185][0m |           0.0114 |         144.0926 |         -23.6214 |
[32m[20221214 00:25:50 @agent_ppo2.py:185][0m |          -0.0029 |         135.9102 |         -23.9904 |
[32m[20221214 00:25:50 @agent_ppo2.py:185][0m |          -0.0057 |         132.2974 |         -23.9442 |
[32m[20221214 00:25:51 @agent_ppo2.py:185][0m |          -0.0082 |         128.3997 |         -24.0141 |
[32m[20221214 00:25:51 @agent_ppo2.py:185][0m |          -0.0063 |         125.7661 |         -23.8774 |
[32m[20221214 00:25:51 @agent_ppo2.py:185][0m |          -0.0065 |         124.7044 |         -23.9320 |
[32m[20221214 00:25:51 @agent_ppo2.py:185][0m |           0.0032 |         135.8720 |         -24.0392 |
[32m[20221214 00:25:51 @agent_ppo2.py:185][0m |           0.0003 |         124.3559 |         -24.1423 |
[32m[20221214 00:25:51 @agent_ppo2.py:185][0m |          -0.0095 |         121.9760 |         -24.3244 |
[32m[20221214 00:25:51 @agent_ppo2.py:185][0m |          -0.0127 |         121.0454 |         -24.3993 |
[32m[20221214 00:25:51 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:25:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 547.09
[32m[20221214 00:25:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 685.70
[32m[20221214 00:25:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 758.84
[32m[20221214 00:25:51 @agent_ppo2.py:143][0m Total time:      28.35 min
[32m[20221214 00:25:51 @agent_ppo2.py:145][0m 2580480 total steps have happened
[32m[20221214 00:25:51 @agent_ppo2.py:121][0m #------------------------ Iteration 5260 --------------------------#
[32m[20221214 00:25:51 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:25:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:25:52 @agent_ppo2.py:185][0m |           0.0061 |         124.2816 |         -24.3422 |
[32m[20221214 00:25:52 @agent_ppo2.py:185][0m |          -0.0043 |         118.9127 |         -24.5573 |
[32m[20221214 00:25:52 @agent_ppo2.py:185][0m |          -0.0027 |         113.6473 |         -24.6236 |
[32m[20221214 00:25:52 @agent_ppo2.py:185][0m |           0.0082 |         130.5186 |         -24.6622 |
[32m[20221214 00:25:52 @agent_ppo2.py:185][0m |           0.0010 |         121.6480 |         -24.5321 |
[32m[20221214 00:25:52 @agent_ppo2.py:185][0m |          -0.0009 |         111.7968 |         -24.4685 |
[32m[20221214 00:25:52 @agent_ppo2.py:185][0m |          -0.0003 |         116.3639 |         -24.7573 |
[32m[20221214 00:25:52 @agent_ppo2.py:185][0m |          -0.0064 |         110.3531 |         -24.6415 |
[32m[20221214 00:25:52 @agent_ppo2.py:185][0m |           0.0019 |         113.1134 |         -24.8819 |
[32m[20221214 00:25:52 @agent_ppo2.py:185][0m |          -0.0092 |         110.1558 |         -24.5965 |
[32m[20221214 00:25:52 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:25:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 672.49
[32m[20221214 00:25:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 763.34
[32m[20221214 00:25:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 836.63
[32m[20221214 00:25:53 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 836.63
[32m[20221214 00:25:53 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 836.63
[32m[20221214 00:25:53 @agent_ppo2.py:143][0m Total time:      28.37 min
[32m[20221214 00:25:53 @agent_ppo2.py:145][0m 2582528 total steps have happened
[32m[20221214 00:25:53 @agent_ppo2.py:121][0m #------------------------ Iteration 5261 --------------------------#
[32m[20221214 00:25:53 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:25:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:25:53 @agent_ppo2.py:185][0m |           0.0028 |          90.8966 |         -25.4454 |
[32m[20221214 00:25:53 @agent_ppo2.py:185][0m |          -0.0013 |          85.0165 |         -25.2126 |
[32m[20221214 00:25:53 @agent_ppo2.py:185][0m |          -0.0080 |          83.3400 |         -25.2698 |
[32m[20221214 00:25:53 @agent_ppo2.py:185][0m |          -0.0057 |          82.4387 |         -25.3771 |
[32m[20221214 00:25:53 @agent_ppo2.py:185][0m |          -0.0061 |          80.4177 |         -25.3592 |
[32m[20221214 00:25:53 @agent_ppo2.py:185][0m |          -0.0093 |          79.6191 |         -25.3180 |
[32m[20221214 00:25:54 @agent_ppo2.py:185][0m |          -0.0093 |          79.2651 |         -25.6121 |
[32m[20221214 00:25:54 @agent_ppo2.py:185][0m |          -0.0093 |          78.5700 |         -25.4005 |
[32m[20221214 00:25:54 @agent_ppo2.py:185][0m |          -0.0081 |          78.2312 |         -25.3800 |
[32m[20221214 00:25:54 @agent_ppo2.py:185][0m |          -0.0040 |          78.7069 |         -25.5978 |
[32m[20221214 00:25:54 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:25:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 490.80
[32m[20221214 00:25:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 628.36
[32m[20221214 00:25:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 842.83
[32m[20221214 00:25:54 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 842.83
[32m[20221214 00:25:54 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 842.83
[32m[20221214 00:25:54 @agent_ppo2.py:143][0m Total time:      28.39 min
[32m[20221214 00:25:54 @agent_ppo2.py:145][0m 2584576 total steps have happened
[32m[20221214 00:25:54 @agent_ppo2.py:121][0m #------------------------ Iteration 5262 --------------------------#
[32m[20221214 00:25:54 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:25:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:25:54 @agent_ppo2.py:185][0m |          -0.0006 |         106.5815 |         -24.2411 |
[32m[20221214 00:25:54 @agent_ppo2.py:185][0m |          -0.0058 |          95.0768 |         -23.7486 |
[32m[20221214 00:25:55 @agent_ppo2.py:185][0m |          -0.0122 |          91.2955 |         -23.8094 |
[32m[20221214 00:25:55 @agent_ppo2.py:185][0m |          -0.0135 |          88.5022 |         -23.6553 |
[32m[20221214 00:25:55 @agent_ppo2.py:185][0m |          -0.0127 |          86.9932 |         -23.6899 |
[32m[20221214 00:25:55 @agent_ppo2.py:185][0m |          -0.0148 |          84.9299 |         -23.4956 |
[32m[20221214 00:25:55 @agent_ppo2.py:185][0m |          -0.0167 |          83.7505 |         -23.5642 |
[32m[20221214 00:25:55 @agent_ppo2.py:185][0m |          -0.0225 |          82.6474 |         -23.5367 |
[32m[20221214 00:25:55 @agent_ppo2.py:185][0m |          -0.0234 |          82.3807 |         -23.5361 |
[32m[20221214 00:25:55 @agent_ppo2.py:185][0m |          -0.0230 |          80.9695 |         -23.5300 |
[32m[20221214 00:25:55 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:25:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 676.59
[32m[20221214 00:25:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 781.10
[32m[20221214 00:25:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 581.49
[32m[20221214 00:25:55 @agent_ppo2.py:143][0m Total time:      28.42 min
[32m[20221214 00:25:55 @agent_ppo2.py:145][0m 2586624 total steps have happened
[32m[20221214 00:25:55 @agent_ppo2.py:121][0m #------------------------ Iteration 5263 --------------------------#
[32m[20221214 00:25:55 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:25:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:25:56 @agent_ppo2.py:185][0m |           0.0054 |         136.8466 |         -26.3108 |
[32m[20221214 00:25:56 @agent_ppo2.py:185][0m |          -0.0026 |         124.9807 |         -26.2115 |
[32m[20221214 00:25:56 @agent_ppo2.py:185][0m |          -0.0116 |         119.4597 |         -26.1813 |
[32m[20221214 00:25:56 @agent_ppo2.py:185][0m |          -0.0113 |         116.9368 |         -26.2496 |
[32m[20221214 00:25:56 @agent_ppo2.py:185][0m |          -0.0146 |         115.7325 |         -26.1814 |
[32m[20221214 00:25:56 @agent_ppo2.py:185][0m |          -0.0166 |         113.7461 |         -26.3140 |
[32m[20221214 00:25:56 @agent_ppo2.py:185][0m |          -0.0151 |         112.0462 |         -26.2316 |
[32m[20221214 00:25:56 @agent_ppo2.py:185][0m |          -0.0160 |         110.7634 |         -26.0487 |
[32m[20221214 00:25:56 @agent_ppo2.py:185][0m |          -0.0168 |         110.4406 |         -26.3103 |
[32m[20221214 00:25:57 @agent_ppo2.py:185][0m |          -0.0194 |         110.4995 |         -26.2405 |
[32m[20221214 00:25:57 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:25:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 520.61
[32m[20221214 00:25:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 715.41
[32m[20221214 00:25:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 787.91
[32m[20221214 00:25:57 @agent_ppo2.py:143][0m Total time:      28.44 min
[32m[20221214 00:25:57 @agent_ppo2.py:145][0m 2588672 total steps have happened
[32m[20221214 00:25:57 @agent_ppo2.py:121][0m #------------------------ Iteration 5264 --------------------------#
[32m[20221214 00:25:57 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:25:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:25:57 @agent_ppo2.py:185][0m |          -0.0020 |         147.8307 |         -27.0864 |
[32m[20221214 00:25:57 @agent_ppo2.py:185][0m |          -0.0048 |         142.5301 |         -27.2234 |
[32m[20221214 00:25:57 @agent_ppo2.py:185][0m |          -0.0028 |         138.8262 |         -26.9924 |
[32m[20221214 00:25:57 @agent_ppo2.py:185][0m |          -0.0040 |         137.7554 |         -26.8145 |
[32m[20221214 00:25:57 @agent_ppo2.py:185][0m |           0.0035 |         150.8037 |         -26.6946 |
[32m[20221214 00:25:58 @agent_ppo2.py:185][0m |          -0.0080 |         136.8508 |         -26.7396 |
[32m[20221214 00:25:58 @agent_ppo2.py:185][0m |          -0.0044 |         135.0621 |         -26.2047 |
[32m[20221214 00:25:58 @agent_ppo2.py:185][0m |          -0.0090 |         135.5061 |         -26.5199 |
[32m[20221214 00:25:58 @agent_ppo2.py:185][0m |          -0.0042 |         134.0773 |         -26.4599 |
[32m[20221214 00:25:58 @agent_ppo2.py:185][0m |          -0.0050 |         133.4299 |         -26.2072 |
[32m[20221214 00:25:58 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:25:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 678.77
[32m[20221214 00:25:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 763.29
[32m[20221214 00:25:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 168.47
[32m[20221214 00:25:58 @agent_ppo2.py:143][0m Total time:      28.46 min
[32m[20221214 00:25:58 @agent_ppo2.py:145][0m 2590720 total steps have happened
[32m[20221214 00:25:58 @agent_ppo2.py:121][0m #------------------------ Iteration 5265 --------------------------#
[32m[20221214 00:25:58 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:25:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:25:58 @agent_ppo2.py:185][0m |           0.0062 |          47.2499 |         -24.3463 |
[32m[20221214 00:25:59 @agent_ppo2.py:185][0m |          -0.0027 |          37.3228 |         -24.9388 |
[32m[20221214 00:25:59 @agent_ppo2.py:185][0m |          -0.0033 |          34.8565 |         -24.9977 |
[32m[20221214 00:25:59 @agent_ppo2.py:185][0m |          -0.0087 |          32.3727 |         -24.8661 |
[32m[20221214 00:25:59 @agent_ppo2.py:185][0m |          -0.0102 |          30.9718 |         -25.2855 |
[32m[20221214 00:25:59 @agent_ppo2.py:185][0m |          -0.0108 |          30.1164 |         -25.3357 |
[32m[20221214 00:25:59 @agent_ppo2.py:185][0m |          -0.0160 |          29.0518 |         -25.4365 |
[32m[20221214 00:25:59 @agent_ppo2.py:185][0m |          -0.0090 |          28.0367 |         -25.4443 |
[32m[20221214 00:25:59 @agent_ppo2.py:185][0m |          -0.0154 |          27.2745 |         -25.7925 |
[32m[20221214 00:25:59 @agent_ppo2.py:185][0m |          -0.0125 |          26.6318 |         -25.5553 |
[32m[20221214 00:25:59 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:25:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 453.08
[32m[20221214 00:25:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 682.09
[32m[20221214 00:25:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 315.41
[32m[20221214 00:25:59 @agent_ppo2.py:143][0m Total time:      28.48 min
[32m[20221214 00:25:59 @agent_ppo2.py:145][0m 2592768 total steps have happened
[32m[20221214 00:25:59 @agent_ppo2.py:121][0m #------------------------ Iteration 5266 --------------------------#
[32m[20221214 00:26:00 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:26:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:26:00 @agent_ppo2.py:185][0m |          -0.0027 |          46.1234 |         -27.2902 |
[32m[20221214 00:26:00 @agent_ppo2.py:185][0m |          -0.0153 |          37.4297 |         -26.7976 |
[32m[20221214 00:26:00 @agent_ppo2.py:185][0m |          -0.0137 |          33.4109 |         -26.8839 |
[32m[20221214 00:26:00 @agent_ppo2.py:185][0m |          -0.0171 |          31.9250 |         -27.0020 |
[32m[20221214 00:26:00 @agent_ppo2.py:185][0m |          -0.0179 |          30.1220 |         -26.8041 |
[32m[20221214 00:26:00 @agent_ppo2.py:185][0m |          -0.0191 |          29.5792 |         -26.8934 |
[32m[20221214 00:26:00 @agent_ppo2.py:185][0m |          -0.0275 |          28.9291 |         -26.7932 |
[32m[20221214 00:26:00 @agent_ppo2.py:185][0m |          -0.0228 |          27.6169 |         -26.8282 |
[32m[20221214 00:26:01 @agent_ppo2.py:185][0m |          -0.0177 |          28.0280 |         -26.7704 |
[32m[20221214 00:26:01 @agent_ppo2.py:185][0m |          -0.0220 |          26.5518 |         -26.6105 |
[32m[20221214 00:26:01 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:26:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 368.92
[32m[20221214 00:26:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 600.94
[32m[20221214 00:26:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 722.45
[32m[20221214 00:26:01 @agent_ppo2.py:143][0m Total time:      28.51 min
[32m[20221214 00:26:01 @agent_ppo2.py:145][0m 2594816 total steps have happened
[32m[20221214 00:26:01 @agent_ppo2.py:121][0m #------------------------ Iteration 5267 --------------------------#
[32m[20221214 00:26:01 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:26:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:26:01 @agent_ppo2.py:185][0m |          -0.0020 |         164.7380 |         -28.2265 |
[32m[20221214 00:26:01 @agent_ppo2.py:185][0m |          -0.0090 |         155.5630 |         -28.1216 |
[32m[20221214 00:26:01 @agent_ppo2.py:185][0m |          -0.0019 |         166.9908 |         -28.2253 |
[32m[20221214 00:26:01 @agent_ppo2.py:185][0m |          -0.0086 |         151.4002 |         -28.0554 |
[32m[20221214 00:26:02 @agent_ppo2.py:185][0m |          -0.0151 |         149.2856 |         -27.9972 |
[32m[20221214 00:26:02 @agent_ppo2.py:185][0m |          -0.0154 |         147.0792 |         -27.9869 |
[32m[20221214 00:26:02 @agent_ppo2.py:185][0m |          -0.0159 |         146.7003 |         -28.2387 |
[32m[20221214 00:26:02 @agent_ppo2.py:185][0m |          -0.0137 |         146.0470 |         -28.2418 |
[32m[20221214 00:26:02 @agent_ppo2.py:185][0m |          -0.0149 |         144.6683 |         -28.4303 |
[32m[20221214 00:26:02 @agent_ppo2.py:185][0m |          -0.0140 |         143.8443 |         -28.2770 |
[32m[20221214 00:26:02 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:26:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 567.48
[32m[20221214 00:26:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 738.73
[32m[20221214 00:26:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 560.23
[32m[20221214 00:26:02 @agent_ppo2.py:143][0m Total time:      28.53 min
[32m[20221214 00:26:02 @agent_ppo2.py:145][0m 2596864 total steps have happened
[32m[20221214 00:26:02 @agent_ppo2.py:121][0m #------------------------ Iteration 5268 --------------------------#
[32m[20221214 00:26:02 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:26:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:26:03 @agent_ppo2.py:185][0m |          -0.0020 |          51.8228 |         -27.0852 |
[32m[20221214 00:26:03 @agent_ppo2.py:185][0m |          -0.0140 |          41.5048 |         -27.0006 |
[32m[20221214 00:26:03 @agent_ppo2.py:185][0m |          -0.0118 |          38.3402 |         -27.2734 |
[32m[20221214 00:26:03 @agent_ppo2.py:185][0m |          -0.0167 |          35.9494 |         -27.3598 |
[32m[20221214 00:26:03 @agent_ppo2.py:185][0m |          -0.0171 |          34.9325 |         -27.4203 |
[32m[20221214 00:26:03 @agent_ppo2.py:185][0m |          -0.0165 |          33.4009 |         -27.4982 |
[32m[20221214 00:26:03 @agent_ppo2.py:185][0m |          -0.0182 |          32.6514 |         -27.5849 |
[32m[20221214 00:26:03 @agent_ppo2.py:185][0m |          -0.0197 |          31.6423 |         -27.6859 |
[32m[20221214 00:26:03 @agent_ppo2.py:185][0m |          -0.0218 |          30.9673 |         -27.8706 |
[32m[20221214 00:26:03 @agent_ppo2.py:185][0m |          -0.0209 |          30.6275 |         -27.8540 |
[32m[20221214 00:26:03 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:26:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 434.10
[32m[20221214 00:26:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 560.55
[32m[20221214 00:26:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 762.13
[32m[20221214 00:26:03 @agent_ppo2.py:143][0m Total time:      28.55 min
[32m[20221214 00:26:03 @agent_ppo2.py:145][0m 2598912 total steps have happened
[32m[20221214 00:26:03 @agent_ppo2.py:121][0m #------------------------ Iteration 5269 --------------------------#
[32m[20221214 00:26:04 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:26:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:26:04 @agent_ppo2.py:185][0m |           0.0078 |          69.3097 |         -29.6112 |
[32m[20221214 00:26:04 @agent_ppo2.py:185][0m |          -0.0038 |          59.3723 |         -29.7743 |
[32m[20221214 00:26:04 @agent_ppo2.py:185][0m |          -0.0047 |          55.3715 |         -29.6492 |
[32m[20221214 00:26:04 @agent_ppo2.py:185][0m |          -0.0066 |          50.8106 |         -29.6461 |
[32m[20221214 00:26:04 @agent_ppo2.py:185][0m |          -0.0137 |          48.9564 |         -29.9141 |
[32m[20221214 00:26:04 @agent_ppo2.py:185][0m |          -0.0132 |          47.1811 |         -29.8239 |
[32m[20221214 00:26:04 @agent_ppo2.py:185][0m |          -0.0129 |          46.6146 |         -29.8573 |
[32m[20221214 00:26:04 @agent_ppo2.py:185][0m |          -0.0149 |          45.6259 |         -29.7477 |
[32m[20221214 00:26:05 @agent_ppo2.py:185][0m |          -0.0170 |          44.9143 |         -29.8955 |
[32m[20221214 00:26:05 @agent_ppo2.py:185][0m |          -0.0182 |          44.5209 |         -29.5434 |
[32m[20221214 00:26:05 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:26:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 412.16
[32m[20221214 00:26:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 664.03
[32m[20221214 00:26:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 692.22
[32m[20221214 00:26:05 @agent_ppo2.py:143][0m Total time:      28.57 min
[32m[20221214 00:26:05 @agent_ppo2.py:145][0m 2600960 total steps have happened
[32m[20221214 00:26:05 @agent_ppo2.py:121][0m #------------------------ Iteration 5270 --------------------------#
[32m[20221214 00:26:05 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:26:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:26:05 @agent_ppo2.py:185][0m |           0.0006 |         125.3858 |         -31.3143 |
[32m[20221214 00:26:05 @agent_ppo2.py:185][0m |          -0.0070 |         115.1211 |         -31.3506 |
[32m[20221214 00:26:05 @agent_ppo2.py:185][0m |          -0.0054 |         111.1533 |         -31.0489 |
[32m[20221214 00:26:05 @agent_ppo2.py:185][0m |          -0.0127 |         107.9000 |         -31.2633 |
[32m[20221214 00:26:06 @agent_ppo2.py:185][0m |          -0.0113 |         105.7075 |         -31.2032 |
[32m[20221214 00:26:06 @agent_ppo2.py:185][0m |          -0.0101 |         103.4137 |         -31.2339 |
[32m[20221214 00:26:06 @agent_ppo2.py:185][0m |           0.0093 |         124.1891 |         -31.1192 |
[32m[20221214 00:26:06 @agent_ppo2.py:185][0m |          -0.0104 |         109.9178 |         -31.1513 |
[32m[20221214 00:26:06 @agent_ppo2.py:185][0m |          -0.0126 |         101.3688 |         -31.4297 |
[32m[20221214 00:26:06 @agent_ppo2.py:185][0m |          -0.0164 |         100.3268 |         -31.1413 |
[32m[20221214 00:26:06 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:26:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 590.41
[32m[20221214 00:26:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 814.40
[32m[20221214 00:26:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 724.22
[32m[20221214 00:26:06 @agent_ppo2.py:143][0m Total time:      28.60 min
[32m[20221214 00:26:06 @agent_ppo2.py:145][0m 2603008 total steps have happened
[32m[20221214 00:26:06 @agent_ppo2.py:121][0m #------------------------ Iteration 5271 --------------------------#
[32m[20221214 00:26:06 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:26:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:26:07 @agent_ppo2.py:185][0m |          -0.0001 |         146.1989 |         -29.3441 |
[32m[20221214 00:26:07 @agent_ppo2.py:185][0m |           0.0016 |         142.7440 |         -29.0414 |
[32m[20221214 00:26:07 @agent_ppo2.py:185][0m |          -0.0069 |         140.0937 |         -29.1041 |
[32m[20221214 00:26:07 @agent_ppo2.py:185][0m |           0.0054 |         146.8745 |         -29.1000 |
[32m[20221214 00:26:07 @agent_ppo2.py:185][0m |          -0.0063 |         137.9198 |         -28.9980 |
[32m[20221214 00:26:07 @agent_ppo2.py:185][0m |          -0.0089 |         137.0792 |         -29.0662 |
[32m[20221214 00:26:07 @agent_ppo2.py:185][0m |          -0.0087 |         135.8117 |         -28.9649 |
[32m[20221214 00:26:07 @agent_ppo2.py:185][0m |          -0.0075 |         134.1156 |         -28.9114 |
[32m[20221214 00:26:07 @agent_ppo2.py:185][0m |          -0.0058 |         133.4169 |         -28.8747 |
[32m[20221214 00:26:07 @agent_ppo2.py:185][0m |          -0.0065 |         132.9697 |         -28.8306 |
[32m[20221214 00:26:07 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:26:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 784.17
[32m[20221214 00:26:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 818.32
[32m[20221214 00:26:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 647.93
[32m[20221214 00:26:08 @agent_ppo2.py:143][0m Total time:      28.62 min
[32m[20221214 00:26:08 @agent_ppo2.py:145][0m 2605056 total steps have happened
[32m[20221214 00:26:08 @agent_ppo2.py:121][0m #------------------------ Iteration 5272 --------------------------#
[32m[20221214 00:26:08 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:26:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:26:08 @agent_ppo2.py:185][0m |           0.0017 |         154.9129 |         -31.3822 |
[32m[20221214 00:26:08 @agent_ppo2.py:185][0m |          -0.0027 |         145.2827 |         -31.5133 |
[32m[20221214 00:26:08 @agent_ppo2.py:185][0m |          -0.0020 |         142.4107 |         -31.2705 |
[32m[20221214 00:26:08 @agent_ppo2.py:185][0m |          -0.0044 |         140.2043 |         -31.0795 |
[32m[20221214 00:26:08 @agent_ppo2.py:185][0m |          -0.0083 |         142.4663 |         -31.5255 |
[32m[20221214 00:26:08 @agent_ppo2.py:185][0m |          -0.0075 |         140.3667 |         -31.2570 |
[32m[20221214 00:26:08 @agent_ppo2.py:185][0m |          -0.0085 |         137.9765 |         -31.3873 |
[32m[20221214 00:26:09 @agent_ppo2.py:185][0m |          -0.0099 |         135.0961 |         -31.3085 |
[32m[20221214 00:26:09 @agent_ppo2.py:185][0m |          -0.0082 |         135.6550 |         -31.3098 |
[32m[20221214 00:26:09 @agent_ppo2.py:185][0m |          -0.0160 |         135.1979 |         -31.4177 |
[32m[20221214 00:26:09 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:26:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 602.31
[32m[20221214 00:26:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 783.32
[32m[20221214 00:26:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 765.32
[32m[20221214 00:26:09 @agent_ppo2.py:143][0m Total time:      28.64 min
[32m[20221214 00:26:09 @agent_ppo2.py:145][0m 2607104 total steps have happened
[32m[20221214 00:26:09 @agent_ppo2.py:121][0m #------------------------ Iteration 5273 --------------------------#
[32m[20221214 00:26:09 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:26:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:26:09 @agent_ppo2.py:185][0m |           0.0026 |          50.1518 |         -30.1024 |
[32m[20221214 00:26:09 @agent_ppo2.py:185][0m |          -0.0108 |          34.6239 |         -30.0649 |
[32m[20221214 00:26:09 @agent_ppo2.py:185][0m |          -0.0118 |          31.1664 |         -30.0258 |
[32m[20221214 00:26:09 @agent_ppo2.py:185][0m |          -0.0109 |          29.2334 |         -30.0964 |
[32m[20221214 00:26:10 @agent_ppo2.py:185][0m |          -0.0151 |          28.6489 |         -30.0926 |
[32m[20221214 00:26:10 @agent_ppo2.py:185][0m |          -0.0108 |          28.0924 |         -30.1614 |
[32m[20221214 00:26:10 @agent_ppo2.py:185][0m |          -0.0152 |          26.3404 |         -30.2765 |
[32m[20221214 00:26:10 @agent_ppo2.py:185][0m |          -0.0188 |          25.8589 |         -30.1431 |
[32m[20221214 00:26:10 @agent_ppo2.py:185][0m |          -0.0166 |          25.3322 |         -30.3921 |
[32m[20221214 00:26:10 @agent_ppo2.py:185][0m |          -0.0252 |          25.1335 |         -30.2561 |
[32m[20221214 00:26:10 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:26:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 514.08
[32m[20221214 00:26:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 655.07
[32m[20221214 00:26:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 680.61
[32m[20221214 00:26:10 @agent_ppo2.py:143][0m Total time:      28.66 min
[32m[20221214 00:26:10 @agent_ppo2.py:145][0m 2609152 total steps have happened
[32m[20221214 00:26:10 @agent_ppo2.py:121][0m #------------------------ Iteration 5274 --------------------------#
[32m[20221214 00:26:10 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:26:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:26:11 @agent_ppo2.py:185][0m |           0.0138 |          56.8612 |         -32.2309 |
[32m[20221214 00:26:11 @agent_ppo2.py:185][0m |          -0.0039 |          45.3494 |         -32.0501 |
[32m[20221214 00:26:11 @agent_ppo2.py:185][0m |          -0.0007 |          40.4073 |         -32.3334 |
[32m[20221214 00:26:11 @agent_ppo2.py:185][0m |          -0.0103 |          39.4097 |         -32.1675 |
[32m[20221214 00:26:11 @agent_ppo2.py:185][0m |          -0.0077 |          38.4071 |         -32.0992 |
[32m[20221214 00:26:11 @agent_ppo2.py:185][0m |          -0.0142 |          37.8482 |         -32.0905 |
[32m[20221214 00:26:11 @agent_ppo2.py:185][0m |          -0.0146 |          36.5200 |         -31.9553 |
[32m[20221214 00:26:11 @agent_ppo2.py:185][0m |          -0.0146 |          35.9490 |         -31.9432 |
[32m[20221214 00:26:11 @agent_ppo2.py:185][0m |          -0.0149 |          35.9718 |         -31.9400 |
[32m[20221214 00:26:11 @agent_ppo2.py:185][0m |          -0.0232 |          34.4109 |         -31.8119 |
[32m[20221214 00:26:11 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:26:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 482.71
[32m[20221214 00:26:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 634.63
[32m[20221214 00:26:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 672.52
[32m[20221214 00:26:12 @agent_ppo2.py:143][0m Total time:      28.69 min
[32m[20221214 00:26:12 @agent_ppo2.py:145][0m 2611200 total steps have happened
[32m[20221214 00:26:12 @agent_ppo2.py:121][0m #------------------------ Iteration 5275 --------------------------#
[32m[20221214 00:26:12 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:26:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:26:12 @agent_ppo2.py:185][0m |           0.0006 |         112.8030 |         -29.1772 |
[32m[20221214 00:26:12 @agent_ppo2.py:185][0m |          -0.0069 |         104.0044 |         -29.3237 |
[32m[20221214 00:26:12 @agent_ppo2.py:185][0m |          -0.0060 |          99.8388 |         -29.1476 |
[32m[20221214 00:26:12 @agent_ppo2.py:185][0m |          -0.0118 |          97.8735 |         -29.0011 |
[32m[20221214 00:26:12 @agent_ppo2.py:185][0m |          -0.0111 |          96.3460 |         -29.0852 |
[32m[20221214 00:26:12 @agent_ppo2.py:185][0m |          -0.0119 |          95.2416 |         -29.1821 |
[32m[20221214 00:26:12 @agent_ppo2.py:185][0m |          -0.0142 |          94.6628 |         -29.0739 |
[32m[20221214 00:26:13 @agent_ppo2.py:185][0m |          -0.0150 |          94.1801 |         -28.9836 |
[32m[20221214 00:26:13 @agent_ppo2.py:185][0m |          -0.0172 |          93.9670 |         -28.9162 |
[32m[20221214 00:26:13 @agent_ppo2.py:185][0m |          -0.0156 |          93.0135 |         -29.0831 |
[32m[20221214 00:26:13 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:26:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 626.95
[32m[20221214 00:26:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 741.20
[32m[20221214 00:26:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 627.54
[32m[20221214 00:26:13 @agent_ppo2.py:143][0m Total time:      28.71 min
[32m[20221214 00:26:13 @agent_ppo2.py:145][0m 2613248 total steps have happened
[32m[20221214 00:26:13 @agent_ppo2.py:121][0m #------------------------ Iteration 5276 --------------------------#
[32m[20221214 00:26:13 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:26:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:26:13 @agent_ppo2.py:185][0m |           0.0048 |          80.8074 |         -31.8014 |
[32m[20221214 00:26:13 @agent_ppo2.py:185][0m |          -0.0054 |          70.0731 |         -31.9312 |
[32m[20221214 00:26:13 @agent_ppo2.py:185][0m |          -0.0016 |          66.8844 |         -32.0330 |
[32m[20221214 00:26:14 @agent_ppo2.py:185][0m |          -0.0078 |          64.7563 |         -32.1058 |
[32m[20221214 00:26:14 @agent_ppo2.py:185][0m |          -0.0104 |          61.6227 |         -31.9471 |
[32m[20221214 00:26:14 @agent_ppo2.py:185][0m |          -0.0154 |          62.5145 |         -32.0479 |
[32m[20221214 00:26:14 @agent_ppo2.py:185][0m |          -0.0141 |          62.8282 |         -32.0135 |
[32m[20221214 00:26:14 @agent_ppo2.py:185][0m |          -0.0148 |          59.1020 |         -32.0542 |
[32m[20221214 00:26:14 @agent_ppo2.py:185][0m |           0.0002 |          63.9723 |         -32.1722 |
[32m[20221214 00:26:14 @agent_ppo2.py:185][0m |          -0.0183 |          59.9024 |         -32.1082 |
[32m[20221214 00:26:14 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:26:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 357.31
[32m[20221214 00:26:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 461.60
[32m[20221214 00:26:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 813.58
[32m[20221214 00:26:14 @agent_ppo2.py:143][0m Total time:      28.73 min
[32m[20221214 00:26:14 @agent_ppo2.py:145][0m 2615296 total steps have happened
[32m[20221214 00:26:14 @agent_ppo2.py:121][0m #------------------------ Iteration 5277 --------------------------#
[32m[20221214 00:26:14 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:26:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:26:15 @agent_ppo2.py:185][0m |           0.0191 |         142.4460 |         -30.5468 |
[32m[20221214 00:26:15 @agent_ppo2.py:185][0m |          -0.0043 |         125.3034 |         -30.6381 |
[32m[20221214 00:26:15 @agent_ppo2.py:185][0m |          -0.0076 |         122.1576 |         -30.5743 |
[32m[20221214 00:26:15 @agent_ppo2.py:185][0m |           0.0102 |         134.3468 |         -30.3995 |
[32m[20221214 00:26:15 @agent_ppo2.py:185][0m |          -0.0139 |         118.9481 |         -30.4304 |
[32m[20221214 00:26:15 @agent_ppo2.py:185][0m |          -0.0092 |         116.9229 |         -30.2587 |
[32m[20221214 00:26:15 @agent_ppo2.py:185][0m |          -0.0124 |         116.0376 |         -30.2997 |
[32m[20221214 00:26:15 @agent_ppo2.py:185][0m |          -0.0104 |         115.4941 |         -30.4661 |
[32m[20221214 00:26:15 @agent_ppo2.py:185][0m |          -0.0104 |         113.7982 |         -30.1457 |
[32m[20221214 00:26:16 @agent_ppo2.py:185][0m |          -0.0113 |         113.5619 |         -30.3489 |
[32m[20221214 00:26:16 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:26:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 652.00
[32m[20221214 00:26:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 725.42
[32m[20221214 00:26:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 795.97
[32m[20221214 00:26:16 @agent_ppo2.py:143][0m Total time:      28.75 min
[32m[20221214 00:26:16 @agent_ppo2.py:145][0m 2617344 total steps have happened
[32m[20221214 00:26:16 @agent_ppo2.py:121][0m #------------------------ Iteration 5278 --------------------------#
[32m[20221214 00:26:16 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:26:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:26:16 @agent_ppo2.py:185][0m |           0.0053 |          80.8533 |         -29.6964 |
[32m[20221214 00:26:16 @agent_ppo2.py:185][0m |          -0.0009 |          70.4658 |         -29.3608 |
[32m[20221214 00:26:16 @agent_ppo2.py:185][0m |           0.0031 |          80.7760 |         -29.5555 |
[32m[20221214 00:26:16 @agent_ppo2.py:185][0m |          -0.0126 |          65.7187 |         -29.1447 |
[32m[20221214 00:26:16 @agent_ppo2.py:185][0m |          -0.0122 |          65.0633 |         -29.4305 |
[32m[20221214 00:26:16 @agent_ppo2.py:185][0m |          -0.0189 |          64.5898 |         -29.5867 |
[32m[20221214 00:26:17 @agent_ppo2.py:185][0m |          -0.0146 |          65.8106 |         -29.5525 |
[32m[20221214 00:26:17 @agent_ppo2.py:185][0m |          -0.0182 |          63.8801 |         -29.2369 |
[32m[20221214 00:26:17 @agent_ppo2.py:185][0m |          -0.0233 |          62.2595 |         -29.6868 |
[32m[20221214 00:26:17 @agent_ppo2.py:185][0m |          -0.0215 |          62.0538 |         -29.5839 |
[32m[20221214 00:26:17 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:26:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 513.50
[32m[20221214 00:26:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 677.53
[32m[20221214 00:26:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 644.18
[32m[20221214 00:26:17 @agent_ppo2.py:143][0m Total time:      28.78 min
[32m[20221214 00:26:17 @agent_ppo2.py:145][0m 2619392 total steps have happened
[32m[20221214 00:26:17 @agent_ppo2.py:121][0m #------------------------ Iteration 5279 --------------------------#
[32m[20221214 00:26:17 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:26:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:26:17 @agent_ppo2.py:185][0m |           0.0021 |         146.7332 |         -30.4061 |
[32m[20221214 00:26:17 @agent_ppo2.py:185][0m |           0.0054 |         142.5591 |         -30.1833 |
[32m[20221214 00:26:18 @agent_ppo2.py:185][0m |          -0.0071 |         135.1394 |         -30.1207 |
[32m[20221214 00:26:18 @agent_ppo2.py:185][0m |          -0.0027 |         136.0188 |         -30.1882 |
[32m[20221214 00:26:18 @agent_ppo2.py:185][0m |           0.0009 |         140.8309 |         -30.4851 |
[32m[20221214 00:26:18 @agent_ppo2.py:185][0m |          -0.0086 |         130.7122 |         -30.2023 |
[32m[20221214 00:26:18 @agent_ppo2.py:185][0m |          -0.0083 |         129.7229 |         -30.4092 |
[32m[20221214 00:26:18 @agent_ppo2.py:185][0m |          -0.0009 |         141.2140 |         -30.2773 |
[32m[20221214 00:26:18 @agent_ppo2.py:185][0m |          -0.0112 |         129.0774 |         -30.0634 |
[32m[20221214 00:26:18 @agent_ppo2.py:185][0m |          -0.0121 |         128.8202 |         -30.3281 |
[32m[20221214 00:26:18 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:26:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 657.19
[32m[20221214 00:26:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 768.88
[32m[20221214 00:26:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 686.62
[32m[20221214 00:26:18 @agent_ppo2.py:143][0m Total time:      28.80 min
[32m[20221214 00:26:18 @agent_ppo2.py:145][0m 2621440 total steps have happened
[32m[20221214 00:26:18 @agent_ppo2.py:121][0m #------------------------ Iteration 5280 --------------------------#
[32m[20221214 00:26:19 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:26:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:26:19 @agent_ppo2.py:185][0m |          -0.0031 |         124.1972 |         -31.4217 |
[32m[20221214 00:26:19 @agent_ppo2.py:185][0m |          -0.0035 |         116.2430 |         -31.2403 |
[32m[20221214 00:26:19 @agent_ppo2.py:185][0m |          -0.0113 |         113.1515 |         -31.2872 |
[32m[20221214 00:26:19 @agent_ppo2.py:185][0m |           0.0011 |         120.6134 |         -31.3754 |
[32m[20221214 00:26:19 @agent_ppo2.py:185][0m |          -0.0079 |         109.7762 |         -31.2534 |
[32m[20221214 00:26:19 @agent_ppo2.py:185][0m |          -0.0144 |         108.2995 |         -31.4385 |
[32m[20221214 00:26:19 @agent_ppo2.py:185][0m |          -0.0152 |         107.5359 |         -31.4974 |
[32m[20221214 00:26:19 @agent_ppo2.py:185][0m |          -0.0079 |         111.5010 |         -31.6058 |
[32m[20221214 00:26:19 @agent_ppo2.py:185][0m |          -0.0108 |         106.3894 |         -31.6322 |
[32m[20221214 00:26:20 @agent_ppo2.py:185][0m |          -0.0160 |         105.5213 |         -31.5221 |
[32m[20221214 00:26:20 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:26:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 591.10
[32m[20221214 00:26:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 709.07
[32m[20221214 00:26:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 647.05
[32m[20221214 00:26:20 @agent_ppo2.py:143][0m Total time:      28.82 min
[32m[20221214 00:26:20 @agent_ppo2.py:145][0m 2623488 total steps have happened
[32m[20221214 00:26:20 @agent_ppo2.py:121][0m #------------------------ Iteration 5281 --------------------------#
[32m[20221214 00:26:20 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:26:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:26:20 @agent_ppo2.py:185][0m |           0.0019 |         120.9530 |         -32.1607 |
[32m[20221214 00:26:20 @agent_ppo2.py:185][0m |          -0.0054 |         109.6078 |         -32.2668 |
[32m[20221214 00:26:20 @agent_ppo2.py:185][0m |          -0.0077 |         105.8106 |         -32.0041 |
[32m[20221214 00:26:20 @agent_ppo2.py:185][0m |          -0.0123 |         103.4458 |         -32.2800 |
[32m[20221214 00:26:20 @agent_ppo2.py:185][0m |          -0.0102 |         101.2593 |         -32.1515 |
[32m[20221214 00:26:20 @agent_ppo2.py:185][0m |          -0.0068 |         100.9058 |         -32.3248 |
[32m[20221214 00:26:21 @agent_ppo2.py:185][0m |          -0.0147 |          99.1330 |         -32.3715 |
[32m[20221214 00:26:21 @agent_ppo2.py:185][0m |          -0.0111 |          98.4705 |         -32.3120 |
[32m[20221214 00:26:21 @agent_ppo2.py:185][0m |          -0.0150 |          97.7836 |         -32.4544 |
[32m[20221214 00:26:21 @agent_ppo2.py:185][0m |          -0.0173 |          97.2140 |         -32.2892 |
[32m[20221214 00:26:21 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:26:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 353.67
[32m[20221214 00:26:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 478.68
[32m[20221214 00:26:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 777.71
[32m[20221214 00:26:21 @agent_ppo2.py:143][0m Total time:      28.84 min
[32m[20221214 00:26:21 @agent_ppo2.py:145][0m 2625536 total steps have happened
[32m[20221214 00:26:21 @agent_ppo2.py:121][0m #------------------------ Iteration 5282 --------------------------#
[32m[20221214 00:26:21 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:26:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:26:21 @agent_ppo2.py:185][0m |           0.0036 |          56.3287 |         -31.6859 |
[32m[20221214 00:26:21 @agent_ppo2.py:185][0m |          -0.0071 |          47.5319 |         -31.5053 |
[32m[20221214 00:26:22 @agent_ppo2.py:185][0m |          -0.0080 |          46.5761 |         -31.4191 |
[32m[20221214 00:26:22 @agent_ppo2.py:185][0m |          -0.0066 |          44.8924 |         -31.8511 |
[32m[20221214 00:26:22 @agent_ppo2.py:185][0m |          -0.0023 |          44.8949 |         -31.7812 |
[32m[20221214 00:26:22 @agent_ppo2.py:185][0m |          -0.0140 |          43.3326 |         -31.8747 |
[32m[20221214 00:26:22 @agent_ppo2.py:185][0m |          -0.0179 |          42.2635 |         -31.8226 |
[32m[20221214 00:26:22 @agent_ppo2.py:185][0m |          -0.0151 |          42.7612 |         -31.9837 |
[32m[20221214 00:26:22 @agent_ppo2.py:185][0m |          -0.0162 |          41.4247 |         -31.9826 |
[32m[20221214 00:26:22 @agent_ppo2.py:185][0m |          -0.0264 |          40.8711 |         -31.7808 |
[32m[20221214 00:26:22 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:26:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 414.51
[32m[20221214 00:26:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 573.05
[32m[20221214 00:26:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 716.67
[32m[20221214 00:26:22 @agent_ppo2.py:143][0m Total time:      28.87 min
[32m[20221214 00:26:22 @agent_ppo2.py:145][0m 2627584 total steps have happened
[32m[20221214 00:26:22 @agent_ppo2.py:121][0m #------------------------ Iteration 5283 --------------------------#
[32m[20221214 00:26:23 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:26:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:26:23 @agent_ppo2.py:185][0m |          -0.0020 |          57.1674 |         -34.0017 |
[32m[20221214 00:26:23 @agent_ppo2.py:185][0m |          -0.0048 |          53.2424 |         -34.0036 |
[32m[20221214 00:26:23 @agent_ppo2.py:185][0m |           0.0005 |          52.2980 |         -33.7333 |
[32m[20221214 00:26:23 @agent_ppo2.py:185][0m |           0.0056 |          60.2708 |         -33.6318 |
[32m[20221214 00:26:23 @agent_ppo2.py:185][0m |          -0.0028 |          51.8637 |         -33.7495 |
[32m[20221214 00:26:23 @agent_ppo2.py:185][0m |          -0.0073 |          51.0595 |         -33.5905 |
[32m[20221214 00:26:23 @agent_ppo2.py:185][0m |          -0.0123 |          49.2537 |         -33.6034 |
[32m[20221214 00:26:23 @agent_ppo2.py:185][0m |          -0.0101 |          49.3861 |         -33.5524 |
[32m[20221214 00:26:23 @agent_ppo2.py:185][0m |          -0.0121 |          48.6202 |         -33.5984 |
[32m[20221214 00:26:24 @agent_ppo2.py:185][0m |          -0.0163 |          48.1780 |         -33.4682 |
[32m[20221214 00:26:24 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:26:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 488.68
[32m[20221214 00:26:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 640.57
[32m[20221214 00:26:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 776.37
[32m[20221214 00:26:24 @agent_ppo2.py:143][0m Total time:      28.89 min
[32m[20221214 00:26:24 @agent_ppo2.py:145][0m 2629632 total steps have happened
[32m[20221214 00:26:24 @agent_ppo2.py:121][0m #------------------------ Iteration 5284 --------------------------#
[32m[20221214 00:26:24 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:26:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:26:24 @agent_ppo2.py:185][0m |           0.0024 |         113.9565 |         -34.7049 |
[32m[20221214 00:26:24 @agent_ppo2.py:185][0m |          -0.0030 |         108.5174 |         -34.6314 |
[32m[20221214 00:26:24 @agent_ppo2.py:185][0m |          -0.0037 |         104.9736 |         -34.6039 |
[32m[20221214 00:26:24 @agent_ppo2.py:185][0m |          -0.0009 |         103.4634 |         -34.8288 |
[32m[20221214 00:26:24 @agent_ppo2.py:185][0m |          -0.0077 |         102.3314 |         -34.6963 |
[32m[20221214 00:26:25 @agent_ppo2.py:185][0m |          -0.0090 |         100.8645 |         -34.5122 |
[32m[20221214 00:26:25 @agent_ppo2.py:185][0m |          -0.0070 |          99.5574 |         -34.6450 |
[32m[20221214 00:26:25 @agent_ppo2.py:185][0m |          -0.0098 |         101.7209 |         -34.7132 |
[32m[20221214 00:26:25 @agent_ppo2.py:185][0m |          -0.0125 |          99.5265 |         -34.7766 |
[32m[20221214 00:26:25 @agent_ppo2.py:185][0m |          -0.0172 |         100.4518 |         -34.7870 |
[32m[20221214 00:26:25 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:26:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 497.80
[32m[20221214 00:26:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 612.39
[32m[20221214 00:26:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 836.50
[32m[20221214 00:26:25 @agent_ppo2.py:143][0m Total time:      28.91 min
[32m[20221214 00:26:25 @agent_ppo2.py:145][0m 2631680 total steps have happened
[32m[20221214 00:26:25 @agent_ppo2.py:121][0m #------------------------ Iteration 5285 --------------------------#
[32m[20221214 00:26:25 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:26:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:26:25 @agent_ppo2.py:185][0m |           0.0031 |         108.6010 |         -32.8942 |
[32m[20221214 00:26:25 @agent_ppo2.py:185][0m |           0.0003 |          97.1820 |         -32.6328 |
[32m[20221214 00:26:26 @agent_ppo2.py:185][0m |          -0.0020 |          93.1251 |         -32.3316 |
[32m[20221214 00:26:26 @agent_ppo2.py:185][0m |           0.0024 |         105.0228 |         -32.2818 |
[32m[20221214 00:26:26 @agent_ppo2.py:185][0m |           0.0003 |          96.0279 |         -32.3613 |
[32m[20221214 00:26:26 @agent_ppo2.py:185][0m |          -0.0085 |          87.4870 |         -32.1375 |
[32m[20221214 00:26:26 @agent_ppo2.py:185][0m |          -0.0133 |          85.5484 |         -32.2804 |
[32m[20221214 00:26:26 @agent_ppo2.py:185][0m |          -0.0130 |          84.8397 |         -32.6805 |
[32m[20221214 00:26:26 @agent_ppo2.py:185][0m |          -0.0137 |          83.9927 |         -32.2053 |
[32m[20221214 00:26:26 @agent_ppo2.py:185][0m |          -0.0156 |          83.8360 |         -32.1152 |
[32m[20221214 00:26:26 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:26:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 611.62
[32m[20221214 00:26:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 694.74
[32m[20221214 00:26:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 794.41
[32m[20221214 00:26:26 @agent_ppo2.py:143][0m Total time:      28.93 min
[32m[20221214 00:26:26 @agent_ppo2.py:145][0m 2633728 total steps have happened
[32m[20221214 00:26:26 @agent_ppo2.py:121][0m #------------------------ Iteration 5286 --------------------------#
[32m[20221214 00:26:27 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:26:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:26:27 @agent_ppo2.py:185][0m |           0.0009 |         108.6588 |         -32.4058 |
[32m[20221214 00:26:27 @agent_ppo2.py:185][0m |          -0.0114 |          98.8215 |         -32.0501 |
[32m[20221214 00:26:27 @agent_ppo2.py:185][0m |          -0.0118 |          95.5448 |         -31.8707 |
[32m[20221214 00:26:27 @agent_ppo2.py:185][0m |          -0.0164 |          92.8303 |         -31.7515 |
[32m[20221214 00:26:27 @agent_ppo2.py:185][0m |          -0.0183 |          91.8161 |         -31.8271 |
[32m[20221214 00:26:27 @agent_ppo2.py:185][0m |          -0.0152 |          90.0654 |         -31.6474 |
[32m[20221214 00:26:27 @agent_ppo2.py:185][0m |          -0.0179 |          89.0553 |         -31.6895 |
[32m[20221214 00:26:27 @agent_ppo2.py:185][0m |          -0.0178 |          88.2608 |         -31.5825 |
[32m[20221214 00:26:27 @agent_ppo2.py:185][0m |          -0.0181 |          86.9810 |         -31.7297 |
[32m[20221214 00:26:28 @agent_ppo2.py:185][0m |          -0.0197 |          86.6436 |         -31.5380 |
[32m[20221214 00:26:28 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:26:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 500.14
[32m[20221214 00:26:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 655.64
[32m[20221214 00:26:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 629.14
[32m[20221214 00:26:28 @agent_ppo2.py:143][0m Total time:      28.96 min
[32m[20221214 00:26:28 @agent_ppo2.py:145][0m 2635776 total steps have happened
[32m[20221214 00:26:28 @agent_ppo2.py:121][0m #------------------------ Iteration 5287 --------------------------#
[32m[20221214 00:26:28 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:26:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:26:28 @agent_ppo2.py:185][0m |           0.0007 |         158.3749 |         -33.5612 |
[32m[20221214 00:26:28 @agent_ppo2.py:185][0m |          -0.0061 |         147.9642 |         -33.3905 |
[32m[20221214 00:26:28 @agent_ppo2.py:185][0m |          -0.0039 |         143.2743 |         -33.5806 |
[32m[20221214 00:26:28 @agent_ppo2.py:185][0m |          -0.0126 |         141.1186 |         -33.3620 |
[32m[20221214 00:26:28 @agent_ppo2.py:185][0m |          -0.0119 |         140.0006 |         -33.4565 |
[32m[20221214 00:26:29 @agent_ppo2.py:185][0m |           0.0064 |         160.3021 |         -33.3373 |
[32m[20221214 00:26:29 @agent_ppo2.py:185][0m |          -0.0103 |         139.3344 |         -33.4672 |
[32m[20221214 00:26:29 @agent_ppo2.py:185][0m |          -0.0086 |         136.2492 |         -33.2193 |
[32m[20221214 00:26:29 @agent_ppo2.py:185][0m |           0.0018 |         156.6574 |         -33.4617 |
[32m[20221214 00:26:29 @agent_ppo2.py:185][0m |          -0.0143 |         134.9849 |         -33.3915 |
[32m[20221214 00:26:29 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:26:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 612.52
[32m[20221214 00:26:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 743.11
[32m[20221214 00:26:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 823.35
[32m[20221214 00:26:29 @agent_ppo2.py:143][0m Total time:      28.98 min
[32m[20221214 00:26:29 @agent_ppo2.py:145][0m 2637824 total steps have happened
[32m[20221214 00:26:29 @agent_ppo2.py:121][0m #------------------------ Iteration 5288 --------------------------#
[32m[20221214 00:26:29 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:26:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:26:29 @agent_ppo2.py:185][0m |           0.0045 |         103.4695 |         -34.4469 |
[32m[20221214 00:26:29 @agent_ppo2.py:185][0m |          -0.0031 |          88.0142 |         -33.8918 |
[32m[20221214 00:26:30 @agent_ppo2.py:185][0m |          -0.0027 |          84.2349 |         -34.0207 |
[32m[20221214 00:26:30 @agent_ppo2.py:185][0m |          -0.0088 |          82.3794 |         -34.0026 |
[32m[20221214 00:26:30 @agent_ppo2.py:185][0m |          -0.0030 |          84.6415 |         -33.6100 |
[32m[20221214 00:26:30 @agent_ppo2.py:185][0m |          -0.0073 |          82.3346 |         -33.4514 |
[32m[20221214 00:26:30 @agent_ppo2.py:185][0m |          -0.0152 |          79.7318 |         -33.3739 |
[32m[20221214 00:26:30 @agent_ppo2.py:185][0m |          -0.0079 |          80.9766 |         -33.7122 |
[32m[20221214 00:26:30 @agent_ppo2.py:185][0m |          -0.0153 |          77.2917 |         -32.9857 |
[32m[20221214 00:26:30 @agent_ppo2.py:185][0m |          -0.0196 |          77.2067 |         -33.0446 |
[32m[20221214 00:26:30 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:26:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 608.45
[32m[20221214 00:26:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 719.27
[32m[20221214 00:26:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 887.54
[32m[20221214 00:26:30 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 887.54
[32m[20221214 00:26:30 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 887.54
[32m[20221214 00:26:30 @agent_ppo2.py:143][0m Total time:      29.00 min
[32m[20221214 00:26:30 @agent_ppo2.py:145][0m 2639872 total steps have happened
[32m[20221214 00:26:30 @agent_ppo2.py:121][0m #------------------------ Iteration 5289 --------------------------#
[32m[20221214 00:26:31 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:26:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:26:31 @agent_ppo2.py:185][0m |           0.0050 |         102.4043 |         -33.5737 |
[32m[20221214 00:26:31 @agent_ppo2.py:185][0m |           0.0009 |          90.8741 |         -33.4213 |
[32m[20221214 00:26:31 @agent_ppo2.py:185][0m |           0.0097 |         105.1217 |         -33.7729 |
[32m[20221214 00:26:31 @agent_ppo2.py:185][0m |          -0.0086 |          88.5328 |         -33.5823 |
[32m[20221214 00:26:31 @agent_ppo2.py:185][0m |          -0.0061 |          85.2269 |         -33.7137 |
[32m[20221214 00:26:31 @agent_ppo2.py:185][0m |          -0.0112 |          82.6385 |         -33.9505 |
[32m[20221214 00:26:31 @agent_ppo2.py:185][0m |          -0.0094 |          81.5671 |         -33.7103 |
[32m[20221214 00:26:31 @agent_ppo2.py:185][0m |          -0.0098 |          80.7254 |         -33.8290 |
[32m[20221214 00:26:32 @agent_ppo2.py:185][0m |          -0.0119 |          82.1107 |         -34.0933 |
[32m[20221214 00:26:32 @agent_ppo2.py:185][0m |          -0.0156 |          81.8092 |         -34.0472 |
[32m[20221214 00:26:32 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:26:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 562.94
[32m[20221214 00:26:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 663.12
[32m[20221214 00:26:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 836.06
[32m[20221214 00:26:32 @agent_ppo2.py:143][0m Total time:      29.02 min
[32m[20221214 00:26:32 @agent_ppo2.py:145][0m 2641920 total steps have happened
[32m[20221214 00:26:32 @agent_ppo2.py:121][0m #------------------------ Iteration 5290 --------------------------#
[32m[20221214 00:26:32 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:26:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:26:32 @agent_ppo2.py:185][0m |           0.0026 |         124.5605 |         -31.6807 |
[32m[20221214 00:26:32 @agent_ppo2.py:185][0m |           0.0006 |         114.7068 |         -31.7801 |
[32m[20221214 00:26:32 @agent_ppo2.py:185][0m |          -0.0070 |         109.1030 |         -31.6997 |
[32m[20221214 00:26:32 @agent_ppo2.py:185][0m |          -0.0068 |         107.3518 |         -31.6772 |
[32m[20221214 00:26:32 @agent_ppo2.py:185][0m |          -0.0110 |         104.6350 |         -31.5751 |
[32m[20221214 00:26:33 @agent_ppo2.py:185][0m |          -0.0058 |         104.8400 |         -31.6110 |
[32m[20221214 00:26:33 @agent_ppo2.py:185][0m |          -0.0155 |         104.2405 |         -31.7604 |
[32m[20221214 00:26:33 @agent_ppo2.py:185][0m |          -0.0085 |         102.3124 |         -31.7329 |
[32m[20221214 00:26:33 @agent_ppo2.py:185][0m |          -0.0097 |         101.7882 |         -31.9863 |
[32m[20221214 00:26:33 @agent_ppo2.py:185][0m |           0.0021 |         116.0527 |         -32.0096 |
[32m[20221214 00:26:33 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:26:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 631.74
[32m[20221214 00:26:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 812.77
[32m[20221214 00:26:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 762.74
[32m[20221214 00:26:33 @agent_ppo2.py:143][0m Total time:      29.05 min
[32m[20221214 00:26:33 @agent_ppo2.py:145][0m 2643968 total steps have happened
[32m[20221214 00:26:33 @agent_ppo2.py:121][0m #------------------------ Iteration 5291 --------------------------#
[32m[20221214 00:26:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:26:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:26:33 @agent_ppo2.py:185][0m |          -0.0020 |         121.1500 |         -34.2740 |
[32m[20221214 00:26:34 @agent_ppo2.py:185][0m |           0.0148 |         126.5064 |         -34.0123 |
[32m[20221214 00:26:34 @agent_ppo2.py:185][0m |           0.0027 |         121.4794 |         -33.9136 |
[32m[20221214 00:26:34 @agent_ppo2.py:185][0m |          -0.0097 |         111.4884 |         -34.0833 |
[32m[20221214 00:26:34 @agent_ppo2.py:185][0m |          -0.0013 |         120.2422 |         -33.8465 |
[32m[20221214 00:26:34 @agent_ppo2.py:185][0m |          -0.0130 |         110.1351 |         -33.8101 |
[32m[20221214 00:26:34 @agent_ppo2.py:185][0m |          -0.0175 |         109.1778 |         -33.8026 |
[32m[20221214 00:26:34 @agent_ppo2.py:185][0m |          -0.0158 |         108.6821 |         -33.6552 |
[32m[20221214 00:26:34 @agent_ppo2.py:185][0m |          -0.0064 |         116.9006 |         -33.4360 |
[32m[20221214 00:26:34 @agent_ppo2.py:185][0m |          -0.0178 |         108.5304 |         -33.4675 |
[32m[20221214 00:26:34 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:26:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 477.19
[32m[20221214 00:26:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 516.81
[32m[20221214 00:26:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 752.79
[32m[20221214 00:26:34 @agent_ppo2.py:143][0m Total time:      29.07 min
[32m[20221214 00:26:34 @agent_ppo2.py:145][0m 2646016 total steps have happened
[32m[20221214 00:26:34 @agent_ppo2.py:121][0m #------------------------ Iteration 5292 --------------------------#
[32m[20221214 00:26:35 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:26:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:26:35 @agent_ppo2.py:185][0m |           0.0113 |          95.2867 |         -30.4734 |
[32m[20221214 00:26:35 @agent_ppo2.py:185][0m |          -0.0008 |          80.8635 |         -30.6762 |
[32m[20221214 00:26:35 @agent_ppo2.py:185][0m |          -0.0084 |          77.8331 |         -30.4053 |
[32m[20221214 00:26:35 @agent_ppo2.py:185][0m |          -0.0080 |          76.1106 |         -30.4975 |
[32m[20221214 00:26:35 @agent_ppo2.py:185][0m |          -0.0067 |          74.9165 |         -30.6817 |
[32m[20221214 00:26:35 @agent_ppo2.py:185][0m |          -0.0095 |          74.1963 |         -30.4254 |
[32m[20221214 00:26:35 @agent_ppo2.py:185][0m |          -0.0113 |          73.3161 |         -30.2601 |
[32m[20221214 00:26:35 @agent_ppo2.py:185][0m |          -0.0152 |          72.4262 |         -30.6644 |
[32m[20221214 00:26:36 @agent_ppo2.py:185][0m |          -0.0140 |          72.3169 |         -30.5123 |
[32m[20221214 00:26:36 @agent_ppo2.py:185][0m |          -0.0124 |          71.5616 |         -30.6252 |
[32m[20221214 00:26:36 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:26:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 529.55
[32m[20221214 00:26:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 627.39
[32m[20221214 00:26:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 801.60
[32m[20221214 00:26:36 @agent_ppo2.py:143][0m Total time:      29.09 min
[32m[20221214 00:26:36 @agent_ppo2.py:145][0m 2648064 total steps have happened
[32m[20221214 00:26:36 @agent_ppo2.py:121][0m #------------------------ Iteration 5293 --------------------------#
[32m[20221214 00:26:36 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:26:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:26:36 @agent_ppo2.py:185][0m |          -0.0026 |          92.9373 |         -35.6207 |
[32m[20221214 00:26:36 @agent_ppo2.py:185][0m |          -0.0092 |          83.6394 |         -35.3944 |
[32m[20221214 00:26:36 @agent_ppo2.py:185][0m |          -0.0137 |          79.6799 |         -35.1641 |
[32m[20221214 00:26:36 @agent_ppo2.py:185][0m |          -0.0014 |          85.7983 |         -35.3606 |
[32m[20221214 00:26:37 @agent_ppo2.py:185][0m |          -0.0080 |          81.7847 |         -35.2182 |
[32m[20221214 00:26:37 @agent_ppo2.py:185][0m |          -0.0168 |          74.8809 |         -35.0081 |
[32m[20221214 00:26:37 @agent_ppo2.py:185][0m |          -0.0212 |          73.8072 |         -35.0550 |
[32m[20221214 00:26:37 @agent_ppo2.py:185][0m |          -0.0231 |          72.9799 |         -35.0022 |
[32m[20221214 00:26:37 @agent_ppo2.py:185][0m |          -0.0201 |          72.5222 |         -34.7868 |
[32m[20221214 00:26:37 @agent_ppo2.py:185][0m |          -0.0177 |          72.1563 |         -34.8334 |
[32m[20221214 00:26:37 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:26:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 472.33
[32m[20221214 00:26:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 573.03
[32m[20221214 00:26:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 576.87
[32m[20221214 00:26:37 @agent_ppo2.py:143][0m Total time:      29.11 min
[32m[20221214 00:26:37 @agent_ppo2.py:145][0m 2650112 total steps have happened
[32m[20221214 00:26:37 @agent_ppo2.py:121][0m #------------------------ Iteration 5294 --------------------------#
[32m[20221214 00:26:37 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:26:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:26:38 @agent_ppo2.py:185][0m |          -0.0025 |          83.5426 |         -34.9125 |
[32m[20221214 00:26:38 @agent_ppo2.py:185][0m |           0.0007 |          75.4917 |         -34.6716 |
[32m[20221214 00:26:38 @agent_ppo2.py:185][0m |          -0.0088 |          72.2401 |         -34.5977 |
[32m[20221214 00:26:38 @agent_ppo2.py:185][0m |          -0.0077 |          71.8416 |         -34.4038 |
[32m[20221214 00:26:38 @agent_ppo2.py:185][0m |          -0.0109 |          69.6329 |         -34.3401 |
[32m[20221214 00:26:38 @agent_ppo2.py:185][0m |          -0.0102 |          68.7509 |         -34.5757 |
[32m[20221214 00:26:38 @agent_ppo2.py:185][0m |          -0.0117 |          68.4040 |         -34.4566 |
[32m[20221214 00:26:38 @agent_ppo2.py:185][0m |          -0.0073 |          68.0015 |         -34.1534 |
[32m[20221214 00:26:38 @agent_ppo2.py:185][0m |          -0.0098 |          66.9050 |         -34.3290 |
[32m[20221214 00:26:38 @agent_ppo2.py:185][0m |          -0.0096 |          66.5740 |         -34.3521 |
[32m[20221214 00:26:38 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:26:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 442.79
[32m[20221214 00:26:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 494.25
[32m[20221214 00:26:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 679.79
[32m[20221214 00:26:38 @agent_ppo2.py:143][0m Total time:      29.14 min
[32m[20221214 00:26:38 @agent_ppo2.py:145][0m 2652160 total steps have happened
[32m[20221214 00:26:38 @agent_ppo2.py:121][0m #------------------------ Iteration 5295 --------------------------#
[32m[20221214 00:26:39 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:26:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:26:39 @agent_ppo2.py:185][0m |          -0.0005 |         102.9946 |         -31.9111 |
[32m[20221214 00:26:39 @agent_ppo2.py:185][0m |          -0.0027 |          91.3329 |         -31.7122 |
[32m[20221214 00:26:39 @agent_ppo2.py:185][0m |          -0.0079 |          85.2632 |         -31.9952 |
[32m[20221214 00:26:39 @agent_ppo2.py:185][0m |          -0.0009 |         100.4416 |         -31.8636 |
[32m[20221214 00:26:39 @agent_ppo2.py:185][0m |          -0.0101 |          81.8966 |         -31.9221 |
[32m[20221214 00:26:39 @agent_ppo2.py:185][0m |          -0.0123 |          81.7682 |         -31.9116 |
[32m[20221214 00:26:39 @agent_ppo2.py:185][0m |          -0.0130 |          79.7089 |         -32.1832 |
[32m[20221214 00:26:39 @agent_ppo2.py:185][0m |          -0.0140 |          78.2909 |         -32.0642 |
[32m[20221214 00:26:40 @agent_ppo2.py:185][0m |          -0.0193 |          78.2393 |         -32.0723 |
[32m[20221214 00:26:40 @agent_ppo2.py:185][0m |          -0.0229 |          76.6061 |         -32.1632 |
[32m[20221214 00:26:40 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:26:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 341.69
[32m[20221214 00:26:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 509.34
[32m[20221214 00:26:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 762.37
[32m[20221214 00:26:40 @agent_ppo2.py:143][0m Total time:      29.16 min
[32m[20221214 00:26:40 @agent_ppo2.py:145][0m 2654208 total steps have happened
[32m[20221214 00:26:40 @agent_ppo2.py:121][0m #------------------------ Iteration 5296 --------------------------#
[32m[20221214 00:26:40 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:26:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:26:40 @agent_ppo2.py:185][0m |           0.0012 |         104.0227 |         -33.9926 |
[32m[20221214 00:26:40 @agent_ppo2.py:185][0m |          -0.0049 |          92.9345 |         -34.0623 |
[32m[20221214 00:26:40 @agent_ppo2.py:185][0m |          -0.0020 |          88.8214 |         -34.1206 |
[32m[20221214 00:26:40 @agent_ppo2.py:185][0m |          -0.0045 |          85.6990 |         -34.1835 |
[32m[20221214 00:26:41 @agent_ppo2.py:185][0m |          -0.0107 |          83.5764 |         -34.3536 |
[32m[20221214 00:26:41 @agent_ppo2.py:185][0m |          -0.0095 |          82.2733 |         -34.4470 |
[32m[20221214 00:26:41 @agent_ppo2.py:185][0m |          -0.0130 |          82.0765 |         -34.3199 |
[32m[20221214 00:26:41 @agent_ppo2.py:185][0m |           0.0003 |          83.9917 |         -34.4484 |
[32m[20221214 00:26:41 @agent_ppo2.py:185][0m |          -0.0136 |          79.4177 |         -34.4523 |
[32m[20221214 00:26:41 @agent_ppo2.py:185][0m |          -0.0129 |          78.4231 |         -34.1896 |
[32m[20221214 00:26:41 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:26:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 509.10
[32m[20221214 00:26:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 581.83
[32m[20221214 00:26:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 809.65
[32m[20221214 00:26:41 @agent_ppo2.py:143][0m Total time:      29.18 min
[32m[20221214 00:26:41 @agent_ppo2.py:145][0m 2656256 total steps have happened
[32m[20221214 00:26:41 @agent_ppo2.py:121][0m #------------------------ Iteration 5297 --------------------------#
[32m[20221214 00:26:41 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:26:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:26:42 @agent_ppo2.py:185][0m |          -0.0010 |          87.7174 |         -32.6495 |
[32m[20221214 00:26:42 @agent_ppo2.py:185][0m |          -0.0052 |          80.6012 |         -32.5752 |
[32m[20221214 00:26:42 @agent_ppo2.py:185][0m |          -0.0082 |          78.4992 |         -32.6975 |
[32m[20221214 00:26:42 @agent_ppo2.py:185][0m |          -0.0072 |          77.6173 |         -32.4924 |
[32m[20221214 00:26:42 @agent_ppo2.py:185][0m |          -0.0120 |          75.9505 |         -32.4646 |
[32m[20221214 00:26:42 @agent_ppo2.py:185][0m |          -0.0098 |          74.7946 |         -32.5011 |
[32m[20221214 00:26:42 @agent_ppo2.py:185][0m |          -0.0130 |          74.2292 |         -32.6603 |
[32m[20221214 00:26:42 @agent_ppo2.py:185][0m |          -0.0179 |          73.9823 |         -32.6276 |
[32m[20221214 00:26:42 @agent_ppo2.py:185][0m |          -0.0118 |          73.6425 |         -32.5707 |
[32m[20221214 00:26:42 @agent_ppo2.py:185][0m |          -0.0106 |          73.2986 |         -32.6952 |
[32m[20221214 00:26:42 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:26:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 456.27
[32m[20221214 00:26:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 585.45
[32m[20221214 00:26:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 747.52
[32m[20221214 00:26:43 @agent_ppo2.py:143][0m Total time:      29.20 min
[32m[20221214 00:26:43 @agent_ppo2.py:145][0m 2658304 total steps have happened
[32m[20221214 00:26:43 @agent_ppo2.py:121][0m #------------------------ Iteration 5298 --------------------------#
[32m[20221214 00:26:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:26:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:26:43 @agent_ppo2.py:185][0m |           0.0025 |          90.6180 |         -33.4673 |
[32m[20221214 00:26:43 @agent_ppo2.py:185][0m |          -0.0053 |          84.5366 |         -33.4439 |
[32m[20221214 00:26:43 @agent_ppo2.py:185][0m |          -0.0047 |          81.8896 |         -33.2150 |
[32m[20221214 00:26:43 @agent_ppo2.py:185][0m |          -0.0078 |          80.8239 |         -33.3945 |
[32m[20221214 00:26:43 @agent_ppo2.py:185][0m |          -0.0063 |          79.8740 |         -33.3230 |
[32m[20221214 00:26:43 @agent_ppo2.py:185][0m |          -0.0041 |          81.0397 |         -33.2706 |
[32m[20221214 00:26:43 @agent_ppo2.py:185][0m |          -0.0102 |          78.2975 |         -33.4710 |
[32m[20221214 00:26:44 @agent_ppo2.py:185][0m |          -0.0120 |          78.0363 |         -33.3854 |
[32m[20221214 00:26:44 @agent_ppo2.py:185][0m |          -0.0082 |          80.9216 |         -33.5334 |
[32m[20221214 00:26:44 @agent_ppo2.py:185][0m |          -0.0074 |          78.0968 |         -33.2796 |
[32m[20221214 00:26:44 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:26:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 540.74
[32m[20221214 00:26:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 705.70
[32m[20221214 00:26:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 775.00
[32m[20221214 00:26:44 @agent_ppo2.py:143][0m Total time:      29.22 min
[32m[20221214 00:26:44 @agent_ppo2.py:145][0m 2660352 total steps have happened
[32m[20221214 00:26:44 @agent_ppo2.py:121][0m #------------------------ Iteration 5299 --------------------------#
[32m[20221214 00:26:44 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:26:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:26:44 @agent_ppo2.py:185][0m |           0.0018 |          88.3290 |         -31.9740 |
[32m[20221214 00:26:44 @agent_ppo2.py:185][0m |          -0.0079 |          79.1942 |         -31.9483 |
[32m[20221214 00:26:44 @agent_ppo2.py:185][0m |          -0.0095 |          75.6215 |         -31.8477 |
[32m[20221214 00:26:44 @agent_ppo2.py:185][0m |          -0.0130 |          73.6241 |         -31.9165 |
[32m[20221214 00:26:45 @agent_ppo2.py:185][0m |          -0.0133 |          71.2384 |         -31.9260 |
[32m[20221214 00:26:45 @agent_ppo2.py:185][0m |          -0.0182 |          70.6531 |         -31.6415 |
[32m[20221214 00:26:45 @agent_ppo2.py:185][0m |          -0.0231 |          70.0060 |         -31.6886 |
[32m[20221214 00:26:45 @agent_ppo2.py:185][0m |          -0.0178 |          69.0183 |         -31.6930 |
[32m[20221214 00:26:45 @agent_ppo2.py:185][0m |          -0.0194 |          68.3449 |         -31.5712 |
[32m[20221214 00:26:45 @agent_ppo2.py:185][0m |          -0.0191 |          67.6544 |         -31.5631 |
[32m[20221214 00:26:45 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:26:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 328.53
[32m[20221214 00:26:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 617.53
[32m[20221214 00:26:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 652.11
[32m[20221214 00:26:45 @agent_ppo2.py:143][0m Total time:      29.25 min
[32m[20221214 00:26:45 @agent_ppo2.py:145][0m 2662400 total steps have happened
[32m[20221214 00:26:45 @agent_ppo2.py:121][0m #------------------------ Iteration 5300 --------------------------#
[32m[20221214 00:26:45 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:26:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:26:46 @agent_ppo2.py:185][0m |           0.0023 |         113.2789 |         -30.7537 |
[32m[20221214 00:26:46 @agent_ppo2.py:185][0m |           0.0012 |         103.5340 |         -31.0555 |
[32m[20221214 00:26:46 @agent_ppo2.py:185][0m |          -0.0024 |         100.6529 |         -31.0102 |
[32m[20221214 00:26:46 @agent_ppo2.py:185][0m |          -0.0033 |          99.3985 |         -30.8759 |
[32m[20221214 00:26:46 @agent_ppo2.py:185][0m |           0.0117 |         104.4597 |         -30.6146 |
[32m[20221214 00:26:46 @agent_ppo2.py:185][0m |          -0.0048 |          97.1917 |         -30.8109 |
[32m[20221214 00:26:46 @agent_ppo2.py:185][0m |          -0.0070 |          96.0704 |         -30.6453 |
[32m[20221214 00:26:46 @agent_ppo2.py:185][0m |          -0.0048 |          95.6335 |         -30.9714 |
[32m[20221214 00:26:46 @agent_ppo2.py:185][0m |          -0.0088 |          94.6125 |         -30.9217 |
[32m[20221214 00:26:46 @agent_ppo2.py:185][0m |          -0.0103 |          94.8831 |         -30.6838 |
[32m[20221214 00:26:46 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:26:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 564.65
[32m[20221214 00:26:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 650.07
[32m[20221214 00:26:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 522.40
[32m[20221214 00:26:47 @agent_ppo2.py:143][0m Total time:      29.27 min
[32m[20221214 00:26:47 @agent_ppo2.py:145][0m 2664448 total steps have happened
[32m[20221214 00:26:47 @agent_ppo2.py:121][0m #------------------------ Iteration 5301 --------------------------#
[32m[20221214 00:26:47 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:26:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:26:47 @agent_ppo2.py:185][0m |           0.0018 |         120.1081 |         -34.5874 |
[32m[20221214 00:26:47 @agent_ppo2.py:185][0m |          -0.0078 |         112.2268 |         -34.3515 |
[32m[20221214 00:26:47 @agent_ppo2.py:185][0m |          -0.0033 |         108.9859 |         -33.8768 |
[32m[20221214 00:26:47 @agent_ppo2.py:185][0m |          -0.0085 |         106.5101 |         -34.1494 |
[32m[20221214 00:26:47 @agent_ppo2.py:185][0m |          -0.0030 |         118.8988 |         -34.0043 |
[32m[20221214 00:26:47 @agent_ppo2.py:185][0m |          -0.0111 |         106.5326 |         -33.8474 |
[32m[20221214 00:26:48 @agent_ppo2.py:185][0m |          -0.0130 |         103.0328 |         -33.6900 |
[32m[20221214 00:26:48 @agent_ppo2.py:185][0m |          -0.0140 |         102.5443 |         -33.5060 |
[32m[20221214 00:26:48 @agent_ppo2.py:185][0m |          -0.0129 |         101.4494 |         -33.6659 |
[32m[20221214 00:26:48 @agent_ppo2.py:185][0m |          -0.0120 |         101.2640 |         -33.8149 |
[32m[20221214 00:26:48 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:26:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 565.72
[32m[20221214 00:26:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 652.03
[32m[20221214 00:26:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 632.88
[32m[20221214 00:26:48 @agent_ppo2.py:143][0m Total time:      29.29 min
[32m[20221214 00:26:48 @agent_ppo2.py:145][0m 2666496 total steps have happened
[32m[20221214 00:26:48 @agent_ppo2.py:121][0m #------------------------ Iteration 5302 --------------------------#
[32m[20221214 00:26:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:26:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:26:48 @agent_ppo2.py:185][0m |           0.0062 |          81.2791 |         -31.8190 |
[32m[20221214 00:26:48 @agent_ppo2.py:185][0m |           0.0032 |          71.3022 |         -31.9088 |
[32m[20221214 00:26:48 @agent_ppo2.py:185][0m |          -0.0018 |          68.4906 |         -32.0184 |
[32m[20221214 00:26:49 @agent_ppo2.py:185][0m |          -0.0071 |          67.3262 |         -32.2682 |
[32m[20221214 00:26:49 @agent_ppo2.py:185][0m |          -0.0104 |          66.6361 |         -32.1748 |
[32m[20221214 00:26:49 @agent_ppo2.py:185][0m |           0.0082 |          71.7330 |         -32.7594 |
[32m[20221214 00:26:49 @agent_ppo2.py:185][0m |          -0.0071 |          65.5394 |         -32.4747 |
[32m[20221214 00:26:49 @agent_ppo2.py:185][0m |          -0.0068 |          63.8913 |         -32.6318 |
[32m[20221214 00:26:49 @agent_ppo2.py:185][0m |          -0.0087 |          63.4603 |         -32.6863 |
[32m[20221214 00:26:49 @agent_ppo2.py:185][0m |          -0.0123 |          63.1580 |         -32.6653 |
[32m[20221214 00:26:49 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:26:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 500.03
[32m[20221214 00:26:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 554.26
[32m[20221214 00:26:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 535.62
[32m[20221214 00:26:49 @agent_ppo2.py:143][0m Total time:      29.32 min
[32m[20221214 00:26:49 @agent_ppo2.py:145][0m 2668544 total steps have happened
[32m[20221214 00:26:49 @agent_ppo2.py:121][0m #------------------------ Iteration 5303 --------------------------#
[32m[20221214 00:26:49 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:26:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:26:50 @agent_ppo2.py:185][0m |          -0.0015 |          94.1982 |         -34.7262 |
[32m[20221214 00:26:50 @agent_ppo2.py:185][0m |          -0.0084 |          84.0586 |         -34.7362 |
[32m[20221214 00:26:50 @agent_ppo2.py:185][0m |          -0.0067 |          80.9825 |         -34.7032 |
[32m[20221214 00:26:50 @agent_ppo2.py:185][0m |          -0.0089 |          78.6114 |         -34.6423 |
[32m[20221214 00:26:50 @agent_ppo2.py:185][0m |          -0.0104 |          77.0563 |         -34.7818 |
[32m[20221214 00:26:50 @agent_ppo2.py:185][0m |          -0.0132 |          75.9971 |         -34.5276 |
[32m[20221214 00:26:50 @agent_ppo2.py:185][0m |          -0.0162 |          75.1831 |         -34.6905 |
[32m[20221214 00:26:50 @agent_ppo2.py:185][0m |          -0.0159 |          74.8913 |         -34.4832 |
[32m[20221214 00:26:50 @agent_ppo2.py:185][0m |          -0.0178 |          73.8231 |         -34.6024 |
[32m[20221214 00:26:50 @agent_ppo2.py:185][0m |          -0.0172 |          73.1871 |         -34.5926 |
[32m[20221214 00:26:50 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:26:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 511.50
[32m[20221214 00:26:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 627.21
[32m[20221214 00:26:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 629.18
[32m[20221214 00:26:51 @agent_ppo2.py:143][0m Total time:      29.34 min
[32m[20221214 00:26:51 @agent_ppo2.py:145][0m 2670592 total steps have happened
[32m[20221214 00:26:51 @agent_ppo2.py:121][0m #------------------------ Iteration 5304 --------------------------#
[32m[20221214 00:26:51 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:26:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:26:51 @agent_ppo2.py:185][0m |           0.0050 |          44.7474 |         -34.5956 |
[32m[20221214 00:26:51 @agent_ppo2.py:185][0m |          -0.0043 |          37.1546 |         -34.6408 |
[32m[20221214 00:26:51 @agent_ppo2.py:185][0m |          -0.0074 |          33.9993 |         -34.7113 |
[32m[20221214 00:26:51 @agent_ppo2.py:185][0m |          -0.0170 |          32.1083 |         -35.0315 |
[32m[20221214 00:26:51 @agent_ppo2.py:185][0m |          -0.0146 |          30.0106 |         -34.9205 |
[32m[20221214 00:26:51 @agent_ppo2.py:185][0m |          -0.0120 |          28.9968 |         -35.3828 |
[32m[20221214 00:26:52 @agent_ppo2.py:185][0m |          -0.0172 |          28.0822 |         -35.3526 |
[32m[20221214 00:26:52 @agent_ppo2.py:185][0m |          -0.0230 |          27.1431 |         -35.6368 |
[32m[20221214 00:26:52 @agent_ppo2.py:185][0m |          -0.0191 |          26.6890 |         -35.4931 |
[32m[20221214 00:26:52 @agent_ppo2.py:185][0m |          -0.0266 |          26.1670 |         -35.7794 |
[32m[20221214 00:26:52 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:26:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 345.42
[32m[20221214 00:26:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 588.89
[32m[20221214 00:26:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 625.48
[32m[20221214 00:26:52 @agent_ppo2.py:143][0m Total time:      29.36 min
[32m[20221214 00:26:52 @agent_ppo2.py:145][0m 2672640 total steps have happened
[32m[20221214 00:26:52 @agent_ppo2.py:121][0m #------------------------ Iteration 5305 --------------------------#
[32m[20221214 00:26:52 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:26:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:26:52 @agent_ppo2.py:185][0m |           0.0067 |          87.5515 |         -34.9209 |
[32m[20221214 00:26:52 @agent_ppo2.py:185][0m |          -0.0143 |          76.0735 |         -34.7516 |
[32m[20221214 00:26:53 @agent_ppo2.py:185][0m |          -0.0018 |          73.6697 |         -34.6970 |
[32m[20221214 00:26:53 @agent_ppo2.py:185][0m |          -0.0134 |          71.1121 |         -34.6636 |
[32m[20221214 00:26:53 @agent_ppo2.py:185][0m |          -0.0129 |          69.9288 |         -34.8357 |
[32m[20221214 00:26:53 @agent_ppo2.py:185][0m |          -0.0106 |          68.7925 |         -34.7526 |
[32m[20221214 00:26:53 @agent_ppo2.py:185][0m |          -0.0154 |          68.1927 |         -34.7585 |
[32m[20221214 00:26:53 @agent_ppo2.py:185][0m |          -0.0143 |          67.7656 |         -34.8404 |
[32m[20221214 00:26:53 @agent_ppo2.py:185][0m |          -0.0128 |          67.5658 |         -34.9954 |
[32m[20221214 00:26:53 @agent_ppo2.py:185][0m |          -0.0194 |          66.3911 |         -34.7364 |
[32m[20221214 00:26:53 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:26:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 617.30
[32m[20221214 00:26:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 710.42
[32m[20221214 00:26:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 250.35
[32m[20221214 00:26:53 @agent_ppo2.py:143][0m Total time:      29.38 min
[32m[20221214 00:26:53 @agent_ppo2.py:145][0m 2674688 total steps have happened
[32m[20221214 00:26:53 @agent_ppo2.py:121][0m #------------------------ Iteration 5306 --------------------------#
[32m[20221214 00:26:53 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:26:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:26:54 @agent_ppo2.py:185][0m |          -0.0030 |          88.9862 |         -35.6258 |
[32m[20221214 00:26:54 @agent_ppo2.py:185][0m |          -0.0049 |          83.7205 |         -35.5698 |
[32m[20221214 00:26:54 @agent_ppo2.py:185][0m |          -0.0061 |          82.6108 |         -35.8210 |
[32m[20221214 00:26:54 @agent_ppo2.py:185][0m |          -0.0031 |          82.4060 |         -35.4415 |
[32m[20221214 00:26:54 @agent_ppo2.py:185][0m |          -0.0077 |          81.5213 |         -35.6846 |
[32m[20221214 00:26:54 @agent_ppo2.py:185][0m |          -0.0087 |          81.5852 |         -35.7129 |
[32m[20221214 00:26:54 @agent_ppo2.py:185][0m |          -0.0098 |          81.3590 |         -35.3938 |
[32m[20221214 00:26:54 @agent_ppo2.py:185][0m |          -0.0050 |          82.8278 |         -35.6114 |
[32m[20221214 00:26:54 @agent_ppo2.py:185][0m |          -0.0044 |          82.4941 |         -35.7075 |
[32m[20221214 00:26:54 @agent_ppo2.py:185][0m |          -0.0113 |          80.2958 |         -35.6692 |
[32m[20221214 00:26:54 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:26:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 710.42
[32m[20221214 00:26:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 754.79
[32m[20221214 00:26:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 741.72
[32m[20221214 00:26:55 @agent_ppo2.py:143][0m Total time:      29.41 min
[32m[20221214 00:26:55 @agent_ppo2.py:145][0m 2676736 total steps have happened
[32m[20221214 00:26:55 @agent_ppo2.py:121][0m #------------------------ Iteration 5307 --------------------------#
[32m[20221214 00:26:55 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:26:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:26:55 @agent_ppo2.py:185][0m |          -0.0004 |         101.5215 |         -36.2453 |
[32m[20221214 00:26:55 @agent_ppo2.py:185][0m |          -0.0033 |          89.6892 |         -35.7157 |
[32m[20221214 00:26:55 @agent_ppo2.py:185][0m |          -0.0100 |          86.3346 |         -35.7005 |
[32m[20221214 00:26:55 @agent_ppo2.py:185][0m |          -0.0111 |          84.6253 |         -35.5054 |
[32m[20221214 00:26:55 @agent_ppo2.py:185][0m |          -0.0197 |          82.7013 |         -35.4381 |
[32m[20221214 00:26:55 @agent_ppo2.py:185][0m |          -0.0209 |          81.5513 |         -35.3454 |
[32m[20221214 00:26:56 @agent_ppo2.py:185][0m |          -0.0147 |          82.8140 |         -35.1932 |
[32m[20221214 00:26:56 @agent_ppo2.py:185][0m |          -0.0094 |          83.2503 |         -35.0149 |
[32m[20221214 00:26:56 @agent_ppo2.py:185][0m |          -0.0234 |          79.0657 |         -34.8744 |
[32m[20221214 00:26:56 @agent_ppo2.py:185][0m |          -0.0266 |          78.9255 |         -34.9827 |
[32m[20221214 00:26:56 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:26:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 514.61
[32m[20221214 00:26:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 603.37
[32m[20221214 00:26:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 570.43
[32m[20221214 00:26:56 @agent_ppo2.py:143][0m Total time:      29.43 min
[32m[20221214 00:26:56 @agent_ppo2.py:145][0m 2678784 total steps have happened
[32m[20221214 00:26:56 @agent_ppo2.py:121][0m #------------------------ Iteration 5308 --------------------------#
[32m[20221214 00:26:56 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:26:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:26:56 @agent_ppo2.py:185][0m |           0.0072 |         105.3113 |         -37.4955 |
[32m[20221214 00:26:56 @agent_ppo2.py:185][0m |          -0.0043 |          93.8006 |         -37.5309 |
[32m[20221214 00:26:57 @agent_ppo2.py:185][0m |          -0.0014 |          94.4017 |         -37.7662 |
[32m[20221214 00:26:57 @agent_ppo2.py:185][0m |          -0.0117 |          84.8524 |         -37.6235 |
[32m[20221214 00:26:57 @agent_ppo2.py:185][0m |          -0.0153 |          81.5778 |         -37.8453 |
[32m[20221214 00:26:57 @agent_ppo2.py:185][0m |          -0.0151 |          80.1015 |         -37.8704 |
[32m[20221214 00:26:57 @agent_ppo2.py:185][0m |          -0.0156 |          78.5801 |         -37.8434 |
[32m[20221214 00:26:57 @agent_ppo2.py:185][0m |          -0.0097 |          77.0133 |         -37.8208 |
[32m[20221214 00:26:57 @agent_ppo2.py:185][0m |          -0.0134 |          76.0701 |         -37.6367 |
[32m[20221214 00:26:57 @agent_ppo2.py:185][0m |          -0.0134 |          78.4484 |         -37.9745 |
[32m[20221214 00:26:57 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:26:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 522.47
[32m[20221214 00:26:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 600.44
[32m[20221214 00:26:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 594.93
[32m[20221214 00:26:57 @agent_ppo2.py:143][0m Total time:      29.45 min
[32m[20221214 00:26:57 @agent_ppo2.py:145][0m 2680832 total steps have happened
[32m[20221214 00:26:57 @agent_ppo2.py:121][0m #------------------------ Iteration 5309 --------------------------#
[32m[20221214 00:26:57 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:26:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:26:58 @agent_ppo2.py:185][0m |           0.0059 |          92.5683 |         -35.8551 |
[32m[20221214 00:26:58 @agent_ppo2.py:185][0m |           0.0018 |          84.8441 |         -36.1660 |
[32m[20221214 00:26:58 @agent_ppo2.py:185][0m |          -0.0017 |          81.4937 |         -36.1520 |
[32m[20221214 00:26:58 @agent_ppo2.py:185][0m |          -0.0084 |          79.9934 |         -36.2811 |
[32m[20221214 00:26:58 @agent_ppo2.py:185][0m |          -0.0053 |          79.5254 |         -36.3049 |
[32m[20221214 00:26:58 @agent_ppo2.py:185][0m |          -0.0110 |          78.1965 |         -36.2224 |
[32m[20221214 00:26:58 @agent_ppo2.py:185][0m |          -0.0044 |          78.2883 |         -36.4856 |
[32m[20221214 00:26:58 @agent_ppo2.py:185][0m |           0.0019 |          82.5651 |         -36.3029 |
[32m[20221214 00:26:58 @agent_ppo2.py:185][0m |           0.0013 |          79.2898 |         -36.2514 |
[32m[20221214 00:26:59 @agent_ppo2.py:185][0m |          -0.0121 |          76.4071 |         -36.2135 |
[32m[20221214 00:26:59 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:26:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 579.91
[32m[20221214 00:26:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 643.13
[32m[20221214 00:26:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 555.33
[32m[20221214 00:26:59 @agent_ppo2.py:143][0m Total time:      29.47 min
[32m[20221214 00:26:59 @agent_ppo2.py:145][0m 2682880 total steps have happened
[32m[20221214 00:26:59 @agent_ppo2.py:121][0m #------------------------ Iteration 5310 --------------------------#
[32m[20221214 00:26:59 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:26:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:26:59 @agent_ppo2.py:185][0m |           0.0037 |          66.8745 |         -37.9610 |
[32m[20221214 00:26:59 @agent_ppo2.py:185][0m |          -0.0045 |          60.2929 |         -37.7704 |
[32m[20221214 00:26:59 @agent_ppo2.py:185][0m |           0.0009 |          64.1876 |         -37.8657 |
[32m[20221214 00:26:59 @agent_ppo2.py:185][0m |          -0.0134 |          57.3433 |         -37.8606 |
[32m[20221214 00:26:59 @agent_ppo2.py:185][0m |          -0.0170 |          57.1324 |         -38.1962 |
[32m[20221214 00:27:00 @agent_ppo2.py:185][0m |          -0.0146 |          55.9291 |         -37.9232 |
[32m[20221214 00:27:00 @agent_ppo2.py:185][0m |          -0.0189 |          54.7641 |         -37.8941 |
[32m[20221214 00:27:00 @agent_ppo2.py:185][0m |          -0.0191 |          54.3201 |         -37.9358 |
[32m[20221214 00:27:00 @agent_ppo2.py:185][0m |          -0.0144 |          55.0680 |         -37.9103 |
[32m[20221214 00:27:00 @agent_ppo2.py:185][0m |          -0.0210 |          53.4938 |         -37.7580 |
[32m[20221214 00:27:00 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:27:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 528.08
[32m[20221214 00:27:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 566.49
[32m[20221214 00:27:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 801.39
[32m[20221214 00:27:00 @agent_ppo2.py:143][0m Total time:      29.50 min
[32m[20221214 00:27:00 @agent_ppo2.py:145][0m 2684928 total steps have happened
[32m[20221214 00:27:00 @agent_ppo2.py:121][0m #------------------------ Iteration 5311 --------------------------#
[32m[20221214 00:27:00 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:27:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:27:00 @agent_ppo2.py:185][0m |           0.0016 |          88.6837 |         -39.0506 |
[32m[20221214 00:27:01 @agent_ppo2.py:185][0m |          -0.0083 |          78.9579 |         -39.2516 |
[32m[20221214 00:27:01 @agent_ppo2.py:185][0m |          -0.0057 |          76.7884 |         -39.2934 |
[32m[20221214 00:27:01 @agent_ppo2.py:185][0m |          -0.0105 |          75.3172 |         -39.3782 |
[32m[20221214 00:27:01 @agent_ppo2.py:185][0m |          -0.0153 |          74.0172 |         -39.3489 |
[32m[20221214 00:27:01 @agent_ppo2.py:185][0m |          -0.0077 |          72.6596 |         -39.5458 |
[32m[20221214 00:27:01 @agent_ppo2.py:185][0m |          -0.0128 |          71.9600 |         -39.6170 |
[32m[20221214 00:27:01 @agent_ppo2.py:185][0m |          -0.0128 |          71.5841 |         -39.5558 |
[32m[20221214 00:27:01 @agent_ppo2.py:185][0m |          -0.0149 |          70.6273 |         -39.5157 |
[32m[20221214 00:27:01 @agent_ppo2.py:185][0m |          -0.0193 |          70.2199 |         -39.5720 |
[32m[20221214 00:27:01 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:27:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 472.56
[32m[20221214 00:27:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 570.54
[32m[20221214 00:27:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 530.46
[32m[20221214 00:27:01 @agent_ppo2.py:143][0m Total time:      29.52 min
[32m[20221214 00:27:01 @agent_ppo2.py:145][0m 2686976 total steps have happened
[32m[20221214 00:27:01 @agent_ppo2.py:121][0m #------------------------ Iteration 5312 --------------------------#
[32m[20221214 00:27:02 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:27:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:27:02 @agent_ppo2.py:185][0m |          -0.0017 |         105.0432 |         -39.2040 |
[32m[20221214 00:27:02 @agent_ppo2.py:185][0m |          -0.0070 |          93.6826 |         -38.7735 |
[32m[20221214 00:27:02 @agent_ppo2.py:185][0m |          -0.0098 |          90.0906 |         -38.5388 |
[32m[20221214 00:27:02 @agent_ppo2.py:185][0m |          -0.0126 |          88.2856 |         -38.5707 |
[32m[20221214 00:27:02 @agent_ppo2.py:185][0m |          -0.0144 |          87.1040 |         -38.6273 |
[32m[20221214 00:27:02 @agent_ppo2.py:185][0m |          -0.0050 |          89.8122 |         -38.3226 |
[32m[20221214 00:27:02 @agent_ppo2.py:185][0m |          -0.0116 |          86.1797 |         -38.4478 |
[32m[20221214 00:27:02 @agent_ppo2.py:185][0m |          -0.0183 |          85.2804 |         -38.3926 |
[32m[20221214 00:27:03 @agent_ppo2.py:185][0m |          -0.0156 |          84.4714 |         -38.4960 |
[32m[20221214 00:27:03 @agent_ppo2.py:185][0m |          -0.0208 |          83.7599 |         -38.3591 |
[32m[20221214 00:27:03 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:27:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 440.31
[32m[20221214 00:27:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 523.81
[32m[20221214 00:27:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 598.66
[32m[20221214 00:27:03 @agent_ppo2.py:143][0m Total time:      29.54 min
[32m[20221214 00:27:03 @agent_ppo2.py:145][0m 2689024 total steps have happened
[32m[20221214 00:27:03 @agent_ppo2.py:121][0m #------------------------ Iteration 5313 --------------------------#
[32m[20221214 00:27:03 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:27:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:27:03 @agent_ppo2.py:185][0m |           0.0124 |         116.9620 |         -39.1507 |
[32m[20221214 00:27:03 @agent_ppo2.py:185][0m |          -0.0020 |         101.9473 |         -39.2594 |
[32m[20221214 00:27:03 @agent_ppo2.py:185][0m |          -0.0046 |          97.9548 |         -39.2501 |
[32m[20221214 00:27:03 @agent_ppo2.py:185][0m |          -0.0079 |          96.2904 |         -39.3093 |
[32m[20221214 00:27:04 @agent_ppo2.py:185][0m |          -0.0122 |          93.6704 |         -39.4098 |
[32m[20221214 00:27:04 @agent_ppo2.py:185][0m |          -0.0071 |          92.8156 |         -39.2516 |
[32m[20221214 00:27:04 @agent_ppo2.py:185][0m |          -0.0156 |          91.2714 |         -39.4353 |
[32m[20221214 00:27:04 @agent_ppo2.py:185][0m |          -0.0065 |          90.1179 |         -39.6308 |
[32m[20221214 00:27:04 @agent_ppo2.py:185][0m |          -0.0151 |          89.2460 |         -39.5831 |
[32m[20221214 00:27:04 @agent_ppo2.py:185][0m |          -0.0161 |          88.3668 |         -39.7285 |
[32m[20221214 00:27:04 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:27:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 341.34
[32m[20221214 00:27:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 566.07
[32m[20221214 00:27:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 606.55
[32m[20221214 00:27:04 @agent_ppo2.py:143][0m Total time:      29.56 min
[32m[20221214 00:27:04 @agent_ppo2.py:145][0m 2691072 total steps have happened
[32m[20221214 00:27:04 @agent_ppo2.py:121][0m #------------------------ Iteration 5314 --------------------------#
[32m[20221214 00:27:04 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:27:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:27:04 @agent_ppo2.py:185][0m |           0.0123 |          82.7680 |         -39.5464 |
[32m[20221214 00:27:05 @agent_ppo2.py:185][0m |          -0.0057 |          72.6625 |         -39.1891 |
[32m[20221214 00:27:05 @agent_ppo2.py:185][0m |          -0.0076 |          70.7105 |         -39.5363 |
[32m[20221214 00:27:05 @agent_ppo2.py:185][0m |          -0.0099 |          69.6267 |         -39.1650 |
[32m[20221214 00:27:05 @agent_ppo2.py:185][0m |          -0.0047 |          69.3118 |         -39.2261 |
[32m[20221214 00:27:05 @agent_ppo2.py:185][0m |          -0.0160 |          68.0426 |         -38.9291 |
[32m[20221214 00:27:05 @agent_ppo2.py:185][0m |          -0.0142 |          67.9330 |         -39.0131 |
[32m[20221214 00:27:05 @agent_ppo2.py:185][0m |          -0.0189 |          67.3970 |         -39.0843 |
[32m[20221214 00:27:05 @agent_ppo2.py:185][0m |          -0.0202 |          66.9562 |         -39.0076 |
[32m[20221214 00:27:05 @agent_ppo2.py:185][0m |          -0.0190 |          66.3958 |         -38.7802 |
[32m[20221214 00:27:05 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:27:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 488.03
[32m[20221214 00:27:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 524.84
[32m[20221214 00:27:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 628.74
[32m[20221214 00:27:05 @agent_ppo2.py:143][0m Total time:      29.59 min
[32m[20221214 00:27:05 @agent_ppo2.py:145][0m 2693120 total steps have happened
[32m[20221214 00:27:05 @agent_ppo2.py:121][0m #------------------------ Iteration 5315 --------------------------#
[32m[20221214 00:27:06 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:27:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:27:06 @agent_ppo2.py:185][0m |          -0.0001 |         112.0383 |         -41.2708 |
[32m[20221214 00:27:06 @agent_ppo2.py:185][0m |          -0.0024 |         104.3659 |         -40.8474 |
[32m[20221214 00:27:06 @agent_ppo2.py:185][0m |          -0.0042 |         102.1983 |         -41.0261 |
[32m[20221214 00:27:06 @agent_ppo2.py:185][0m |          -0.0022 |         101.8092 |         -41.2131 |
[32m[20221214 00:27:06 @agent_ppo2.py:185][0m |          -0.0062 |          99.5167 |         -40.8706 |
[32m[20221214 00:27:06 @agent_ppo2.py:185][0m |          -0.0059 |          98.7885 |         -41.0361 |
[32m[20221214 00:27:06 @agent_ppo2.py:185][0m |          -0.0014 |         105.4626 |         -41.2700 |
[32m[20221214 00:27:06 @agent_ppo2.py:185][0m |          -0.0095 |          98.0654 |         -41.2728 |
[32m[20221214 00:27:07 @agent_ppo2.py:185][0m |          -0.0121 |          96.1656 |         -41.3192 |
[32m[20221214 00:27:07 @agent_ppo2.py:185][0m |          -0.0125 |          95.2586 |         -41.4759 |
[32m[20221214 00:27:07 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:27:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 680.66
[32m[20221214 00:27:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 826.89
[32m[20221214 00:27:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 756.54
[32m[20221214 00:27:07 @agent_ppo2.py:143][0m Total time:      29.61 min
[32m[20221214 00:27:07 @agent_ppo2.py:145][0m 2695168 total steps have happened
[32m[20221214 00:27:07 @agent_ppo2.py:121][0m #------------------------ Iteration 5316 --------------------------#
[32m[20221214 00:27:07 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:27:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:27:07 @agent_ppo2.py:185][0m |           0.0020 |          74.1862 |         -39.4952 |
[32m[20221214 00:27:07 @agent_ppo2.py:185][0m |          -0.0010 |          59.5886 |         -39.6354 |
[32m[20221214 00:27:07 @agent_ppo2.py:185][0m |          -0.0082 |          56.6749 |         -39.5257 |
[32m[20221214 00:27:07 @agent_ppo2.py:185][0m |          -0.0078 |          53.8378 |         -39.3176 |
[32m[20221214 00:27:08 @agent_ppo2.py:185][0m |          -0.0097 |          54.1565 |         -39.1354 |
[32m[20221214 00:27:08 @agent_ppo2.py:185][0m |          -0.0077 |          53.0139 |         -39.4346 |
[32m[20221214 00:27:08 @agent_ppo2.py:185][0m |          -0.0072 |          50.0939 |         -39.7545 |
[32m[20221214 00:27:08 @agent_ppo2.py:185][0m |          -0.0087 |          49.0102 |         -39.6602 |
[32m[20221214 00:27:08 @agent_ppo2.py:185][0m |          -0.0151 |          49.6509 |         -39.6913 |
[32m[20221214 00:27:08 @agent_ppo2.py:185][0m |          -0.0071 |          49.0577 |         -39.7679 |
[32m[20221214 00:27:08 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:27:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 527.84
[32m[20221214 00:27:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 672.08
[32m[20221214 00:27:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 629.33
[32m[20221214 00:27:08 @agent_ppo2.py:143][0m Total time:      29.63 min
[32m[20221214 00:27:08 @agent_ppo2.py:145][0m 2697216 total steps have happened
[32m[20221214 00:27:08 @agent_ppo2.py:121][0m #------------------------ Iteration 5317 --------------------------#
[32m[20221214 00:27:08 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:27:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:27:09 @agent_ppo2.py:185][0m |           0.0010 |         113.5835 |         -41.1314 |
[32m[20221214 00:27:09 @agent_ppo2.py:185][0m |          -0.0065 |         104.3240 |         -40.9571 |
[32m[20221214 00:27:09 @agent_ppo2.py:185][0m |          -0.0039 |         100.8290 |         -41.2584 |
[32m[20221214 00:27:09 @agent_ppo2.py:185][0m |          -0.0044 |          98.5011 |         -41.3410 |
[32m[20221214 00:27:09 @agent_ppo2.py:185][0m |          -0.0081 |          96.8038 |         -41.1855 |
[32m[20221214 00:27:09 @agent_ppo2.py:185][0m |          -0.0089 |          96.0489 |         -41.2747 |
[32m[20221214 00:27:09 @agent_ppo2.py:185][0m |          -0.0054 |          95.0460 |         -41.4708 |
[32m[20221214 00:27:09 @agent_ppo2.py:185][0m |          -0.0077 |          94.1687 |         -41.5519 |
[32m[20221214 00:27:09 @agent_ppo2.py:185][0m |          -0.0099 |          93.6848 |         -41.6860 |
[32m[20221214 00:27:09 @agent_ppo2.py:185][0m |          -0.0102 |          92.8211 |         -41.6072 |
[32m[20221214 00:27:09 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:27:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 636.97
[32m[20221214 00:27:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 780.82
[32m[20221214 00:27:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 721.44
[32m[20221214 00:27:10 @agent_ppo2.py:143][0m Total time:      29.65 min
[32m[20221214 00:27:10 @agent_ppo2.py:145][0m 2699264 total steps have happened
[32m[20221214 00:27:10 @agent_ppo2.py:121][0m #------------------------ Iteration 5318 --------------------------#
[32m[20221214 00:27:10 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:27:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:27:10 @agent_ppo2.py:185][0m |           0.0104 |         122.8706 |         -39.6536 |
[32m[20221214 00:27:10 @agent_ppo2.py:185][0m |          -0.0015 |         102.7787 |         -39.5299 |
[32m[20221214 00:27:10 @agent_ppo2.py:185][0m |          -0.0051 |          99.0091 |         -39.5033 |
[32m[20221214 00:27:10 @agent_ppo2.py:185][0m |          -0.0083 |          97.4946 |         -39.4836 |
[32m[20221214 00:27:10 @agent_ppo2.py:185][0m |          -0.0083 |          95.1856 |         -39.2238 |
[32m[20221214 00:27:10 @agent_ppo2.py:185][0m |          -0.0067 |          93.1869 |         -39.4185 |
[32m[20221214 00:27:10 @agent_ppo2.py:185][0m |          -0.0130 |          93.1447 |         -39.6082 |
[32m[20221214 00:27:11 @agent_ppo2.py:185][0m |          -0.0137 |          92.0770 |         -39.3464 |
[32m[20221214 00:27:11 @agent_ppo2.py:185][0m |          -0.0129 |          92.1500 |         -39.5245 |
[32m[20221214 00:27:11 @agent_ppo2.py:185][0m |          -0.0164 |          91.7227 |         -39.1734 |
[32m[20221214 00:27:11 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:27:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 519.56
[32m[20221214 00:27:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 580.69
[32m[20221214 00:27:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 803.52
[32m[20221214 00:27:11 @agent_ppo2.py:143][0m Total time:      29.68 min
[32m[20221214 00:27:11 @agent_ppo2.py:145][0m 2701312 total steps have happened
[32m[20221214 00:27:11 @agent_ppo2.py:121][0m #------------------------ Iteration 5319 --------------------------#
[32m[20221214 00:27:11 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:27:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:27:11 @agent_ppo2.py:185][0m |          -0.0002 |         103.8227 |         -41.0720 |
[32m[20221214 00:27:11 @agent_ppo2.py:185][0m |          -0.0031 |          98.2568 |         -41.2451 |
[32m[20221214 00:27:11 @agent_ppo2.py:185][0m |          -0.0029 |          96.5545 |         -41.6043 |
[32m[20221214 00:27:12 @agent_ppo2.py:185][0m |          -0.0038 |          95.1013 |         -41.8240 |
[32m[20221214 00:27:12 @agent_ppo2.py:185][0m |          -0.0014 |          94.6175 |         -41.6616 |
[32m[20221214 00:27:12 @agent_ppo2.py:185][0m |          -0.0114 |          93.9232 |         -41.9974 |
[32m[20221214 00:27:12 @agent_ppo2.py:185][0m |          -0.0027 |          93.4125 |         -42.1515 |
[32m[20221214 00:27:12 @agent_ppo2.py:185][0m |          -0.0055 |          92.6340 |         -42.0041 |
[32m[20221214 00:27:12 @agent_ppo2.py:185][0m |          -0.0043 |          93.1591 |         -42.0183 |
[32m[20221214 00:27:12 @agent_ppo2.py:185][0m |          -0.0123 |          91.1021 |         -42.1393 |
[32m[20221214 00:27:12 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:27:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 593.09
[32m[20221214 00:27:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 695.48
[32m[20221214 00:27:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 843.35
[32m[20221214 00:27:12 @agent_ppo2.py:143][0m Total time:      29.70 min
[32m[20221214 00:27:12 @agent_ppo2.py:145][0m 2703360 total steps have happened
[32m[20221214 00:27:12 @agent_ppo2.py:121][0m #------------------------ Iteration 5320 --------------------------#
[32m[20221214 00:27:12 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:27:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:27:13 @agent_ppo2.py:185][0m |           0.0077 |         106.3203 |         -43.9886 |
[32m[20221214 00:27:13 @agent_ppo2.py:185][0m |          -0.0020 |          90.2743 |         -43.3907 |
[32m[20221214 00:27:13 @agent_ppo2.py:185][0m |          -0.0078 |          85.4045 |         -43.3367 |
[32m[20221214 00:27:13 @agent_ppo2.py:185][0m |          -0.0070 |          82.4994 |         -43.6423 |
[32m[20221214 00:27:13 @agent_ppo2.py:185][0m |          -0.0024 |          80.4831 |         -43.4140 |
[32m[20221214 00:27:13 @agent_ppo2.py:185][0m |          -0.0059 |          78.3094 |         -43.7228 |
[32m[20221214 00:27:13 @agent_ppo2.py:185][0m |          -0.0145 |          76.7113 |         -44.0279 |
[32m[20221214 00:27:13 @agent_ppo2.py:185][0m |          -0.0125 |          75.9295 |         -43.7851 |
[32m[20221214 00:27:13 @agent_ppo2.py:185][0m |          -0.0130 |          74.2726 |         -43.5988 |
[32m[20221214 00:27:13 @agent_ppo2.py:185][0m |          -0.0172 |          73.2934 |         -43.9539 |
[32m[20221214 00:27:13 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:27:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 531.75
[32m[20221214 00:27:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 594.04
[32m[20221214 00:27:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 654.03
[32m[20221214 00:27:14 @agent_ppo2.py:143][0m Total time:      29.72 min
[32m[20221214 00:27:14 @agent_ppo2.py:145][0m 2705408 total steps have happened
[32m[20221214 00:27:14 @agent_ppo2.py:121][0m #------------------------ Iteration 5321 --------------------------#
[32m[20221214 00:27:14 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:27:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:27:14 @agent_ppo2.py:185][0m |          -0.0006 |         100.6163 |         -45.9300 |
[32m[20221214 00:27:14 @agent_ppo2.py:185][0m |           0.0016 |          94.6650 |         -45.5010 |
[32m[20221214 00:27:14 @agent_ppo2.py:185][0m |          -0.0002 |          91.6706 |         -45.7084 |
[32m[20221214 00:27:14 @agent_ppo2.py:185][0m |          -0.0069 |          91.3277 |         -45.7815 |
[32m[20221214 00:27:14 @agent_ppo2.py:185][0m |          -0.0044 |          88.7625 |         -45.6450 |
[32m[20221214 00:27:14 @agent_ppo2.py:185][0m |          -0.0068 |          87.8609 |         -45.7639 |
[32m[20221214 00:27:14 @agent_ppo2.py:185][0m |          -0.0068 |          87.7777 |         -45.8124 |
[32m[20221214 00:27:15 @agent_ppo2.py:185][0m |          -0.0099 |          87.2693 |         -45.6089 |
[32m[20221214 00:27:15 @agent_ppo2.py:185][0m |          -0.0107 |          86.4696 |         -45.9012 |
[32m[20221214 00:27:15 @agent_ppo2.py:185][0m |          -0.0094 |          86.4281 |         -45.3138 |
[32m[20221214 00:27:15 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:27:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 713.84
[32m[20221214 00:27:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 819.50
[32m[20221214 00:27:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 669.84
[32m[20221214 00:27:15 @agent_ppo2.py:143][0m Total time:      29.74 min
[32m[20221214 00:27:15 @agent_ppo2.py:145][0m 2707456 total steps have happened
[32m[20221214 00:27:15 @agent_ppo2.py:121][0m #------------------------ Iteration 5322 --------------------------#
[32m[20221214 00:27:15 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:27:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:27:15 @agent_ppo2.py:185][0m |          -0.0015 |         102.8707 |         -45.2865 |
[32m[20221214 00:27:15 @agent_ppo2.py:185][0m |          -0.0143 |          92.0132 |         -45.3371 |
[32m[20221214 00:27:15 @agent_ppo2.py:185][0m |          -0.0122 |          87.6057 |         -45.2695 |
[32m[20221214 00:27:16 @agent_ppo2.py:185][0m |          -0.0128 |          85.4387 |         -45.3338 |
[32m[20221214 00:27:16 @agent_ppo2.py:185][0m |          -0.0141 |          83.6534 |         -45.5148 |
[32m[20221214 00:27:16 @agent_ppo2.py:185][0m |          -0.0168 |          83.0393 |         -45.2693 |
[32m[20221214 00:27:16 @agent_ppo2.py:185][0m |          -0.0198 |          81.8346 |         -45.4474 |
[32m[20221214 00:27:16 @agent_ppo2.py:185][0m |          -0.0209 |          81.4173 |         -45.3247 |
[32m[20221214 00:27:16 @agent_ppo2.py:185][0m |          -0.0143 |          82.9584 |         -45.4326 |
[32m[20221214 00:27:16 @agent_ppo2.py:185][0m |          -0.0223 |          79.9163 |         -45.6464 |
[32m[20221214 00:27:16 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:27:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 427.88
[32m[20221214 00:27:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 589.61
[32m[20221214 00:27:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 372.42
[32m[20221214 00:27:16 @agent_ppo2.py:143][0m Total time:      29.76 min
[32m[20221214 00:27:16 @agent_ppo2.py:145][0m 2709504 total steps have happened
[32m[20221214 00:27:16 @agent_ppo2.py:121][0m #------------------------ Iteration 5323 --------------------------#
[32m[20221214 00:27:16 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:27:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:27:17 @agent_ppo2.py:185][0m |           0.0144 |          61.2909 |         -50.8795 |
[32m[20221214 00:27:17 @agent_ppo2.py:185][0m |          -0.0040 |          52.3072 |         -50.9343 |
[32m[20221214 00:27:17 @agent_ppo2.py:185][0m |           0.0012 |          49.0136 |         -50.9331 |
[32m[20221214 00:27:17 @agent_ppo2.py:185][0m |          -0.0061 |          47.0076 |         -51.1082 |
[32m[20221214 00:27:17 @agent_ppo2.py:185][0m |          -0.0075 |          45.5368 |         -51.1244 |
[32m[20221214 00:27:17 @agent_ppo2.py:185][0m |          -0.0131 |          44.2363 |         -51.1831 |
[32m[20221214 00:27:17 @agent_ppo2.py:185][0m |          -0.0115 |          47.3634 |         -50.9554 |
[32m[20221214 00:27:17 @agent_ppo2.py:185][0m |          -0.0144 |          42.6498 |         -50.8699 |
[32m[20221214 00:27:17 @agent_ppo2.py:185][0m |          -0.0146 |          41.4832 |         -50.9737 |
[32m[20221214 00:27:17 @agent_ppo2.py:185][0m |          -0.0188 |          41.0467 |         -51.1620 |
[32m[20221214 00:27:17 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:27:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 504.30
[32m[20221214 00:27:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 671.72
[32m[20221214 00:27:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 617.31
[32m[20221214 00:27:18 @agent_ppo2.py:143][0m Total time:      29.79 min
[32m[20221214 00:27:18 @agent_ppo2.py:145][0m 2711552 total steps have happened
[32m[20221214 00:27:18 @agent_ppo2.py:121][0m #------------------------ Iteration 5324 --------------------------#
[32m[20221214 00:27:18 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:27:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:27:18 @agent_ppo2.py:185][0m |           0.0030 |         104.5245 |         -46.2841 |
[32m[20221214 00:27:18 @agent_ppo2.py:185][0m |          -0.0074 |          89.4850 |         -46.3695 |
[32m[20221214 00:27:18 @agent_ppo2.py:185][0m |          -0.0090 |          85.4247 |         -46.0598 |
[32m[20221214 00:27:18 @agent_ppo2.py:185][0m |          -0.0115 |          83.0376 |         -46.0611 |
[32m[20221214 00:27:18 @agent_ppo2.py:185][0m |          -0.0144 |          81.8029 |         -46.0466 |
[32m[20221214 00:27:18 @agent_ppo2.py:185][0m |          -0.0132 |          79.8383 |         -46.1627 |
[32m[20221214 00:27:19 @agent_ppo2.py:185][0m |          -0.0139 |          78.6432 |         -45.7431 |
[32m[20221214 00:27:19 @agent_ppo2.py:185][0m |          -0.0111 |          78.3473 |         -46.1715 |
[32m[20221214 00:27:19 @agent_ppo2.py:185][0m |          -0.0163 |          77.2216 |         -46.1760 |
[32m[20221214 00:27:19 @agent_ppo2.py:185][0m |          -0.0173 |          76.0045 |         -45.8897 |
[32m[20221214 00:27:19 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:27:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 580.14
[32m[20221214 00:27:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 637.16
[32m[20221214 00:27:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 761.75
[32m[20221214 00:27:19 @agent_ppo2.py:143][0m Total time:      29.81 min
[32m[20221214 00:27:19 @agent_ppo2.py:145][0m 2713600 total steps have happened
[32m[20221214 00:27:19 @agent_ppo2.py:121][0m #------------------------ Iteration 5325 --------------------------#
[32m[20221214 00:27:19 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:27:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:27:19 @agent_ppo2.py:185][0m |           0.0097 |          52.2317 |         -48.7301 |
[32m[20221214 00:27:19 @agent_ppo2.py:185][0m |          -0.0038 |          41.8833 |         -49.0802 |
[32m[20221214 00:27:19 @agent_ppo2.py:185][0m |          -0.0049 |          38.9251 |         -48.8615 |
[32m[20221214 00:27:20 @agent_ppo2.py:185][0m |          -0.0090 |          37.2983 |         -48.8042 |
[32m[20221214 00:27:20 @agent_ppo2.py:185][0m |          -0.0107 |          36.1852 |         -48.8278 |
[32m[20221214 00:27:20 @agent_ppo2.py:185][0m |          -0.0106 |          35.2592 |         -48.5431 |
[32m[20221214 00:27:20 @agent_ppo2.py:185][0m |          -0.0110 |          34.6376 |         -48.7938 |
[32m[20221214 00:27:20 @agent_ppo2.py:185][0m |          -0.0113 |          34.2379 |         -48.8950 |
[32m[20221214 00:27:20 @agent_ppo2.py:185][0m |          -0.0141 |          33.4816 |         -48.9822 |
[32m[20221214 00:27:20 @agent_ppo2.py:185][0m |          -0.0156 |          33.1845 |         -48.7683 |
[32m[20221214 00:27:20 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:27:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 400.65
[32m[20221214 00:27:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 672.17
[32m[20221214 00:27:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 748.38
[32m[20221214 00:27:20 @agent_ppo2.py:143][0m Total time:      29.83 min
[32m[20221214 00:27:20 @agent_ppo2.py:145][0m 2715648 total steps have happened
[32m[20221214 00:27:20 @agent_ppo2.py:121][0m #------------------------ Iteration 5326 --------------------------#
[32m[20221214 00:27:20 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:27:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:27:21 @agent_ppo2.py:185][0m |           0.0051 |          97.5482 |         -47.0919 |
[32m[20221214 00:27:21 @agent_ppo2.py:185][0m |          -0.0034 |          84.4841 |         -47.1345 |
[32m[20221214 00:27:21 @agent_ppo2.py:185][0m |          -0.0079 |          81.2892 |         -46.8208 |
[32m[20221214 00:27:21 @agent_ppo2.py:185][0m |          -0.0071 |          79.3364 |         -46.8710 |
[32m[20221214 00:27:21 @agent_ppo2.py:185][0m |          -0.0188 |          77.4366 |         -46.9202 |
[32m[20221214 00:27:21 @agent_ppo2.py:185][0m |          -0.0139 |          76.0888 |         -46.7961 |
[32m[20221214 00:27:21 @agent_ppo2.py:185][0m |          -0.0187 |          74.7300 |         -46.5656 |
[32m[20221214 00:27:21 @agent_ppo2.py:185][0m |          -0.0173 |          73.6669 |         -46.6323 |
[32m[20221214 00:27:21 @agent_ppo2.py:185][0m |          -0.0121 |          81.9200 |         -46.6394 |
[32m[20221214 00:27:21 @agent_ppo2.py:185][0m |          -0.0217 |          73.0632 |         -46.5200 |
[32m[20221214 00:27:21 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:27:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 504.72
[32m[20221214 00:27:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 557.10
[32m[20221214 00:27:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 845.16
[32m[20221214 00:27:22 @agent_ppo2.py:143][0m Total time:      29.85 min
[32m[20221214 00:27:22 @agent_ppo2.py:145][0m 2717696 total steps have happened
[32m[20221214 00:27:22 @agent_ppo2.py:121][0m #------------------------ Iteration 5327 --------------------------#
[32m[20221214 00:27:22 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:27:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:27:22 @agent_ppo2.py:185][0m |           0.0047 |          78.4918 |         -46.0455 |
[32m[20221214 00:27:22 @agent_ppo2.py:185][0m |           0.0047 |          62.6379 |         -45.7068 |
[32m[20221214 00:27:22 @agent_ppo2.py:185][0m |          -0.0033 |          57.5688 |         -45.7994 |
[32m[20221214 00:27:22 @agent_ppo2.py:185][0m |          -0.0026 |          55.3696 |         -46.1319 |
[32m[20221214 00:27:22 @agent_ppo2.py:185][0m |          -0.0089 |          54.8012 |         -45.9503 |
[32m[20221214 00:27:22 @agent_ppo2.py:185][0m |          -0.0120 |          53.3401 |         -45.7954 |
[32m[20221214 00:27:23 @agent_ppo2.py:185][0m |          -0.0117 |          52.5979 |         -45.7843 |
[32m[20221214 00:27:23 @agent_ppo2.py:185][0m |          -0.0070 |          52.4048 |         -45.8107 |
[32m[20221214 00:27:23 @agent_ppo2.py:185][0m |          -0.0129 |          51.9038 |         -46.1615 |
[32m[20221214 00:27:23 @agent_ppo2.py:185][0m |          -0.0130 |          51.0931 |         -46.0631 |
[32m[20221214 00:27:23 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:27:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 655.41
[32m[20221214 00:27:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 755.33
[32m[20221214 00:27:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 509.38
[32m[20221214 00:27:23 @agent_ppo2.py:143][0m Total time:      29.88 min
[32m[20221214 00:27:23 @agent_ppo2.py:145][0m 2719744 total steps have happened
[32m[20221214 00:27:23 @agent_ppo2.py:121][0m #------------------------ Iteration 5328 --------------------------#
[32m[20221214 00:27:23 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:27:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:27:23 @agent_ppo2.py:185][0m |          -0.0023 |          77.8557 |         -49.0291 |
[32m[20221214 00:27:23 @agent_ppo2.py:185][0m |          -0.0095 |          61.1945 |         -48.8703 |
[32m[20221214 00:27:23 @agent_ppo2.py:185][0m |          -0.0056 |          57.4715 |         -48.6028 |
[32m[20221214 00:27:24 @agent_ppo2.py:185][0m |          -0.0136 |          55.1510 |         -49.0973 |
[32m[20221214 00:27:24 @agent_ppo2.py:185][0m |          -0.0133 |          53.5688 |         -49.1942 |
[32m[20221214 00:27:24 @agent_ppo2.py:185][0m |          -0.0132 |          52.1580 |         -48.7759 |
[32m[20221214 00:27:24 @agent_ppo2.py:185][0m |          -0.0195 |          51.7839 |         -49.2505 |
[32m[20221214 00:27:24 @agent_ppo2.py:185][0m |          -0.0089 |          53.4818 |         -49.1430 |
[32m[20221214 00:27:24 @agent_ppo2.py:185][0m |          -0.0149 |          49.9352 |         -48.9521 |
[32m[20221214 00:27:24 @agent_ppo2.py:185][0m |          -0.0210 |          49.1141 |         -48.9853 |
[32m[20221214 00:27:24 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:27:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 584.51
[32m[20221214 00:27:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 642.07
[32m[20221214 00:27:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 656.34
[32m[20221214 00:27:24 @agent_ppo2.py:143][0m Total time:      29.90 min
[32m[20221214 00:27:24 @agent_ppo2.py:145][0m 2721792 total steps have happened
[32m[20221214 00:27:24 @agent_ppo2.py:121][0m #------------------------ Iteration 5329 --------------------------#
[32m[20221214 00:27:24 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:27:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:27:25 @agent_ppo2.py:185][0m |           0.0000 |         114.8482 |         -48.9798 |
[32m[20221214 00:27:25 @agent_ppo2.py:185][0m |          -0.0015 |          97.3101 |         -48.7301 |
[32m[20221214 00:27:25 @agent_ppo2.py:185][0m |          -0.0043 |          91.8358 |         -48.1115 |
[32m[20221214 00:27:25 @agent_ppo2.py:185][0m |          -0.0143 |          87.7934 |         -47.8833 |
[32m[20221214 00:27:25 @agent_ppo2.py:185][0m |          -0.0069 |          85.0182 |         -48.0015 |
[32m[20221214 00:27:25 @agent_ppo2.py:185][0m |          -0.0105 |          83.3219 |         -47.8378 |
[32m[20221214 00:27:25 @agent_ppo2.py:185][0m |          -0.0146 |          81.9053 |         -47.8567 |
[32m[20221214 00:27:25 @agent_ppo2.py:185][0m |          -0.0184 |          80.9186 |         -47.5411 |
[32m[20221214 00:27:25 @agent_ppo2.py:185][0m |          -0.0015 |          89.8549 |         -47.4546 |
[32m[20221214 00:27:25 @agent_ppo2.py:185][0m |          -0.0172 |          80.0803 |         -47.0993 |
[32m[20221214 00:27:25 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:27:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 513.03
[32m[20221214 00:27:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 648.23
[32m[20221214 00:27:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 677.49
[32m[20221214 00:27:26 @agent_ppo2.py:143][0m Total time:      29.92 min
[32m[20221214 00:27:26 @agent_ppo2.py:145][0m 2723840 total steps have happened
[32m[20221214 00:27:26 @agent_ppo2.py:121][0m #------------------------ Iteration 5330 --------------------------#
[32m[20221214 00:27:26 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:27:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:27:26 @agent_ppo2.py:185][0m |           0.0001 |          38.9766 |         -45.0500 |
[32m[20221214 00:27:26 @agent_ppo2.py:185][0m |          -0.0050 |          31.0682 |         -44.9315 |
[32m[20221214 00:27:26 @agent_ppo2.py:185][0m |          -0.0015 |          28.6054 |         -44.8505 |
[32m[20221214 00:27:26 @agent_ppo2.py:185][0m |          -0.0037 |          27.3073 |         -44.7296 |
[32m[20221214 00:27:26 @agent_ppo2.py:185][0m |          -0.0173 |          26.3145 |         -45.0473 |
[32m[20221214 00:27:26 @agent_ppo2.py:185][0m |          -0.0146 |          25.4145 |         -44.9173 |
[32m[20221214 00:27:27 @agent_ppo2.py:185][0m |          -0.0211 |          24.8686 |         -45.3466 |
[32m[20221214 00:27:27 @agent_ppo2.py:185][0m |          -0.0133 |          24.4566 |         -45.5157 |
[32m[20221214 00:27:27 @agent_ppo2.py:185][0m |          -0.0068 |          27.3594 |         -45.3695 |
[32m[20221214 00:27:27 @agent_ppo2.py:185][0m |          -0.0153 |          23.6396 |         -45.1644 |
[32m[20221214 00:27:27 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:27:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 473.75
[32m[20221214 00:27:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 745.02
[32m[20221214 00:27:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 741.96
[32m[20221214 00:27:27 @agent_ppo2.py:143][0m Total time:      29.94 min
[32m[20221214 00:27:27 @agent_ppo2.py:145][0m 2725888 total steps have happened
[32m[20221214 00:27:27 @agent_ppo2.py:121][0m #------------------------ Iteration 5331 --------------------------#
[32m[20221214 00:27:27 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:27:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:27:27 @agent_ppo2.py:185][0m |           0.0019 |          99.4204 |         -48.0693 |
[32m[20221214 00:27:27 @agent_ppo2.py:185][0m |           0.0041 |          96.2975 |         -47.9881 |
[32m[20221214 00:27:27 @agent_ppo2.py:185][0m |          -0.0003 |          85.1520 |         -47.8696 |
[32m[20221214 00:27:28 @agent_ppo2.py:185][0m |          -0.0086 |          82.3631 |         -48.0262 |
[32m[20221214 00:27:28 @agent_ppo2.py:185][0m |          -0.0097 |          80.8066 |         -47.7843 |
[32m[20221214 00:27:28 @agent_ppo2.py:185][0m |          -0.0093 |          81.2728 |         -47.9125 |
[32m[20221214 00:27:28 @agent_ppo2.py:185][0m |          -0.0061 |          79.3549 |         -47.6923 |
[32m[20221214 00:27:28 @agent_ppo2.py:185][0m |          -0.0125 |          78.5464 |         -47.6362 |
[32m[20221214 00:27:28 @agent_ppo2.py:185][0m |          -0.0086 |          78.4186 |         -47.2706 |
[32m[20221214 00:27:28 @agent_ppo2.py:185][0m |          -0.0109 |          77.9496 |         -47.3180 |
[32m[20221214 00:27:28 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:27:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 717.88
[32m[20221214 00:27:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 792.38
[32m[20221214 00:27:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 808.67
[32m[20221214 00:27:28 @agent_ppo2.py:143][0m Total time:      29.97 min
[32m[20221214 00:27:28 @agent_ppo2.py:145][0m 2727936 total steps have happened
[32m[20221214 00:27:28 @agent_ppo2.py:121][0m #------------------------ Iteration 5332 --------------------------#
[32m[20221214 00:27:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:27:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:27:29 @agent_ppo2.py:185][0m |           0.0023 |          94.4538 |         -48.1208 |
[32m[20221214 00:27:29 @agent_ppo2.py:185][0m |          -0.0101 |          82.2948 |         -47.5744 |
[32m[20221214 00:27:29 @agent_ppo2.py:185][0m |          -0.0059 |          78.3822 |         -47.8669 |
[32m[20221214 00:27:29 @agent_ppo2.py:185][0m |          -0.0057 |          78.1295 |         -47.4922 |
[32m[20221214 00:27:29 @agent_ppo2.py:185][0m |          -0.0122 |          74.8361 |         -47.6979 |
[32m[20221214 00:27:29 @agent_ppo2.py:185][0m |          -0.0079 |          73.5782 |         -47.3678 |
[32m[20221214 00:27:29 @agent_ppo2.py:185][0m |          -0.0120 |          73.1142 |         -47.7637 |
[32m[20221214 00:27:29 @agent_ppo2.py:185][0m |          -0.0182 |          72.4838 |         -47.6665 |
[32m[20221214 00:27:29 @agent_ppo2.py:185][0m |          -0.0158 |          72.0463 |         -47.5657 |
[32m[20221214 00:27:29 @agent_ppo2.py:185][0m |          -0.0155 |          71.1288 |         -47.8275 |
[32m[20221214 00:27:29 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:27:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 627.41
[32m[20221214 00:27:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 735.32
[32m[20221214 00:27:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 665.86
[32m[20221214 00:27:30 @agent_ppo2.py:143][0m Total time:      29.99 min
[32m[20221214 00:27:30 @agent_ppo2.py:145][0m 2729984 total steps have happened
[32m[20221214 00:27:30 @agent_ppo2.py:121][0m #------------------------ Iteration 5333 --------------------------#
[32m[20221214 00:27:30 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:27:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:27:30 @agent_ppo2.py:185][0m |          -0.0002 |         127.6276 |         -47.1009 |
[32m[20221214 00:27:30 @agent_ppo2.py:185][0m |          -0.0019 |         124.6216 |         -46.9022 |
[32m[20221214 00:27:30 @agent_ppo2.py:185][0m |          -0.0025 |         122.8772 |         -47.1155 |
[32m[20221214 00:27:30 @agent_ppo2.py:185][0m |           0.0015 |         124.9742 |         -47.0757 |
[32m[20221214 00:27:30 @agent_ppo2.py:185][0m |          -0.0070 |         123.4095 |         -47.3287 |
[32m[20221214 00:27:30 @agent_ppo2.py:185][0m |           0.0026 |         133.0821 |         -47.5995 |
[32m[20221214 00:27:31 @agent_ppo2.py:185][0m |          -0.0011 |         125.3708 |         -47.1157 |
[32m[20221214 00:27:31 @agent_ppo2.py:185][0m |          -0.0102 |         120.1779 |         -47.4843 |
[32m[20221214 00:27:31 @agent_ppo2.py:185][0m |          -0.0087 |         120.0335 |         -47.7514 |
[32m[20221214 00:27:31 @agent_ppo2.py:185][0m |          -0.0059 |         119.9573 |         -47.8044 |
[32m[20221214 00:27:31 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:27:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 683.73
[32m[20221214 00:27:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.83
[32m[20221214 00:27:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 614.97
[32m[20221214 00:27:31 @agent_ppo2.py:143][0m Total time:      30.01 min
[32m[20221214 00:27:31 @agent_ppo2.py:145][0m 2732032 total steps have happened
[32m[20221214 00:27:31 @agent_ppo2.py:121][0m #------------------------ Iteration 5334 --------------------------#
[32m[20221214 00:27:31 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:27:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:27:31 @agent_ppo2.py:185][0m |          -0.0032 |         172.9848 |         -48.9331 |
[32m[20221214 00:27:31 @agent_ppo2.py:185][0m |          -0.0042 |         169.2143 |         -49.0733 |
[32m[20221214 00:27:32 @agent_ppo2.py:185][0m |          -0.0035 |         167.5131 |         -48.9364 |
[32m[20221214 00:27:32 @agent_ppo2.py:185][0m |          -0.0019 |         167.1099 |         -48.4300 |
[32m[20221214 00:27:32 @agent_ppo2.py:185][0m |          -0.0055 |         167.0076 |         -49.0155 |
[32m[20221214 00:27:32 @agent_ppo2.py:185][0m |           0.0014 |         169.8853 |         -48.8454 |
[32m[20221214 00:27:32 @agent_ppo2.py:185][0m |           0.0052 |         176.8390 |         -49.0183 |
[32m[20221214 00:27:32 @agent_ppo2.py:185][0m |          -0.0036 |         166.3577 |         -48.5861 |
[32m[20221214 00:27:32 @agent_ppo2.py:185][0m |          -0.0065 |         166.0568 |         -48.6965 |
[32m[20221214 00:27:32 @agent_ppo2.py:185][0m |          -0.0025 |         165.6288 |         -48.8115 |
[32m[20221214 00:27:32 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:27:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 738.64
[32m[20221214 00:27:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 758.80
[32m[20221214 00:27:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 710.80
[32m[20221214 00:27:32 @agent_ppo2.py:143][0m Total time:      30.03 min
[32m[20221214 00:27:32 @agent_ppo2.py:145][0m 2734080 total steps have happened
[32m[20221214 00:27:32 @agent_ppo2.py:121][0m #------------------------ Iteration 5335 --------------------------#
[32m[20221214 00:27:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:27:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:27:33 @agent_ppo2.py:185][0m |           0.0040 |         177.5990 |         -47.7011 |
[32m[20221214 00:27:33 @agent_ppo2.py:185][0m |           0.0021 |         174.0300 |         -47.4495 |
[32m[20221214 00:27:33 @agent_ppo2.py:185][0m |           0.0048 |         172.8237 |         -47.1298 |
[32m[20221214 00:27:33 @agent_ppo2.py:185][0m |          -0.0078 |         171.3312 |         -47.6900 |
[32m[20221214 00:27:33 @agent_ppo2.py:185][0m |          -0.0009 |         172.4788 |         -47.1541 |
[32m[20221214 00:27:33 @agent_ppo2.py:185][0m |          -0.0066 |         170.5432 |         -47.8784 |
[32m[20221214 00:27:33 @agent_ppo2.py:185][0m |          -0.0096 |         169.6061 |         -47.3009 |
[32m[20221214 00:27:33 @agent_ppo2.py:185][0m |          -0.0087 |         169.4479 |         -47.4977 |
[32m[20221214 00:27:34 @agent_ppo2.py:185][0m |          -0.0077 |         170.5469 |         -46.7415 |
[32m[20221214 00:27:34 @agent_ppo2.py:185][0m |          -0.0084 |         169.3191 |         -47.2221 |
[32m[20221214 00:27:34 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 00:27:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 641.97
[32m[20221214 00:27:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 750.43
[32m[20221214 00:27:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 624.36
[32m[20221214 00:27:34 @agent_ppo2.py:143][0m Total time:      30.06 min
[32m[20221214 00:27:34 @agent_ppo2.py:145][0m 2736128 total steps have happened
[32m[20221214 00:27:34 @agent_ppo2.py:121][0m #------------------------ Iteration 5336 --------------------------#
[32m[20221214 00:27:34 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:27:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:27:34 @agent_ppo2.py:185][0m |           0.0130 |         164.4890 |         -50.8997 |
[32m[20221214 00:27:34 @agent_ppo2.py:185][0m |          -0.0001 |         154.7422 |         -51.0087 |
[32m[20221214 00:27:34 @agent_ppo2.py:185][0m |          -0.0055 |         149.8757 |         -50.6397 |
[32m[20221214 00:27:35 @agent_ppo2.py:185][0m |          -0.0058 |         147.7543 |         -50.2661 |
[32m[20221214 00:27:35 @agent_ppo2.py:185][0m |           0.0010 |         155.3578 |         -50.6544 |
[32m[20221214 00:27:35 @agent_ppo2.py:185][0m |          -0.0081 |         145.6436 |         -50.4652 |
[32m[20221214 00:27:35 @agent_ppo2.py:185][0m |          -0.0072 |         144.6132 |         -50.2273 |
[32m[20221214 00:27:35 @agent_ppo2.py:185][0m |          -0.0006 |         152.8285 |         -49.9697 |
[32m[20221214 00:27:35 @agent_ppo2.py:185][0m |          -0.0089 |         143.8449 |         -49.8676 |
[32m[20221214 00:27:35 @agent_ppo2.py:185][0m |          -0.0104 |         142.6821 |         -50.1100 |
[32m[20221214 00:27:35 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:27:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 718.57
[32m[20221214 00:27:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 793.95
[32m[20221214 00:27:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 677.15
[32m[20221214 00:27:35 @agent_ppo2.py:143][0m Total time:      30.08 min
[32m[20221214 00:27:35 @agent_ppo2.py:145][0m 2738176 total steps have happened
[32m[20221214 00:27:35 @agent_ppo2.py:121][0m #------------------------ Iteration 5337 --------------------------#
[32m[20221214 00:27:35 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:27:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:27:36 @agent_ppo2.py:185][0m |           0.0058 |         102.9487 |         -49.2170 |
[32m[20221214 00:27:36 @agent_ppo2.py:185][0m |          -0.0007 |          93.1405 |         -48.7024 |
[32m[20221214 00:27:36 @agent_ppo2.py:185][0m |          -0.0056 |          91.5394 |         -48.8482 |
[32m[20221214 00:27:36 @agent_ppo2.py:185][0m |          -0.0048 |          89.2706 |         -48.5234 |
[32m[20221214 00:27:36 @agent_ppo2.py:185][0m |          -0.0079 |          88.0009 |         -48.5654 |
[32m[20221214 00:27:36 @agent_ppo2.py:185][0m |          -0.0086 |          86.7917 |         -48.3875 |
[32m[20221214 00:27:36 @agent_ppo2.py:185][0m |          -0.0010 |          90.8877 |         -48.4325 |
[32m[20221214 00:27:36 @agent_ppo2.py:185][0m |          -0.0102 |          86.8482 |         -48.4803 |
[32m[20221214 00:27:36 @agent_ppo2.py:185][0m |          -0.0117 |          85.4843 |         -48.4487 |
[32m[20221214 00:27:37 @agent_ppo2.py:185][0m |          -0.0049 |          85.5285 |         -48.7107 |
[32m[20221214 00:27:37 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:27:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 519.09
[32m[20221214 00:27:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 692.71
[32m[20221214 00:27:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 708.15
[32m[20221214 00:27:37 @agent_ppo2.py:143][0m Total time:      30.10 min
[32m[20221214 00:27:37 @agent_ppo2.py:145][0m 2740224 total steps have happened
[32m[20221214 00:27:37 @agent_ppo2.py:121][0m #------------------------ Iteration 5338 --------------------------#
[32m[20221214 00:27:37 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:27:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:27:37 @agent_ppo2.py:185][0m |           0.0092 |         127.8693 |         -47.2006 |
[32m[20221214 00:27:37 @agent_ppo2.py:185][0m |          -0.0014 |         117.1219 |         -47.3375 |
[32m[20221214 00:27:37 @agent_ppo2.py:185][0m |          -0.0055 |         115.1116 |         -47.4690 |
[32m[20221214 00:27:37 @agent_ppo2.py:185][0m |          -0.0040 |         114.1481 |         -47.3486 |
[32m[20221214 00:27:37 @agent_ppo2.py:185][0m |          -0.0059 |         113.8562 |         -47.3179 |
[32m[20221214 00:27:38 @agent_ppo2.py:185][0m |          -0.0117 |         112.9776 |         -47.5215 |
[32m[20221214 00:27:38 @agent_ppo2.py:185][0m |          -0.0117 |         112.5479 |         -47.4528 |
[32m[20221214 00:27:38 @agent_ppo2.py:185][0m |          -0.0101 |         112.7525 |         -47.3971 |
[32m[20221214 00:27:38 @agent_ppo2.py:185][0m |          -0.0127 |         112.5094 |         -47.5587 |
[32m[20221214 00:27:38 @agent_ppo2.py:185][0m |          -0.0170 |         111.5187 |         -47.7025 |
[32m[20221214 00:27:38 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:27:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 625.78
[32m[20221214 00:27:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 773.15
[32m[20221214 00:27:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 694.23
[32m[20221214 00:27:38 @agent_ppo2.py:143][0m Total time:      30.13 min
[32m[20221214 00:27:38 @agent_ppo2.py:145][0m 2742272 total steps have happened
[32m[20221214 00:27:38 @agent_ppo2.py:121][0m #------------------------ Iteration 5339 --------------------------#
[32m[20221214 00:27:38 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:27:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:27:38 @agent_ppo2.py:185][0m |           0.0025 |          93.5312 |         -45.7673 |
[32m[20221214 00:27:38 @agent_ppo2.py:185][0m |          -0.0034 |          83.7508 |         -45.6123 |
[32m[20221214 00:27:39 @agent_ppo2.py:185][0m |          -0.0085 |          81.3407 |         -45.6389 |
[32m[20221214 00:27:39 @agent_ppo2.py:185][0m |          -0.0151 |          78.7138 |         -45.7233 |
[32m[20221214 00:27:39 @agent_ppo2.py:185][0m |          -0.0157 |          77.0238 |         -45.7066 |
[32m[20221214 00:27:39 @agent_ppo2.py:185][0m |          -0.0205 |          76.2686 |         -45.5300 |
[32m[20221214 00:27:39 @agent_ppo2.py:185][0m |          -0.0181 |          75.4418 |         -45.2821 |
[32m[20221214 00:27:39 @agent_ppo2.py:185][0m |          -0.0091 |          76.4478 |         -45.3637 |
[32m[20221214 00:27:39 @agent_ppo2.py:185][0m |          -0.0185 |          73.6033 |         -45.4228 |
[32m[20221214 00:27:39 @agent_ppo2.py:185][0m |          -0.0220 |          72.8554 |         -45.3790 |
[32m[20221214 00:27:39 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:27:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 488.26
[32m[20221214 00:27:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 604.37
[32m[20221214 00:27:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 599.74
[32m[20221214 00:27:39 @agent_ppo2.py:143][0m Total time:      30.15 min
[32m[20221214 00:27:39 @agent_ppo2.py:145][0m 2744320 total steps have happened
[32m[20221214 00:27:39 @agent_ppo2.py:121][0m #------------------------ Iteration 5340 --------------------------#
[32m[20221214 00:27:40 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:27:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:27:40 @agent_ppo2.py:185][0m |          -0.0009 |          72.2364 |         -49.3658 |
[32m[20221214 00:27:40 @agent_ppo2.py:185][0m |          -0.0038 |          61.1693 |         -48.9106 |
[32m[20221214 00:27:40 @agent_ppo2.py:185][0m |          -0.0096 |          58.7233 |         -49.1118 |
[32m[20221214 00:27:40 @agent_ppo2.py:185][0m |          -0.0154 |          55.4400 |         -49.3026 |
[32m[20221214 00:27:40 @agent_ppo2.py:185][0m |          -0.0203 |          52.8271 |         -49.1889 |
[32m[20221214 00:27:40 @agent_ppo2.py:185][0m |          -0.0085 |          53.1537 |         -49.1823 |
[32m[20221214 00:27:40 @agent_ppo2.py:185][0m |          -0.0170 |          50.0087 |         -48.9937 |
[32m[20221214 00:27:40 @agent_ppo2.py:185][0m |          -0.0230 |          49.5653 |         -48.9542 |
[32m[20221214 00:27:41 @agent_ppo2.py:185][0m |          -0.0212 |          48.3510 |         -49.1773 |
[32m[20221214 00:27:41 @agent_ppo2.py:185][0m |          -0.0202 |          47.9880 |         -48.9370 |
[32m[20221214 00:27:41 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:27:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 444.65
[32m[20221214 00:27:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 559.07
[32m[20221214 00:27:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 519.39
[32m[20221214 00:27:41 @agent_ppo2.py:143][0m Total time:      30.17 min
[32m[20221214 00:27:41 @agent_ppo2.py:145][0m 2746368 total steps have happened
[32m[20221214 00:27:41 @agent_ppo2.py:121][0m #------------------------ Iteration 5341 --------------------------#
[32m[20221214 00:27:41 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:27:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:27:41 @agent_ppo2.py:185][0m |          -0.0003 |         141.2442 |         -49.3326 |
[32m[20221214 00:27:41 @agent_ppo2.py:185][0m |          -0.0019 |         130.2249 |         -49.2969 |
[32m[20221214 00:27:41 @agent_ppo2.py:185][0m |          -0.0018 |         125.8636 |         -49.3674 |
[32m[20221214 00:27:41 @agent_ppo2.py:185][0m |          -0.0073 |         125.4096 |         -49.2583 |
[32m[20221214 00:27:41 @agent_ppo2.py:185][0m |          -0.0033 |         122.4505 |         -49.4422 |
[32m[20221214 00:27:42 @agent_ppo2.py:185][0m |          -0.0112 |         121.5937 |         -49.0991 |
[32m[20221214 00:27:42 @agent_ppo2.py:185][0m |          -0.0082 |         120.0089 |         -49.1034 |
[32m[20221214 00:27:42 @agent_ppo2.py:185][0m |          -0.0112 |         119.8764 |         -49.7856 |
[32m[20221214 00:27:42 @agent_ppo2.py:185][0m |          -0.0085 |         121.9421 |         -49.4138 |
[32m[20221214 00:27:42 @agent_ppo2.py:185][0m |          -0.0082 |         121.2457 |         -49.7424 |
[32m[20221214 00:27:42 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:27:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 664.26
[32m[20221214 00:27:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 744.26
[32m[20221214 00:27:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 717.01
[32m[20221214 00:27:42 @agent_ppo2.py:143][0m Total time:      30.20 min
[32m[20221214 00:27:42 @agent_ppo2.py:145][0m 2748416 total steps have happened
[32m[20221214 00:27:42 @agent_ppo2.py:121][0m #------------------------ Iteration 5342 --------------------------#
[32m[20221214 00:27:42 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:27:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:27:42 @agent_ppo2.py:185][0m |           0.0016 |         110.6287 |         -50.4951 |
[32m[20221214 00:27:43 @agent_ppo2.py:185][0m |          -0.0063 |         104.1868 |         -50.2170 |
[32m[20221214 00:27:43 @agent_ppo2.py:185][0m |          -0.0095 |         101.7068 |         -50.1644 |
[32m[20221214 00:27:43 @agent_ppo2.py:185][0m |          -0.0054 |         100.4178 |         -49.9490 |
[32m[20221214 00:27:43 @agent_ppo2.py:185][0m |          -0.0129 |          98.5996 |         -49.9440 |
[32m[20221214 00:27:43 @agent_ppo2.py:185][0m |          -0.0116 |          96.9814 |         -49.6958 |
[32m[20221214 00:27:43 @agent_ppo2.py:185][0m |          -0.0089 |          99.7385 |         -49.8169 |
[32m[20221214 00:27:43 @agent_ppo2.py:185][0m |          -0.0147 |          95.0628 |         -49.3401 |
[32m[20221214 00:27:43 @agent_ppo2.py:185][0m |          -0.0152 |          94.2180 |         -49.4583 |
[32m[20221214 00:27:43 @agent_ppo2.py:185][0m |          -0.0148 |          93.7488 |         -49.4936 |
[32m[20221214 00:27:43 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:27:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 439.26
[32m[20221214 00:27:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 530.84
[32m[20221214 00:27:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 791.07
[32m[20221214 00:27:43 @agent_ppo2.py:143][0m Total time:      30.22 min
[32m[20221214 00:27:43 @agent_ppo2.py:145][0m 2750464 total steps have happened
[32m[20221214 00:27:43 @agent_ppo2.py:121][0m #------------------------ Iteration 5343 --------------------------#
[32m[20221214 00:27:44 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:27:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:27:44 @agent_ppo2.py:185][0m |           0.0049 |         128.4802 |         -47.4937 |
[32m[20221214 00:27:44 @agent_ppo2.py:185][0m |          -0.0013 |         114.7173 |         -46.9439 |
[32m[20221214 00:27:44 @agent_ppo2.py:185][0m |          -0.0034 |         109.8025 |         -46.4275 |
[32m[20221214 00:27:44 @agent_ppo2.py:185][0m |          -0.0025 |         106.8068 |         -46.4709 |
[32m[20221214 00:27:44 @agent_ppo2.py:185][0m |          -0.0061 |         105.1640 |         -46.6151 |
[32m[20221214 00:27:44 @agent_ppo2.py:185][0m |          -0.0067 |         103.2690 |         -46.4733 |
[32m[20221214 00:27:44 @agent_ppo2.py:185][0m |          -0.0024 |         103.9178 |         -46.6302 |
[32m[20221214 00:27:44 @agent_ppo2.py:185][0m |          -0.0122 |         100.8379 |         -46.3797 |
[32m[20221214 00:27:45 @agent_ppo2.py:185][0m |          -0.0082 |          99.8513 |         -46.2799 |
[32m[20221214 00:27:45 @agent_ppo2.py:185][0m |          -0.0127 |          98.9108 |         -45.9086 |
[32m[20221214 00:27:45 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:27:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 594.11
[32m[20221214 00:27:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 780.97
[32m[20221214 00:27:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 841.46
[32m[20221214 00:27:45 @agent_ppo2.py:143][0m Total time:      30.24 min
[32m[20221214 00:27:45 @agent_ppo2.py:145][0m 2752512 total steps have happened
[32m[20221214 00:27:45 @agent_ppo2.py:121][0m #------------------------ Iteration 5344 --------------------------#
[32m[20221214 00:27:45 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:27:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:27:45 @agent_ppo2.py:185][0m |           0.0067 |         135.0528 |         -48.2193 |
[32m[20221214 00:27:45 @agent_ppo2.py:185][0m |           0.0009 |         123.3903 |         -48.1782 |
[32m[20221214 00:27:45 @agent_ppo2.py:185][0m |          -0.0093 |         120.6772 |         -47.8905 |
[32m[20221214 00:27:45 @agent_ppo2.py:185][0m |          -0.0105 |         119.5227 |         -48.2847 |
[32m[20221214 00:27:46 @agent_ppo2.py:185][0m |          -0.0077 |         117.9388 |         -47.9489 |
[32m[20221214 00:27:46 @agent_ppo2.py:185][0m |          -0.0108 |         117.4393 |         -48.4456 |
[32m[20221214 00:27:46 @agent_ppo2.py:185][0m |          -0.0122 |         115.6216 |         -48.4994 |
[32m[20221214 00:27:46 @agent_ppo2.py:185][0m |          -0.0134 |         115.1540 |         -48.5151 |
[32m[20221214 00:27:46 @agent_ppo2.py:185][0m |          -0.0148 |         114.4293 |         -48.1656 |
[32m[20221214 00:27:46 @agent_ppo2.py:185][0m |          -0.0146 |         113.8234 |         -48.3306 |
[32m[20221214 00:27:46 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:27:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 509.74
[32m[20221214 00:27:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 639.11
[32m[20221214 00:27:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 541.46
[32m[20221214 00:27:46 @agent_ppo2.py:143][0m Total time:      30.26 min
[32m[20221214 00:27:46 @agent_ppo2.py:145][0m 2754560 total steps have happened
[32m[20221214 00:27:46 @agent_ppo2.py:121][0m #------------------------ Iteration 5345 --------------------------#
[32m[20221214 00:27:46 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:27:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:27:46 @agent_ppo2.py:185][0m |           0.0041 |         102.0279 |         -46.1397 |
[32m[20221214 00:27:47 @agent_ppo2.py:185][0m |          -0.0037 |          91.6045 |         -45.9186 |
[32m[20221214 00:27:47 @agent_ppo2.py:185][0m |          -0.0016 |          89.3305 |         -46.2154 |
[32m[20221214 00:27:47 @agent_ppo2.py:185][0m |          -0.0088 |          88.4052 |         -45.8481 |
[32m[20221214 00:27:47 @agent_ppo2.py:185][0m |          -0.0067 |          85.6517 |         -45.8756 |
[32m[20221214 00:27:47 @agent_ppo2.py:185][0m |          -0.0060 |          85.2357 |         -45.6218 |
[32m[20221214 00:27:47 @agent_ppo2.py:185][0m |          -0.0072 |          84.8535 |         -45.7392 |
[32m[20221214 00:27:47 @agent_ppo2.py:185][0m |          -0.0107 |          83.7200 |         -45.9375 |
[32m[20221214 00:27:47 @agent_ppo2.py:185][0m |          -0.0082 |          83.3330 |         -45.6576 |
[32m[20221214 00:27:47 @agent_ppo2.py:185][0m |          -0.0134 |          82.4135 |         -45.7901 |
[32m[20221214 00:27:47 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:27:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 450.83
[32m[20221214 00:27:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 587.25
[32m[20221214 00:27:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 472.84
[32m[20221214 00:27:47 @agent_ppo2.py:143][0m Total time:      30.28 min
[32m[20221214 00:27:47 @agent_ppo2.py:145][0m 2756608 total steps have happened
[32m[20221214 00:27:47 @agent_ppo2.py:121][0m #------------------------ Iteration 5346 --------------------------#
[32m[20221214 00:27:48 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:27:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:27:48 @agent_ppo2.py:185][0m |           0.0018 |          94.4655 |         -47.6901 |
[32m[20221214 00:27:48 @agent_ppo2.py:185][0m |          -0.0019 |          87.4892 |         -47.8101 |
[32m[20221214 00:27:48 @agent_ppo2.py:185][0m |          -0.0073 |          85.5836 |         -48.0359 |
[32m[20221214 00:27:48 @agent_ppo2.py:185][0m |          -0.0049 |          84.4518 |         -48.0868 |
[32m[20221214 00:27:48 @agent_ppo2.py:185][0m |          -0.0059 |          82.7650 |         -47.9668 |
[32m[20221214 00:27:48 @agent_ppo2.py:185][0m |          -0.0096 |          82.6163 |         -47.9644 |
[32m[20221214 00:27:48 @agent_ppo2.py:185][0m |          -0.0083 |          82.4732 |         -48.1079 |
[32m[20221214 00:27:48 @agent_ppo2.py:185][0m |          -0.0096 |          81.4452 |         -48.0766 |
[32m[20221214 00:27:49 @agent_ppo2.py:185][0m |          -0.0127 |          80.8787 |         -48.2265 |
[32m[20221214 00:27:49 @agent_ppo2.py:185][0m |          -0.0143 |          80.4960 |         -48.6773 |
[32m[20221214 00:27:49 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:27:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 456.57
[32m[20221214 00:27:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 528.87
[32m[20221214 00:27:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 359.32
[32m[20221214 00:27:49 @agent_ppo2.py:143][0m Total time:      30.31 min
[32m[20221214 00:27:49 @agent_ppo2.py:145][0m 2758656 total steps have happened
[32m[20221214 00:27:49 @agent_ppo2.py:121][0m #------------------------ Iteration 5347 --------------------------#
[32m[20221214 00:27:49 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:27:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:27:49 @agent_ppo2.py:185][0m |          -0.0011 |         101.0444 |         -45.1541 |
[32m[20221214 00:27:49 @agent_ppo2.py:185][0m |          -0.0009 |          91.8038 |         -45.8202 |
[32m[20221214 00:27:49 @agent_ppo2.py:185][0m |          -0.0036 |          88.2168 |         -45.8134 |
[32m[20221214 00:27:49 @agent_ppo2.py:185][0m |          -0.0018 |          86.9515 |         -45.5362 |
[32m[20221214 00:27:50 @agent_ppo2.py:185][0m |          -0.0081 |          86.4585 |         -45.7503 |
[32m[20221214 00:27:50 @agent_ppo2.py:185][0m |          -0.0124 |          83.3402 |         -45.6597 |
[32m[20221214 00:27:50 @agent_ppo2.py:185][0m |          -0.0096 |          82.9150 |         -45.9692 |
[32m[20221214 00:27:50 @agent_ppo2.py:185][0m |          -0.0076 |          82.5250 |         -45.8385 |
[32m[20221214 00:27:50 @agent_ppo2.py:185][0m |          -0.0120 |          80.2485 |         -45.7718 |
[32m[20221214 00:27:50 @agent_ppo2.py:185][0m |          -0.0169 |          79.3314 |         -45.6763 |
[32m[20221214 00:27:50 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:27:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 511.85
[32m[20221214 00:27:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 667.84
[32m[20221214 00:27:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 687.80
[32m[20221214 00:27:50 @agent_ppo2.py:143][0m Total time:      30.33 min
[32m[20221214 00:27:50 @agent_ppo2.py:145][0m 2760704 total steps have happened
[32m[20221214 00:27:50 @agent_ppo2.py:121][0m #------------------------ Iteration 5348 --------------------------#
[32m[20221214 00:27:50 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:27:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:27:51 @agent_ppo2.py:185][0m |           0.0039 |          73.3917 |         -50.0255 |
[32m[20221214 00:27:51 @agent_ppo2.py:185][0m |          -0.0092 |          66.3554 |         -50.1465 |
[32m[20221214 00:27:51 @agent_ppo2.py:185][0m |          -0.0112 |          65.6267 |         -49.8010 |
[32m[20221214 00:27:51 @agent_ppo2.py:185][0m |          -0.0140 |          62.4964 |         -49.7873 |
[32m[20221214 00:27:51 @agent_ppo2.py:185][0m |          -0.0108 |          60.9738 |         -49.7568 |
[32m[20221214 00:27:51 @agent_ppo2.py:185][0m |          -0.0166 |          60.0680 |         -49.8350 |
[32m[20221214 00:27:51 @agent_ppo2.py:185][0m |          -0.0181 |          59.0735 |         -49.9106 |
[32m[20221214 00:27:51 @agent_ppo2.py:185][0m |          -0.0198 |          58.2762 |         -50.0118 |
[32m[20221214 00:27:51 @agent_ppo2.py:185][0m |          -0.0148 |          60.0505 |         -50.0448 |
[32m[20221214 00:27:51 @agent_ppo2.py:185][0m |          -0.0108 |          59.7970 |         -49.6119 |
[32m[20221214 00:27:51 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:27:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 493.63
[32m[20221214 00:27:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 563.54
[32m[20221214 00:27:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 679.30
[32m[20221214 00:27:52 @agent_ppo2.py:143][0m Total time:      30.35 min
[32m[20221214 00:27:52 @agent_ppo2.py:145][0m 2762752 total steps have happened
[32m[20221214 00:27:52 @agent_ppo2.py:121][0m #------------------------ Iteration 5349 --------------------------#
[32m[20221214 00:27:52 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:27:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:27:52 @agent_ppo2.py:185][0m |          -0.0012 |         103.3549 |         -47.1498 |
[32m[20221214 00:27:52 @agent_ppo2.py:185][0m |          -0.0095 |          98.1918 |         -47.0784 |
[32m[20221214 00:27:52 @agent_ppo2.py:185][0m |          -0.0118 |          96.5961 |         -46.8042 |
[32m[20221214 00:27:52 @agent_ppo2.py:185][0m |          -0.0088 |          94.9432 |         -46.7875 |
[32m[20221214 00:27:52 @agent_ppo2.py:185][0m |          -0.0080 |          98.8710 |         -46.9709 |
[32m[20221214 00:27:52 @agent_ppo2.py:185][0m |          -0.0154 |          93.3837 |         -46.8582 |
[32m[20221214 00:27:52 @agent_ppo2.py:185][0m |          -0.0179 |          92.5296 |         -47.0989 |
[32m[20221214 00:27:52 @agent_ppo2.py:185][0m |          -0.0121 |          92.4846 |         -46.9003 |
[32m[20221214 00:27:53 @agent_ppo2.py:185][0m |          -0.0158 |          91.1627 |         -47.0337 |
[32m[20221214 00:27:53 @agent_ppo2.py:185][0m |          -0.0168 |          90.9681 |         -47.2521 |
[32m[20221214 00:27:53 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:27:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 544.10
[32m[20221214 00:27:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 587.99
[32m[20221214 00:27:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 608.45
[32m[20221214 00:27:53 @agent_ppo2.py:143][0m Total time:      30.37 min
[32m[20221214 00:27:53 @agent_ppo2.py:145][0m 2764800 total steps have happened
[32m[20221214 00:27:53 @agent_ppo2.py:121][0m #------------------------ Iteration 5350 --------------------------#
[32m[20221214 00:27:53 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:27:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:27:53 @agent_ppo2.py:185][0m |          -0.0016 |         134.0811 |         -51.7169 |
[32m[20221214 00:27:53 @agent_ppo2.py:185][0m |          -0.0097 |         125.7345 |         -51.5140 |
[32m[20221214 00:27:53 @agent_ppo2.py:185][0m |          -0.0121 |         122.6017 |         -51.3278 |
[32m[20221214 00:27:53 @agent_ppo2.py:185][0m |          -0.0131 |         121.0178 |         -51.3505 |
[32m[20221214 00:27:54 @agent_ppo2.py:185][0m |          -0.0122 |         119.8991 |         -51.2168 |
[32m[20221214 00:27:54 @agent_ppo2.py:185][0m |          -0.0165 |         118.5839 |         -51.1492 |
[32m[20221214 00:27:54 @agent_ppo2.py:185][0m |          -0.0175 |         118.2123 |         -51.4243 |
[32m[20221214 00:27:54 @agent_ppo2.py:185][0m |          -0.0141 |         117.8980 |         -51.0787 |
[32m[20221214 00:27:54 @agent_ppo2.py:185][0m |          -0.0108 |         117.4476 |         -51.0459 |
[32m[20221214 00:27:54 @agent_ppo2.py:185][0m |          -0.0148 |         117.1366 |         -50.6260 |
[32m[20221214 00:27:54 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:27:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 713.70
[32m[20221214 00:27:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 821.64
[32m[20221214 00:27:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 549.36
[32m[20221214 00:27:54 @agent_ppo2.py:143][0m Total time:      30.40 min
[32m[20221214 00:27:54 @agent_ppo2.py:145][0m 2766848 total steps have happened
[32m[20221214 00:27:54 @agent_ppo2.py:121][0m #------------------------ Iteration 5351 --------------------------#
[32m[20221214 00:27:54 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:27:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:27:55 @agent_ppo2.py:185][0m |           0.0025 |         138.7353 |         -51.3366 |
[32m[20221214 00:27:55 @agent_ppo2.py:185][0m |          -0.0009 |         134.2190 |         -51.2711 |
[32m[20221214 00:27:55 @agent_ppo2.py:185][0m |           0.0031 |         133.3227 |         -51.5704 |
[32m[20221214 00:27:55 @agent_ppo2.py:185][0m |          -0.0074 |         132.1549 |         -51.2456 |
[32m[20221214 00:27:55 @agent_ppo2.py:185][0m |           0.0096 |         150.6902 |         -51.5979 |
[32m[20221214 00:27:55 @agent_ppo2.py:185][0m |           0.0028 |         140.1969 |         -51.4326 |
[32m[20221214 00:27:55 @agent_ppo2.py:185][0m |          -0.0015 |         131.0140 |         -50.9120 |
[32m[20221214 00:27:55 @agent_ppo2.py:185][0m |          -0.0060 |         131.4141 |         -51.5942 |
[32m[20221214 00:27:55 @agent_ppo2.py:185][0m |          -0.0093 |         130.7665 |         -51.5533 |
[32m[20221214 00:27:55 @agent_ppo2.py:185][0m |          -0.0107 |         130.6950 |         -51.3915 |
[32m[20221214 00:27:55 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:27:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 668.75
[32m[20221214 00:27:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 821.14
[32m[20221214 00:27:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 800.87
[32m[20221214 00:27:56 @agent_ppo2.py:143][0m Total time:      30.42 min
[32m[20221214 00:27:56 @agent_ppo2.py:145][0m 2768896 total steps have happened
[32m[20221214 00:27:56 @agent_ppo2.py:121][0m #------------------------ Iteration 5352 --------------------------#
[32m[20221214 00:27:56 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:27:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:27:56 @agent_ppo2.py:185][0m |           0.0069 |         139.9319 |         -51.4665 |
[32m[20221214 00:27:56 @agent_ppo2.py:185][0m |           0.0007 |         130.1327 |         -51.4968 |
[32m[20221214 00:27:56 @agent_ppo2.py:185][0m |          -0.0020 |         126.8226 |         -51.9506 |
[32m[20221214 00:27:56 @agent_ppo2.py:185][0m |          -0.0029 |         125.9101 |         -51.9035 |
[32m[20221214 00:27:56 @agent_ppo2.py:185][0m |          -0.0058 |         122.1607 |         -52.1132 |
[32m[20221214 00:27:56 @agent_ppo2.py:185][0m |          -0.0034 |         121.5473 |         -52.3915 |
[32m[20221214 00:27:56 @agent_ppo2.py:185][0m |          -0.0062 |         121.0471 |         -52.6921 |
[32m[20221214 00:27:56 @agent_ppo2.py:185][0m |           0.0049 |         124.1030 |         -52.6772 |
[32m[20221214 00:27:57 @agent_ppo2.py:185][0m |          -0.0050 |         119.4821 |         -52.8696 |
[32m[20221214 00:27:57 @agent_ppo2.py:185][0m |          -0.0072 |         120.8146 |         -53.2755 |
[32m[20221214 00:27:57 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:27:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 711.95
[32m[20221214 00:27:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 867.78
[32m[20221214 00:27:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 499.07
[32m[20221214 00:27:57 @agent_ppo2.py:143][0m Total time:      30.44 min
[32m[20221214 00:27:57 @agent_ppo2.py:145][0m 2770944 total steps have happened
[32m[20221214 00:27:57 @agent_ppo2.py:121][0m #------------------------ Iteration 5353 --------------------------#
[32m[20221214 00:27:57 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:27:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:27:57 @agent_ppo2.py:185][0m |           0.0044 |         111.5974 |         -54.8680 |
[32m[20221214 00:27:57 @agent_ppo2.py:185][0m |          -0.0074 |         104.3812 |         -54.7650 |
[32m[20221214 00:27:57 @agent_ppo2.py:185][0m |          -0.0063 |         100.8344 |         -55.0607 |
[32m[20221214 00:27:57 @agent_ppo2.py:185][0m |          -0.0137 |          98.4004 |         -54.4995 |
[32m[20221214 00:27:58 @agent_ppo2.py:185][0m |          -0.0106 |          97.2257 |         -54.6820 |
[32m[20221214 00:27:58 @agent_ppo2.py:185][0m |          -0.0130 |          95.5960 |         -54.7521 |
[32m[20221214 00:27:58 @agent_ppo2.py:185][0m |          -0.0136 |          94.6061 |         -54.9677 |
[32m[20221214 00:27:58 @agent_ppo2.py:185][0m |          -0.0085 |          95.2427 |         -54.8473 |
[32m[20221214 00:27:58 @agent_ppo2.py:185][0m |          -0.0180 |          93.2153 |         -54.8335 |
[32m[20221214 00:27:58 @agent_ppo2.py:185][0m |          -0.0145 |          92.8196 |         -54.6058 |
[32m[20221214 00:27:58 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:27:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 513.95
[32m[20221214 00:27:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 568.87
[32m[20221214 00:27:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 460.99
[32m[20221214 00:27:58 @agent_ppo2.py:143][0m Total time:      30.46 min
[32m[20221214 00:27:58 @agent_ppo2.py:145][0m 2772992 total steps have happened
[32m[20221214 00:27:58 @agent_ppo2.py:121][0m #------------------------ Iteration 5354 --------------------------#
[32m[20221214 00:27:58 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:27:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:27:59 @agent_ppo2.py:185][0m |           0.0025 |         124.0560 |         -53.3598 |
[32m[20221214 00:27:59 @agent_ppo2.py:185][0m |          -0.0043 |         112.6602 |         -53.3193 |
[32m[20221214 00:27:59 @agent_ppo2.py:185][0m |          -0.0093 |         107.7700 |         -53.1683 |
[32m[20221214 00:27:59 @agent_ppo2.py:185][0m |          -0.0099 |         104.5779 |         -53.1115 |
[32m[20221214 00:27:59 @agent_ppo2.py:185][0m |          -0.0112 |         102.9522 |         -53.0200 |
[32m[20221214 00:27:59 @agent_ppo2.py:185][0m |          -0.0129 |         101.5729 |         -53.1334 |
[32m[20221214 00:27:59 @agent_ppo2.py:185][0m |          -0.0086 |         101.9537 |         -52.8888 |
[32m[20221214 00:27:59 @agent_ppo2.py:185][0m |          -0.0154 |          99.1620 |         -52.7413 |
[32m[20221214 00:27:59 @agent_ppo2.py:185][0m |          -0.0164 |          98.3170 |         -52.7224 |
[32m[20221214 00:27:59 @agent_ppo2.py:185][0m |          -0.0201 |          98.4531 |         -52.5486 |
[32m[20221214 00:27:59 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:28:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 411.53
[32m[20221214 00:28:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 593.65
[32m[20221214 00:28:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 622.04
[32m[20221214 00:28:00 @agent_ppo2.py:143][0m Total time:      30.49 min
[32m[20221214 00:28:00 @agent_ppo2.py:145][0m 2775040 total steps have happened
[32m[20221214 00:28:00 @agent_ppo2.py:121][0m #------------------------ Iteration 5355 --------------------------#
[32m[20221214 00:28:00 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:28:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:28:00 @agent_ppo2.py:185][0m |          -0.0003 |          99.9981 |         -53.9330 |
[32m[20221214 00:28:00 @agent_ppo2.py:185][0m |          -0.0018 |          92.3679 |         -53.8439 |
[32m[20221214 00:28:00 @agent_ppo2.py:185][0m |          -0.0016 |          89.6443 |         -53.8876 |
[32m[20221214 00:28:00 @agent_ppo2.py:185][0m |          -0.0040 |          87.9565 |         -53.6526 |
[32m[20221214 00:28:00 @agent_ppo2.py:185][0m |          -0.0055 |          86.8812 |         -52.9276 |
[32m[20221214 00:28:00 @agent_ppo2.py:185][0m |          -0.0080 |          85.9875 |         -54.0380 |
[32m[20221214 00:28:00 @agent_ppo2.py:185][0m |          -0.0102 |          85.1760 |         -54.2690 |
[32m[20221214 00:28:01 @agent_ppo2.py:185][0m |          -0.0119 |          84.2828 |         -54.2340 |
[32m[20221214 00:28:01 @agent_ppo2.py:185][0m |          -0.0133 |          83.9858 |         -54.2959 |
[32m[20221214 00:28:01 @agent_ppo2.py:185][0m |          -0.0118 |          83.7377 |         -54.0152 |
[32m[20221214 00:28:01 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:28:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 486.04
[32m[20221214 00:28:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 586.86
[32m[20221214 00:28:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 383.00
[32m[20221214 00:28:01 @agent_ppo2.py:143][0m Total time:      30.51 min
[32m[20221214 00:28:01 @agent_ppo2.py:145][0m 2777088 total steps have happened
[32m[20221214 00:28:01 @agent_ppo2.py:121][0m #------------------------ Iteration 5356 --------------------------#
[32m[20221214 00:28:01 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:28:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:28:01 @agent_ppo2.py:185][0m |           0.0134 |         141.3141 |         -55.5947 |
[32m[20221214 00:28:01 @agent_ppo2.py:185][0m |          -0.0058 |         126.2426 |         -55.0143 |
[32m[20221214 00:28:01 @agent_ppo2.py:185][0m |          -0.0001 |         114.4016 |         -55.1643 |
[32m[20221214 00:28:01 @agent_ppo2.py:185][0m |          -0.0080 |         112.7018 |         -55.1046 |
[32m[20221214 00:28:02 @agent_ppo2.py:185][0m |          -0.0116 |         110.8341 |         -54.9715 |
[32m[20221214 00:28:02 @agent_ppo2.py:185][0m |          -0.0150 |         110.6424 |         -55.1022 |
[32m[20221214 00:28:02 @agent_ppo2.py:185][0m |          -0.0142 |         108.9683 |         -54.8297 |
[32m[20221214 00:28:02 @agent_ppo2.py:185][0m |          -0.0039 |         110.8327 |         -54.5194 |
[32m[20221214 00:28:02 @agent_ppo2.py:185][0m |          -0.0103 |         109.5194 |         -54.4997 |
[32m[20221214 00:28:02 @agent_ppo2.py:185][0m |          -0.0141 |         105.6987 |         -54.3697 |
[32m[20221214 00:28:02 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:28:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 615.60
[32m[20221214 00:28:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 795.72
[32m[20221214 00:28:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 854.83
[32m[20221214 00:28:02 @agent_ppo2.py:143][0m Total time:      30.53 min
[32m[20221214 00:28:02 @agent_ppo2.py:145][0m 2779136 total steps have happened
[32m[20221214 00:28:02 @agent_ppo2.py:121][0m #------------------------ Iteration 5357 --------------------------#
[32m[20221214 00:28:02 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:28:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:28:03 @agent_ppo2.py:185][0m |           0.0010 |         118.8381 |         -51.9095 |
[32m[20221214 00:28:03 @agent_ppo2.py:185][0m |          -0.0046 |         105.5181 |         -52.1065 |
[32m[20221214 00:28:03 @agent_ppo2.py:185][0m |          -0.0059 |         101.8350 |         -51.6311 |
[32m[20221214 00:28:03 @agent_ppo2.py:185][0m |          -0.0082 |          99.6780 |         -52.0985 |
[32m[20221214 00:28:03 @agent_ppo2.py:185][0m |          -0.0113 |          98.5030 |         -51.6920 |
[32m[20221214 00:28:03 @agent_ppo2.py:185][0m |          -0.0130 |          97.6234 |         -51.5418 |
[32m[20221214 00:28:03 @agent_ppo2.py:185][0m |          -0.0093 |          96.3988 |         -51.3154 |
[32m[20221214 00:28:03 @agent_ppo2.py:185][0m |          -0.0171 |          96.1596 |         -51.3367 |
[32m[20221214 00:28:03 @agent_ppo2.py:185][0m |          -0.0048 |          95.2240 |         -51.2773 |
[32m[20221214 00:28:03 @agent_ppo2.py:185][0m |          -0.0107 |          94.8208 |         -51.6390 |
[32m[20221214 00:28:03 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:28:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 640.39
[32m[20221214 00:28:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 820.20
[32m[20221214 00:28:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 638.58
[32m[20221214 00:28:04 @agent_ppo2.py:143][0m Total time:      30.55 min
[32m[20221214 00:28:04 @agent_ppo2.py:145][0m 2781184 total steps have happened
[32m[20221214 00:28:04 @agent_ppo2.py:121][0m #------------------------ Iteration 5358 --------------------------#
[32m[20221214 00:28:04 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:28:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:28:04 @agent_ppo2.py:185][0m |           0.0026 |         126.4928 |         -48.5128 |
[32m[20221214 00:28:04 @agent_ppo2.py:185][0m |           0.0027 |         123.5587 |         -47.3713 |
[32m[20221214 00:28:04 @agent_ppo2.py:185][0m |           0.0058 |         123.9116 |         -48.0665 |
[32m[20221214 00:28:04 @agent_ppo2.py:185][0m |          -0.0004 |         121.9690 |         -48.0016 |
[32m[20221214 00:28:04 @agent_ppo2.py:185][0m |          -0.0013 |         122.6775 |         -47.5623 |
[32m[20221214 00:28:04 @agent_ppo2.py:185][0m |           0.0016 |         121.5159 |         -48.3030 |
[32m[20221214 00:28:04 @agent_ppo2.py:185][0m |          -0.0044 |         121.2958 |         -48.6227 |
[32m[20221214 00:28:05 @agent_ppo2.py:185][0m |          -0.0019 |         120.8481 |         -48.5663 |
[32m[20221214 00:28:05 @agent_ppo2.py:185][0m |           0.0011 |         121.5513 |         -48.4744 |
[32m[20221214 00:28:05 @agent_ppo2.py:185][0m |           0.0004 |         120.6519 |         -48.5688 |
[32m[20221214 00:28:05 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:28:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 796.43
[32m[20221214 00:28:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.53
[32m[20221214 00:28:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 624.76
[32m[20221214 00:28:05 @agent_ppo2.py:143][0m Total time:      30.58 min
[32m[20221214 00:28:05 @agent_ppo2.py:145][0m 2783232 total steps have happened
[32m[20221214 00:28:05 @agent_ppo2.py:121][0m #------------------------ Iteration 5359 --------------------------#
[32m[20221214 00:28:05 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:28:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:28:05 @agent_ppo2.py:185][0m |           0.0027 |          58.9179 |         -50.3474 |
[32m[20221214 00:28:05 @agent_ppo2.py:185][0m |          -0.0061 |          44.8746 |         -50.3507 |
[32m[20221214 00:28:05 @agent_ppo2.py:185][0m |          -0.0166 |          40.0241 |         -50.0731 |
[32m[20221214 00:28:06 @agent_ppo2.py:185][0m |          -0.0152 |          37.9640 |         -50.3731 |
[32m[20221214 00:28:06 @agent_ppo2.py:185][0m |          -0.0216 |          36.4818 |         -50.1483 |
[32m[20221214 00:28:06 @agent_ppo2.py:185][0m |          -0.0153 |          34.9838 |         -50.1697 |
[32m[20221214 00:28:06 @agent_ppo2.py:185][0m |          -0.0152 |          34.4443 |         -50.0881 |
[32m[20221214 00:28:06 @agent_ppo2.py:185][0m |          -0.0209 |          35.1689 |         -50.0976 |
[32m[20221214 00:28:06 @agent_ppo2.py:185][0m |          -0.0199 |          34.3463 |         -49.8922 |
[32m[20221214 00:28:06 @agent_ppo2.py:185][0m |          -0.0237 |          33.1725 |         -49.9797 |
[32m[20221214 00:28:06 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:28:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 373.60
[32m[20221214 00:28:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 565.84
[32m[20221214 00:28:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 648.16
[32m[20221214 00:28:06 @agent_ppo2.py:143][0m Total time:      30.60 min
[32m[20221214 00:28:06 @agent_ppo2.py:145][0m 2785280 total steps have happened
[32m[20221214 00:28:06 @agent_ppo2.py:121][0m #------------------------ Iteration 5360 --------------------------#
[32m[20221214 00:28:06 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:28:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:28:07 @agent_ppo2.py:185][0m |           0.0061 |         112.0360 |         -50.9220 |
[32m[20221214 00:28:07 @agent_ppo2.py:185][0m |          -0.0072 |         100.9261 |         -50.8980 |
[32m[20221214 00:28:07 @agent_ppo2.py:185][0m |          -0.0043 |          99.6227 |         -50.5630 |
[32m[20221214 00:28:07 @agent_ppo2.py:185][0m |           0.0002 |         100.4929 |         -50.2605 |
[32m[20221214 00:28:07 @agent_ppo2.py:185][0m |          -0.0104 |          93.8872 |         -50.1973 |
[32m[20221214 00:28:07 @agent_ppo2.py:185][0m |          -0.0134 |          92.4029 |         -50.1636 |
[32m[20221214 00:28:07 @agent_ppo2.py:185][0m |          -0.0144 |          91.5943 |         -50.3406 |
[32m[20221214 00:28:07 @agent_ppo2.py:185][0m |          -0.0152 |          90.3886 |         -49.8115 |
[32m[20221214 00:28:07 @agent_ppo2.py:185][0m |          -0.0205 |          90.3150 |         -50.1185 |
[32m[20221214 00:28:07 @agent_ppo2.py:185][0m |          -0.0199 |          89.3064 |         -50.0291 |
[32m[20221214 00:28:07 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:28:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 458.93
[32m[20221214 00:28:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 595.80
[32m[20221214 00:28:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 849.15
[32m[20221214 00:28:08 @agent_ppo2.py:143][0m Total time:      30.62 min
[32m[20221214 00:28:08 @agent_ppo2.py:145][0m 2787328 total steps have happened
[32m[20221214 00:28:08 @agent_ppo2.py:121][0m #------------------------ Iteration 5361 --------------------------#
[32m[20221214 00:28:08 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:28:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:28:08 @agent_ppo2.py:185][0m |          -0.0027 |         126.5140 |         -52.7869 |
[32m[20221214 00:28:08 @agent_ppo2.py:185][0m |          -0.0045 |         112.3626 |         -52.9074 |
[32m[20221214 00:28:08 @agent_ppo2.py:185][0m |          -0.0106 |         108.1394 |         -52.9772 |
[32m[20221214 00:28:08 @agent_ppo2.py:185][0m |          -0.0087 |         106.3459 |         -52.8676 |
[32m[20221214 00:28:08 @agent_ppo2.py:185][0m |          -0.0133 |         104.7123 |         -53.1811 |
[32m[20221214 00:28:08 @agent_ppo2.py:185][0m |          -0.0169 |         104.1814 |         -52.7380 |
[32m[20221214 00:28:08 @agent_ppo2.py:185][0m |          -0.0113 |         110.5261 |         -52.8832 |
[32m[20221214 00:28:09 @agent_ppo2.py:185][0m |          -0.0103 |         104.0774 |         -52.6655 |
[32m[20221214 00:28:09 @agent_ppo2.py:185][0m |          -0.0194 |         101.6086 |         -52.6982 |
[32m[20221214 00:28:09 @agent_ppo2.py:185][0m |          -0.0183 |         100.9124 |         -52.6119 |
[32m[20221214 00:28:09 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:28:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 541.10
[32m[20221214 00:28:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 570.18
[32m[20221214 00:28:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 564.71
[32m[20221214 00:28:09 @agent_ppo2.py:143][0m Total time:      30.64 min
[32m[20221214 00:28:09 @agent_ppo2.py:145][0m 2789376 total steps have happened
[32m[20221214 00:28:09 @agent_ppo2.py:121][0m #------------------------ Iteration 5362 --------------------------#
[32m[20221214 00:28:09 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:28:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:28:09 @agent_ppo2.py:185][0m |          -0.0004 |         131.3368 |         -51.9023 |
[32m[20221214 00:28:09 @agent_ppo2.py:185][0m |          -0.0009 |         125.6468 |         -51.6860 |
[32m[20221214 00:28:09 @agent_ppo2.py:185][0m |          -0.0021 |         124.3582 |         -51.8645 |
[32m[20221214 00:28:10 @agent_ppo2.py:185][0m |          -0.0035 |         123.5775 |         -52.0937 |
[32m[20221214 00:28:10 @agent_ppo2.py:185][0m |           0.0049 |         127.0500 |         -52.0620 |
[32m[20221214 00:28:10 @agent_ppo2.py:185][0m |          -0.0027 |         122.3757 |         -52.3710 |
[32m[20221214 00:28:10 @agent_ppo2.py:185][0m |          -0.0002 |         128.6380 |         -52.2620 |
[32m[20221214 00:28:10 @agent_ppo2.py:185][0m |          -0.0038 |         121.6659 |         -52.5251 |
[32m[20221214 00:28:10 @agent_ppo2.py:185][0m |          -0.0097 |         120.6273 |         -52.6643 |
[32m[20221214 00:28:10 @agent_ppo2.py:185][0m |          -0.0076 |         120.8924 |         -52.7451 |
[32m[20221214 00:28:10 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:28:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 638.00
[32m[20221214 00:28:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 782.02
[32m[20221214 00:28:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 743.55
[32m[20221214 00:28:10 @agent_ppo2.py:143][0m Total time:      30.66 min
[32m[20221214 00:28:10 @agent_ppo2.py:145][0m 2791424 total steps have happened
[32m[20221214 00:28:10 @agent_ppo2.py:121][0m #------------------------ Iteration 5363 --------------------------#
[32m[20221214 00:28:10 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:28:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:28:11 @agent_ppo2.py:185][0m |           0.0008 |         114.8713 |         -52.9105 |
[32m[20221214 00:28:11 @agent_ppo2.py:185][0m |          -0.0014 |         111.7543 |         -52.7691 |
[32m[20221214 00:28:11 @agent_ppo2.py:185][0m |          -0.0080 |         104.6435 |         -52.8008 |
[32m[20221214 00:28:11 @agent_ppo2.py:185][0m |          -0.0142 |         101.6029 |         -52.5582 |
[32m[20221214 00:28:11 @agent_ppo2.py:185][0m |          -0.0143 |         101.0606 |         -52.6742 |
[32m[20221214 00:28:11 @agent_ppo2.py:185][0m |          -0.0178 |          98.5091 |         -52.7976 |
[32m[20221214 00:28:11 @agent_ppo2.py:185][0m |          -0.0176 |          96.8590 |         -53.0004 |
[32m[20221214 00:28:11 @agent_ppo2.py:185][0m |          -0.0190 |          95.7735 |         -52.9523 |
[32m[20221214 00:28:11 @agent_ppo2.py:185][0m |          -0.0254 |          95.8386 |         -53.0763 |
[32m[20221214 00:28:11 @agent_ppo2.py:185][0m |          -0.0157 |          94.9512 |         -52.9350 |
[32m[20221214 00:28:11 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:28:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 446.54
[32m[20221214 00:28:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 551.28
[32m[20221214 00:28:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 509.79
[32m[20221214 00:28:12 @agent_ppo2.py:143][0m Total time:      30.69 min
[32m[20221214 00:28:12 @agent_ppo2.py:145][0m 2793472 total steps have happened
[32m[20221214 00:28:12 @agent_ppo2.py:121][0m #------------------------ Iteration 5364 --------------------------#
[32m[20221214 00:28:12 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:28:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:28:12 @agent_ppo2.py:185][0m |          -0.0003 |          94.8300 |         -54.7045 |
[32m[20221214 00:28:12 @agent_ppo2.py:185][0m |          -0.0085 |          92.4520 |         -54.5664 |
[32m[20221214 00:28:12 @agent_ppo2.py:185][0m |          -0.0098 |          91.0304 |         -54.8744 |
[32m[20221214 00:28:12 @agent_ppo2.py:185][0m |          -0.0081 |          89.7690 |         -54.6933 |
[32m[20221214 00:28:12 @agent_ppo2.py:185][0m |          -0.0146 |          89.2201 |         -54.5464 |
[32m[20221214 00:28:12 @agent_ppo2.py:185][0m |          -0.0057 |          95.0699 |         -54.5819 |
[32m[20221214 00:28:13 @agent_ppo2.py:185][0m |          -0.0154 |          88.3128 |         -54.6779 |
[32m[20221214 00:28:13 @agent_ppo2.py:185][0m |          -0.0191 |          87.5663 |         -54.5295 |
[32m[20221214 00:28:13 @agent_ppo2.py:185][0m |          -0.0191 |          87.1675 |         -54.3245 |
[32m[20221214 00:28:13 @agent_ppo2.py:185][0m |          -0.0125 |          88.3708 |         -54.3268 |
[32m[20221214 00:28:13 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:28:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 491.62
[32m[20221214 00:28:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 561.24
[32m[20221214 00:28:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 669.55
[32m[20221214 00:28:13 @agent_ppo2.py:143][0m Total time:      30.71 min
[32m[20221214 00:28:13 @agent_ppo2.py:145][0m 2795520 total steps have happened
[32m[20221214 00:28:13 @agent_ppo2.py:121][0m #------------------------ Iteration 5365 --------------------------#
[32m[20221214 00:28:13 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:28:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:28:13 @agent_ppo2.py:185][0m |           0.0032 |         148.2990 |         -52.1522 |
[32m[20221214 00:28:13 @agent_ppo2.py:185][0m |           0.0081 |         146.1537 |         -52.3651 |
[32m[20221214 00:28:13 @agent_ppo2.py:185][0m |          -0.0052 |         136.2017 |         -52.2462 |
[32m[20221214 00:28:14 @agent_ppo2.py:185][0m |          -0.0082 |         133.7061 |         -52.2501 |
[32m[20221214 00:28:14 @agent_ppo2.py:185][0m |          -0.0088 |         130.5135 |         -52.4895 |
[32m[20221214 00:28:14 @agent_ppo2.py:185][0m |          -0.0033 |         135.2332 |         -52.1176 |
[32m[20221214 00:28:14 @agent_ppo2.py:185][0m |          -0.0070 |         129.5974 |         -52.1667 |
[32m[20221214 00:28:14 @agent_ppo2.py:185][0m |          -0.0129 |         128.9128 |         -52.6999 |
[32m[20221214 00:28:14 @agent_ppo2.py:185][0m |          -0.0119 |         127.3819 |         -52.8599 |
[32m[20221214 00:28:14 @agent_ppo2.py:185][0m |          -0.0148 |         127.2387 |         -52.7809 |
[32m[20221214 00:28:14 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:28:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 680.22
[32m[20221214 00:28:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 794.41
[32m[20221214 00:28:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 832.48
[32m[20221214 00:28:14 @agent_ppo2.py:143][0m Total time:      30.73 min
[32m[20221214 00:28:14 @agent_ppo2.py:145][0m 2797568 total steps have happened
[32m[20221214 00:28:14 @agent_ppo2.py:121][0m #------------------------ Iteration 5366 --------------------------#
[32m[20221214 00:28:14 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:28:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:28:15 @agent_ppo2.py:185][0m |           0.0011 |          22.1846 |         -54.9677 |
[32m[20221214 00:28:15 @agent_ppo2.py:185][0m |          -0.0014 |          19.3584 |         -55.1553 |
[32m[20221214 00:28:15 @agent_ppo2.py:185][0m |          -0.0018 |          18.8274 |         -55.5406 |
[32m[20221214 00:28:15 @agent_ppo2.py:185][0m |          -0.0032 |          18.6098 |         -55.4421 |
[32m[20221214 00:28:15 @agent_ppo2.py:185][0m |           0.0045 |          18.7582 |         -55.6572 |
[32m[20221214 00:28:15 @agent_ppo2.py:185][0m |          -0.0046 |          18.0744 |         -55.6599 |
[32m[20221214 00:28:15 @agent_ppo2.py:185][0m |          -0.0061 |          17.9525 |         -55.7838 |
[32m[20221214 00:28:15 @agent_ppo2.py:185][0m |          -0.0052 |          17.8607 |         -55.8882 |
[32m[20221214 00:28:15 @agent_ppo2.py:185][0m |          -0.0065 |          17.8848 |         -55.8647 |
[32m[20221214 00:28:15 @agent_ppo2.py:185][0m |           0.0009 |          18.2279 |         -55.7823 |
[32m[20221214 00:28:15 @agent_ppo2.py:130][0m Policy update time: 1.00 s
[32m[20221214 00:28:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221214 00:28:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221214 00:28:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 508.15
[32m[20221214 00:28:16 @agent_ppo2.py:143][0m Total time:      30.75 min
[32m[20221214 00:28:16 @agent_ppo2.py:145][0m 2799616 total steps have happened
[32m[20221214 00:28:16 @agent_ppo2.py:121][0m #------------------------ Iteration 5367 --------------------------#
[32m[20221214 00:28:16 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:28:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:28:16 @agent_ppo2.py:185][0m |           0.0019 |         140.1760 |         -56.5886 |
[32m[20221214 00:28:16 @agent_ppo2.py:185][0m |          -0.0006 |         138.4778 |         -56.3461 |
[32m[20221214 00:28:16 @agent_ppo2.py:185][0m |          -0.0106 |         135.3444 |         -56.0085 |
[32m[20221214 00:28:16 @agent_ppo2.py:185][0m |          -0.0084 |         134.0033 |         -56.1884 |
[32m[20221214 00:28:16 @agent_ppo2.py:185][0m |          -0.0055 |         133.2317 |         -56.0170 |
[32m[20221214 00:28:16 @agent_ppo2.py:185][0m |          -0.0088 |         132.8903 |         -55.6486 |
[32m[20221214 00:28:17 @agent_ppo2.py:185][0m |          -0.0103 |         132.1132 |         -55.6602 |
[32m[20221214 00:28:17 @agent_ppo2.py:185][0m |          -0.0122 |         132.5759 |         -55.7223 |
[32m[20221214 00:28:17 @agent_ppo2.py:185][0m |          -0.0121 |         131.6441 |         -55.6967 |
[32m[20221214 00:28:17 @agent_ppo2.py:185][0m |          -0.0075 |         133.2362 |         -55.5257 |
[32m[20221214 00:28:17 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:28:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 670.56
[32m[20221214 00:28:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 733.70
[32m[20221214 00:28:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 645.20
[32m[20221214 00:28:17 @agent_ppo2.py:143][0m Total time:      30.78 min
[32m[20221214 00:28:17 @agent_ppo2.py:145][0m 2801664 total steps have happened
[32m[20221214 00:28:17 @agent_ppo2.py:121][0m #------------------------ Iteration 5368 --------------------------#
[32m[20221214 00:28:17 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:28:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:28:17 @agent_ppo2.py:185][0m |           0.0045 |          81.1030 |         -53.8021 |
[32m[20221214 00:28:17 @agent_ppo2.py:185][0m |          -0.0002 |          66.2787 |         -53.6038 |
[32m[20221214 00:28:17 @agent_ppo2.py:185][0m |          -0.0104 |          62.6002 |         -53.2646 |
[32m[20221214 00:28:18 @agent_ppo2.py:185][0m |          -0.0027 |          62.1588 |         -53.4904 |
[32m[20221214 00:28:18 @agent_ppo2.py:185][0m |          -0.0093 |          59.3658 |         -53.3185 |
[32m[20221214 00:28:18 @agent_ppo2.py:185][0m |          -0.0128 |          58.1397 |         -53.0363 |
[32m[20221214 00:28:18 @agent_ppo2.py:185][0m |          -0.0082 |          58.0687 |         -53.3056 |
[32m[20221214 00:28:18 @agent_ppo2.py:185][0m |          -0.0150 |          56.8235 |         -53.1657 |
[32m[20221214 00:28:18 @agent_ppo2.py:185][0m |          -0.0159 |          56.6085 |         -53.0997 |
[32m[20221214 00:28:18 @agent_ppo2.py:185][0m |          -0.0095 |          57.6176 |         -53.0378 |
[32m[20221214 00:28:18 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:28:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 543.83
[32m[20221214 00:28:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 594.76
[32m[20221214 00:28:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 827.32
[32m[20221214 00:28:18 @agent_ppo2.py:143][0m Total time:      30.80 min
[32m[20221214 00:28:18 @agent_ppo2.py:145][0m 2803712 total steps have happened
[32m[20221214 00:28:18 @agent_ppo2.py:121][0m #------------------------ Iteration 5369 --------------------------#
[32m[20221214 00:28:18 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:28:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:28:19 @agent_ppo2.py:185][0m |           0.0006 |          83.5599 |         -50.5856 |
[32m[20221214 00:28:19 @agent_ppo2.py:185][0m |          -0.0094 |          77.7449 |         -50.5364 |
[32m[20221214 00:28:19 @agent_ppo2.py:185][0m |          -0.0123 |          76.1102 |         -50.6066 |
[32m[20221214 00:28:19 @agent_ppo2.py:185][0m |          -0.0131 |          74.8054 |         -50.3217 |
[32m[20221214 00:28:19 @agent_ppo2.py:185][0m |          -0.0161 |          73.8560 |         -50.3676 |
[32m[20221214 00:28:19 @agent_ppo2.py:185][0m |          -0.0090 |          79.7794 |         -50.0561 |
[32m[20221214 00:28:19 @agent_ppo2.py:185][0m |          -0.0154 |          72.5043 |         -50.1161 |
[32m[20221214 00:28:19 @agent_ppo2.py:185][0m |          -0.0209 |          71.8773 |         -50.1075 |
[32m[20221214 00:28:19 @agent_ppo2.py:185][0m |          -0.0164 |          71.7169 |         -50.2803 |
[32m[20221214 00:28:19 @agent_ppo2.py:185][0m |          -0.0189 |          70.9155 |         -50.0342 |
[32m[20221214 00:28:19 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:28:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 521.10
[32m[20221214 00:28:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 552.36
[32m[20221214 00:28:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 598.23
[32m[20221214 00:28:20 @agent_ppo2.py:143][0m Total time:      30.82 min
[32m[20221214 00:28:20 @agent_ppo2.py:145][0m 2805760 total steps have happened
[32m[20221214 00:28:20 @agent_ppo2.py:121][0m #------------------------ Iteration 5370 --------------------------#
[32m[20221214 00:28:20 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:28:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:28:20 @agent_ppo2.py:185][0m |           0.0052 |         112.7153 |         -54.2054 |
[32m[20221214 00:28:20 @agent_ppo2.py:185][0m |          -0.0028 |         101.3087 |         -53.6278 |
[32m[20221214 00:28:20 @agent_ppo2.py:185][0m |          -0.0058 |          96.8386 |         -53.4407 |
[32m[20221214 00:28:20 @agent_ppo2.py:185][0m |          -0.0109 |          94.1771 |         -53.6621 |
[32m[20221214 00:28:20 @agent_ppo2.py:185][0m |          -0.0091 |          92.8096 |         -53.7275 |
[32m[20221214 00:28:20 @agent_ppo2.py:185][0m |          -0.0103 |          91.0058 |         -53.0454 |
[32m[20221214 00:28:21 @agent_ppo2.py:185][0m |          -0.0138 |          90.1353 |         -53.5704 |
[32m[20221214 00:28:21 @agent_ppo2.py:185][0m |          -0.0096 |          89.6457 |         -53.2471 |
[32m[20221214 00:28:21 @agent_ppo2.py:185][0m |          -0.0139 |          88.7168 |         -53.1196 |
[32m[20221214 00:28:21 @agent_ppo2.py:185][0m |          -0.0157 |          87.8137 |         -53.1327 |
[32m[20221214 00:28:21 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:28:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 637.00
[32m[20221214 00:28:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 777.66
[32m[20221214 00:28:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 520.25
[32m[20221214 00:28:21 @agent_ppo2.py:143][0m Total time:      30.84 min
[32m[20221214 00:28:21 @agent_ppo2.py:145][0m 2807808 total steps have happened
[32m[20221214 00:28:21 @agent_ppo2.py:121][0m #------------------------ Iteration 5371 --------------------------#
[32m[20221214 00:28:21 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:28:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:28:21 @agent_ppo2.py:185][0m |           0.0028 |          72.2441 |         -51.8061 |
[32m[20221214 00:28:21 @agent_ppo2.py:185][0m |          -0.0079 |          61.6297 |         -52.1635 |
[32m[20221214 00:28:21 @agent_ppo2.py:185][0m |          -0.0102 |          59.1018 |         -52.3810 |
[32m[20221214 00:28:22 @agent_ppo2.py:185][0m |          -0.0093 |          58.5980 |         -52.2315 |
[32m[20221214 00:28:22 @agent_ppo2.py:185][0m |          -0.0106 |          55.8447 |         -52.1310 |
[32m[20221214 00:28:22 @agent_ppo2.py:185][0m |          -0.0172 |          55.3519 |         -51.9548 |
[32m[20221214 00:28:22 @agent_ppo2.py:185][0m |          -0.0174 |          54.1187 |         -52.2451 |
[32m[20221214 00:28:22 @agent_ppo2.py:185][0m |          -0.0140 |          53.4366 |         -52.2171 |
[32m[20221214 00:28:22 @agent_ppo2.py:185][0m |          -0.0096 |          57.9178 |         -52.4168 |
[32m[20221214 00:28:22 @agent_ppo2.py:185][0m |          -0.0130 |          52.4114 |         -52.0735 |
[32m[20221214 00:28:22 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:28:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 564.19
[32m[20221214 00:28:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 632.63
[32m[20221214 00:28:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 682.92
[32m[20221214 00:28:22 @agent_ppo2.py:143][0m Total time:      30.87 min
[32m[20221214 00:28:22 @agent_ppo2.py:145][0m 2809856 total steps have happened
[32m[20221214 00:28:22 @agent_ppo2.py:121][0m #------------------------ Iteration 5372 --------------------------#
[32m[20221214 00:28:22 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:28:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:28:23 @agent_ppo2.py:185][0m |           0.0078 |         117.4256 |         -56.2073 |
[32m[20221214 00:28:23 @agent_ppo2.py:185][0m |          -0.0045 |         104.3899 |         -55.9355 |
[32m[20221214 00:28:23 @agent_ppo2.py:185][0m |          -0.0028 |          99.1093 |         -55.8473 |
[32m[20221214 00:28:23 @agent_ppo2.py:185][0m |          -0.0097 |          96.9642 |         -56.1624 |
[32m[20221214 00:28:23 @agent_ppo2.py:185][0m |          -0.0088 |          95.2283 |         -56.1158 |
[32m[20221214 00:28:23 @agent_ppo2.py:185][0m |          -0.0104 |          93.2980 |         -55.9527 |
[32m[20221214 00:28:23 @agent_ppo2.py:185][0m |          -0.0140 |          92.3149 |         -56.0309 |
[32m[20221214 00:28:23 @agent_ppo2.py:185][0m |          -0.0132 |          91.3884 |         -55.7640 |
[32m[20221214 00:28:23 @agent_ppo2.py:185][0m |          -0.0189 |          89.9474 |         -55.7672 |
[32m[20221214 00:28:23 @agent_ppo2.py:185][0m |          -0.0166 |          89.1383 |         -55.8037 |
[32m[20221214 00:28:24 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:28:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 582.16
[32m[20221214 00:28:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 759.03
[32m[20221214 00:28:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 796.96
[32m[20221214 00:28:24 @agent_ppo2.py:143][0m Total time:      30.89 min
[32m[20221214 00:28:24 @agent_ppo2.py:145][0m 2811904 total steps have happened
[32m[20221214 00:28:24 @agent_ppo2.py:121][0m #------------------------ Iteration 5373 --------------------------#
[32m[20221214 00:28:24 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:28:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:28:24 @agent_ppo2.py:185][0m |          -0.0001 |          90.6979 |         -56.0710 |
[32m[20221214 00:28:24 @agent_ppo2.py:185][0m |           0.0001 |          79.0252 |         -55.6349 |
[32m[20221214 00:28:24 @agent_ppo2.py:185][0m |          -0.0056 |          73.5367 |         -55.2266 |
[32m[20221214 00:28:24 @agent_ppo2.py:185][0m |          -0.0107 |          71.4457 |         -55.2406 |
[32m[20221214 00:28:24 @agent_ppo2.py:185][0m |          -0.0116 |          69.9908 |         -55.4366 |
[32m[20221214 00:28:24 @agent_ppo2.py:185][0m |          -0.0137 |          68.9306 |         -54.9286 |
[32m[20221214 00:28:25 @agent_ppo2.py:185][0m |          -0.0169 |          67.8469 |         -55.1977 |
[32m[20221214 00:28:25 @agent_ppo2.py:185][0m |          -0.0172 |          66.6754 |         -54.8711 |
[32m[20221214 00:28:25 @agent_ppo2.py:185][0m |          -0.0116 |          65.9416 |         -55.0558 |
[32m[20221214 00:28:25 @agent_ppo2.py:185][0m |          -0.0223 |          65.1271 |         -54.6947 |
[32m[20221214 00:28:25 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:28:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 423.52
[32m[20221214 00:28:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 520.11
[32m[20221214 00:28:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 835.24
[32m[20221214 00:28:25 @agent_ppo2.py:143][0m Total time:      30.91 min
[32m[20221214 00:28:25 @agent_ppo2.py:145][0m 2813952 total steps have happened
[32m[20221214 00:28:25 @agent_ppo2.py:121][0m #------------------------ Iteration 5374 --------------------------#
[32m[20221214 00:28:25 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:28:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:28:25 @agent_ppo2.py:185][0m |           0.0007 |         135.2436 |         -53.9185 |
[32m[20221214 00:28:25 @agent_ppo2.py:185][0m |          -0.0008 |         123.9453 |         -53.8453 |
[32m[20221214 00:28:26 @agent_ppo2.py:185][0m |          -0.0045 |         119.4120 |         -53.8277 |
[32m[20221214 00:28:26 @agent_ppo2.py:185][0m |          -0.0027 |         118.5964 |         -54.2183 |
[32m[20221214 00:28:26 @agent_ppo2.py:185][0m |          -0.0087 |         114.9821 |         -53.7986 |
[32m[20221214 00:28:26 @agent_ppo2.py:185][0m |          -0.0065 |         114.3843 |         -53.6045 |
[32m[20221214 00:28:26 @agent_ppo2.py:185][0m |          -0.0051 |         114.0669 |         -53.7797 |
[32m[20221214 00:28:26 @agent_ppo2.py:185][0m |          -0.0015 |         120.0656 |         -53.9744 |
[32m[20221214 00:28:26 @agent_ppo2.py:185][0m |          -0.0117 |         113.0173 |         -53.3904 |
[32m[20221214 00:28:26 @agent_ppo2.py:185][0m |          -0.0128 |         111.5157 |         -53.7742 |
[32m[20221214 00:28:26 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:28:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 542.23
[32m[20221214 00:28:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 685.46
[32m[20221214 00:28:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 610.71
[32m[20221214 00:28:26 @agent_ppo2.py:143][0m Total time:      30.93 min
[32m[20221214 00:28:26 @agent_ppo2.py:145][0m 2816000 total steps have happened
[32m[20221214 00:28:26 @agent_ppo2.py:121][0m #------------------------ Iteration 5375 --------------------------#
[32m[20221214 00:28:27 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:28:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:28:27 @agent_ppo2.py:185][0m |           0.0054 |          73.4509 |         -55.3071 |
[32m[20221214 00:28:27 @agent_ppo2.py:185][0m |           0.0014 |          67.6312 |         -54.9979 |
[32m[20221214 00:28:27 @agent_ppo2.py:185][0m |          -0.0019 |          66.0605 |         -54.8536 |
[32m[20221214 00:28:27 @agent_ppo2.py:185][0m |          -0.0093 |          64.9540 |         -54.7136 |
[32m[20221214 00:28:27 @agent_ppo2.py:185][0m |          -0.0091 |          63.9833 |         -54.5617 |
[32m[20221214 00:28:27 @agent_ppo2.py:185][0m |          -0.0010 |          66.0375 |         -54.3653 |
[32m[20221214 00:28:27 @agent_ppo2.py:185][0m |          -0.0090 |          62.5891 |         -54.1614 |
[32m[20221214 00:28:27 @agent_ppo2.py:185][0m |          -0.0078 |          62.1375 |         -54.7056 |
[32m[20221214 00:28:27 @agent_ppo2.py:185][0m |          -0.0092 |          62.2833 |         -54.6454 |
[32m[20221214 00:28:28 @agent_ppo2.py:185][0m |          -0.0099 |          61.5339 |         -54.3793 |
[32m[20221214 00:28:28 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:28:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 478.17
[32m[20221214 00:28:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 556.13
[32m[20221214 00:28:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 474.66
[32m[20221214 00:28:28 @agent_ppo2.py:143][0m Total time:      30.96 min
[32m[20221214 00:28:28 @agent_ppo2.py:145][0m 2818048 total steps have happened
[32m[20221214 00:28:28 @agent_ppo2.py:121][0m #------------------------ Iteration 5376 --------------------------#
[32m[20221214 00:28:28 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:28:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:28:28 @agent_ppo2.py:185][0m |           0.0038 |          93.8525 |         -54.4979 |
[32m[20221214 00:28:28 @agent_ppo2.py:185][0m |           0.0160 |          98.8272 |         -54.5043 |
[32m[20221214 00:28:28 @agent_ppo2.py:185][0m |          -0.0060 |          84.3439 |         -54.1674 |
[32m[20221214 00:28:28 @agent_ppo2.py:185][0m |          -0.0078 |          81.9042 |         -53.7814 |
[32m[20221214 00:28:28 @agent_ppo2.py:185][0m |          -0.0135 |          80.5638 |         -54.0576 |
[32m[20221214 00:28:29 @agent_ppo2.py:185][0m |          -0.0138 |          79.7941 |         -53.6675 |
[32m[20221214 00:28:29 @agent_ppo2.py:185][0m |          -0.0147 |          79.0431 |         -53.6695 |
[32m[20221214 00:28:29 @agent_ppo2.py:185][0m |          -0.0150 |          78.6473 |         -53.2924 |
[32m[20221214 00:28:29 @agent_ppo2.py:185][0m |          -0.0201 |          77.5586 |         -53.4729 |
[32m[20221214 00:28:29 @agent_ppo2.py:185][0m |          -0.0175 |          77.2606 |         -52.7264 |
[32m[20221214 00:28:29 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:28:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 490.52
[32m[20221214 00:28:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 543.98
[32m[20221214 00:28:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 797.90
[32m[20221214 00:28:29 @agent_ppo2.py:143][0m Total time:      30.98 min
[32m[20221214 00:28:29 @agent_ppo2.py:145][0m 2820096 total steps have happened
[32m[20221214 00:28:29 @agent_ppo2.py:121][0m #------------------------ Iteration 5377 --------------------------#
[32m[20221214 00:28:29 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:28:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:28:29 @agent_ppo2.py:185][0m |           0.0026 |         110.6376 |         -51.9503 |
[32m[20221214 00:28:30 @agent_ppo2.py:185][0m |          -0.0086 |          98.7128 |         -51.9766 |
[32m[20221214 00:28:30 @agent_ppo2.py:185][0m |          -0.0083 |          95.8509 |         -51.3918 |
[32m[20221214 00:28:30 @agent_ppo2.py:185][0m |          -0.0136 |          94.5718 |         -51.4639 |
[32m[20221214 00:28:30 @agent_ppo2.py:185][0m |          -0.0161 |          92.6054 |         -51.3221 |
[32m[20221214 00:28:30 @agent_ppo2.py:185][0m |          -0.0195 |          91.5453 |         -51.1034 |
[32m[20221214 00:28:30 @agent_ppo2.py:185][0m |          -0.0138 |          92.7148 |         -50.7602 |
[32m[20221214 00:28:30 @agent_ppo2.py:185][0m |          -0.0219 |          89.5332 |         -50.9120 |
[32m[20221214 00:28:30 @agent_ppo2.py:185][0m |           0.0041 |         108.2946 |         -50.7350 |
[32m[20221214 00:28:30 @agent_ppo2.py:185][0m |          -0.0078 |          99.0957 |         -50.1487 |
[32m[20221214 00:28:30 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:28:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 537.34
[32m[20221214 00:28:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 588.03
[32m[20221214 00:28:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 474.74
[32m[20221214 00:28:30 @agent_ppo2.py:143][0m Total time:      31.00 min
[32m[20221214 00:28:30 @agent_ppo2.py:145][0m 2822144 total steps have happened
[32m[20221214 00:28:30 @agent_ppo2.py:121][0m #------------------------ Iteration 5378 --------------------------#
[32m[20221214 00:28:31 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:28:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:28:31 @agent_ppo2.py:185][0m |          -0.0005 |         100.4725 |         -51.2164 |
[32m[20221214 00:28:31 @agent_ppo2.py:185][0m |          -0.0070 |          94.5315 |         -51.1479 |
[32m[20221214 00:28:31 @agent_ppo2.py:185][0m |          -0.0060 |          92.3301 |         -50.7375 |
[32m[20221214 00:28:31 @agent_ppo2.py:185][0m |          -0.0071 |          90.8213 |         -51.0704 |
[32m[20221214 00:28:31 @agent_ppo2.py:185][0m |          -0.0121 |          89.3988 |         -51.2417 |
[32m[20221214 00:28:31 @agent_ppo2.py:185][0m |          -0.0110 |          88.2732 |         -51.0204 |
[32m[20221214 00:28:31 @agent_ppo2.py:185][0m |          -0.0127 |          87.4120 |         -51.0642 |
[32m[20221214 00:28:31 @agent_ppo2.py:185][0m |          -0.0071 |          86.8578 |         -51.1446 |
[32m[20221214 00:28:32 @agent_ppo2.py:185][0m |          -0.0142 |          86.0246 |         -51.0487 |
[32m[20221214 00:28:32 @agent_ppo2.py:185][0m |          -0.0176 |          85.1228 |         -51.0128 |
[32m[20221214 00:28:32 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:28:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 504.14
[32m[20221214 00:28:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 541.12
[32m[20221214 00:28:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 465.83
[32m[20221214 00:28:32 @agent_ppo2.py:143][0m Total time:      31.02 min
[32m[20221214 00:28:32 @agent_ppo2.py:145][0m 2824192 total steps have happened
[32m[20221214 00:28:32 @agent_ppo2.py:121][0m #------------------------ Iteration 5379 --------------------------#
[32m[20221214 00:28:32 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:28:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:28:32 @agent_ppo2.py:185][0m |           0.0003 |          95.4375 |         -49.7974 |
[32m[20221214 00:28:32 @agent_ppo2.py:185][0m |          -0.0054 |          92.1185 |         -49.2758 |
[32m[20221214 00:28:32 @agent_ppo2.py:185][0m |          -0.0044 |          89.1461 |         -48.9275 |
[32m[20221214 00:28:32 @agent_ppo2.py:185][0m |          -0.0012 |          89.9075 |         -48.8382 |
[32m[20221214 00:28:32 @agent_ppo2.py:185][0m |          -0.0114 |          86.6041 |         -49.0140 |
[32m[20221214 00:28:33 @agent_ppo2.py:185][0m |          -0.0098 |          85.6100 |         -49.3075 |
[32m[20221214 00:28:33 @agent_ppo2.py:185][0m |          -0.0081 |          86.2663 |         -48.8718 |
[32m[20221214 00:28:33 @agent_ppo2.py:185][0m |          -0.0141 |          84.7724 |         -48.6985 |
[32m[20221214 00:28:33 @agent_ppo2.py:185][0m |           0.0027 |          96.3946 |         -48.7200 |
[32m[20221214 00:28:33 @agent_ppo2.py:185][0m |          -0.0125 |          84.1704 |         -48.6786 |
[32m[20221214 00:28:33 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:28:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 508.51
[32m[20221214 00:28:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 570.66
[32m[20221214 00:28:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 391.68
[32m[20221214 00:28:33 @agent_ppo2.py:143][0m Total time:      31.05 min
[32m[20221214 00:28:33 @agent_ppo2.py:145][0m 2826240 total steps have happened
[32m[20221214 00:28:33 @agent_ppo2.py:121][0m #------------------------ Iteration 5380 --------------------------#
[32m[20221214 00:28:33 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:28:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:28:33 @agent_ppo2.py:185][0m |           0.0054 |         122.8370 |         -52.5964 |
[32m[20221214 00:28:34 @agent_ppo2.py:185][0m |          -0.0060 |         110.7040 |         -52.3446 |
[32m[20221214 00:28:34 @agent_ppo2.py:185][0m |          -0.0082 |         107.3760 |         -52.6219 |
[32m[20221214 00:28:34 @agent_ppo2.py:185][0m |          -0.0117 |         104.0757 |         -52.1486 |
[32m[20221214 00:28:34 @agent_ppo2.py:185][0m |          -0.0082 |         103.3677 |         -52.2290 |
[32m[20221214 00:28:34 @agent_ppo2.py:185][0m |          -0.0109 |         100.9156 |         -52.2507 |
[32m[20221214 00:28:34 @agent_ppo2.py:185][0m |           0.0020 |         113.8910 |         -52.1880 |
[32m[20221214 00:28:34 @agent_ppo2.py:185][0m |           0.0019 |         111.6223 |         -52.1359 |
[32m[20221214 00:28:34 @agent_ppo2.py:185][0m |          -0.0137 |          99.1856 |         -52.0460 |
[32m[20221214 00:28:34 @agent_ppo2.py:185][0m |          -0.0127 |          98.8866 |         -52.0339 |
[32m[20221214 00:28:34 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:28:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 600.14
[32m[20221214 00:28:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 697.48
[32m[20221214 00:28:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 554.66
[32m[20221214 00:28:34 @agent_ppo2.py:143][0m Total time:      31.07 min
[32m[20221214 00:28:34 @agent_ppo2.py:145][0m 2828288 total steps have happened
[32m[20221214 00:28:34 @agent_ppo2.py:121][0m #------------------------ Iteration 5381 --------------------------#
[32m[20221214 00:28:35 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:28:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:28:35 @agent_ppo2.py:185][0m |           0.0067 |          99.9963 |         -50.9984 |
[32m[20221214 00:28:35 @agent_ppo2.py:185][0m |          -0.0098 |          90.8393 |         -51.1583 |
[32m[20221214 00:28:35 @agent_ppo2.py:185][0m |          -0.0104 |          88.0795 |         -51.2475 |
[32m[20221214 00:28:35 @agent_ppo2.py:185][0m |          -0.0014 |          94.6102 |         -51.0188 |
[32m[20221214 00:28:35 @agent_ppo2.py:185][0m |          -0.0078 |          86.6092 |         -51.0833 |
[32m[20221214 00:28:35 @agent_ppo2.py:185][0m |          -0.0174 |          84.7450 |         -50.7327 |
[32m[20221214 00:28:35 @agent_ppo2.py:185][0m |          -0.0177 |          83.4702 |         -50.9913 |
[32m[20221214 00:28:35 @agent_ppo2.py:185][0m |          -0.0172 |          82.7733 |         -51.0504 |
[32m[20221214 00:28:36 @agent_ppo2.py:185][0m |          -0.0163 |          81.8986 |         -50.7125 |
[32m[20221214 00:28:36 @agent_ppo2.py:185][0m |          -0.0163 |          81.0797 |         -50.6210 |
[32m[20221214 00:28:36 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:28:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 456.06
[32m[20221214 00:28:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 544.78
[32m[20221214 00:28:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 564.09
[32m[20221214 00:28:36 @agent_ppo2.py:143][0m Total time:      31.09 min
[32m[20221214 00:28:36 @agent_ppo2.py:145][0m 2830336 total steps have happened
[32m[20221214 00:28:36 @agent_ppo2.py:121][0m #------------------------ Iteration 5382 --------------------------#
[32m[20221214 00:28:36 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:28:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:28:36 @agent_ppo2.py:185][0m |           0.0041 |         106.7387 |         -51.0533 |
[32m[20221214 00:28:36 @agent_ppo2.py:185][0m |          -0.0003 |          89.8627 |         -51.0840 |
[32m[20221214 00:28:36 @agent_ppo2.py:185][0m |           0.0019 |          86.4925 |         -51.0220 |
[32m[20221214 00:28:36 @agent_ppo2.py:185][0m |          -0.0090 |          78.3290 |         -50.8467 |
[32m[20221214 00:28:37 @agent_ppo2.py:185][0m |          -0.0129 |          76.4229 |         -50.8981 |
[32m[20221214 00:28:37 @agent_ppo2.py:185][0m |          -0.0127 |          74.2705 |         -50.7394 |
[32m[20221214 00:28:37 @agent_ppo2.py:185][0m |          -0.0040 |          72.9441 |         -50.6052 |
[32m[20221214 00:28:37 @agent_ppo2.py:185][0m |          -0.0128 |          71.2683 |         -50.8988 |
[32m[20221214 00:28:37 @agent_ppo2.py:185][0m |          -0.0146 |          70.3652 |         -50.6963 |
[32m[20221214 00:28:37 @agent_ppo2.py:185][0m |          -0.0160 |          70.0036 |         -50.6559 |
[32m[20221214 00:28:37 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:28:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 537.33
[32m[20221214 00:28:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 730.53
[32m[20221214 00:28:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 639.76
[32m[20221214 00:28:37 @agent_ppo2.py:143][0m Total time:      31.11 min
[32m[20221214 00:28:37 @agent_ppo2.py:145][0m 2832384 total steps have happened
[32m[20221214 00:28:37 @agent_ppo2.py:121][0m #------------------------ Iteration 5383 --------------------------#
[32m[20221214 00:28:37 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:28:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:28:38 @agent_ppo2.py:185][0m |           0.0063 |         150.5459 |         -54.2125 |
[32m[20221214 00:28:38 @agent_ppo2.py:185][0m |          -0.0011 |         141.6895 |         -54.0105 |
[32m[20221214 00:28:38 @agent_ppo2.py:185][0m |          -0.0024 |         138.1928 |         -54.2356 |
[32m[20221214 00:28:38 @agent_ppo2.py:185][0m |          -0.0050 |         136.5894 |         -54.4032 |
[32m[20221214 00:28:38 @agent_ppo2.py:185][0m |          -0.0020 |         134.8675 |         -53.7787 |
[32m[20221214 00:28:38 @agent_ppo2.py:185][0m |          -0.0100 |         135.2431 |         -53.6916 |
[32m[20221214 00:28:38 @agent_ppo2.py:185][0m |          -0.0106 |         134.7219 |         -53.7539 |
[32m[20221214 00:28:38 @agent_ppo2.py:185][0m |          -0.0022 |         138.6528 |         -53.7424 |
[32m[20221214 00:28:38 @agent_ppo2.py:185][0m |          -0.0064 |         133.8716 |         -53.5367 |
[32m[20221214 00:28:38 @agent_ppo2.py:185][0m |          -0.0099 |         132.5704 |         -53.6189 |
[32m[20221214 00:28:38 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:28:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 641.09
[32m[20221214 00:28:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 710.85
[32m[20221214 00:28:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 497.83
[32m[20221214 00:28:39 @agent_ppo2.py:143][0m Total time:      31.14 min
[32m[20221214 00:28:39 @agent_ppo2.py:145][0m 2834432 total steps have happened
[32m[20221214 00:28:39 @agent_ppo2.py:121][0m #------------------------ Iteration 5384 --------------------------#
[32m[20221214 00:28:39 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:28:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:28:39 @agent_ppo2.py:185][0m |           0.0062 |         100.4402 |         -49.9945 |
[32m[20221214 00:28:39 @agent_ppo2.py:185][0m |          -0.0002 |          86.4221 |         -50.3629 |
[32m[20221214 00:28:39 @agent_ppo2.py:185][0m |          -0.0044 |          81.6694 |         -50.0193 |
[32m[20221214 00:28:39 @agent_ppo2.py:185][0m |          -0.0059 |          79.2444 |         -49.9728 |
[32m[20221214 00:28:39 @agent_ppo2.py:185][0m |          -0.0052 |          77.7393 |         -50.0457 |
[32m[20221214 00:28:39 @agent_ppo2.py:185][0m |          -0.0127 |          76.7618 |         -49.4232 |
[32m[20221214 00:28:39 @agent_ppo2.py:185][0m |          -0.0119 |          75.4262 |         -49.9757 |
[32m[20221214 00:28:40 @agent_ppo2.py:185][0m |          -0.0120 |          74.4184 |         -50.0441 |
[32m[20221214 00:28:40 @agent_ppo2.py:185][0m |          -0.0102 |          77.8370 |         -50.0756 |
[32m[20221214 00:28:40 @agent_ppo2.py:185][0m |          -0.0151 |          73.7311 |         -49.9276 |
[32m[20221214 00:28:40 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:28:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 514.92
[32m[20221214 00:28:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 692.93
[32m[20221214 00:28:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 590.99
[32m[20221214 00:28:40 @agent_ppo2.py:143][0m Total time:      31.16 min
[32m[20221214 00:28:40 @agent_ppo2.py:145][0m 2836480 total steps have happened
[32m[20221214 00:28:40 @agent_ppo2.py:121][0m #------------------------ Iteration 5385 --------------------------#
[32m[20221214 00:28:40 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:28:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:28:40 @agent_ppo2.py:185][0m |           0.0110 |          97.0935 |         -50.4145 |
[32m[20221214 00:28:40 @agent_ppo2.py:185][0m |          -0.0017 |          86.4539 |         -50.5738 |
[32m[20221214 00:28:40 @agent_ppo2.py:185][0m |          -0.0073 |          84.4839 |         -50.3098 |
[32m[20221214 00:28:41 @agent_ppo2.py:185][0m |          -0.0127 |          82.8246 |         -50.5154 |
[32m[20221214 00:28:41 @agent_ppo2.py:185][0m |          -0.0085 |          81.8013 |         -50.1750 |
[32m[20221214 00:28:41 @agent_ppo2.py:185][0m |          -0.0085 |          81.1582 |         -50.4950 |
[32m[20221214 00:28:41 @agent_ppo2.py:185][0m |          -0.0162 |          80.0937 |         -50.4074 |
[32m[20221214 00:28:41 @agent_ppo2.py:185][0m |          -0.0049 |          84.3654 |         -50.4635 |
[32m[20221214 00:28:41 @agent_ppo2.py:185][0m |          -0.0174 |          78.8775 |         -50.4870 |
[32m[20221214 00:28:41 @agent_ppo2.py:185][0m |          -0.0176 |          78.2153 |         -50.7770 |
[32m[20221214 00:28:41 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:28:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 505.48
[32m[20221214 00:28:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 609.13
[32m[20221214 00:28:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 798.68
[32m[20221214 00:28:41 @agent_ppo2.py:143][0m Total time:      31.18 min
[32m[20221214 00:28:41 @agent_ppo2.py:145][0m 2838528 total steps have happened
[32m[20221214 00:28:41 @agent_ppo2.py:121][0m #------------------------ Iteration 5386 --------------------------#
[32m[20221214 00:28:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:28:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:28:42 @agent_ppo2.py:185][0m |           0.0061 |         111.2604 |         -50.9788 |
[32m[20221214 00:28:42 @agent_ppo2.py:185][0m |          -0.0073 |          99.4030 |         -50.8525 |
[32m[20221214 00:28:42 @agent_ppo2.py:185][0m |          -0.0057 |          96.0699 |         -51.0945 |
[32m[20221214 00:28:42 @agent_ppo2.py:185][0m |          -0.0080 |          94.3243 |         -51.0179 |
[32m[20221214 00:28:42 @agent_ppo2.py:185][0m |          -0.0119 |          93.0845 |         -51.0483 |
[32m[20221214 00:28:42 @agent_ppo2.py:185][0m |          -0.0116 |          91.8229 |         -51.0343 |
[32m[20221214 00:28:42 @agent_ppo2.py:185][0m |          -0.0110 |          91.2938 |         -50.9789 |
[32m[20221214 00:28:42 @agent_ppo2.py:185][0m |          -0.0104 |          90.4460 |         -51.1965 |
[32m[20221214 00:28:42 @agent_ppo2.py:185][0m |          -0.0099 |          92.0114 |         -51.2854 |
[32m[20221214 00:28:42 @agent_ppo2.py:185][0m |          -0.0165 |          88.9497 |         -51.4267 |
[32m[20221214 00:28:42 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:28:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 608.07
[32m[20221214 00:28:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 666.47
[32m[20221214 00:28:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 571.34
[32m[20221214 00:28:43 @agent_ppo2.py:143][0m Total time:      31.20 min
[32m[20221214 00:28:43 @agent_ppo2.py:145][0m 2840576 total steps have happened
[32m[20221214 00:28:43 @agent_ppo2.py:121][0m #------------------------ Iteration 5387 --------------------------#
[32m[20221214 00:28:43 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:28:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:28:43 @agent_ppo2.py:185][0m |          -0.0052 |          69.6500 |         -54.3095 |
[32m[20221214 00:28:43 @agent_ppo2.py:185][0m |          -0.0055 |          61.0867 |         -54.0585 |
[32m[20221214 00:28:43 @agent_ppo2.py:185][0m |          -0.0116 |          58.6077 |         -53.6444 |
[32m[20221214 00:28:43 @agent_ppo2.py:185][0m |          -0.0086 |          57.4527 |         -54.0108 |
[32m[20221214 00:28:43 @agent_ppo2.py:185][0m |          -0.0097 |          56.5778 |         -53.7533 |
[32m[20221214 00:28:43 @agent_ppo2.py:185][0m |          -0.0125 |          55.2806 |         -54.1201 |
[32m[20221214 00:28:44 @agent_ppo2.py:185][0m |          -0.0138 |          54.5445 |         -53.8023 |
[32m[20221214 00:28:44 @agent_ppo2.py:185][0m |          -0.0075 |          58.5700 |         -54.2115 |
[32m[20221214 00:28:44 @agent_ppo2.py:185][0m |          -0.0188 |          53.7129 |         -54.2994 |
[32m[20221214 00:28:44 @agent_ppo2.py:185][0m |          -0.0060 |          55.1369 |         -54.4714 |
[32m[20221214 00:28:44 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:28:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 503.62
[32m[20221214 00:28:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 597.55
[32m[20221214 00:28:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 756.94
[32m[20221214 00:28:44 @agent_ppo2.py:143][0m Total time:      31.23 min
[32m[20221214 00:28:44 @agent_ppo2.py:145][0m 2842624 total steps have happened
[32m[20221214 00:28:44 @agent_ppo2.py:121][0m #------------------------ Iteration 5388 --------------------------#
[32m[20221214 00:28:44 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:28:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:28:44 @agent_ppo2.py:185][0m |           0.0021 |          89.5561 |         -52.9097 |
[32m[20221214 00:28:44 @agent_ppo2.py:185][0m |          -0.0061 |          84.0890 |         -52.6432 |
[32m[20221214 00:28:44 @agent_ppo2.py:185][0m |          -0.0121 |          82.5190 |         -52.5000 |
[32m[20221214 00:28:45 @agent_ppo2.py:185][0m |          -0.0117 |          81.1483 |         -52.4463 |
[32m[20221214 00:28:45 @agent_ppo2.py:185][0m |          -0.0120 |          80.1649 |         -52.3088 |
[32m[20221214 00:28:45 @agent_ppo2.py:185][0m |          -0.0143 |          79.4967 |         -52.3685 |
[32m[20221214 00:28:45 @agent_ppo2.py:185][0m |          -0.0168 |          79.0467 |         -52.1920 |
[32m[20221214 00:28:45 @agent_ppo2.py:185][0m |          -0.0124 |          78.3558 |         -51.7076 |
[32m[20221214 00:28:45 @agent_ppo2.py:185][0m |          -0.0179 |          78.4729 |         -51.7620 |
[32m[20221214 00:28:45 @agent_ppo2.py:185][0m |          -0.0078 |          84.1093 |         -51.8959 |
[32m[20221214 00:28:45 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:28:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 459.94
[32m[20221214 00:28:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 507.34
[32m[20221214 00:28:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 646.08
[32m[20221214 00:28:45 @agent_ppo2.py:143][0m Total time:      31.25 min
[32m[20221214 00:28:45 @agent_ppo2.py:145][0m 2844672 total steps have happened
[32m[20221214 00:28:45 @agent_ppo2.py:121][0m #------------------------ Iteration 5389 --------------------------#
[32m[20221214 00:28:45 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:28:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:28:46 @agent_ppo2.py:185][0m |           0.0034 |         123.9158 |         -50.8911 |
[32m[20221214 00:28:46 @agent_ppo2.py:185][0m |          -0.0017 |         118.3050 |         -50.8220 |
[32m[20221214 00:28:46 @agent_ppo2.py:185][0m |           0.0018 |         121.1525 |         -50.7269 |
[32m[20221214 00:28:46 @agent_ppo2.py:185][0m |          -0.0092 |         115.0163 |         -50.8462 |
[32m[20221214 00:28:46 @agent_ppo2.py:185][0m |          -0.0110 |         114.3260 |         -50.7107 |
[32m[20221214 00:28:46 @agent_ppo2.py:185][0m |          -0.0108 |         113.6527 |         -50.8349 |
[32m[20221214 00:28:46 @agent_ppo2.py:185][0m |          -0.0123 |         113.1588 |         -50.7210 |
[32m[20221214 00:28:46 @agent_ppo2.py:185][0m |          -0.0025 |         121.4561 |         -50.4130 |
[32m[20221214 00:28:46 @agent_ppo2.py:185][0m |          -0.0084 |         113.3802 |         -50.3405 |
[32m[20221214 00:28:46 @agent_ppo2.py:185][0m |           0.0045 |         129.7669 |         -50.4259 |
[32m[20221214 00:28:46 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:28:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 652.89
[32m[20221214 00:28:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 713.75
[32m[20221214 00:28:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 453.37
[32m[20221214 00:28:47 @agent_ppo2.py:143][0m Total time:      31.27 min
[32m[20221214 00:28:47 @agent_ppo2.py:145][0m 2846720 total steps have happened
[32m[20221214 00:28:47 @agent_ppo2.py:121][0m #------------------------ Iteration 5390 --------------------------#
[32m[20221214 00:28:47 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:28:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:28:47 @agent_ppo2.py:185][0m |           0.0018 |         108.5639 |         -50.7315 |
[32m[20221214 00:28:47 @agent_ppo2.py:185][0m |          -0.0016 |         105.7746 |         -50.6973 |
[32m[20221214 00:28:47 @agent_ppo2.py:185][0m |          -0.0053 |          99.9565 |         -50.5356 |
[32m[20221214 00:28:47 @agent_ppo2.py:185][0m |          -0.0134 |          96.8934 |         -50.6798 |
[32m[20221214 00:28:47 @agent_ppo2.py:185][0m |          -0.0179 |          95.4778 |         -50.8361 |
[32m[20221214 00:28:47 @agent_ppo2.py:185][0m |          -0.0169 |          95.0280 |         -50.7934 |
[32m[20221214 00:28:48 @agent_ppo2.py:185][0m |          -0.0168 |          93.2121 |         -50.6850 |
[32m[20221214 00:28:48 @agent_ppo2.py:185][0m |          -0.0190 |          93.0218 |         -50.6514 |
[32m[20221214 00:28:48 @agent_ppo2.py:185][0m |          -0.0164 |          91.6782 |         -50.5893 |
[32m[20221214 00:28:48 @agent_ppo2.py:185][0m |          -0.0169 |          91.1385 |         -50.6246 |
[32m[20221214 00:28:48 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:28:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 444.16
[32m[20221214 00:28:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 551.02
[32m[20221214 00:28:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 515.96
[32m[20221214 00:28:48 @agent_ppo2.py:143][0m Total time:      31.29 min
[32m[20221214 00:28:48 @agent_ppo2.py:145][0m 2848768 total steps have happened
[32m[20221214 00:28:48 @agent_ppo2.py:121][0m #------------------------ Iteration 5391 --------------------------#
[32m[20221214 00:28:48 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:28:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:28:48 @agent_ppo2.py:185][0m |           0.0023 |          84.2953 |         -54.2158 |
[32m[20221214 00:28:48 @agent_ppo2.py:185][0m |          -0.0084 |          79.7409 |         -53.9380 |
[32m[20221214 00:28:49 @agent_ppo2.py:185][0m |          -0.0123 |          78.0710 |         -53.5389 |
[32m[20221214 00:28:49 @agent_ppo2.py:185][0m |          -0.0119 |          76.8007 |         -53.5102 |
[32m[20221214 00:28:49 @agent_ppo2.py:185][0m |          -0.0070 |          76.9884 |         -53.2989 |
[32m[20221214 00:28:49 @agent_ppo2.py:185][0m |          -0.0140 |          75.3483 |         -53.3108 |
[32m[20221214 00:28:49 @agent_ppo2.py:185][0m |          -0.0129 |          75.0609 |         -53.3287 |
[32m[20221214 00:28:49 @agent_ppo2.py:185][0m |          -0.0029 |          78.2227 |         -52.7666 |
[32m[20221214 00:28:49 @agent_ppo2.py:185][0m |          -0.0071 |          75.3495 |         -52.8585 |
[32m[20221214 00:28:49 @agent_ppo2.py:185][0m |          -0.0166 |          73.2446 |         -52.7715 |
[32m[20221214 00:28:49 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:28:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 641.11
[32m[20221214 00:28:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 699.16
[32m[20221214 00:28:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 534.32
[32m[20221214 00:28:49 @agent_ppo2.py:143][0m Total time:      31.32 min
[32m[20221214 00:28:49 @agent_ppo2.py:145][0m 2850816 total steps have happened
[32m[20221214 00:28:49 @agent_ppo2.py:121][0m #------------------------ Iteration 5392 --------------------------#
[32m[20221214 00:28:50 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:28:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:28:50 @agent_ppo2.py:185][0m |           0.0017 |         101.6592 |         -48.2532 |
[32m[20221214 00:28:50 @agent_ppo2.py:185][0m |          -0.0076 |          96.9045 |         -48.0546 |
[32m[20221214 00:28:50 @agent_ppo2.py:185][0m |          -0.0004 |          98.0233 |         -48.1766 |
[32m[20221214 00:28:50 @agent_ppo2.py:185][0m |          -0.0075 |          94.0379 |         -47.9591 |
[32m[20221214 00:28:50 @agent_ppo2.py:185][0m |          -0.0064 |          92.5492 |         -47.7083 |
[32m[20221214 00:28:50 @agent_ppo2.py:185][0m |          -0.0093 |          91.9910 |         -48.2674 |
[32m[20221214 00:28:50 @agent_ppo2.py:185][0m |          -0.0100 |          91.5047 |         -48.2723 |
[32m[20221214 00:28:50 @agent_ppo2.py:185][0m |          -0.0085 |          90.8722 |         -48.2241 |
[32m[20221214 00:28:51 @agent_ppo2.py:185][0m |          -0.0162 |          90.6090 |         -48.3842 |
[32m[20221214 00:28:51 @agent_ppo2.py:185][0m |          -0.0156 |          90.2759 |         -48.2929 |
[32m[20221214 00:28:51 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:28:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 498.20
[32m[20221214 00:28:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 520.82
[32m[20221214 00:28:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 550.71
[32m[20221214 00:28:51 @agent_ppo2.py:143][0m Total time:      31.34 min
[32m[20221214 00:28:51 @agent_ppo2.py:145][0m 2852864 total steps have happened
[32m[20221214 00:28:51 @agent_ppo2.py:121][0m #------------------------ Iteration 5393 --------------------------#
[32m[20221214 00:28:51 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:28:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:28:51 @agent_ppo2.py:185][0m |           0.0022 |          80.4982 |         -48.7221 |
[32m[20221214 00:28:51 @agent_ppo2.py:185][0m |          -0.0039 |          71.8371 |         -48.8670 |
[32m[20221214 00:28:51 @agent_ppo2.py:185][0m |          -0.0032 |          68.7157 |         -49.0851 |
[32m[20221214 00:28:51 @agent_ppo2.py:185][0m |          -0.0100 |          66.8496 |         -49.1414 |
[32m[20221214 00:28:51 @agent_ppo2.py:185][0m |          -0.0140 |          65.8182 |         -49.4551 |
[32m[20221214 00:28:52 @agent_ppo2.py:185][0m |          -0.0134 |          64.9841 |         -49.3890 |
[32m[20221214 00:28:52 @agent_ppo2.py:185][0m |          -0.0134 |          64.5693 |         -49.5717 |
[32m[20221214 00:28:52 @agent_ppo2.py:185][0m |          -0.0141 |          63.4859 |         -49.6487 |
[32m[20221214 00:28:52 @agent_ppo2.py:185][0m |          -0.0160 |          63.5624 |         -49.7796 |
[32m[20221214 00:28:52 @agent_ppo2.py:185][0m |          -0.0106 |          63.2407 |         -49.8610 |
[32m[20221214 00:28:52 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:28:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 504.66
[32m[20221214 00:28:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 556.86
[32m[20221214 00:28:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 607.85
[32m[20221214 00:28:52 @agent_ppo2.py:143][0m Total time:      31.36 min
[32m[20221214 00:28:52 @agent_ppo2.py:145][0m 2854912 total steps have happened
[32m[20221214 00:28:52 @agent_ppo2.py:121][0m #------------------------ Iteration 5394 --------------------------#
[32m[20221214 00:28:52 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:28:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:28:52 @agent_ppo2.py:185][0m |           0.0022 |         104.7655 |         -49.1077 |
[32m[20221214 00:28:53 @agent_ppo2.py:185][0m |          -0.0093 |         100.7444 |         -48.5702 |
[32m[20221214 00:28:53 @agent_ppo2.py:185][0m |          -0.0128 |          99.2460 |         -48.9953 |
[32m[20221214 00:28:53 @agent_ppo2.py:185][0m |          -0.0079 |         100.6676 |         -48.5527 |
[32m[20221214 00:28:53 @agent_ppo2.py:185][0m |          -0.0145 |          97.7681 |         -48.4404 |
[32m[20221214 00:28:53 @agent_ppo2.py:185][0m |          -0.0172 |          97.0875 |         -48.5725 |
[32m[20221214 00:28:53 @agent_ppo2.py:185][0m |          -0.0172 |          96.8375 |         -48.3707 |
[32m[20221214 00:28:53 @agent_ppo2.py:185][0m |          -0.0193 |          96.2342 |         -48.3775 |
[32m[20221214 00:28:53 @agent_ppo2.py:185][0m |          -0.0217 |          95.7896 |         -48.3530 |
[32m[20221214 00:28:53 @agent_ppo2.py:185][0m |          -0.0206 |          96.7612 |         -48.2266 |
[32m[20221214 00:28:53 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:28:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 526.38
[32m[20221214 00:28:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 551.88
[32m[20221214 00:28:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 688.84
[32m[20221214 00:28:53 @agent_ppo2.py:143][0m Total time:      31.39 min
[32m[20221214 00:28:53 @agent_ppo2.py:145][0m 2856960 total steps have happened
[32m[20221214 00:28:53 @agent_ppo2.py:121][0m #------------------------ Iteration 5395 --------------------------#
[32m[20221214 00:28:54 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:28:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:28:54 @agent_ppo2.py:185][0m |           0.0011 |         117.7620 |         -50.9174 |
[32m[20221214 00:28:54 @agent_ppo2.py:185][0m |          -0.0076 |         108.2413 |         -50.6341 |
[32m[20221214 00:28:54 @agent_ppo2.py:185][0m |          -0.0051 |         105.8682 |         -50.7275 |
[32m[20221214 00:28:54 @agent_ppo2.py:185][0m |          -0.0140 |         100.7689 |         -50.6527 |
[32m[20221214 00:28:54 @agent_ppo2.py:185][0m |          -0.0145 |          98.9400 |         -50.3675 |
[32m[20221214 00:28:54 @agent_ppo2.py:185][0m |          -0.0143 |          97.2948 |         -50.6495 |
[32m[20221214 00:28:54 @agent_ppo2.py:185][0m |          -0.0220 |          96.6027 |         -50.7356 |
[32m[20221214 00:28:54 @agent_ppo2.py:185][0m |          -0.0194 |          95.8567 |         -50.3863 |
[32m[20221214 00:28:55 @agent_ppo2.py:185][0m |          -0.0101 |         107.0239 |         -50.5308 |
[32m[20221214 00:28:55 @agent_ppo2.py:185][0m |          -0.0137 |          97.2182 |         -50.8743 |
[32m[20221214 00:28:55 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:28:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 493.93
[32m[20221214 00:28:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 659.27
[32m[20221214 00:28:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 478.64
[32m[20221214 00:28:55 @agent_ppo2.py:143][0m Total time:      31.41 min
[32m[20221214 00:28:55 @agent_ppo2.py:145][0m 2859008 total steps have happened
[32m[20221214 00:28:55 @agent_ppo2.py:121][0m #------------------------ Iteration 5396 --------------------------#
[32m[20221214 00:28:55 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:28:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:28:55 @agent_ppo2.py:185][0m |           0.0014 |          77.5332 |         -51.0987 |
[32m[20221214 00:28:55 @agent_ppo2.py:185][0m |          -0.0035 |          67.1558 |         -51.3986 |
[32m[20221214 00:28:55 @agent_ppo2.py:185][0m |          -0.0036 |          66.2200 |         -51.1244 |
[32m[20221214 00:28:55 @agent_ppo2.py:185][0m |          -0.0088 |          64.9760 |         -51.4490 |
[32m[20221214 00:28:56 @agent_ppo2.py:185][0m |          -0.0093 |          64.1848 |         -51.3524 |
[32m[20221214 00:28:56 @agent_ppo2.py:185][0m |          -0.0106 |          63.7669 |         -51.2601 |
[32m[20221214 00:28:56 @agent_ppo2.py:185][0m |          -0.0104 |          63.4460 |         -51.5194 |
[32m[20221214 00:28:56 @agent_ppo2.py:185][0m |          -0.0120 |          63.1998 |         -51.4369 |
[32m[20221214 00:28:56 @agent_ppo2.py:185][0m |          -0.0138 |          62.8418 |         -51.6563 |
[32m[20221214 00:28:56 @agent_ppo2.py:185][0m |          -0.0137 |          62.5990 |         -51.6692 |
[32m[20221214 00:28:56 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:28:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 463.50
[32m[20221214 00:28:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 594.12
[32m[20221214 00:28:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 522.68
[32m[20221214 00:28:56 @agent_ppo2.py:143][0m Total time:      31.43 min
[32m[20221214 00:28:56 @agent_ppo2.py:145][0m 2861056 total steps have happened
[32m[20221214 00:28:56 @agent_ppo2.py:121][0m #------------------------ Iteration 5397 --------------------------#
[32m[20221214 00:28:56 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:28:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:28:57 @agent_ppo2.py:185][0m |           0.0041 |          86.1304 |         -52.4647 |
[32m[20221214 00:28:57 @agent_ppo2.py:185][0m |          -0.0040 |          80.0525 |         -52.1869 |
[32m[20221214 00:28:57 @agent_ppo2.py:185][0m |          -0.0104 |          77.6574 |         -52.3115 |
[32m[20221214 00:28:57 @agent_ppo2.py:185][0m |          -0.0147 |          76.5323 |         -52.1750 |
[32m[20221214 00:28:57 @agent_ppo2.py:185][0m |          -0.0050 |          83.5165 |         -52.2072 |
[32m[20221214 00:28:57 @agent_ppo2.py:185][0m |          -0.0059 |          76.5872 |         -51.7062 |
[32m[20221214 00:28:57 @agent_ppo2.py:185][0m |          -0.0120 |          74.0992 |         -51.8839 |
[32m[20221214 00:28:57 @agent_ppo2.py:185][0m |          -0.0197 |          73.7758 |         -51.6929 |
[32m[20221214 00:28:57 @agent_ppo2.py:185][0m |          -0.0217 |          73.6556 |         -51.6550 |
[32m[20221214 00:28:57 @agent_ppo2.py:185][0m |          -0.0214 |          73.6998 |         -51.4212 |
[32m[20221214 00:28:57 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:28:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 424.86
[32m[20221214 00:28:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 554.47
[32m[20221214 00:28:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 813.04
[32m[20221214 00:28:58 @agent_ppo2.py:143][0m Total time:      31.45 min
[32m[20221214 00:28:58 @agent_ppo2.py:145][0m 2863104 total steps have happened
[32m[20221214 00:28:58 @agent_ppo2.py:121][0m #------------------------ Iteration 5398 --------------------------#
[32m[20221214 00:28:58 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:28:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:28:58 @agent_ppo2.py:185][0m |           0.0002 |          92.3039 |         -54.9069 |
[32m[20221214 00:28:58 @agent_ppo2.py:185][0m |          -0.0042 |          83.9308 |         -54.5118 |
[32m[20221214 00:28:58 @agent_ppo2.py:185][0m |          -0.0062 |          84.2641 |         -54.4120 |
[32m[20221214 00:28:58 @agent_ppo2.py:185][0m |          -0.0071 |          79.7884 |         -54.1948 |
[32m[20221214 00:28:58 @agent_ppo2.py:185][0m |          -0.0126 |          78.6300 |         -54.3874 |
[32m[20221214 00:28:58 @agent_ppo2.py:185][0m |          -0.0156 |          77.8021 |         -54.0368 |
[32m[20221214 00:28:58 @agent_ppo2.py:185][0m |          -0.0152 |          76.6551 |         -54.0627 |
[32m[20221214 00:28:59 @agent_ppo2.py:185][0m |          -0.0184 |          76.0674 |         -53.9432 |
[32m[20221214 00:28:59 @agent_ppo2.py:185][0m |          -0.0200 |          75.5148 |         -53.9794 |
[32m[20221214 00:28:59 @agent_ppo2.py:185][0m |          -0.0203 |          75.0271 |         -53.7585 |
[32m[20221214 00:28:59 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:28:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 421.52
[32m[20221214 00:28:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 460.44
[32m[20221214 00:28:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 490.67
[32m[20221214 00:28:59 @agent_ppo2.py:143][0m Total time:      31.48 min
[32m[20221214 00:28:59 @agent_ppo2.py:145][0m 2865152 total steps have happened
[32m[20221214 00:28:59 @agent_ppo2.py:121][0m #------------------------ Iteration 5399 --------------------------#
[32m[20221214 00:28:59 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:28:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:28:59 @agent_ppo2.py:185][0m |           0.0047 |          96.8030 |         -50.8394 |
[32m[20221214 00:28:59 @agent_ppo2.py:185][0m |          -0.0039 |          84.9020 |         -50.8789 |
[32m[20221214 00:28:59 @agent_ppo2.py:185][0m |          -0.0052 |          81.9062 |         -50.5910 |
[32m[20221214 00:29:00 @agent_ppo2.py:185][0m |          -0.0136 |          79.6385 |         -50.8992 |
[32m[20221214 00:29:00 @agent_ppo2.py:185][0m |          -0.0170 |          78.3690 |         -50.8373 |
[32m[20221214 00:29:00 @agent_ppo2.py:185][0m |          -0.0177 |          77.3673 |         -50.5012 |
[32m[20221214 00:29:00 @agent_ppo2.py:185][0m |          -0.0185 |          76.4841 |         -50.8498 |
[32m[20221214 00:29:00 @agent_ppo2.py:185][0m |          -0.0123 |          75.6167 |         -50.9093 |
[32m[20221214 00:29:00 @agent_ppo2.py:185][0m |          -0.0166 |          74.5410 |         -50.8076 |
[32m[20221214 00:29:00 @agent_ppo2.py:185][0m |          -0.0197 |          74.2310 |         -50.9131 |
[32m[20221214 00:29:00 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:29:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 508.96
[32m[20221214 00:29:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 591.83
[32m[20221214 00:29:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 515.91
[32m[20221214 00:29:00 @agent_ppo2.py:143][0m Total time:      31.50 min
[32m[20221214 00:29:00 @agent_ppo2.py:145][0m 2867200 total steps have happened
[32m[20221214 00:29:00 @agent_ppo2.py:121][0m #------------------------ Iteration 5400 --------------------------#
[32m[20221214 00:29:01 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:29:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:29:01 @agent_ppo2.py:185][0m |           0.0049 |          63.1120 |         -51.3018 |
[32m[20221214 00:29:01 @agent_ppo2.py:185][0m |          -0.0045 |          57.9313 |         -51.5618 |
[32m[20221214 00:29:01 @agent_ppo2.py:185][0m |          -0.0066 |          57.3748 |         -51.7761 |
[32m[20221214 00:29:01 @agent_ppo2.py:185][0m |          -0.0078 |          55.7199 |         -51.7335 |
[32m[20221214 00:29:01 @agent_ppo2.py:185][0m |          -0.0131 |          54.7814 |         -51.9657 |
[32m[20221214 00:29:01 @agent_ppo2.py:185][0m |          -0.0154 |          53.8489 |         -52.3057 |
[32m[20221214 00:29:01 @agent_ppo2.py:185][0m |          -0.0009 |          57.6451 |         -52.6423 |
[32m[20221214 00:29:01 @agent_ppo2.py:185][0m |          -0.0024 |          56.2829 |         -52.5519 |
[32m[20221214 00:29:01 @agent_ppo2.py:185][0m |          -0.0147 |          52.8421 |         -52.8900 |
[32m[20221214 00:29:02 @agent_ppo2.py:185][0m |          -0.0210 |          52.3617 |         -52.8398 |
[32m[20221214 00:29:02 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:29:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 455.27
[32m[20221214 00:29:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 526.83
[32m[20221214 00:29:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 537.11
[32m[20221214 00:29:02 @agent_ppo2.py:143][0m Total time:      31.52 min
[32m[20221214 00:29:02 @agent_ppo2.py:145][0m 2869248 total steps have happened
[32m[20221214 00:29:02 @agent_ppo2.py:121][0m #------------------------ Iteration 5401 --------------------------#
[32m[20221214 00:29:02 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:29:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:29:02 @agent_ppo2.py:185][0m |           0.0060 |          79.6155 |         -57.1597 |
[32m[20221214 00:29:02 @agent_ppo2.py:185][0m |          -0.0059 |          74.2296 |         -56.8586 |
[32m[20221214 00:29:02 @agent_ppo2.py:185][0m |          -0.0069 |          72.9301 |         -57.1206 |
[32m[20221214 00:29:02 @agent_ppo2.py:185][0m |          -0.0092 |          72.1677 |         -57.0196 |
[32m[20221214 00:29:02 @agent_ppo2.py:185][0m |          -0.0085 |          71.0843 |         -57.0055 |
[32m[20221214 00:29:03 @agent_ppo2.py:185][0m |          -0.0077 |          70.7176 |         -57.1058 |
[32m[20221214 00:29:03 @agent_ppo2.py:185][0m |          -0.0012 |          76.4862 |         -57.0244 |
[32m[20221214 00:29:03 @agent_ppo2.py:185][0m |          -0.0123 |          71.0071 |         -57.1931 |
[32m[20221214 00:29:03 @agent_ppo2.py:185][0m |          -0.0203 |          69.2814 |         -57.1580 |
[32m[20221214 00:29:03 @agent_ppo2.py:185][0m |          -0.0169 |          68.9877 |         -57.1518 |
[32m[20221214 00:29:03 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:29:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 487.01
[32m[20221214 00:29:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 524.55
[32m[20221214 00:29:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 527.43
[32m[20221214 00:29:03 @agent_ppo2.py:143][0m Total time:      31.54 min
[32m[20221214 00:29:03 @agent_ppo2.py:145][0m 2871296 total steps have happened
[32m[20221214 00:29:03 @agent_ppo2.py:121][0m #------------------------ Iteration 5402 --------------------------#
[32m[20221214 00:29:03 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:29:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:29:03 @agent_ppo2.py:185][0m |           0.0018 |          89.2730 |         -54.5869 |
[32m[20221214 00:29:03 @agent_ppo2.py:185][0m |          -0.0047 |          80.5876 |         -54.6720 |
[32m[20221214 00:29:04 @agent_ppo2.py:185][0m |          -0.0117 |          76.9753 |         -54.4352 |
[32m[20221214 00:29:04 @agent_ppo2.py:185][0m |          -0.0117 |          75.3454 |         -54.3677 |
[32m[20221214 00:29:04 @agent_ppo2.py:185][0m |          -0.0083 |          76.8511 |         -54.5824 |
[32m[20221214 00:29:04 @agent_ppo2.py:185][0m |          -0.0148 |          72.9823 |         -54.4415 |
[32m[20221214 00:29:04 @agent_ppo2.py:185][0m |          -0.0182 |          72.3798 |         -54.1079 |
[32m[20221214 00:29:04 @agent_ppo2.py:185][0m |          -0.0186 |          71.7399 |         -54.1813 |
[32m[20221214 00:29:04 @agent_ppo2.py:185][0m |          -0.0218 |          71.1491 |         -54.0578 |
[32m[20221214 00:29:04 @agent_ppo2.py:185][0m |          -0.0130 |          72.5153 |         -54.2590 |
[32m[20221214 00:29:04 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:29:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 511.54
[32m[20221214 00:29:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 591.37
[32m[20221214 00:29:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 423.28
[32m[20221214 00:29:04 @agent_ppo2.py:143][0m Total time:      31.57 min
[32m[20221214 00:29:04 @agent_ppo2.py:145][0m 2873344 total steps have happened
[32m[20221214 00:29:04 @agent_ppo2.py:121][0m #------------------------ Iteration 5403 --------------------------#
[32m[20221214 00:29:05 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:29:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:29:05 @agent_ppo2.py:185][0m |          -0.0005 |          71.5107 |         -52.9783 |
[32m[20221214 00:29:05 @agent_ppo2.py:185][0m |          -0.0128 |          59.8866 |         -53.0384 |
[32m[20221214 00:29:05 @agent_ppo2.py:185][0m |          -0.0053 |          56.2728 |         -52.9233 |
[32m[20221214 00:29:05 @agent_ppo2.py:185][0m |          -0.0081 |          54.8472 |         -53.1106 |
[32m[20221214 00:29:05 @agent_ppo2.py:185][0m |          -0.0116 |          53.2443 |         -52.9631 |
[32m[20221214 00:29:05 @agent_ppo2.py:185][0m |          -0.0089 |          52.9903 |         -52.9668 |
[32m[20221214 00:29:05 @agent_ppo2.py:185][0m |          -0.0068 |          58.4037 |         -52.8637 |
[32m[20221214 00:29:05 @agent_ppo2.py:185][0m |          -0.0126 |          51.5423 |         -53.0407 |
[32m[20221214 00:29:06 @agent_ppo2.py:185][0m |          -0.0248 |          50.8683 |         -53.1481 |
[32m[20221214 00:29:06 @agent_ppo2.py:185][0m |          -0.0219 |          50.4562 |         -52.7778 |
[32m[20221214 00:29:06 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:29:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 462.70
[32m[20221214 00:29:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 564.43
[32m[20221214 00:29:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 451.30
[32m[20221214 00:29:06 @agent_ppo2.py:143][0m Total time:      31.59 min
[32m[20221214 00:29:06 @agent_ppo2.py:145][0m 2875392 total steps have happened
[32m[20221214 00:29:06 @agent_ppo2.py:121][0m #------------------------ Iteration 5404 --------------------------#
[32m[20221214 00:29:06 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:29:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:29:06 @agent_ppo2.py:185][0m |           0.0033 |          96.5241 |         -57.0090 |
[32m[20221214 00:29:06 @agent_ppo2.py:185][0m |          -0.0168 |          91.3457 |         -56.5904 |
[32m[20221214 00:29:06 @agent_ppo2.py:185][0m |          -0.0007 |          95.0103 |         -56.8154 |
[32m[20221214 00:29:06 @agent_ppo2.py:185][0m |          -0.0129 |          88.6854 |         -56.7126 |
[32m[20221214 00:29:07 @agent_ppo2.py:185][0m |          -0.0139 |          87.7477 |         -56.6762 |
[32m[20221214 00:29:07 @agent_ppo2.py:185][0m |          -0.0130 |          87.0616 |         -56.7096 |
[32m[20221214 00:29:07 @agent_ppo2.py:185][0m |          -0.0125 |          86.8501 |         -56.7150 |
[32m[20221214 00:29:07 @agent_ppo2.py:185][0m |          -0.0181 |          86.4056 |         -56.5291 |
[32m[20221214 00:29:07 @agent_ppo2.py:185][0m |          -0.0202 |          85.6511 |         -56.3998 |
[32m[20221214 00:29:07 @agent_ppo2.py:185][0m |          -0.0202 |          85.4652 |         -56.3786 |
[32m[20221214 00:29:07 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:29:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 542.55
[32m[20221214 00:29:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 571.87
[32m[20221214 00:29:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 518.12
[32m[20221214 00:29:07 @agent_ppo2.py:143][0m Total time:      31.61 min
[32m[20221214 00:29:07 @agent_ppo2.py:145][0m 2877440 total steps have happened
[32m[20221214 00:29:07 @agent_ppo2.py:121][0m #------------------------ Iteration 5405 --------------------------#
[32m[20221214 00:29:07 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:29:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:29:08 @agent_ppo2.py:185][0m |          -0.0004 |          81.5622 |         -52.9294 |
[32m[20221214 00:29:08 @agent_ppo2.py:185][0m |          -0.0064 |          77.4078 |         -52.8575 |
[32m[20221214 00:29:08 @agent_ppo2.py:185][0m |          -0.0107 |          75.6428 |         -52.8323 |
[32m[20221214 00:29:08 @agent_ppo2.py:185][0m |          -0.0029 |          77.4578 |         -52.7671 |
[32m[20221214 00:29:08 @agent_ppo2.py:185][0m |          -0.0071 |          73.8074 |         -53.0227 |
[32m[20221214 00:29:08 @agent_ppo2.py:185][0m |          -0.0071 |          77.5092 |         -52.7332 |
[32m[20221214 00:29:08 @agent_ppo2.py:185][0m |          -0.0151 |          72.7721 |         -52.7162 |
[32m[20221214 00:29:08 @agent_ppo2.py:185][0m |          -0.0167 |          72.3329 |         -53.1138 |
[32m[20221214 00:29:08 @agent_ppo2.py:185][0m |          -0.0165 |          72.0679 |         -52.8921 |
[32m[20221214 00:29:08 @agent_ppo2.py:185][0m |          -0.0169 |          71.6992 |         -52.9793 |
[32m[20221214 00:29:08 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:29:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 421.00
[32m[20221214 00:29:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 499.34
[32m[20221214 00:29:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 547.78
[32m[20221214 00:29:09 @agent_ppo2.py:143][0m Total time:      31.64 min
[32m[20221214 00:29:09 @agent_ppo2.py:145][0m 2879488 total steps have happened
[32m[20221214 00:29:09 @agent_ppo2.py:121][0m #------------------------ Iteration 5406 --------------------------#
[32m[20221214 00:29:09 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:29:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:29:09 @agent_ppo2.py:185][0m |           0.0002 |          85.5924 |         -58.3134 |
[32m[20221214 00:29:09 @agent_ppo2.py:185][0m |          -0.0073 |          78.1047 |         -58.1516 |
[32m[20221214 00:29:09 @agent_ppo2.py:185][0m |          -0.0090 |          75.0435 |         -57.6815 |
[32m[20221214 00:29:09 @agent_ppo2.py:185][0m |          -0.0112 |          73.6586 |         -57.9979 |
[32m[20221214 00:29:09 @agent_ppo2.py:185][0m |          -0.0116 |          72.6977 |         -57.9268 |
[32m[20221214 00:29:09 @agent_ppo2.py:185][0m |          -0.0170 |          71.6962 |         -57.8024 |
[32m[20221214 00:29:10 @agent_ppo2.py:185][0m |          -0.0136 |          70.7078 |         -57.8090 |
[32m[20221214 00:29:10 @agent_ppo2.py:185][0m |          -0.0102 |          72.1288 |         -57.8536 |
[32m[20221214 00:29:10 @agent_ppo2.py:185][0m |          -0.0183 |          70.1229 |         -58.0864 |
[32m[20221214 00:29:10 @agent_ppo2.py:185][0m |          -0.0072 |          73.8817 |         -57.5394 |
[32m[20221214 00:29:10 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:29:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 546.64
[32m[20221214 00:29:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 568.52
[32m[20221214 00:29:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 388.60
[32m[20221214 00:29:10 @agent_ppo2.py:143][0m Total time:      31.66 min
[32m[20221214 00:29:10 @agent_ppo2.py:145][0m 2881536 total steps have happened
[32m[20221214 00:29:10 @agent_ppo2.py:121][0m #------------------------ Iteration 5407 --------------------------#
[32m[20221214 00:29:10 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:29:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:29:10 @agent_ppo2.py:185][0m |           0.0029 |          71.1894 |         -55.6993 |
[32m[20221214 00:29:10 @agent_ppo2.py:185][0m |          -0.0087 |          54.7148 |         -56.1030 |
[32m[20221214 00:29:11 @agent_ppo2.py:185][0m |          -0.0128 |          50.3356 |         -55.8636 |
[32m[20221214 00:29:11 @agent_ppo2.py:185][0m |          -0.0146 |          47.1620 |         -55.7212 |
[32m[20221214 00:29:11 @agent_ppo2.py:185][0m |          -0.0195 |          45.2682 |         -55.9766 |
[32m[20221214 00:29:11 @agent_ppo2.py:185][0m |          -0.0179 |          43.9178 |         -55.5837 |
[32m[20221214 00:29:11 @agent_ppo2.py:185][0m |          -0.0190 |          42.6480 |         -55.7221 |
[32m[20221214 00:29:11 @agent_ppo2.py:185][0m |          -0.0216 |          41.6010 |         -55.8519 |
[32m[20221214 00:29:11 @agent_ppo2.py:185][0m |          -0.0208 |          40.7865 |         -55.6074 |
[32m[20221214 00:29:11 @agent_ppo2.py:185][0m |          -0.0142 |          40.1695 |         -55.7744 |
[32m[20221214 00:29:11 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:29:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 465.76
[32m[20221214 00:29:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 515.79
[32m[20221214 00:29:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 487.24
[32m[20221214 00:29:11 @agent_ppo2.py:143][0m Total time:      31.68 min
[32m[20221214 00:29:11 @agent_ppo2.py:145][0m 2883584 total steps have happened
[32m[20221214 00:29:11 @agent_ppo2.py:121][0m #------------------------ Iteration 5408 --------------------------#
[32m[20221214 00:29:12 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:29:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:29:12 @agent_ppo2.py:185][0m |           0.0025 |          59.5106 |         -59.9185 |
[32m[20221214 00:29:12 @agent_ppo2.py:185][0m |          -0.0060 |          41.3046 |         -59.7915 |
[32m[20221214 00:29:12 @agent_ppo2.py:185][0m |          -0.0002 |          39.0257 |         -59.9612 |
[32m[20221214 00:29:12 @agent_ppo2.py:185][0m |          -0.0118 |          37.9672 |         -60.1954 |
[32m[20221214 00:29:12 @agent_ppo2.py:185][0m |           0.0008 |          38.0974 |         -60.0218 |
[32m[20221214 00:29:12 @agent_ppo2.py:185][0m |          -0.0123 |          36.8436 |         -59.9026 |
[32m[20221214 00:29:12 @agent_ppo2.py:185][0m |          -0.0125 |          36.0979 |         -60.2117 |
[32m[20221214 00:29:12 @agent_ppo2.py:185][0m |          -0.0127 |          35.7797 |         -60.4335 |
[32m[20221214 00:29:12 @agent_ppo2.py:185][0m |          -0.0141 |          35.4181 |         -60.2862 |
[32m[20221214 00:29:13 @agent_ppo2.py:185][0m |          -0.0151 |          35.2033 |         -60.7883 |
[32m[20221214 00:29:13 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:29:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 495.96
[32m[20221214 00:29:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 629.24
[32m[20221214 00:29:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 520.04
[32m[20221214 00:29:13 @agent_ppo2.py:143][0m Total time:      31.71 min
[32m[20221214 00:29:13 @agent_ppo2.py:145][0m 2885632 total steps have happened
[32m[20221214 00:29:13 @agent_ppo2.py:121][0m #------------------------ Iteration 5409 --------------------------#
[32m[20221214 00:29:13 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:29:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:29:13 @agent_ppo2.py:185][0m |           0.0062 |          75.2137 |         -57.5487 |
[32m[20221214 00:29:13 @agent_ppo2.py:185][0m |          -0.0071 |          66.6414 |         -57.6412 |
[32m[20221214 00:29:13 @agent_ppo2.py:185][0m |          -0.0142 |          64.2623 |         -57.6917 |
[32m[20221214 00:29:13 @agent_ppo2.py:185][0m |          -0.0126 |          62.9178 |         -57.7558 |
[32m[20221214 00:29:13 @agent_ppo2.py:185][0m |          -0.0168 |          61.7225 |         -57.7093 |
[32m[20221214 00:29:14 @agent_ppo2.py:185][0m |          -0.0150 |          60.9695 |         -57.3380 |
[32m[20221214 00:29:14 @agent_ppo2.py:185][0m |          -0.0243 |          60.3000 |         -57.6700 |
[32m[20221214 00:29:14 @agent_ppo2.py:185][0m |          -0.0216 |          60.0110 |         -57.6059 |
[32m[20221214 00:29:14 @agent_ppo2.py:185][0m |          -0.0243 |          59.6015 |         -57.7424 |
[32m[20221214 00:29:14 @agent_ppo2.py:185][0m |          -0.0132 |          61.2500 |         -57.9377 |
[32m[20221214 00:29:14 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:29:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 453.27
[32m[20221214 00:29:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 550.41
[32m[20221214 00:29:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 522.29
[32m[20221214 00:29:14 @agent_ppo2.py:143][0m Total time:      31.73 min
[32m[20221214 00:29:14 @agent_ppo2.py:145][0m 2887680 total steps have happened
[32m[20221214 00:29:14 @agent_ppo2.py:121][0m #------------------------ Iteration 5410 --------------------------#
[32m[20221214 00:29:14 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:29:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:29:14 @agent_ppo2.py:185][0m |          -0.0008 |          84.3034 |         -58.4959 |
[32m[20221214 00:29:15 @agent_ppo2.py:185][0m |          -0.0053 |          74.1551 |         -58.1885 |
[32m[20221214 00:29:15 @agent_ppo2.py:185][0m |          -0.0117 |          71.7386 |         -58.3490 |
[32m[20221214 00:29:15 @agent_ppo2.py:185][0m |          -0.0113 |          70.2573 |         -57.9281 |
[32m[20221214 00:29:15 @agent_ppo2.py:185][0m |          -0.0127 |          69.9258 |         -58.1400 |
[32m[20221214 00:29:15 @agent_ppo2.py:185][0m |          -0.0208 |          68.7643 |         -57.9599 |
[32m[20221214 00:29:15 @agent_ppo2.py:185][0m |          -0.0210 |          68.0902 |         -58.0353 |
[32m[20221214 00:29:15 @agent_ppo2.py:185][0m |          -0.0194 |          67.4658 |         -57.8331 |
[32m[20221214 00:29:15 @agent_ppo2.py:185][0m |          -0.0163 |          66.8687 |         -57.8715 |
[32m[20221214 00:29:15 @agent_ppo2.py:185][0m |          -0.0213 |          66.7761 |         -57.8288 |
[32m[20221214 00:29:15 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:29:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 524.21
[32m[20221214 00:29:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 556.45
[32m[20221214 00:29:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 506.48
[32m[20221214 00:29:15 @agent_ppo2.py:143][0m Total time:      31.75 min
[32m[20221214 00:29:15 @agent_ppo2.py:145][0m 2889728 total steps have happened
[32m[20221214 00:29:15 @agent_ppo2.py:121][0m #------------------------ Iteration 5411 --------------------------#
[32m[20221214 00:29:16 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:29:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:29:16 @agent_ppo2.py:185][0m |          -0.0011 |          64.6844 |         -61.7573 |
[32m[20221214 00:29:16 @agent_ppo2.py:185][0m |          -0.0021 |          55.4298 |         -61.2598 |
[32m[20221214 00:29:16 @agent_ppo2.py:185][0m |           0.0013 |          57.2521 |         -61.5423 |
[32m[20221214 00:29:16 @agent_ppo2.py:185][0m |          -0.0148 |          50.1093 |         -61.3216 |
[32m[20221214 00:29:16 @agent_ppo2.py:185][0m |          -0.0157 |          47.7007 |         -61.2177 |
[32m[20221214 00:29:16 @agent_ppo2.py:185][0m |          -0.0169 |          46.4073 |         -60.9268 |
[32m[20221214 00:29:16 @agent_ppo2.py:185][0m |          -0.0178 |          45.6960 |         -60.8890 |
[32m[20221214 00:29:16 @agent_ppo2.py:185][0m |          -0.0187 |          44.2634 |         -60.7064 |
[32m[20221214 00:29:17 @agent_ppo2.py:185][0m |          -0.0133 |          45.6107 |         -60.4345 |
[32m[20221214 00:29:17 @agent_ppo2.py:185][0m |          -0.0238 |          42.0599 |         -60.1425 |
[32m[20221214 00:29:17 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:29:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 428.04
[32m[20221214 00:29:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 502.57
[32m[20221214 00:29:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 798.73
[32m[20221214 00:29:17 @agent_ppo2.py:143][0m Total time:      31.77 min
[32m[20221214 00:29:17 @agent_ppo2.py:145][0m 2891776 total steps have happened
[32m[20221214 00:29:17 @agent_ppo2.py:121][0m #------------------------ Iteration 5412 --------------------------#
[32m[20221214 00:29:17 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:29:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:29:17 @agent_ppo2.py:185][0m |           0.0012 |          68.5421 |         -60.5754 |
[32m[20221214 00:29:17 @agent_ppo2.py:185][0m |           0.0096 |          64.4831 |         -60.0997 |
[32m[20221214 00:29:17 @agent_ppo2.py:185][0m |          -0.0120 |          53.2242 |         -60.0482 |
[32m[20221214 00:29:17 @agent_ppo2.py:185][0m |          -0.0165 |          51.0120 |         -59.9152 |
[32m[20221214 00:29:18 @agent_ppo2.py:185][0m |          -0.0148 |          49.2884 |         -59.7064 |
[32m[20221214 00:29:18 @agent_ppo2.py:185][0m |          -0.0138 |          48.2449 |         -59.6810 |
[32m[20221214 00:29:18 @agent_ppo2.py:185][0m |          -0.0166 |          47.2539 |         -59.9435 |
[32m[20221214 00:29:18 @agent_ppo2.py:185][0m |          -0.0218 |          46.1909 |         -59.7130 |
[32m[20221214 00:29:18 @agent_ppo2.py:185][0m |          -0.0191 |          45.6998 |         -59.5781 |
[32m[20221214 00:29:18 @agent_ppo2.py:185][0m |          -0.0210 |          44.8877 |         -59.7994 |
[32m[20221214 00:29:18 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:29:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 624.62
[32m[20221214 00:29:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 735.05
[32m[20221214 00:29:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 624.86
[32m[20221214 00:29:18 @agent_ppo2.py:143][0m Total time:      31.80 min
[32m[20221214 00:29:18 @agent_ppo2.py:145][0m 2893824 total steps have happened
[32m[20221214 00:29:18 @agent_ppo2.py:121][0m #------------------------ Iteration 5413 --------------------------#
[32m[20221214 00:29:18 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:29:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:29:19 @agent_ppo2.py:185][0m |           0.0004 |          89.8065 |         -61.4502 |
[32m[20221214 00:29:19 @agent_ppo2.py:185][0m |           0.0005 |          78.9927 |         -61.3802 |
[32m[20221214 00:29:19 @agent_ppo2.py:185][0m |          -0.0066 |          76.1928 |         -61.3539 |
[32m[20221214 00:29:19 @agent_ppo2.py:185][0m |          -0.0119 |          74.6847 |         -61.1031 |
[32m[20221214 00:29:19 @agent_ppo2.py:185][0m |          -0.0017 |          73.7788 |         -61.2232 |
[32m[20221214 00:29:19 @agent_ppo2.py:185][0m |          -0.0107 |          73.0466 |         -61.3733 |
[32m[20221214 00:29:19 @agent_ppo2.py:185][0m |          -0.0069 |          71.8914 |         -61.2756 |
[32m[20221214 00:29:19 @agent_ppo2.py:185][0m |          -0.0138 |          71.6471 |         -61.4856 |
[32m[20221214 00:29:19 @agent_ppo2.py:185][0m |           0.0037 |          79.5458 |         -60.9343 |
[32m[20221214 00:29:19 @agent_ppo2.py:185][0m |          -0.0066 |          71.3835 |         -61.3982 |
[32m[20221214 00:29:19 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:29:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 482.71
[32m[20221214 00:29:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 606.66
[32m[20221214 00:29:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 571.56
[32m[20221214 00:29:20 @agent_ppo2.py:143][0m Total time:      31.82 min
[32m[20221214 00:29:20 @agent_ppo2.py:145][0m 2895872 total steps have happened
[32m[20221214 00:29:20 @agent_ppo2.py:121][0m #------------------------ Iteration 5414 --------------------------#
[32m[20221214 00:29:20 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:29:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:29:20 @agent_ppo2.py:185][0m |           0.0036 |         167.7013 |         -59.3704 |
[32m[20221214 00:29:20 @agent_ppo2.py:185][0m |           0.0036 |         160.8579 |         -59.2698 |
[32m[20221214 00:29:20 @agent_ppo2.py:185][0m |          -0.0001 |         159.7599 |         -59.6033 |
[32m[20221214 00:29:20 @agent_ppo2.py:185][0m |          -0.0032 |         159.5671 |         -59.2510 |
[32m[20221214 00:29:20 @agent_ppo2.py:185][0m |          -0.0044 |         158.8193 |         -59.3397 |
[32m[20221214 00:29:20 @agent_ppo2.py:185][0m |          -0.0041 |         157.8198 |         -59.3600 |
[32m[20221214 00:29:21 @agent_ppo2.py:185][0m |           0.0082 |         171.2119 |         -59.2173 |
[32m[20221214 00:29:21 @agent_ppo2.py:185][0m |          -0.0040 |         157.1698 |         -59.3911 |
[32m[20221214 00:29:21 @agent_ppo2.py:185][0m |          -0.0031 |         156.3755 |         -59.5219 |
[32m[20221214 00:29:21 @agent_ppo2.py:185][0m |          -0.0013 |         156.4186 |         -58.7662 |
[32m[20221214 00:29:21 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:29:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 715.54
[32m[20221214 00:29:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 832.48
[32m[20221214 00:29:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 539.13
[32m[20221214 00:29:21 @agent_ppo2.py:143][0m Total time:      31.84 min
[32m[20221214 00:29:21 @agent_ppo2.py:145][0m 2897920 total steps have happened
[32m[20221214 00:29:21 @agent_ppo2.py:121][0m #------------------------ Iteration 5415 --------------------------#
[32m[20221214 00:29:21 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:29:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:29:21 @agent_ppo2.py:185][0m |          -0.0005 |          74.1564 |         -58.3296 |
[32m[20221214 00:29:21 @agent_ppo2.py:185][0m |          -0.0110 |          66.8776 |         -58.0452 |
[32m[20221214 00:29:21 @agent_ppo2.py:185][0m |          -0.0107 |          64.7011 |         -58.0962 |
[32m[20221214 00:29:22 @agent_ppo2.py:185][0m |          -0.0108 |          63.5206 |         -58.2330 |
[32m[20221214 00:29:22 @agent_ppo2.py:185][0m |          -0.0192 |          62.6052 |         -58.0969 |
[32m[20221214 00:29:22 @agent_ppo2.py:185][0m |          -0.0171 |          61.6372 |         -58.1329 |
[32m[20221214 00:29:22 @agent_ppo2.py:185][0m |          -0.0159 |          61.0119 |         -58.4933 |
[32m[20221214 00:29:22 @agent_ppo2.py:185][0m |          -0.0184 |          60.5054 |         -58.4408 |
[32m[20221214 00:29:22 @agent_ppo2.py:185][0m |          -0.0199 |          60.0703 |         -58.1153 |
[32m[20221214 00:29:22 @agent_ppo2.py:185][0m |          -0.0156 |          60.0119 |         -58.4933 |
[32m[20221214 00:29:22 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:29:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 488.90
[32m[20221214 00:29:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 563.65
[32m[20221214 00:29:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 890.94
[32m[20221214 00:29:22 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 890.94
[32m[20221214 00:29:22 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 890.94
[32m[20221214 00:29:22 @agent_ppo2.py:143][0m Total time:      31.87 min
[32m[20221214 00:29:22 @agent_ppo2.py:145][0m 2899968 total steps have happened
[32m[20221214 00:29:22 @agent_ppo2.py:121][0m #------------------------ Iteration 5416 --------------------------#
[32m[20221214 00:29:23 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:29:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:29:23 @agent_ppo2.py:185][0m |          -0.0023 |          58.1466 |         -57.8893 |
[32m[20221214 00:29:23 @agent_ppo2.py:185][0m |           0.0030 |          49.7185 |         -57.9011 |
[32m[20221214 00:29:23 @agent_ppo2.py:185][0m |          -0.0064 |          46.6384 |         -57.8001 |
[32m[20221214 00:29:23 @agent_ppo2.py:185][0m |          -0.0029 |          48.2698 |         -57.9087 |
[32m[20221214 00:29:23 @agent_ppo2.py:185][0m |          -0.0006 |          47.4677 |         -58.1032 |
[32m[20221214 00:29:23 @agent_ppo2.py:185][0m |          -0.0114 |          42.9052 |         -58.0536 |
[32m[20221214 00:29:23 @agent_ppo2.py:185][0m |          -0.0047 |          43.4846 |         -58.3361 |
[32m[20221214 00:29:23 @agent_ppo2.py:185][0m |          -0.0168 |          41.5206 |         -58.0329 |
[32m[20221214 00:29:23 @agent_ppo2.py:185][0m |          -0.0151 |          41.0199 |         -58.2042 |
[32m[20221214 00:29:24 @agent_ppo2.py:185][0m |          -0.0163 |          40.2836 |         -58.1296 |
[32m[20221214 00:29:24 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:29:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 485.48
[32m[20221214 00:29:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 549.28
[32m[20221214 00:29:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 551.82
[32m[20221214 00:29:24 @agent_ppo2.py:143][0m Total time:      31.89 min
[32m[20221214 00:29:24 @agent_ppo2.py:145][0m 2902016 total steps have happened
[32m[20221214 00:29:24 @agent_ppo2.py:121][0m #------------------------ Iteration 5417 --------------------------#
[32m[20221214 00:29:24 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:29:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:29:24 @agent_ppo2.py:185][0m |           0.0035 |          80.5035 |         -61.7860 |
[32m[20221214 00:29:24 @agent_ppo2.py:185][0m |          -0.0087 |          69.0853 |         -61.1209 |
[32m[20221214 00:29:24 @agent_ppo2.py:185][0m |          -0.0077 |          68.1918 |         -61.3612 |
[32m[20221214 00:29:24 @agent_ppo2.py:185][0m |          -0.0153 |          64.7211 |         -60.9716 |
[32m[20221214 00:29:24 @agent_ppo2.py:185][0m |          -0.0202 |          62.7199 |         -60.7726 |
[32m[20221214 00:29:25 @agent_ppo2.py:185][0m |          -0.0180 |          61.5829 |         -60.9505 |
[32m[20221214 00:29:25 @agent_ppo2.py:185][0m |          -0.0192 |          61.0984 |         -60.3497 |
[32m[20221214 00:29:25 @agent_ppo2.py:185][0m |          -0.0148 |          60.7137 |         -60.2957 |
[32m[20221214 00:29:25 @agent_ppo2.py:185][0m |          -0.0206 |          59.8353 |         -60.4920 |
[32m[20221214 00:29:25 @agent_ppo2.py:185][0m |          -0.0208 |          58.9950 |         -60.4370 |
[32m[20221214 00:29:25 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:29:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 399.92
[32m[20221214 00:29:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 584.27
[32m[20221214 00:29:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 545.77
[32m[20221214 00:29:25 @agent_ppo2.py:143][0m Total time:      31.91 min
[32m[20221214 00:29:25 @agent_ppo2.py:145][0m 2904064 total steps have happened
[32m[20221214 00:29:25 @agent_ppo2.py:121][0m #------------------------ Iteration 5418 --------------------------#
[32m[20221214 00:29:25 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:29:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:29:25 @agent_ppo2.py:185][0m |           0.0017 |          80.1076 |         -59.2672 |
[32m[20221214 00:29:26 @agent_ppo2.py:185][0m |          -0.0092 |          72.4806 |         -59.4389 |
[32m[20221214 00:29:26 @agent_ppo2.py:185][0m |          -0.0026 |          75.1862 |         -59.3685 |
[32m[20221214 00:29:26 @agent_ppo2.py:185][0m |          -0.0038 |          69.8538 |         -58.9929 |
[32m[20221214 00:29:26 @agent_ppo2.py:185][0m |          -0.0107 |          67.8159 |         -59.1104 |
[32m[20221214 00:29:26 @agent_ppo2.py:185][0m |          -0.0078 |          68.3487 |         -59.1165 |
[32m[20221214 00:29:26 @agent_ppo2.py:185][0m |          -0.0107 |          70.7244 |         -59.0008 |
[32m[20221214 00:29:26 @agent_ppo2.py:185][0m |          -0.0171 |          65.2462 |         -58.5651 |
[32m[20221214 00:29:26 @agent_ppo2.py:185][0m |          -0.0189 |          64.5095 |         -59.0023 |
[32m[20221214 00:29:26 @agent_ppo2.py:185][0m |          -0.0184 |          63.6310 |         -58.7722 |
[32m[20221214 00:29:26 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:29:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 503.70
[32m[20221214 00:29:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 582.03
[32m[20221214 00:29:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 590.55
[32m[20221214 00:29:26 @agent_ppo2.py:143][0m Total time:      31.93 min
[32m[20221214 00:29:26 @agent_ppo2.py:145][0m 2906112 total steps have happened
[32m[20221214 00:29:26 @agent_ppo2.py:121][0m #------------------------ Iteration 5419 --------------------------#
[32m[20221214 00:29:27 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:29:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:29:27 @agent_ppo2.py:185][0m |           0.0027 |          98.5092 |         -56.1455 |
[32m[20221214 00:29:27 @agent_ppo2.py:185][0m |          -0.0005 |          87.9573 |         -56.7034 |
[32m[20221214 00:29:27 @agent_ppo2.py:185][0m |          -0.0016 |          83.6276 |         -56.5236 |
[32m[20221214 00:29:27 @agent_ppo2.py:185][0m |          -0.0126 |          82.0792 |         -56.6000 |
[32m[20221214 00:29:27 @agent_ppo2.py:185][0m |          -0.0110 |          80.3821 |         -57.0777 |
[32m[20221214 00:29:27 @agent_ppo2.py:185][0m |          -0.0136 |          79.4071 |         -56.9780 |
[32m[20221214 00:29:27 @agent_ppo2.py:185][0m |          -0.0068 |          78.2056 |         -57.1901 |
[32m[20221214 00:29:28 @agent_ppo2.py:185][0m |          -0.0158 |          77.7434 |         -57.0275 |
[32m[20221214 00:29:28 @agent_ppo2.py:185][0m |          -0.0162 |          77.8886 |         -57.2028 |
[32m[20221214 00:29:28 @agent_ppo2.py:185][0m |          -0.0185 |          76.3882 |         -57.4725 |
[32m[20221214 00:29:28 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:29:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 467.54
[32m[20221214 00:29:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 579.01
[32m[20221214 00:29:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 601.19
[32m[20221214 00:29:28 @agent_ppo2.py:143][0m Total time:      31.96 min
[32m[20221214 00:29:28 @agent_ppo2.py:145][0m 2908160 total steps have happened
[32m[20221214 00:29:28 @agent_ppo2.py:121][0m #------------------------ Iteration 5420 --------------------------#
[32m[20221214 00:29:28 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:29:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:29:28 @agent_ppo2.py:185][0m |          -0.0028 |          72.4634 |         -60.9945 |
[32m[20221214 00:29:28 @agent_ppo2.py:185][0m |          -0.0047 |          62.5755 |         -60.6869 |
[32m[20221214 00:29:28 @agent_ppo2.py:185][0m |          -0.0161 |          58.7781 |         -60.6944 |
[32m[20221214 00:29:29 @agent_ppo2.py:185][0m |          -0.0109 |          56.5058 |         -60.4873 |
[32m[20221214 00:29:29 @agent_ppo2.py:185][0m |          -0.0178 |          54.8156 |         -60.4102 |
[32m[20221214 00:29:29 @agent_ppo2.py:185][0m |          -0.0185 |          53.8298 |         -60.5997 |
[32m[20221214 00:29:29 @agent_ppo2.py:185][0m |          -0.0218 |          52.7925 |         -60.6850 |
[32m[20221214 00:29:29 @agent_ppo2.py:185][0m |          -0.0098 |          52.5847 |         -60.4710 |
[32m[20221214 00:29:29 @agent_ppo2.py:185][0m |          -0.0236 |          52.7013 |         -60.3412 |
[32m[20221214 00:29:29 @agent_ppo2.py:185][0m |          -0.0208 |          51.3635 |         -60.5707 |
[32m[20221214 00:29:29 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:29:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 435.09
[32m[20221214 00:29:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 544.40
[32m[20221214 00:29:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 821.93
[32m[20221214 00:29:29 @agent_ppo2.py:143][0m Total time:      31.98 min
[32m[20221214 00:29:29 @agent_ppo2.py:145][0m 2910208 total steps have happened
[32m[20221214 00:29:29 @agent_ppo2.py:121][0m #------------------------ Iteration 5421 --------------------------#
[32m[20221214 00:29:29 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:29:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:29:30 @agent_ppo2.py:185][0m |          -0.0010 |          73.7683 |         -59.8625 |
[32m[20221214 00:29:30 @agent_ppo2.py:185][0m |          -0.0049 |          62.7385 |         -60.3901 |
[32m[20221214 00:29:30 @agent_ppo2.py:185][0m |           0.0050 |          61.3208 |         -60.9865 |
[32m[20221214 00:29:30 @agent_ppo2.py:185][0m |           0.0033 |          60.6694 |         -60.3716 |
[32m[20221214 00:29:30 @agent_ppo2.py:185][0m |          -0.0107 |          58.2989 |         -60.8757 |
[32m[20221214 00:29:30 @agent_ppo2.py:185][0m |          -0.0175 |          58.1188 |         -60.7495 |
[32m[20221214 00:29:30 @agent_ppo2.py:185][0m |          -0.0135 |          56.8449 |         -60.8957 |
[32m[20221214 00:29:30 @agent_ppo2.py:185][0m |          -0.0138 |          56.2048 |         -61.1701 |
[32m[20221214 00:29:30 @agent_ppo2.py:185][0m |          -0.0140 |          56.3008 |         -61.1317 |
[32m[20221214 00:29:31 @agent_ppo2.py:185][0m |          -0.0127 |          55.6190 |         -61.2348 |
[32m[20221214 00:29:31 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:29:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 478.92
[32m[20221214 00:29:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 553.98
[32m[20221214 00:29:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 787.51
[32m[20221214 00:29:31 @agent_ppo2.py:143][0m Total time:      32.01 min
[32m[20221214 00:29:31 @agent_ppo2.py:145][0m 2912256 total steps have happened
[32m[20221214 00:29:31 @agent_ppo2.py:121][0m #------------------------ Iteration 5422 --------------------------#
[32m[20221214 00:29:31 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:29:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:29:31 @agent_ppo2.py:185][0m |           0.0032 |          76.0739 |         -61.4558 |
[32m[20221214 00:29:31 @agent_ppo2.py:185][0m |          -0.0002 |          69.6323 |         -61.7886 |
[32m[20221214 00:29:31 @agent_ppo2.py:185][0m |          -0.0105 |          65.8634 |         -61.4198 |
[32m[20221214 00:29:31 @agent_ppo2.py:185][0m |          -0.0077 |          63.9742 |         -61.2976 |
[32m[20221214 00:29:31 @agent_ppo2.py:185][0m |          -0.0103 |          63.2933 |         -61.2470 |
[32m[20221214 00:29:32 @agent_ppo2.py:185][0m |          -0.0104 |          62.4881 |         -61.4535 |
[32m[20221214 00:29:32 @agent_ppo2.py:185][0m |          -0.0177 |          61.1708 |         -61.4121 |
[32m[20221214 00:29:32 @agent_ppo2.py:185][0m |          -0.0167 |          61.0857 |         -61.3715 |
[32m[20221214 00:29:32 @agent_ppo2.py:185][0m |          -0.0124 |          60.2890 |         -61.3991 |
[32m[20221214 00:29:32 @agent_ppo2.py:185][0m |          -0.0152 |          60.3438 |         -61.5724 |
[32m[20221214 00:29:32 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:29:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 553.47
[32m[20221214 00:29:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 629.46
[32m[20221214 00:29:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 547.09
[32m[20221214 00:29:32 @agent_ppo2.py:143][0m Total time:      32.03 min
[32m[20221214 00:29:32 @agent_ppo2.py:145][0m 2914304 total steps have happened
[32m[20221214 00:29:32 @agent_ppo2.py:121][0m #------------------------ Iteration 5423 --------------------------#
[32m[20221214 00:29:32 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:29:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:29:32 @agent_ppo2.py:185][0m |          -0.0034 |          81.8114 |         -64.0135 |
[32m[20221214 00:29:33 @agent_ppo2.py:185][0m |          -0.0064 |          71.9968 |         -63.8099 |
[32m[20221214 00:29:33 @agent_ppo2.py:185][0m |          -0.0062 |          70.4191 |         -64.1028 |
[32m[20221214 00:29:33 @agent_ppo2.py:185][0m |          -0.0093 |          68.8128 |         -64.3172 |
[32m[20221214 00:29:33 @agent_ppo2.py:185][0m |          -0.0159 |          67.8738 |         -64.2152 |
[32m[20221214 00:29:33 @agent_ppo2.py:185][0m |          -0.0149 |          67.1844 |         -64.1643 |
[32m[20221214 00:29:33 @agent_ppo2.py:185][0m |          -0.0110 |          66.6001 |         -64.2202 |
[32m[20221214 00:29:33 @agent_ppo2.py:185][0m |          -0.0077 |          72.5978 |         -64.1334 |
[32m[20221214 00:29:33 @agent_ppo2.py:185][0m |          -0.0185 |          65.9755 |         -64.3582 |
[32m[20221214 00:29:33 @agent_ppo2.py:185][0m |          -0.0196 |          65.6028 |         -64.7322 |
[32m[20221214 00:29:33 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:29:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 554.86
[32m[20221214 00:29:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 598.90
[32m[20221214 00:29:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 786.90
[32m[20221214 00:29:33 @agent_ppo2.py:143][0m Total time:      32.05 min
[32m[20221214 00:29:33 @agent_ppo2.py:145][0m 2916352 total steps have happened
[32m[20221214 00:29:33 @agent_ppo2.py:121][0m #------------------------ Iteration 5424 --------------------------#
[32m[20221214 00:29:34 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:29:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:29:34 @agent_ppo2.py:185][0m |           0.0004 |          70.5512 |         -62.6787 |
[32m[20221214 00:29:34 @agent_ppo2.py:185][0m |          -0.0081 |          61.5233 |         -62.4567 |
[32m[20221214 00:29:34 @agent_ppo2.py:185][0m |          -0.0147 |          58.9828 |         -62.4449 |
[32m[20221214 00:29:34 @agent_ppo2.py:185][0m |          -0.0114 |          57.5657 |         -62.3915 |
[32m[20221214 00:29:34 @agent_ppo2.py:185][0m |          -0.0059 |          59.7881 |         -62.3918 |
[32m[20221214 00:29:34 @agent_ppo2.py:185][0m |          -0.0145 |          55.2079 |         -62.0078 |
[32m[20221214 00:29:34 @agent_ppo2.py:185][0m |          -0.0128 |          54.4002 |         -61.7349 |
[32m[20221214 00:29:34 @agent_ppo2.py:185][0m |          -0.0192 |          53.4748 |         -61.9861 |
[32m[20221214 00:29:35 @agent_ppo2.py:185][0m |          -0.0187 |          52.7944 |         -61.7488 |
[32m[20221214 00:29:35 @agent_ppo2.py:185][0m |          -0.0214 |          52.4760 |         -61.7748 |
[32m[20221214 00:29:35 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:29:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 574.55
[32m[20221214 00:29:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 597.16
[32m[20221214 00:29:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 552.38
[32m[20221214 00:29:35 @agent_ppo2.py:143][0m Total time:      32.07 min
[32m[20221214 00:29:35 @agent_ppo2.py:145][0m 2918400 total steps have happened
[32m[20221214 00:29:35 @agent_ppo2.py:121][0m #------------------------ Iteration 5425 --------------------------#
[32m[20221214 00:29:35 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:29:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:29:35 @agent_ppo2.py:185][0m |          -0.0001 |          73.9724 |         -59.3224 |
[32m[20221214 00:29:35 @agent_ppo2.py:185][0m |          -0.0102 |          66.1980 |         -59.2250 |
[32m[20221214 00:29:35 @agent_ppo2.py:185][0m |          -0.0087 |          64.1773 |         -59.0747 |
[32m[20221214 00:29:35 @agent_ppo2.py:185][0m |          -0.0170 |          63.0182 |         -58.7844 |
[32m[20221214 00:29:36 @agent_ppo2.py:185][0m |          -0.0159 |          63.0181 |         -58.5954 |
[32m[20221214 00:29:36 @agent_ppo2.py:185][0m |          -0.0173 |          61.2116 |         -58.5499 |
[32m[20221214 00:29:36 @agent_ppo2.py:185][0m |          -0.0159 |          60.9546 |         -58.5205 |
[32m[20221214 00:29:36 @agent_ppo2.py:185][0m |          -0.0191 |          60.4102 |         -58.2411 |
[32m[20221214 00:29:36 @agent_ppo2.py:185][0m |          -0.0211 |          60.4861 |         -58.1316 |
[32m[20221214 00:29:36 @agent_ppo2.py:185][0m |          -0.0247 |          59.8998 |         -58.2187 |
[32m[20221214 00:29:36 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:29:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 474.36
[32m[20221214 00:29:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 514.05
[32m[20221214 00:29:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 522.15
[32m[20221214 00:29:36 @agent_ppo2.py:143][0m Total time:      32.10 min
[32m[20221214 00:29:36 @agent_ppo2.py:145][0m 2920448 total steps have happened
[32m[20221214 00:29:36 @agent_ppo2.py:121][0m #------------------------ Iteration 5426 --------------------------#
[32m[20221214 00:29:36 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:29:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:29:37 @agent_ppo2.py:185][0m |           0.0003 |          74.1563 |         -61.8451 |
[32m[20221214 00:29:37 @agent_ppo2.py:185][0m |           0.0077 |          69.8422 |         -62.5846 |
[32m[20221214 00:29:37 @agent_ppo2.py:185][0m |          -0.0052 |          61.5483 |         -61.7654 |
[32m[20221214 00:29:37 @agent_ppo2.py:185][0m |          -0.0032 |          57.9433 |         -62.1433 |
[32m[20221214 00:29:37 @agent_ppo2.py:185][0m |          -0.0091 |          57.2432 |         -62.1656 |
[32m[20221214 00:29:37 @agent_ppo2.py:185][0m |          -0.0153 |          55.7043 |         -62.0489 |
[32m[20221214 00:29:37 @agent_ppo2.py:185][0m |          -0.0135 |          55.0393 |         -62.3398 |
[32m[20221214 00:29:37 @agent_ppo2.py:185][0m |          -0.0167 |          53.8942 |         -62.1328 |
[32m[20221214 00:29:37 @agent_ppo2.py:185][0m |          -0.0150 |          53.2977 |         -62.2773 |
[32m[20221214 00:29:37 @agent_ppo2.py:185][0m |          -0.0127 |          52.5608 |         -62.4956 |
[32m[20221214 00:29:37 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:29:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 487.23
[32m[20221214 00:29:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 516.37
[32m[20221214 00:29:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 587.88
[32m[20221214 00:29:38 @agent_ppo2.py:143][0m Total time:      32.12 min
[32m[20221214 00:29:38 @agent_ppo2.py:145][0m 2922496 total steps have happened
[32m[20221214 00:29:38 @agent_ppo2.py:121][0m #------------------------ Iteration 5427 --------------------------#
[32m[20221214 00:29:38 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:29:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:29:38 @agent_ppo2.py:185][0m |           0.0005 |          85.6237 |         -61.9629 |
[32m[20221214 00:29:38 @agent_ppo2.py:185][0m |          -0.0065 |          79.1518 |         -61.9150 |
[32m[20221214 00:29:38 @agent_ppo2.py:185][0m |          -0.0090 |          76.9103 |         -61.6200 |
[32m[20221214 00:29:38 @agent_ppo2.py:185][0m |          -0.0121 |          74.8596 |         -61.6066 |
[32m[20221214 00:29:38 @agent_ppo2.py:185][0m |          -0.0131 |          73.6858 |         -61.3639 |
[32m[20221214 00:29:38 @agent_ppo2.py:185][0m |          -0.0171 |          72.8211 |         -61.9685 |
[32m[20221214 00:29:39 @agent_ppo2.py:185][0m |          -0.0142 |          71.4182 |         -61.7688 |
[32m[20221214 00:29:39 @agent_ppo2.py:185][0m |          -0.0165 |          70.9707 |         -61.5655 |
[32m[20221214 00:29:39 @agent_ppo2.py:185][0m |          -0.0127 |          71.6325 |         -61.7296 |
[32m[20221214 00:29:39 @agent_ppo2.py:185][0m |          -0.0208 |          70.5654 |         -61.6462 |
[32m[20221214 00:29:39 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:29:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 490.73
[32m[20221214 00:29:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 576.77
[32m[20221214 00:29:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 656.44
[32m[20221214 00:29:39 @agent_ppo2.py:143][0m Total time:      32.14 min
[32m[20221214 00:29:39 @agent_ppo2.py:145][0m 2924544 total steps have happened
[32m[20221214 00:29:39 @agent_ppo2.py:121][0m #------------------------ Iteration 5428 --------------------------#
[32m[20221214 00:29:39 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:29:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:29:39 @agent_ppo2.py:185][0m |           0.0053 |          88.4875 |         -58.6815 |
[32m[20221214 00:29:39 @agent_ppo2.py:185][0m |           0.0036 |          80.3693 |         -58.7377 |
[32m[20221214 00:29:39 @agent_ppo2.py:185][0m |          -0.0032 |          76.9471 |         -58.5930 |
[32m[20221214 00:29:40 @agent_ppo2.py:185][0m |          -0.0101 |          75.2833 |         -58.7914 |
[32m[20221214 00:29:40 @agent_ppo2.py:185][0m |          -0.0115 |          74.6433 |         -58.4766 |
[32m[20221214 00:29:40 @agent_ppo2.py:185][0m |          -0.0147 |          73.5591 |         -58.4928 |
[32m[20221214 00:29:40 @agent_ppo2.py:185][0m |          -0.0112 |          72.5500 |         -58.6815 |
[32m[20221214 00:29:40 @agent_ppo2.py:185][0m |          -0.0100 |          72.3853 |         -58.5913 |
[32m[20221214 00:29:40 @agent_ppo2.py:185][0m |          -0.0161 |          71.6704 |         -59.1361 |
[32m[20221214 00:29:40 @agent_ppo2.py:185][0m |          -0.0157 |          71.2291 |         -59.0361 |
[32m[20221214 00:29:40 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:29:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 520.74
[32m[20221214 00:29:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 599.20
[32m[20221214 00:29:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 598.76
[32m[20221214 00:29:40 @agent_ppo2.py:143][0m Total time:      32.17 min
[32m[20221214 00:29:40 @agent_ppo2.py:145][0m 2926592 total steps have happened
[32m[20221214 00:29:40 @agent_ppo2.py:121][0m #------------------------ Iteration 5429 --------------------------#
[32m[20221214 00:29:40 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:29:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:29:41 @agent_ppo2.py:185][0m |           0.0029 |          87.3257 |         -60.6917 |
[32m[20221214 00:29:41 @agent_ppo2.py:185][0m |          -0.0084 |          75.1264 |         -60.5042 |
[32m[20221214 00:29:41 @agent_ppo2.py:185][0m |          -0.0098 |          71.5382 |         -60.8622 |
[32m[20221214 00:29:41 @agent_ppo2.py:185][0m |          -0.0105 |          70.4535 |         -60.1345 |
[32m[20221214 00:29:41 @agent_ppo2.py:185][0m |          -0.0166 |          68.1702 |         -60.1178 |
[32m[20221214 00:29:41 @agent_ppo2.py:185][0m |          -0.0147 |          66.9532 |         -60.1368 |
[32m[20221214 00:29:41 @agent_ppo2.py:185][0m |          -0.0108 |          66.1750 |         -59.8435 |
[32m[20221214 00:29:41 @agent_ppo2.py:185][0m |          -0.0185 |          64.6464 |         -59.8623 |
[32m[20221214 00:29:41 @agent_ppo2.py:185][0m |          -0.0131 |          63.7834 |         -59.8869 |
[32m[20221214 00:29:41 @agent_ppo2.py:185][0m |          -0.0169 |          64.0609 |         -59.6894 |
[32m[20221214 00:29:41 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:29:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 552.54
[32m[20221214 00:29:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 713.14
[32m[20221214 00:29:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 657.38
[32m[20221214 00:29:42 @agent_ppo2.py:143][0m Total time:      32.19 min
[32m[20221214 00:29:42 @agent_ppo2.py:145][0m 2928640 total steps have happened
[32m[20221214 00:29:42 @agent_ppo2.py:121][0m #------------------------ Iteration 5430 --------------------------#
[32m[20221214 00:29:42 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:29:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:29:42 @agent_ppo2.py:185][0m |           0.0015 |          98.7517 |         -61.8767 |
[32m[20221214 00:29:42 @agent_ppo2.py:185][0m |          -0.0040 |          92.4311 |         -61.7820 |
[32m[20221214 00:29:42 @agent_ppo2.py:185][0m |          -0.0109 |          89.7367 |         -61.5305 |
[32m[20221214 00:29:42 @agent_ppo2.py:185][0m |           0.0026 |          95.3401 |         -60.9070 |
[32m[20221214 00:29:42 @agent_ppo2.py:185][0m |          -0.0101 |          86.6509 |         -61.2753 |
[32m[20221214 00:29:42 @agent_ppo2.py:185][0m |          -0.0106 |          83.9667 |         -61.3730 |
[32m[20221214 00:29:43 @agent_ppo2.py:185][0m |          -0.0110 |          84.1263 |         -61.0038 |
[32m[20221214 00:29:43 @agent_ppo2.py:185][0m |          -0.0162 |          83.5005 |         -60.9274 |
[32m[20221214 00:29:43 @agent_ppo2.py:185][0m |          -0.0145 |          81.8522 |         -60.8256 |
[32m[20221214 00:29:43 @agent_ppo2.py:185][0m |          -0.0185 |          82.0699 |         -60.9925 |
[32m[20221214 00:29:43 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:29:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 628.54
[32m[20221214 00:29:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 746.41
[32m[20221214 00:29:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 541.57
[32m[20221214 00:29:43 @agent_ppo2.py:143][0m Total time:      32.21 min
[32m[20221214 00:29:43 @agent_ppo2.py:145][0m 2930688 total steps have happened
[32m[20221214 00:29:43 @agent_ppo2.py:121][0m #------------------------ Iteration 5431 --------------------------#
[32m[20221214 00:29:43 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:29:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:29:43 @agent_ppo2.py:185][0m |          -0.0008 |         128.5025 |         -64.1322 |
[32m[20221214 00:29:43 @agent_ppo2.py:185][0m |           0.0003 |         125.1752 |         -64.4929 |
[32m[20221214 00:29:43 @agent_ppo2.py:185][0m |           0.0002 |         123.7050 |         -64.4908 |
[32m[20221214 00:29:44 @agent_ppo2.py:185][0m |          -0.0035 |         123.8873 |         -64.3787 |
[32m[20221214 00:29:44 @agent_ppo2.py:185][0m |           0.0123 |         131.6078 |         -64.3894 |
[32m[20221214 00:29:44 @agent_ppo2.py:185][0m |          -0.0106 |         122.5123 |         -64.7184 |
[32m[20221214 00:29:44 @agent_ppo2.py:185][0m |          -0.0080 |         122.6084 |         -64.4849 |
[32m[20221214 00:29:44 @agent_ppo2.py:185][0m |          -0.0068 |         122.1833 |         -64.7009 |
[32m[20221214 00:29:44 @agent_ppo2.py:185][0m |           0.0024 |         125.2981 |         -64.9024 |
[32m[20221214 00:29:44 @agent_ppo2.py:185][0m |          -0.0037 |         121.2504 |         -64.1298 |
[32m[20221214 00:29:44 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:29:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 744.71
[32m[20221214 00:29:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 826.01
[32m[20221214 00:29:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 606.59
[32m[20221214 00:29:44 @agent_ppo2.py:143][0m Total time:      32.23 min
[32m[20221214 00:29:44 @agent_ppo2.py:145][0m 2932736 total steps have happened
[32m[20221214 00:29:44 @agent_ppo2.py:121][0m #------------------------ Iteration 5432 --------------------------#
[32m[20221214 00:29:44 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:29:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:29:45 @agent_ppo2.py:185][0m |          -0.0002 |          69.2851 |         -63.1951 |
[32m[20221214 00:29:45 @agent_ppo2.py:185][0m |           0.0006 |          69.2441 |         -62.9304 |
[32m[20221214 00:29:45 @agent_ppo2.py:185][0m |          -0.0179 |          58.0294 |         -63.2401 |
[32m[20221214 00:29:45 @agent_ppo2.py:185][0m |          -0.0150 |          56.6390 |         -62.8095 |
[32m[20221214 00:29:45 @agent_ppo2.py:185][0m |          -0.0162 |          55.6456 |         -63.1289 |
[32m[20221214 00:29:45 @agent_ppo2.py:185][0m |          -0.0177 |          55.2876 |         -63.0054 |
[32m[20221214 00:29:45 @agent_ppo2.py:185][0m |          -0.0189 |          54.5757 |         -62.8246 |
[32m[20221214 00:29:45 @agent_ppo2.py:185][0m |          -0.0255 |          54.1316 |         -62.9574 |
[32m[20221214 00:29:45 @agent_ppo2.py:185][0m |          -0.0266 |          53.8885 |         -63.0702 |
[32m[20221214 00:29:45 @agent_ppo2.py:185][0m |          -0.0275 |          53.4470 |         -62.8213 |
[32m[20221214 00:29:45 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:29:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 505.38
[32m[20221214 00:29:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 608.43
[32m[20221214 00:29:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 771.07
[32m[20221214 00:29:46 @agent_ppo2.py:143][0m Total time:      32.25 min
[32m[20221214 00:29:46 @agent_ppo2.py:145][0m 2934784 total steps have happened
[32m[20221214 00:29:46 @agent_ppo2.py:121][0m #------------------------ Iteration 5433 --------------------------#
[32m[20221214 00:29:46 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:29:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:29:46 @agent_ppo2.py:185][0m |           0.0056 |          66.1841 |         -64.9359 |
[32m[20221214 00:29:46 @agent_ppo2.py:185][0m |          -0.0061 |          56.4016 |         -64.5435 |
[32m[20221214 00:29:46 @agent_ppo2.py:185][0m |          -0.0098 |          53.1970 |         -64.6174 |
[32m[20221214 00:29:46 @agent_ppo2.py:185][0m |          -0.0102 |          51.5613 |         -64.5422 |
[32m[20221214 00:29:46 @agent_ppo2.py:185][0m |          -0.0145 |          49.9604 |         -64.6482 |
[32m[20221214 00:29:46 @agent_ppo2.py:185][0m |          -0.0151 |          48.5719 |         -64.9649 |
[32m[20221214 00:29:47 @agent_ppo2.py:185][0m |          -0.0115 |          47.4865 |         -65.2961 |
[32m[20221214 00:29:47 @agent_ppo2.py:185][0m |          -0.0147 |          46.5989 |         -64.9448 |
[32m[20221214 00:29:47 @agent_ppo2.py:185][0m |          -0.0163 |          45.7501 |         -65.1240 |
[32m[20221214 00:29:47 @agent_ppo2.py:185][0m |          -0.0121 |          44.9541 |         -65.0608 |
[32m[20221214 00:29:47 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:29:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 466.21
[32m[20221214 00:29:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 537.82
[32m[20221214 00:29:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 700.86
[32m[20221214 00:29:47 @agent_ppo2.py:143][0m Total time:      32.28 min
[32m[20221214 00:29:47 @agent_ppo2.py:145][0m 2936832 total steps have happened
[32m[20221214 00:29:47 @agent_ppo2.py:121][0m #------------------------ Iteration 5434 --------------------------#
[32m[20221214 00:29:47 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:29:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:29:47 @agent_ppo2.py:185][0m |           0.0016 |          84.5184 |         -65.6667 |
[32m[20221214 00:29:48 @agent_ppo2.py:185][0m |           0.0018 |          77.1764 |         -65.6031 |
[32m[20221214 00:29:48 @agent_ppo2.py:185][0m |          -0.0057 |          73.0962 |         -65.3290 |
[32m[20221214 00:29:48 @agent_ppo2.py:185][0m |          -0.0099 |          72.7140 |         -65.1179 |
[32m[20221214 00:29:48 @agent_ppo2.py:185][0m |          -0.0082 |          71.4580 |         -65.4886 |
[32m[20221214 00:29:48 @agent_ppo2.py:185][0m |          -0.0121 |          70.7271 |         -65.4696 |
[32m[20221214 00:29:48 @agent_ppo2.py:185][0m |          -0.0134 |          69.8675 |         -65.3312 |
[32m[20221214 00:29:48 @agent_ppo2.py:185][0m |          -0.0186 |          69.5206 |         -65.3221 |
[32m[20221214 00:29:48 @agent_ppo2.py:185][0m |          -0.0101 |          72.6863 |         -65.4026 |
[32m[20221214 00:29:48 @agent_ppo2.py:185][0m |          -0.0189 |          68.8580 |         -65.4010 |
[32m[20221214 00:29:48 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:29:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 501.95
[32m[20221214 00:29:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 575.31
[32m[20221214 00:29:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 757.63
[32m[20221214 00:29:48 @agent_ppo2.py:143][0m Total time:      32.30 min
[32m[20221214 00:29:48 @agent_ppo2.py:145][0m 2938880 total steps have happened
[32m[20221214 00:29:48 @agent_ppo2.py:121][0m #------------------------ Iteration 5435 --------------------------#
[32m[20221214 00:29:49 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:29:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:29:49 @agent_ppo2.py:185][0m |          -0.0027 |          77.9528 |         -64.4905 |
[32m[20221214 00:29:49 @agent_ppo2.py:185][0m |          -0.0031 |          69.7923 |         -64.6080 |
[32m[20221214 00:29:49 @agent_ppo2.py:185][0m |          -0.0101 |          65.1438 |         -64.4759 |
[32m[20221214 00:29:49 @agent_ppo2.py:185][0m |          -0.0178 |          61.6889 |         -64.3330 |
[32m[20221214 00:29:49 @agent_ppo2.py:185][0m |          -0.0176 |          60.2993 |         -64.4762 |
[32m[20221214 00:29:49 @agent_ppo2.py:185][0m |          -0.0211 |          58.8026 |         -64.4920 |
[32m[20221214 00:29:49 @agent_ppo2.py:185][0m |          -0.0209 |          58.2164 |         -64.4166 |
[32m[20221214 00:29:49 @agent_ppo2.py:185][0m |          -0.0242 |          57.3523 |         -64.2406 |
[32m[20221214 00:29:50 @agent_ppo2.py:185][0m |          -0.0250 |          56.1446 |         -64.2461 |
[32m[20221214 00:29:50 @agent_ppo2.py:185][0m |          -0.0246 |          55.6494 |         -64.3575 |
[32m[20221214 00:29:50 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:29:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 428.01
[32m[20221214 00:29:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 518.31
[32m[20221214 00:29:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 559.42
[32m[20221214 00:29:50 @agent_ppo2.py:143][0m Total time:      32.32 min
[32m[20221214 00:29:50 @agent_ppo2.py:145][0m 2940928 total steps have happened
[32m[20221214 00:29:50 @agent_ppo2.py:121][0m #------------------------ Iteration 5436 --------------------------#
[32m[20221214 00:29:50 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:29:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:29:50 @agent_ppo2.py:185][0m |           0.0021 |         147.0574 |         -66.5042 |
[32m[20221214 00:29:50 @agent_ppo2.py:185][0m |          -0.0047 |         143.6657 |         -66.4439 |
[32m[20221214 00:29:50 @agent_ppo2.py:185][0m |          -0.0042 |         144.7467 |         -66.5594 |
[32m[20221214 00:29:50 @agent_ppo2.py:185][0m |           0.0047 |         147.8097 |         -66.6768 |
[32m[20221214 00:29:51 @agent_ppo2.py:185][0m |           0.0092 |         161.1214 |         -66.0852 |
[32m[20221214 00:29:51 @agent_ppo2.py:185][0m |           0.0101 |         162.4123 |         -66.1145 |
[32m[20221214 00:29:51 @agent_ppo2.py:185][0m |          -0.0114 |         142.8610 |         -66.6478 |
[32m[20221214 00:29:51 @agent_ppo2.py:185][0m |          -0.0081 |         142.9459 |         -66.1144 |
[32m[20221214 00:29:51 @agent_ppo2.py:185][0m |          -0.0106 |         140.8338 |         -66.7974 |
[32m[20221214 00:29:51 @agent_ppo2.py:185][0m |          -0.0120 |         140.6081 |         -66.1370 |
[32m[20221214 00:29:51 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:29:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 760.39
[32m[20221214 00:29:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 832.69
[32m[20221214 00:29:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 608.10
[32m[20221214 00:29:51 @agent_ppo2.py:143][0m Total time:      32.35 min
[32m[20221214 00:29:51 @agent_ppo2.py:145][0m 2942976 total steps have happened
[32m[20221214 00:29:51 @agent_ppo2.py:121][0m #------------------------ Iteration 5437 --------------------------#
[32m[20221214 00:29:51 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:29:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:29:51 @agent_ppo2.py:185][0m |           0.0046 |          91.7631 |         -66.7332 |
[32m[20221214 00:29:52 @agent_ppo2.py:185][0m |          -0.0045 |          82.3929 |         -67.1142 |
[32m[20221214 00:29:52 @agent_ppo2.py:185][0m |          -0.0078 |          79.2416 |         -67.0725 |
[32m[20221214 00:29:52 @agent_ppo2.py:185][0m |          -0.0114 |          76.3959 |         -67.4739 |
[32m[20221214 00:29:52 @agent_ppo2.py:185][0m |          -0.0101 |          75.2573 |         -67.3898 |
[32m[20221214 00:29:52 @agent_ppo2.py:185][0m |          -0.0132 |          74.5187 |         -67.2237 |
[32m[20221214 00:29:52 @agent_ppo2.py:185][0m |          -0.0177 |          73.0649 |         -67.6463 |
[32m[20221214 00:29:52 @agent_ppo2.py:185][0m |          -0.0137 |          73.1842 |         -67.8758 |
[32m[20221214 00:29:52 @agent_ppo2.py:185][0m |          -0.0153 |          71.8592 |         -67.7278 |
[32m[20221214 00:29:52 @agent_ppo2.py:185][0m |          -0.0131 |          72.8161 |         -67.6221 |
[32m[20221214 00:29:52 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:29:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 465.18
[32m[20221214 00:29:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 610.98
[32m[20221214 00:29:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 567.73
[32m[20221214 00:29:52 @agent_ppo2.py:143][0m Total time:      32.37 min
[32m[20221214 00:29:52 @agent_ppo2.py:145][0m 2945024 total steps have happened
[32m[20221214 00:29:52 @agent_ppo2.py:121][0m #------------------------ Iteration 5438 --------------------------#
[32m[20221214 00:29:53 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:29:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:29:53 @agent_ppo2.py:185][0m |           0.0004 |          91.7119 |         -67.7608 |
[32m[20221214 00:29:53 @agent_ppo2.py:185][0m |           0.0039 |          80.7155 |         -67.4960 |
[32m[20221214 00:29:53 @agent_ppo2.py:185][0m |          -0.0034 |          77.1945 |         -67.7481 |
[32m[20221214 00:29:53 @agent_ppo2.py:185][0m |          -0.0118 |          74.5808 |         -67.9712 |
[32m[20221214 00:29:53 @agent_ppo2.py:185][0m |          -0.0073 |          73.2114 |         -67.3510 |
[32m[20221214 00:29:53 @agent_ppo2.py:185][0m |          -0.0117 |          72.0609 |         -67.9161 |
[32m[20221214 00:29:53 @agent_ppo2.py:185][0m |          -0.0115 |          72.4114 |         -68.0289 |
[32m[20221214 00:29:54 @agent_ppo2.py:185][0m |          -0.0113 |          70.7966 |         -67.9135 |
[32m[20221214 00:29:54 @agent_ppo2.py:185][0m |          -0.0133 |          70.2143 |         -68.1617 |
[32m[20221214 00:29:54 @agent_ppo2.py:185][0m |          -0.0119 |          68.8826 |         -68.0746 |
[32m[20221214 00:29:54 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:29:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 642.95
[32m[20221214 00:29:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 786.47
[32m[20221214 00:29:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 720.37
[32m[20221214 00:29:54 @agent_ppo2.py:143][0m Total time:      32.39 min
[32m[20221214 00:29:54 @agent_ppo2.py:145][0m 2947072 total steps have happened
[32m[20221214 00:29:54 @agent_ppo2.py:121][0m #------------------------ Iteration 5439 --------------------------#
[32m[20221214 00:29:54 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:29:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:29:54 @agent_ppo2.py:185][0m |           0.0044 |         106.5866 |         -71.4717 |
[32m[20221214 00:29:54 @agent_ppo2.py:185][0m |          -0.0088 |          92.5555 |         -71.0283 |
[32m[20221214 00:29:54 @agent_ppo2.py:185][0m |          -0.0044 |          94.9271 |         -70.8155 |
[32m[20221214 00:29:55 @agent_ppo2.py:185][0m |          -0.0096 |          87.0378 |         -70.8439 |
[32m[20221214 00:29:55 @agent_ppo2.py:185][0m |          -0.0115 |          85.5768 |         -70.7482 |
[32m[20221214 00:29:55 @agent_ppo2.py:185][0m |          -0.0165 |          85.0428 |         -70.5862 |
[32m[20221214 00:29:55 @agent_ppo2.py:185][0m |          -0.0162 |          83.2921 |         -70.6286 |
[32m[20221214 00:29:55 @agent_ppo2.py:185][0m |          -0.0177 |          82.9343 |         -70.3775 |
[32m[20221214 00:29:55 @agent_ppo2.py:185][0m |          -0.0124 |          84.8810 |         -70.1755 |
[32m[20221214 00:29:55 @agent_ppo2.py:185][0m |          -0.0195 |          81.3242 |         -70.1654 |
[32m[20221214 00:29:55 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:29:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 525.33
[32m[20221214 00:29:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 580.23
[32m[20221214 00:29:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 756.67
[32m[20221214 00:29:55 @agent_ppo2.py:143][0m Total time:      32.41 min
[32m[20221214 00:29:55 @agent_ppo2.py:145][0m 2949120 total steps have happened
[32m[20221214 00:29:55 @agent_ppo2.py:121][0m #------------------------ Iteration 5440 --------------------------#
[32m[20221214 00:29:55 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:29:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:29:56 @agent_ppo2.py:185][0m |           0.0036 |          93.0268 |         -67.7395 |
[32m[20221214 00:29:56 @agent_ppo2.py:185][0m |          -0.0062 |          85.6153 |         -67.7386 |
[32m[20221214 00:29:56 @agent_ppo2.py:185][0m |          -0.0097 |          83.5997 |         -67.4409 |
[32m[20221214 00:29:56 @agent_ppo2.py:185][0m |          -0.0111 |          82.4355 |         -67.1686 |
[32m[20221214 00:29:56 @agent_ppo2.py:185][0m |          -0.0089 |          81.5894 |         -67.1268 |
[32m[20221214 00:29:56 @agent_ppo2.py:185][0m |          -0.0139 |          80.7090 |         -67.2504 |
[32m[20221214 00:29:56 @agent_ppo2.py:185][0m |          -0.0114 |          80.2610 |         -66.8555 |
[32m[20221214 00:29:56 @agent_ppo2.py:185][0m |          -0.0170 |          79.8109 |         -66.6709 |
[32m[20221214 00:29:56 @agent_ppo2.py:185][0m |          -0.0163 |          79.1625 |         -67.2404 |
[32m[20221214 00:29:56 @agent_ppo2.py:185][0m |          -0.0173 |          79.3851 |         -66.7764 |
[32m[20221214 00:29:56 @agent_ppo2.py:130][0m Policy update time: 1.01 s
[32m[20221214 00:29:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 622.91
[32m[20221214 00:29:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 690.24
[32m[20221214 00:29:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 525.02
[32m[20221214 00:29:57 @agent_ppo2.py:143][0m Total time:      32.44 min
[32m[20221214 00:29:57 @agent_ppo2.py:145][0m 2951168 total steps have happened
[32m[20221214 00:29:57 @agent_ppo2.py:121][0m #------------------------ Iteration 5441 --------------------------#
[32m[20221214 00:29:57 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:29:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:29:57 @agent_ppo2.py:185][0m |           0.0074 |         132.4798 |         -68.3418 |
[32m[20221214 00:29:57 @agent_ppo2.py:185][0m |           0.0021 |         125.5419 |         -68.3636 |
[32m[20221214 00:29:57 @agent_ppo2.py:185][0m |          -0.0026 |         122.8569 |         -68.4292 |
[32m[20221214 00:29:57 @agent_ppo2.py:185][0m |          -0.0063 |         119.4937 |         -68.8425 |
[32m[20221214 00:29:57 @agent_ppo2.py:185][0m |           0.0018 |         121.0063 |         -69.0077 |
[32m[20221214 00:29:57 @agent_ppo2.py:185][0m |           0.0028 |         118.7266 |         -69.1196 |
[32m[20221214 00:29:58 @agent_ppo2.py:185][0m |          -0.0089 |         116.8386 |         -69.2130 |
[32m[20221214 00:29:58 @agent_ppo2.py:185][0m |          -0.0077 |         116.7746 |         -69.3802 |
[32m[20221214 00:29:58 @agent_ppo2.py:185][0m |          -0.0055 |         115.3902 |         -70.0242 |
[32m[20221214 00:29:58 @agent_ppo2.py:185][0m |          -0.0086 |         114.9628 |         -69.6928 |
[32m[20221214 00:29:58 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:29:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 636.31
[32m[20221214 00:29:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 823.78
[32m[20221214 00:29:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 568.35
[32m[20221214 00:29:58 @agent_ppo2.py:143][0m Total time:      32.46 min
[32m[20221214 00:29:58 @agent_ppo2.py:145][0m 2953216 total steps have happened
[32m[20221214 00:29:58 @agent_ppo2.py:121][0m #------------------------ Iteration 5442 --------------------------#
[32m[20221214 00:29:58 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:29:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:29:58 @agent_ppo2.py:185][0m |          -0.0002 |         169.9797 |         -76.4994 |
[32m[20221214 00:29:58 @agent_ppo2.py:185][0m |           0.0020 |         166.4207 |         -76.3088 |
[32m[20221214 00:29:58 @agent_ppo2.py:185][0m |          -0.0009 |         165.2808 |         -75.7868 |
[32m[20221214 00:29:59 @agent_ppo2.py:185][0m |           0.0026 |         164.7563 |         -76.0295 |
[32m[20221214 00:29:59 @agent_ppo2.py:185][0m |          -0.0013 |         163.4926 |         -76.5318 |
[32m[20221214 00:29:59 @agent_ppo2.py:185][0m |          -0.0017 |         162.9225 |         -76.2062 |
[32m[20221214 00:29:59 @agent_ppo2.py:185][0m |          -0.0011 |         163.1885 |         -76.1845 |
[32m[20221214 00:29:59 @agent_ppo2.py:185][0m |          -0.0017 |         161.9602 |         -76.0666 |
[32m[20221214 00:29:59 @agent_ppo2.py:185][0m |           0.0104 |         178.2484 |         -76.0813 |
[32m[20221214 00:29:59 @agent_ppo2.py:185][0m |           0.0008 |         163.0817 |         -76.6990 |
[32m[20221214 00:29:59 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:29:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 797.07
[32m[20221214 00:29:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 805.39
[32m[20221214 00:29:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 342.76
[32m[20221214 00:29:59 @agent_ppo2.py:143][0m Total time:      32.48 min
[32m[20221214 00:29:59 @agent_ppo2.py:145][0m 2955264 total steps have happened
[32m[20221214 00:29:59 @agent_ppo2.py:121][0m #------------------------ Iteration 5443 --------------------------#
[32m[20221214 00:29:59 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:30:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:30:00 @agent_ppo2.py:185][0m |           0.0025 |          99.4618 |         -71.7627 |
[32m[20221214 00:30:00 @agent_ppo2.py:185][0m |          -0.0038 |          90.5676 |         -71.6463 |
[32m[20221214 00:30:00 @agent_ppo2.py:185][0m |          -0.0085 |          87.7799 |         -71.3775 |
[32m[20221214 00:30:00 @agent_ppo2.py:185][0m |           0.0049 |          91.5814 |         -71.6172 |
[32m[20221214 00:30:00 @agent_ppo2.py:185][0m |          -0.0131 |          86.7283 |         -71.2417 |
[32m[20221214 00:30:00 @agent_ppo2.py:185][0m |          -0.0086 |          89.8023 |         -71.5159 |
[32m[20221214 00:30:00 @agent_ppo2.py:185][0m |          -0.0108 |          84.9789 |         -71.4625 |
[32m[20221214 00:30:00 @agent_ppo2.py:185][0m |          -0.0123 |          84.3706 |         -71.3865 |
[32m[20221214 00:30:00 @agent_ppo2.py:185][0m |          -0.0152 |          83.4790 |         -71.4985 |
[32m[20221214 00:30:00 @agent_ppo2.py:185][0m |          -0.0133 |          83.6316 |         -71.4600 |
[32m[20221214 00:30:00 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:30:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 564.01
[32m[20221214 00:30:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 637.13
[32m[20221214 00:30:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 561.50
[32m[20221214 00:30:01 @agent_ppo2.py:143][0m Total time:      32.50 min
[32m[20221214 00:30:01 @agent_ppo2.py:145][0m 2957312 total steps have happened
[32m[20221214 00:30:01 @agent_ppo2.py:121][0m #------------------------ Iteration 5444 --------------------------#
[32m[20221214 00:30:01 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:30:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:30:01 @agent_ppo2.py:185][0m |          -0.0017 |          93.1611 |         -69.2678 |
[32m[20221214 00:30:01 @agent_ppo2.py:185][0m |          -0.0020 |          88.6227 |         -69.4223 |
[32m[20221214 00:30:01 @agent_ppo2.py:185][0m |          -0.0053 |          85.1655 |         -69.4085 |
[32m[20221214 00:30:01 @agent_ppo2.py:185][0m |          -0.0128 |          83.8089 |         -68.9845 |
[32m[20221214 00:30:01 @agent_ppo2.py:185][0m |          -0.0091 |          83.1152 |         -68.9655 |
[32m[20221214 00:30:01 @agent_ppo2.py:185][0m |          -0.0115 |          82.3342 |         -68.9532 |
[32m[20221214 00:30:02 @agent_ppo2.py:185][0m |          -0.0168 |          81.7124 |         -68.6300 |
[32m[20221214 00:30:02 @agent_ppo2.py:185][0m |          -0.0154 |          81.3234 |         -68.6379 |
[32m[20221214 00:30:02 @agent_ppo2.py:185][0m |          -0.0149 |          80.8483 |         -68.4898 |
[32m[20221214 00:30:02 @agent_ppo2.py:185][0m |          -0.0172 |          80.3550 |         -68.5863 |
[32m[20221214 00:30:02 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:30:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 504.62
[32m[20221214 00:30:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 573.61
[32m[20221214 00:30:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 700.18
[32m[20221214 00:30:02 @agent_ppo2.py:143][0m Total time:      32.53 min
[32m[20221214 00:30:02 @agent_ppo2.py:145][0m 2959360 total steps have happened
[32m[20221214 00:30:02 @agent_ppo2.py:121][0m #------------------------ Iteration 5445 --------------------------#
[32m[20221214 00:30:02 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:30:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:30:02 @agent_ppo2.py:185][0m |          -0.0012 |          97.8797 |         -69.0948 |
[32m[20221214 00:30:02 @agent_ppo2.py:185][0m |           0.0035 |          92.2129 |         -69.4574 |
[32m[20221214 00:30:03 @agent_ppo2.py:185][0m |          -0.0057 |          87.5782 |         -69.2543 |
[32m[20221214 00:30:03 @agent_ppo2.py:185][0m |          -0.0067 |          85.4265 |         -69.5920 |
[32m[20221214 00:30:03 @agent_ppo2.py:185][0m |          -0.0043 |          88.6610 |         -69.2685 |
[32m[20221214 00:30:03 @agent_ppo2.py:185][0m |          -0.0088 |          81.1319 |         -69.5944 |
[32m[20221214 00:30:03 @agent_ppo2.py:185][0m |          -0.0080 |          79.9711 |         -69.4677 |
[32m[20221214 00:30:03 @agent_ppo2.py:185][0m |          -0.0090 |          78.4105 |         -69.7957 |
[32m[20221214 00:30:03 @agent_ppo2.py:185][0m |          -0.0125 |          78.1414 |         -69.3895 |
[32m[20221214 00:30:03 @agent_ppo2.py:185][0m |          -0.0054 |          78.7775 |         -69.5755 |
[32m[20221214 00:30:03 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:30:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 641.05
[32m[20221214 00:30:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 663.42
[32m[20221214 00:30:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 662.06
[32m[20221214 00:30:03 @agent_ppo2.py:143][0m Total time:      32.55 min
[32m[20221214 00:30:03 @agent_ppo2.py:145][0m 2961408 total steps have happened
[32m[20221214 00:30:03 @agent_ppo2.py:121][0m #------------------------ Iteration 5446 --------------------------#
[32m[20221214 00:30:04 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:30:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:30:04 @agent_ppo2.py:185][0m |           0.0049 |         101.7979 |         -72.5869 |
[32m[20221214 00:30:04 @agent_ppo2.py:185][0m |          -0.0039 |          81.4440 |         -72.1794 |
[32m[20221214 00:30:04 @agent_ppo2.py:185][0m |          -0.0093 |          76.5870 |         -72.1833 |
[32m[20221214 00:30:04 @agent_ppo2.py:185][0m |          -0.0045 |          76.5933 |         -72.3588 |
[32m[20221214 00:30:04 @agent_ppo2.py:185][0m |          -0.0080 |          72.6377 |         -72.5118 |
[32m[20221214 00:30:04 @agent_ppo2.py:185][0m |          -0.0104 |          70.8343 |         -72.1166 |
[32m[20221214 00:30:04 @agent_ppo2.py:185][0m |          -0.0101 |          68.6145 |         -71.9657 |
[32m[20221214 00:30:04 @agent_ppo2.py:185][0m |          -0.0147 |          67.0870 |         -72.3261 |
[32m[20221214 00:30:05 @agent_ppo2.py:185][0m |          -0.0200 |          66.4825 |         -72.1577 |
[32m[20221214 00:30:05 @agent_ppo2.py:185][0m |          -0.0157 |          65.8763 |         -72.1350 |
[32m[20221214 00:30:05 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:30:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 511.59
[32m[20221214 00:30:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 576.44
[32m[20221214 00:30:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 799.67
[32m[20221214 00:30:05 @agent_ppo2.py:143][0m Total time:      32.57 min
[32m[20221214 00:30:05 @agent_ppo2.py:145][0m 2963456 total steps have happened
[32m[20221214 00:30:05 @agent_ppo2.py:121][0m #------------------------ Iteration 5447 --------------------------#
[32m[20221214 00:30:05 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:30:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:30:05 @agent_ppo2.py:185][0m |           0.0076 |         175.3844 |         -72.6871 |
[32m[20221214 00:30:05 @agent_ppo2.py:185][0m |           0.0011 |         168.1027 |         -72.0057 |
[32m[20221214 00:30:05 @agent_ppo2.py:185][0m |          -0.0021 |         164.9553 |         -72.0429 |
[32m[20221214 00:30:05 @agent_ppo2.py:185][0m |          -0.0057 |         165.3908 |         -71.8497 |
[32m[20221214 00:30:06 @agent_ppo2.py:185][0m |          -0.0059 |         165.2597 |         -72.3549 |
[32m[20221214 00:30:06 @agent_ppo2.py:185][0m |           0.0024 |         170.1677 |         -72.1447 |
[32m[20221214 00:30:06 @agent_ppo2.py:185][0m |          -0.0012 |         164.5573 |         -71.6203 |
[32m[20221214 00:30:06 @agent_ppo2.py:185][0m |          -0.0097 |         163.2008 |         -71.9301 |
[32m[20221214 00:30:06 @agent_ppo2.py:185][0m |          -0.0083 |         161.1678 |         -72.0663 |
[32m[20221214 00:30:06 @agent_ppo2.py:185][0m |          -0.0004 |         165.6290 |         -71.4956 |
[32m[20221214 00:30:06 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:30:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 737.14
[32m[20221214 00:30:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 793.87
[32m[20221214 00:30:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 285.36
[32m[20221214 00:30:06 @agent_ppo2.py:143][0m Total time:      32.60 min
[32m[20221214 00:30:06 @agent_ppo2.py:145][0m 2965504 total steps have happened
[32m[20221214 00:30:06 @agent_ppo2.py:121][0m #------------------------ Iteration 5448 --------------------------#
[32m[20221214 00:30:06 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:30:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:30:07 @agent_ppo2.py:185][0m |          -0.0022 |         105.4508 |         -68.0905 |
[32m[20221214 00:30:07 @agent_ppo2.py:185][0m |          -0.0074 |          98.8869 |         -67.6884 |
[32m[20221214 00:30:07 @agent_ppo2.py:185][0m |          -0.0059 |          96.3838 |         -67.7301 |
[32m[20221214 00:30:07 @agent_ppo2.py:185][0m |          -0.0071 |          94.9433 |         -68.4720 |
[32m[20221214 00:30:07 @agent_ppo2.py:185][0m |          -0.0066 |          94.2329 |         -67.5923 |
[32m[20221214 00:30:07 @agent_ppo2.py:185][0m |          -0.0063 |          93.1966 |         -67.6004 |
[32m[20221214 00:30:07 @agent_ppo2.py:185][0m |          -0.0085 |          92.9605 |         -68.4235 |
[32m[20221214 00:30:07 @agent_ppo2.py:185][0m |          -0.0106 |          92.2613 |         -68.4279 |
[32m[20221214 00:30:07 @agent_ppo2.py:185][0m |          -0.0108 |          92.2148 |         -68.6134 |
[32m[20221214 00:30:07 @agent_ppo2.py:185][0m |          -0.0098 |          91.5064 |         -68.4125 |
[32m[20221214 00:30:07 @agent_ppo2.py:130][0m Policy update time: 1.02 s
[32m[20221214 00:30:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 640.21
[32m[20221214 00:30:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 694.56
[32m[20221214 00:30:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 420.49
[32m[20221214 00:30:08 @agent_ppo2.py:143][0m Total time:      32.62 min
[32m[20221214 00:30:08 @agent_ppo2.py:145][0m 2967552 total steps have happened
[32m[20221214 00:30:08 @agent_ppo2.py:121][0m #------------------------ Iteration 5449 --------------------------#
[32m[20221214 00:30:08 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:30:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:30:08 @agent_ppo2.py:185][0m |           0.0019 |         111.4789 |         -70.6306 |
[32m[20221214 00:30:08 @agent_ppo2.py:185][0m |          -0.0040 |         104.4631 |         -70.2416 |
[32m[20221214 00:30:08 @agent_ppo2.py:185][0m |          -0.0040 |         101.2171 |         -70.5447 |
[32m[20221214 00:30:08 @agent_ppo2.py:185][0m |          -0.0052 |         100.9331 |         -69.7787 |
[32m[20221214 00:30:08 @agent_ppo2.py:185][0m |          -0.0043 |          98.8157 |         -70.0563 |
[32m[20221214 00:30:08 @agent_ppo2.py:185][0m |          -0.0116 |          97.6557 |         -70.3751 |
[32m[20221214 00:30:08 @agent_ppo2.py:185][0m |          -0.0105 |          96.0308 |         -69.6262 |
[32m[20221214 00:30:09 @agent_ppo2.py:185][0m |          -0.0121 |          95.3306 |         -70.3705 |
[32m[20221214 00:30:09 @agent_ppo2.py:185][0m |          -0.0115 |          94.5031 |         -69.9618 |
[32m[20221214 00:30:09 @agent_ppo2.py:185][0m |          -0.0150 |          93.8875 |         -70.1995 |
[32m[20221214 00:30:09 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221214 00:30:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 647.32
[32m[20221214 00:30:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 699.05
[32m[20221214 00:30:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 620.54
[32m[20221214 00:30:09 @agent_ppo2.py:143][0m Total time:      32.64 min
[32m[20221214 00:30:09 @agent_ppo2.py:145][0m 2969600 total steps have happened
[32m[20221214 00:30:09 @agent_ppo2.py:121][0m #------------------------ Iteration 5450 --------------------------#
[32m[20221214 00:30:09 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221214 00:30:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:30:09 @agent_ppo2.py:185][0m |           0.0004 |          91.9541 |         -69.5224 |
[32m[20221214 00:30:10 @agent_ppo2.py:185][0m |          -0.0057 |          81.9275 |         -69.6512 |
[32m[20221214 00:30:10 @agent_ppo2.py:185][0m |          -0.0015 |          76.3908 |         -69.5443 |
[32m[20221214 00:30:10 @agent_ppo2.py:185][0m |          -0.0106 |          72.9157 |         -69.6748 |
[32m[20221214 00:30:10 @agent_ppo2.py:185][0m |          -0.0083 |          71.1434 |         -69.3717 |
[32m[20221214 00:30:10 @agent_ppo2.py:185][0m |          -0.0148 |          69.1457 |         -69.8121 |
[32m[20221214 00:30:10 @agent_ppo2.py:185][0m |          -0.0179 |          67.8850 |         -69.5694 |
[32m[20221214 00:30:10 @agent_ppo2.py:185][0m |          -0.0204 |          67.3118 |         -69.7478 |
[32m[20221214 00:30:10 @agent_ppo2.py:185][0m |          -0.0194 |          66.4005 |         -69.4249 |
[32m[20221214 00:30:10 @agent_ppo2.py:185][0m |          -0.0157 |          66.2500 |         -69.3750 |
[32m[20221214 00:30:10 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:30:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 496.45
[32m[20221214 00:30:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 607.51
[32m[20221214 00:30:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 487.42
[32m[20221214 00:30:10 @agent_ppo2.py:143][0m Total time:      32.67 min
[32m[20221214 00:30:10 @agent_ppo2.py:145][0m 2971648 total steps have happened
[32m[20221214 00:30:10 @agent_ppo2.py:121][0m #------------------------ Iteration 5451 --------------------------#
[32m[20221214 00:30:11 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:30:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:30:11 @agent_ppo2.py:185][0m |          -0.0017 |          92.2083 |         -69.5206 |
[32m[20221214 00:30:11 @agent_ppo2.py:185][0m |          -0.0046 |          84.6705 |         -68.7910 |
[32m[20221214 00:30:11 @agent_ppo2.py:185][0m |          -0.0081 |          81.8406 |         -68.7766 |
[32m[20221214 00:30:11 @agent_ppo2.py:185][0m |          -0.0146 |          79.8514 |         -68.6053 |
[32m[20221214 00:30:11 @agent_ppo2.py:185][0m |          -0.0152 |          78.6762 |         -68.6589 |
[32m[20221214 00:30:11 @agent_ppo2.py:185][0m |          -0.0152 |          77.7304 |         -68.8811 |
[32m[20221214 00:30:11 @agent_ppo2.py:185][0m |          -0.0197 |          77.7074 |         -68.7593 |
[32m[20221214 00:30:11 @agent_ppo2.py:185][0m |          -0.0157 |          76.7270 |         -68.7259 |
[32m[20221214 00:30:12 @agent_ppo2.py:185][0m |          -0.0174 |          75.5244 |         -68.6908 |
[32m[20221214 00:30:12 @agent_ppo2.py:185][0m |          -0.0143 |          77.4622 |         -68.8377 |
[32m[20221214 00:30:12 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:30:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 499.59
[32m[20221214 00:30:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 567.38
[32m[20221214 00:30:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 581.30
[32m[20221214 00:30:12 @agent_ppo2.py:143][0m Total time:      32.69 min
[32m[20221214 00:30:12 @agent_ppo2.py:145][0m 2973696 total steps have happened
[32m[20221214 00:30:12 @agent_ppo2.py:121][0m #------------------------ Iteration 5452 --------------------------#
[32m[20221214 00:30:12 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:30:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:30:12 @agent_ppo2.py:185][0m |           0.0018 |          91.9616 |         -71.5946 |
[32m[20221214 00:30:12 @agent_ppo2.py:185][0m |           0.0059 |          86.9478 |         -71.6680 |
[32m[20221214 00:30:12 @agent_ppo2.py:185][0m |          -0.0088 |          76.3473 |         -71.1131 |
[32m[20221214 00:30:12 @agent_ppo2.py:185][0m |          -0.0141 |          72.6155 |         -70.8260 |
[32m[20221214 00:30:13 @agent_ppo2.py:185][0m |          -0.0138 |          70.2718 |         -70.9022 |
[32m[20221214 00:30:13 @agent_ppo2.py:185][0m |          -0.0161 |          67.6046 |         -70.6087 |
[32m[20221214 00:30:13 @agent_ppo2.py:185][0m |          -0.0104 |          66.5595 |         -70.5575 |
[32m[20221214 00:30:13 @agent_ppo2.py:185][0m |          -0.0169 |          65.1227 |         -70.5943 |
[32m[20221214 00:30:13 @agent_ppo2.py:185][0m |          -0.0167 |          63.7639 |         -70.5406 |
[32m[20221214 00:30:13 @agent_ppo2.py:185][0m |          -0.0175 |          63.0523 |         -70.1367 |
[32m[20221214 00:30:13 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:30:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 627.97
[32m[20221214 00:30:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 749.90
[32m[20221214 00:30:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 561.78
[32m[20221214 00:30:13 @agent_ppo2.py:143][0m Total time:      32.71 min
[32m[20221214 00:30:13 @agent_ppo2.py:145][0m 2975744 total steps have happened
[32m[20221214 00:30:13 @agent_ppo2.py:121][0m #------------------------ Iteration 5453 --------------------------#
[32m[20221214 00:30:13 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:30:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:30:14 @agent_ppo2.py:185][0m |           0.0072 |         143.5484 |         -67.9972 |
[32m[20221214 00:30:14 @agent_ppo2.py:185][0m |           0.0032 |         141.7443 |         -68.1738 |
[32m[20221214 00:30:14 @agent_ppo2.py:185][0m |           0.0027 |         148.8438 |         -67.8877 |
[32m[20221214 00:30:14 @agent_ppo2.py:185][0m |           0.0005 |         140.8674 |         -67.6509 |
[32m[20221214 00:30:14 @agent_ppo2.py:185][0m |          -0.0024 |         141.0190 |         -67.6679 |
[32m[20221214 00:30:14 @agent_ppo2.py:185][0m |          -0.0031 |         140.7501 |         -67.4399 |
[32m[20221214 00:30:14 @agent_ppo2.py:185][0m |          -0.0007 |         142.0284 |         -67.8131 |
[32m[20221214 00:30:14 @agent_ppo2.py:185][0m |          -0.0053 |         141.0559 |         -67.5507 |
[32m[20221214 00:30:14 @agent_ppo2.py:185][0m |          -0.0035 |         140.8369 |         -67.2447 |
[32m[20221214 00:30:14 @agent_ppo2.py:185][0m |          -0.0041 |         139.0841 |         -67.5727 |
[32m[20221214 00:30:14 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:30:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 742.43
[32m[20221214 00:30:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 812.00
[32m[20221214 00:30:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 605.76
[32m[20221214 00:30:15 @agent_ppo2.py:143][0m Total time:      32.74 min
[32m[20221214 00:30:15 @agent_ppo2.py:145][0m 2977792 total steps have happened
[32m[20221214 00:30:15 @agent_ppo2.py:121][0m #------------------------ Iteration 5454 --------------------------#
[32m[20221214 00:30:15 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:30:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:30:15 @agent_ppo2.py:185][0m |          -0.0019 |         132.8061 |         -71.7262 |
[32m[20221214 00:30:15 @agent_ppo2.py:185][0m |           0.0004 |         127.8142 |         -71.3208 |
[32m[20221214 00:30:15 @agent_ppo2.py:185][0m |          -0.0053 |         124.8881 |         -71.0894 |
[32m[20221214 00:30:15 @agent_ppo2.py:185][0m |          -0.0098 |         123.7992 |         -71.0995 |
[32m[20221214 00:30:15 @agent_ppo2.py:185][0m |          -0.0079 |         123.2923 |         -71.0708 |
[32m[20221214 00:30:15 @agent_ppo2.py:185][0m |          -0.0076 |         122.5200 |         -71.3036 |
[32m[20221214 00:30:16 @agent_ppo2.py:185][0m |          -0.0086 |         121.4078 |         -71.5277 |
[32m[20221214 00:30:16 @agent_ppo2.py:185][0m |          -0.0112 |         122.3492 |         -71.4756 |
[32m[20221214 00:30:16 @agent_ppo2.py:185][0m |          -0.0100 |         121.9174 |         -71.3928 |
[32m[20221214 00:30:16 @agent_ppo2.py:185][0m |          -0.0085 |         121.7458 |         -71.3219 |
[32m[20221214 00:30:16 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:30:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 737.22
[32m[20221214 00:30:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 798.87
[32m[20221214 00:30:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 494.85
[32m[20221214 00:30:16 @agent_ppo2.py:143][0m Total time:      32.76 min
[32m[20221214 00:30:16 @agent_ppo2.py:145][0m 2979840 total steps have happened
[32m[20221214 00:30:16 @agent_ppo2.py:121][0m #------------------------ Iteration 5455 --------------------------#
[32m[20221214 00:30:16 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:30:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:30:16 @agent_ppo2.py:185][0m |           0.0013 |         101.2437 |         -68.8498 |
[32m[20221214 00:30:16 @agent_ppo2.py:185][0m |          -0.0117 |          86.1274 |         -68.3262 |
[32m[20221214 00:30:17 @agent_ppo2.py:185][0m |          -0.0111 |          82.6196 |         -68.1936 |
[32m[20221214 00:30:17 @agent_ppo2.py:185][0m |          -0.0128 |          80.6493 |         -68.0541 |
[32m[20221214 00:30:17 @agent_ppo2.py:185][0m |          -0.0122 |          76.4056 |         -68.0997 |
[32m[20221214 00:30:17 @agent_ppo2.py:185][0m |          -0.0151 |          75.2287 |         -67.9736 |
[32m[20221214 00:30:17 @agent_ppo2.py:185][0m |          -0.0172 |          74.0513 |         -68.5246 |
[32m[20221214 00:30:17 @agent_ppo2.py:185][0m |          -0.0234 |          72.8021 |         -68.5348 |
[32m[20221214 00:30:17 @agent_ppo2.py:185][0m |          -0.0208 |          71.8622 |         -68.3494 |
[32m[20221214 00:30:17 @agent_ppo2.py:185][0m |          -0.0226 |          70.8486 |         -68.3876 |
[32m[20221214 00:30:17 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:30:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 478.37
[32m[20221214 00:30:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 652.78
[32m[20221214 00:30:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 587.93
[32m[20221214 00:30:17 @agent_ppo2.py:143][0m Total time:      32.78 min
[32m[20221214 00:30:17 @agent_ppo2.py:145][0m 2981888 total steps have happened
[32m[20221214 00:30:17 @agent_ppo2.py:121][0m #------------------------ Iteration 5456 --------------------------#
[32m[20221214 00:30:18 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:30:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:30:18 @agent_ppo2.py:185][0m |           0.0033 |         164.3338 |         -67.9901 |
[32m[20221214 00:30:18 @agent_ppo2.py:185][0m |           0.0128 |         177.0409 |         -69.1770 |
[32m[20221214 00:30:18 @agent_ppo2.py:185][0m |           0.0077 |         181.4423 |         -68.9121 |
[32m[20221214 00:30:18 @agent_ppo2.py:185][0m |          -0.0026 |         160.6764 |         -69.0631 |
[32m[20221214 00:30:18 @agent_ppo2.py:185][0m |          -0.0048 |         159.5244 |         -69.3808 |
[32m[20221214 00:30:18 @agent_ppo2.py:185][0m |           0.0016 |         162.3195 |         -70.1464 |
[32m[20221214 00:30:18 @agent_ppo2.py:185][0m |          -0.0049 |         158.9533 |         -70.1009 |
[32m[20221214 00:30:18 @agent_ppo2.py:185][0m |          -0.0025 |         158.3371 |         -69.6281 |
[32m[20221214 00:30:18 @agent_ppo2.py:185][0m |          -0.0069 |         159.1834 |         -70.4294 |
[32m[20221214 00:30:19 @agent_ppo2.py:185][0m |          -0.0053 |         158.5374 |         -70.4817 |
[32m[20221214 00:30:19 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:30:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 708.52
[32m[20221214 00:30:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 835.19
[32m[20221214 00:30:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 596.48
[32m[20221214 00:30:19 @agent_ppo2.py:143][0m Total time:      32.81 min
[32m[20221214 00:30:19 @agent_ppo2.py:145][0m 2983936 total steps have happened
[32m[20221214 00:30:19 @agent_ppo2.py:121][0m #------------------------ Iteration 5457 --------------------------#
[32m[20221214 00:30:19 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:30:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:30:19 @agent_ppo2.py:185][0m |          -0.0029 |         143.2451 |         -66.5772 |
[32m[20221214 00:30:19 @agent_ppo2.py:185][0m |          -0.0020 |         141.7793 |         -66.8528 |
[32m[20221214 00:30:19 @agent_ppo2.py:185][0m |          -0.0026 |         141.8992 |         -66.2246 |
[32m[20221214 00:30:19 @agent_ppo2.py:185][0m |          -0.0022 |         141.1592 |         -66.7261 |
[32m[20221214 00:30:19 @agent_ppo2.py:185][0m |          -0.0033 |         141.3786 |         -66.6018 |
[32m[20221214 00:30:20 @agent_ppo2.py:185][0m |          -0.0020 |         141.3263 |         -66.3150 |
[32m[20221214 00:30:20 @agent_ppo2.py:185][0m |          -0.0073 |         141.1160 |         -65.8921 |
[32m[20221214 00:30:20 @agent_ppo2.py:185][0m |           0.0033 |         141.9088 |         -66.3708 |
[32m[20221214 00:30:20 @agent_ppo2.py:185][0m |          -0.0028 |         140.1059 |         -65.7426 |
[32m[20221214 00:30:20 @agent_ppo2.py:185][0m |          -0.0088 |         140.2830 |         -65.9621 |
[32m[20221214 00:30:20 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:30:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 722.63
[32m[20221214 00:30:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 794.27
[32m[20221214 00:30:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 578.47
[32m[20221214 00:30:20 @agent_ppo2.py:143][0m Total time:      32.83 min
[32m[20221214 00:30:20 @agent_ppo2.py:145][0m 2985984 total steps have happened
[32m[20221214 00:30:20 @agent_ppo2.py:121][0m #------------------------ Iteration 5458 --------------------------#
[32m[20221214 00:30:20 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:30:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:30:20 @agent_ppo2.py:185][0m |           0.0094 |          93.1769 |         -70.2826 |
[32m[20221214 00:30:21 @agent_ppo2.py:185][0m |          -0.0062 |          81.5005 |         -70.4600 |
[32m[20221214 00:30:21 @agent_ppo2.py:185][0m |          -0.0063 |          77.3524 |         -70.1061 |
[32m[20221214 00:30:21 @agent_ppo2.py:185][0m |          -0.0106 |          75.9304 |         -69.9874 |
[32m[20221214 00:30:21 @agent_ppo2.py:185][0m |          -0.0145 |          73.9052 |         -70.0731 |
[32m[20221214 00:30:21 @agent_ppo2.py:185][0m |          -0.0164 |          72.0540 |         -70.2516 |
[32m[20221214 00:30:21 @agent_ppo2.py:185][0m |          -0.0153 |          71.0532 |         -70.2812 |
[32m[20221214 00:30:21 @agent_ppo2.py:185][0m |          -0.0130 |          70.5710 |         -70.0538 |
[32m[20221214 00:30:21 @agent_ppo2.py:185][0m |          -0.0164 |          69.5488 |         -70.4995 |
[32m[20221214 00:30:21 @agent_ppo2.py:185][0m |          -0.0146 |          68.8267 |         -70.4763 |
[32m[20221214 00:30:21 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:30:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 472.85
[32m[20221214 00:30:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 613.35
[32m[20221214 00:30:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 581.71
[32m[20221214 00:30:21 @agent_ppo2.py:143][0m Total time:      32.85 min
[32m[20221214 00:30:21 @agent_ppo2.py:145][0m 2988032 total steps have happened
[32m[20221214 00:30:21 @agent_ppo2.py:121][0m #------------------------ Iteration 5459 --------------------------#
[32m[20221214 00:30:22 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:30:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:30:22 @agent_ppo2.py:185][0m |          -0.0021 |          91.8176 |         -69.9758 |
[32m[20221214 00:30:22 @agent_ppo2.py:185][0m |          -0.0025 |          81.4693 |         -69.3342 |
[32m[20221214 00:30:22 @agent_ppo2.py:185][0m |          -0.0052 |          77.2214 |         -69.6090 |
[32m[20221214 00:30:22 @agent_ppo2.py:185][0m |          -0.0015 |          75.8109 |         -69.6232 |
[32m[20221214 00:30:22 @agent_ppo2.py:185][0m |          -0.0100 |          74.0354 |         -69.3892 |
[32m[20221214 00:30:22 @agent_ppo2.py:185][0m |          -0.0095 |          73.2710 |         -69.7986 |
[32m[20221214 00:30:22 @agent_ppo2.py:185][0m |          -0.0118 |          72.6312 |         -69.4629 |
[32m[20221214 00:30:23 @agent_ppo2.py:185][0m |          -0.0051 |          74.5865 |         -69.6146 |
[32m[20221214 00:30:23 @agent_ppo2.py:185][0m |          -0.0143 |          72.1009 |         -69.7350 |
[32m[20221214 00:30:23 @agent_ppo2.py:185][0m |          -0.0036 |          72.3138 |         -69.7032 |
[32m[20221214 00:30:23 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:30:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 459.14
[32m[20221214 00:30:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 523.22
[32m[20221214 00:30:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 630.02
[32m[20221214 00:30:23 @agent_ppo2.py:143][0m Total time:      32.87 min
[32m[20221214 00:30:23 @agent_ppo2.py:145][0m 2990080 total steps have happened
[32m[20221214 00:30:23 @agent_ppo2.py:121][0m #------------------------ Iteration 5460 --------------------------#
[32m[20221214 00:30:23 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:30:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:30:23 @agent_ppo2.py:185][0m |           0.0003 |         106.9781 |         -67.0889 |
[32m[20221214 00:30:23 @agent_ppo2.py:185][0m |          -0.0075 |          95.0821 |         -67.5459 |
[32m[20221214 00:30:23 @agent_ppo2.py:185][0m |          -0.0114 |          92.9206 |         -67.5656 |
[32m[20221214 00:30:24 @agent_ppo2.py:185][0m |          -0.0173 |          91.0542 |         -67.3771 |
[32m[20221214 00:30:24 @agent_ppo2.py:185][0m |          -0.0152 |          90.3750 |         -67.2967 |
[32m[20221214 00:30:24 @agent_ppo2.py:185][0m |          -0.0124 |          89.6140 |         -67.5622 |
[32m[20221214 00:30:24 @agent_ppo2.py:185][0m |          -0.0205 |          87.3993 |         -67.4478 |
[32m[20221214 00:30:24 @agent_ppo2.py:185][0m |          -0.0200 |          86.5974 |         -67.3787 |
[32m[20221214 00:30:24 @agent_ppo2.py:185][0m |          -0.0211 |          86.3263 |         -67.5621 |
[32m[20221214 00:30:24 @agent_ppo2.py:185][0m |          -0.0239 |          85.5715 |         -67.0999 |
[32m[20221214 00:30:24 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:30:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 475.70
[32m[20221214 00:30:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 592.88
[32m[20221214 00:30:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 677.69
[32m[20221214 00:30:24 @agent_ppo2.py:143][0m Total time:      32.90 min
[32m[20221214 00:30:24 @agent_ppo2.py:145][0m 2992128 total steps have happened
[32m[20221214 00:30:24 @agent_ppo2.py:121][0m #------------------------ Iteration 5461 --------------------------#
[32m[20221214 00:30:24 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:30:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:30:25 @agent_ppo2.py:185][0m |           0.0146 |         141.9225 |         -70.5789 |
[32m[20221214 00:30:25 @agent_ppo2.py:185][0m |           0.0079 |         131.3100 |         -70.6938 |
[32m[20221214 00:30:25 @agent_ppo2.py:185][0m |           0.0068 |         130.6583 |         -70.5198 |
[32m[20221214 00:30:25 @agent_ppo2.py:185][0m |          -0.0062 |         123.6177 |         -70.3440 |
[32m[20221214 00:30:25 @agent_ppo2.py:185][0m |          -0.0041 |         119.9600 |         -70.5424 |
[32m[20221214 00:30:25 @agent_ppo2.py:185][0m |          -0.0087 |         119.0521 |         -70.5418 |
[32m[20221214 00:30:25 @agent_ppo2.py:185][0m |          -0.0100 |         118.3897 |         -70.8081 |
[32m[20221214 00:30:25 @agent_ppo2.py:185][0m |          -0.0081 |         117.1654 |         -70.5624 |
[32m[20221214 00:30:25 @agent_ppo2.py:185][0m |           0.0023 |         125.6768 |         -70.6337 |
[32m[20221214 00:30:25 @agent_ppo2.py:185][0m |          -0.0072 |         117.5923 |         -70.2072 |
[32m[20221214 00:30:25 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:30:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 609.76
[32m[20221214 00:30:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 668.30
[32m[20221214 00:30:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 807.24
[32m[20221214 00:30:26 @agent_ppo2.py:143][0m Total time:      32.92 min
[32m[20221214 00:30:26 @agent_ppo2.py:145][0m 2994176 total steps have happened
[32m[20221214 00:30:26 @agent_ppo2.py:121][0m #------------------------ Iteration 5462 --------------------------#
[32m[20221214 00:30:26 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:30:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:30:26 @agent_ppo2.py:185][0m |           0.0033 |          85.6384 |         -72.9596 |
[32m[20221214 00:30:26 @agent_ppo2.py:185][0m |          -0.0045 |          79.3022 |         -72.9416 |
[32m[20221214 00:30:26 @agent_ppo2.py:185][0m |          -0.0041 |          77.6385 |         -73.0425 |
[32m[20221214 00:30:26 @agent_ppo2.py:185][0m |          -0.0038 |          77.1192 |         -72.8249 |
[32m[20221214 00:30:26 @agent_ppo2.py:185][0m |          -0.0111 |          75.0979 |         -72.8643 |
[32m[20221214 00:30:26 @agent_ppo2.py:185][0m |          -0.0110 |          74.4596 |         -72.9892 |
[32m[20221214 00:30:27 @agent_ppo2.py:185][0m |          -0.0073 |          83.7541 |         -73.1616 |
[32m[20221214 00:30:27 @agent_ppo2.py:185][0m |          -0.0043 |          74.0450 |         -72.9812 |
[32m[20221214 00:30:27 @agent_ppo2.py:185][0m |          -0.0112 |          73.6467 |         -73.6805 |
[32m[20221214 00:30:27 @agent_ppo2.py:185][0m |          -0.0163 |          72.0794 |         -73.3947 |
[32m[20221214 00:30:27 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:30:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 449.29
[32m[20221214 00:30:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 568.69
[32m[20221214 00:30:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 813.30
[32m[20221214 00:30:27 @agent_ppo2.py:143][0m Total time:      32.94 min
[32m[20221214 00:30:27 @agent_ppo2.py:145][0m 2996224 total steps have happened
[32m[20221214 00:30:27 @agent_ppo2.py:121][0m #------------------------ Iteration 5463 --------------------------#
[32m[20221214 00:30:27 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:30:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:30:27 @agent_ppo2.py:185][0m |           0.0011 |         105.5242 |         -72.6305 |
[32m[20221214 00:30:27 @agent_ppo2.py:185][0m |          -0.0062 |          97.8672 |         -72.1160 |
[32m[20221214 00:30:28 @agent_ppo2.py:185][0m |          -0.0067 |          95.7716 |         -72.8958 |
[32m[20221214 00:30:28 @agent_ppo2.py:185][0m |          -0.0099 |          94.4541 |         -72.3054 |
[32m[20221214 00:30:28 @agent_ppo2.py:185][0m |          -0.0061 |          95.2619 |         -72.3427 |
[32m[20221214 00:30:28 @agent_ppo2.py:185][0m |          -0.0126 |          91.8511 |         -72.3126 |
[32m[20221214 00:30:28 @agent_ppo2.py:185][0m |          -0.0112 |          91.3062 |         -72.0609 |
[32m[20221214 00:30:28 @agent_ppo2.py:185][0m |          -0.0168 |          90.8354 |         -72.1755 |
[32m[20221214 00:30:28 @agent_ppo2.py:185][0m |          -0.0148 |          89.6719 |         -72.1566 |
[32m[20221214 00:30:28 @agent_ppo2.py:185][0m |          -0.0176 |          89.3907 |         -71.8279 |
[32m[20221214 00:30:28 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:30:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 530.96
[32m[20221214 00:30:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 688.31
[32m[20221214 00:30:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 652.11
[32m[20221214 00:30:28 @agent_ppo2.py:143][0m Total time:      32.97 min
[32m[20221214 00:30:28 @agent_ppo2.py:145][0m 2998272 total steps have happened
[32m[20221214 00:30:28 @agent_ppo2.py:121][0m #------------------------ Iteration 5464 --------------------------#
[32m[20221214 00:30:29 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:30:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:30:29 @agent_ppo2.py:185][0m |           0.0029 |          70.0591 |         -66.9542 |
[32m[20221214 00:30:29 @agent_ppo2.py:185][0m |          -0.0038 |          65.7003 |         -65.9228 |
[32m[20221214 00:30:29 @agent_ppo2.py:185][0m |          -0.0095 |          63.6234 |         -65.8441 |
[32m[20221214 00:30:29 @agent_ppo2.py:185][0m |          -0.0088 |          62.5153 |         -65.9779 |
[32m[20221214 00:30:29 @agent_ppo2.py:185][0m |          -0.0021 |          61.4787 |         -65.6655 |
[32m[20221214 00:30:29 @agent_ppo2.py:185][0m |          -0.0147 |          61.1608 |         -65.8035 |
[32m[20221214 00:30:29 @agent_ppo2.py:185][0m |          -0.0149 |          60.6858 |         -65.6637 |
[32m[20221214 00:30:29 @agent_ppo2.py:185][0m |          -0.0122 |          59.5380 |         -65.8463 |
[32m[20221214 00:30:30 @agent_ppo2.py:185][0m |          -0.0127 |          59.1209 |         -65.7056 |
[32m[20221214 00:30:30 @agent_ppo2.py:185][0m |          -0.0096 |          58.7852 |         -65.6383 |
[32m[20221214 00:30:30 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:30:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 524.83
[32m[20221214 00:30:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 648.73
[32m[20221214 00:30:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 703.05
[32m[20221214 00:30:30 @agent_ppo2.py:143][0m Total time:      32.99 min
[32m[20221214 00:30:30 @agent_ppo2.py:145][0m 3000320 total steps have happened
[32m[20221214 00:30:30 @agent_ppo2.py:121][0m #------------------------ Iteration 5465 --------------------------#
[32m[20221214 00:30:30 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:30:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:30:30 @agent_ppo2.py:185][0m |          -0.0033 |          79.0612 |         -70.5794 |
[32m[20221214 00:30:30 @agent_ppo2.py:185][0m |          -0.0051 |          74.6786 |         -70.6235 |
[32m[20221214 00:30:30 @agent_ppo2.py:185][0m |          -0.0117 |          70.5698 |         -70.3701 |
[32m[20221214 00:30:30 @agent_ppo2.py:185][0m |          -0.0083 |          70.1402 |         -69.7721 |
[32m[20221214 00:30:31 @agent_ppo2.py:185][0m |          -0.0133 |          68.5812 |         -70.1151 |
[32m[20221214 00:30:31 @agent_ppo2.py:185][0m |          -0.0130 |          67.6607 |         -70.3350 |
[32m[20221214 00:30:31 @agent_ppo2.py:185][0m |          -0.0141 |          67.0406 |         -70.3309 |
[32m[20221214 00:30:31 @agent_ppo2.py:185][0m |          -0.0129 |          65.3515 |         -70.5732 |
[32m[20221214 00:30:31 @agent_ppo2.py:185][0m |          -0.0040 |          73.4790 |         -70.1380 |
[32m[20221214 00:30:31 @agent_ppo2.py:185][0m |          -0.0139 |          65.5183 |         -70.2702 |
[32m[20221214 00:30:31 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:30:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 697.02
[32m[20221214 00:30:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 833.78
[32m[20221214 00:30:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 643.46
[32m[20221214 00:30:31 @agent_ppo2.py:143][0m Total time:      33.01 min
[32m[20221214 00:30:31 @agent_ppo2.py:145][0m 3002368 total steps have happened
[32m[20221214 00:30:31 @agent_ppo2.py:121][0m #------------------------ Iteration 5466 --------------------------#
[32m[20221214 00:30:31 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:30:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:30:32 @agent_ppo2.py:185][0m |           0.0081 |         109.9954 |         -70.3593 |
[32m[20221214 00:30:32 @agent_ppo2.py:185][0m |          -0.0014 |          96.6045 |         -69.7594 |
[32m[20221214 00:30:32 @agent_ppo2.py:185][0m |          -0.0036 |          94.4181 |         -69.3923 |
[32m[20221214 00:30:32 @agent_ppo2.py:185][0m |          -0.0082 |          93.8195 |         -69.7423 |
[32m[20221214 00:30:32 @agent_ppo2.py:185][0m |          -0.0100 |          91.8030 |         -69.8396 |
[32m[20221214 00:30:32 @agent_ppo2.py:185][0m |          -0.0083 |          90.8882 |         -69.8890 |
[32m[20221214 00:30:32 @agent_ppo2.py:185][0m |          -0.0116 |          90.7934 |         -69.9702 |
[32m[20221214 00:30:32 @agent_ppo2.py:185][0m |          -0.0129 |          89.5535 |         -69.4394 |
[32m[20221214 00:30:32 @agent_ppo2.py:185][0m |          -0.0110 |          89.6187 |         -69.1426 |
[32m[20221214 00:30:32 @agent_ppo2.py:185][0m |          -0.0170 |          88.8339 |         -69.1796 |
[32m[20221214 00:30:32 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:30:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 520.19
[32m[20221214 00:30:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 615.21
[32m[20221214 00:30:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 686.74
[32m[20221214 00:30:33 @agent_ppo2.py:143][0m Total time:      33.04 min
[32m[20221214 00:30:33 @agent_ppo2.py:145][0m 3004416 total steps have happened
[32m[20221214 00:30:33 @agent_ppo2.py:121][0m #------------------------ Iteration 5467 --------------------------#
[32m[20221214 00:30:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:30:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:30:33 @agent_ppo2.py:185][0m |           0.0136 |          85.4879 |         -67.4491 |
[32m[20221214 00:30:33 @agent_ppo2.py:185][0m |           0.0006 |          78.1754 |         -67.9280 |
[32m[20221214 00:30:33 @agent_ppo2.py:185][0m |          -0.0098 |          73.8948 |         -67.4560 |
[32m[20221214 00:30:33 @agent_ppo2.py:185][0m |           0.0023 |          81.1740 |         -67.8125 |
[32m[20221214 00:30:33 @agent_ppo2.py:185][0m |          -0.0123 |          71.2875 |         -67.2272 |
[32m[20221214 00:30:33 @agent_ppo2.py:185][0m |          -0.0082 |          70.0301 |         -67.4475 |
[32m[20221214 00:30:33 @agent_ppo2.py:185][0m |          -0.0140 |          69.4891 |         -67.5510 |
[32m[20221214 00:30:34 @agent_ppo2.py:185][0m |          -0.0112 |          69.1762 |         -67.3482 |
[32m[20221214 00:30:34 @agent_ppo2.py:185][0m |          -0.0124 |          68.4037 |         -67.5949 |
[32m[20221214 00:30:34 @agent_ppo2.py:185][0m |          -0.0151 |          68.6740 |         -67.4687 |
[32m[20221214 00:30:34 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:30:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 496.86
[32m[20221214 00:30:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 606.13
[32m[20221214 00:30:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 756.04
[32m[20221214 00:30:34 @agent_ppo2.py:143][0m Total time:      33.06 min
[32m[20221214 00:30:34 @agent_ppo2.py:145][0m 3006464 total steps have happened
[32m[20221214 00:30:34 @agent_ppo2.py:121][0m #------------------------ Iteration 5468 --------------------------#
[32m[20221214 00:30:34 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:30:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:30:34 @agent_ppo2.py:185][0m |          -0.0018 |         111.6739 |         -71.3623 |
[32m[20221214 00:30:34 @agent_ppo2.py:185][0m |          -0.0040 |         109.8977 |         -71.3535 |
[32m[20221214 00:30:34 @agent_ppo2.py:185][0m |          -0.0034 |         106.1821 |         -71.3198 |
[32m[20221214 00:30:35 @agent_ppo2.py:185][0m |          -0.0111 |         105.7798 |         -71.1104 |
[32m[20221214 00:30:35 @agent_ppo2.py:185][0m |           0.0038 |         112.9944 |         -70.8414 |
[32m[20221214 00:30:35 @agent_ppo2.py:185][0m |          -0.0090 |         105.4340 |         -71.0219 |
[32m[20221214 00:30:35 @agent_ppo2.py:185][0m |          -0.0070 |         104.3054 |         -71.2561 |
[32m[20221214 00:30:35 @agent_ppo2.py:185][0m |          -0.0131 |         105.3523 |         -70.9848 |
[32m[20221214 00:30:35 @agent_ppo2.py:185][0m |          -0.0118 |         103.7515 |         -70.9641 |
[32m[20221214 00:30:35 @agent_ppo2.py:185][0m |          -0.0102 |         103.8902 |         -70.8166 |
[32m[20221214 00:30:35 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:30:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 734.38
[32m[20221214 00:30:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 823.42
[32m[20221214 00:30:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 390.05
[32m[20221214 00:30:35 @agent_ppo2.py:143][0m Total time:      33.08 min
[32m[20221214 00:30:35 @agent_ppo2.py:145][0m 3008512 total steps have happened
[32m[20221214 00:30:35 @agent_ppo2.py:121][0m #------------------------ Iteration 5469 --------------------------#
[32m[20221214 00:30:35 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:30:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:30:36 @agent_ppo2.py:185][0m |           0.0014 |         142.1782 |         -70.6123 |
[32m[20221214 00:30:36 @agent_ppo2.py:185][0m |          -0.0033 |         135.7424 |         -70.1693 |
[32m[20221214 00:30:36 @agent_ppo2.py:185][0m |          -0.0091 |         131.5516 |         -70.0656 |
[32m[20221214 00:30:36 @agent_ppo2.py:185][0m |          -0.0033 |         132.1751 |         -70.1683 |
[32m[20221214 00:30:36 @agent_ppo2.py:185][0m |           0.0028 |         138.7155 |         -70.0618 |
[32m[20221214 00:30:36 @agent_ppo2.py:185][0m |          -0.0081 |         127.4323 |         -69.7838 |
[32m[20221214 00:30:36 @agent_ppo2.py:185][0m |          -0.0089 |         126.3463 |         -69.8480 |
[32m[20221214 00:30:36 @agent_ppo2.py:185][0m |          -0.0129 |         124.7542 |         -69.5666 |
[32m[20221214 00:30:36 @agent_ppo2.py:185][0m |          -0.0112 |         123.8712 |         -69.8558 |
[32m[20221214 00:30:37 @agent_ppo2.py:185][0m |          -0.0140 |         122.3379 |         -69.0705 |
[32m[20221214 00:30:37 @agent_ppo2.py:130][0m Policy update time: 1.03 s
[32m[20221214 00:30:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 703.47
[32m[20221214 00:30:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.61
[32m[20221214 00:30:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 592.95
[32m[20221214 00:30:37 @agent_ppo2.py:143][0m Total time:      33.10 min
[32m[20221214 00:30:37 @agent_ppo2.py:145][0m 3010560 total steps have happened
[32m[20221214 00:30:37 @agent_ppo2.py:121][0m #------------------------ Iteration 5470 --------------------------#
[32m[20221214 00:30:37 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:30:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:30:37 @agent_ppo2.py:185][0m |           0.0043 |          73.9549 |         -66.0645 |
[32m[20221214 00:30:37 @agent_ppo2.py:185][0m |          -0.0045 |          64.4779 |         -66.3964 |
[32m[20221214 00:30:37 @agent_ppo2.py:185][0m |          -0.0078 |          61.4880 |         -66.6796 |
[32m[20221214 00:30:37 @agent_ppo2.py:185][0m |          -0.0097 |          59.3921 |         -66.6291 |
[32m[20221214 00:30:37 @agent_ppo2.py:185][0m |          -0.0161 |          58.4389 |         -67.0776 |
[32m[20221214 00:30:38 @agent_ppo2.py:185][0m |          -0.0213 |          57.5093 |         -67.0479 |
[32m[20221214 00:30:38 @agent_ppo2.py:185][0m |          -0.0226 |          56.6018 |         -67.1077 |
[32m[20221214 00:30:38 @agent_ppo2.py:185][0m |          -0.0147 |          57.8165 |         -67.0273 |
[32m[20221214 00:30:38 @agent_ppo2.py:185][0m |          -0.0245 |          55.0811 |         -67.2147 |
[32m[20221214 00:30:38 @agent_ppo2.py:185][0m |          -0.0286 |          54.9017 |         -67.5246 |
[32m[20221214 00:30:38 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:30:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 450.61
[32m[20221214 00:30:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 672.68
[32m[20221214 00:30:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 846.04
[32m[20221214 00:30:38 @agent_ppo2.py:143][0m Total time:      33.13 min
[32m[20221214 00:30:38 @agent_ppo2.py:145][0m 3012608 total steps have happened
[32m[20221214 00:30:38 @agent_ppo2.py:121][0m #------------------------ Iteration 5471 --------------------------#
[32m[20221214 00:30:38 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:30:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:30:38 @agent_ppo2.py:185][0m |           0.0041 |          53.6584 |         -73.4329 |
[32m[20221214 00:30:39 @agent_ppo2.py:185][0m |          -0.0033 |          45.0537 |         -73.5344 |
[32m[20221214 00:30:39 @agent_ppo2.py:185][0m |          -0.0100 |          42.6071 |         -73.5178 |
[32m[20221214 00:30:39 @agent_ppo2.py:185][0m |          -0.0073 |          41.9336 |         -73.2587 |
[32m[20221214 00:30:39 @agent_ppo2.py:185][0m |          -0.0132 |          40.2513 |         -73.7245 |
[32m[20221214 00:30:39 @agent_ppo2.py:185][0m |          -0.0165 |          39.5689 |         -73.6877 |
[32m[20221214 00:30:39 @agent_ppo2.py:185][0m |          -0.0205 |          38.1017 |         -73.8465 |
[32m[20221214 00:30:39 @agent_ppo2.py:185][0m |          -0.0124 |          37.7660 |         -73.4670 |
[32m[20221214 00:30:39 @agent_ppo2.py:185][0m |          -0.0202 |          36.7006 |         -73.7172 |
[32m[20221214 00:30:39 @agent_ppo2.py:185][0m |          -0.0252 |          36.5295 |         -73.7748 |
[32m[20221214 00:30:39 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:30:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 353.11
[32m[20221214 00:30:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 419.88
[32m[20221214 00:30:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 798.42
[32m[20221214 00:30:39 @agent_ppo2.py:143][0m Total time:      33.15 min
[32m[20221214 00:30:39 @agent_ppo2.py:145][0m 3014656 total steps have happened
[32m[20221214 00:30:39 @agent_ppo2.py:121][0m #------------------------ Iteration 5472 --------------------------#
[32m[20221214 00:30:40 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:30:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:30:40 @agent_ppo2.py:185][0m |           0.0010 |          98.8750 |         -75.3518 |
[32m[20221214 00:30:40 @agent_ppo2.py:185][0m |          -0.0030 |          90.2382 |         -75.1962 |
[32m[20221214 00:30:40 @agent_ppo2.py:185][0m |          -0.0042 |          85.0962 |         -74.5413 |
[32m[20221214 00:30:40 @agent_ppo2.py:185][0m |          -0.0128 |          82.0643 |         -74.7958 |
[32m[20221214 00:30:40 @agent_ppo2.py:185][0m |          -0.0093 |          79.8746 |         -74.6247 |
[32m[20221214 00:30:40 @agent_ppo2.py:185][0m |          -0.0119 |          78.2769 |         -74.2468 |
[32m[20221214 00:30:40 @agent_ppo2.py:185][0m |          -0.0143 |          77.6573 |         -74.1754 |
[32m[20221214 00:30:40 @agent_ppo2.py:185][0m |          -0.0155 |          76.2062 |         -74.2450 |
[32m[20221214 00:30:41 @agent_ppo2.py:185][0m |          -0.0021 |          80.8417 |         -74.0580 |
[32m[20221214 00:30:41 @agent_ppo2.py:185][0m |          -0.0161 |          74.4211 |         -73.5901 |
[32m[20221214 00:30:41 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:30:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 576.75
[32m[20221214 00:30:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 638.51
[32m[20221214 00:30:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 850.80
[32m[20221214 00:30:41 @agent_ppo2.py:143][0m Total time:      33.17 min
[32m[20221214 00:30:41 @agent_ppo2.py:145][0m 3016704 total steps have happened
[32m[20221214 00:30:41 @agent_ppo2.py:121][0m #------------------------ Iteration 5473 --------------------------#
[32m[20221214 00:30:41 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:30:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:30:41 @agent_ppo2.py:185][0m |           0.0050 |         150.4880 |         -72.0014 |
[32m[20221214 00:30:41 @agent_ppo2.py:185][0m |           0.0076 |         147.8767 |         -71.5897 |
[32m[20221214 00:30:41 @agent_ppo2.py:185][0m |           0.0013 |         145.4732 |         -71.7858 |
[32m[20221214 00:30:41 @agent_ppo2.py:185][0m |           0.0013 |         145.5250 |         -71.8190 |
[32m[20221214 00:30:42 @agent_ppo2.py:185][0m |          -0.0033 |         145.4292 |         -71.8107 |
[32m[20221214 00:30:42 @agent_ppo2.py:185][0m |           0.0004 |         145.2730 |         -72.1726 |
[32m[20221214 00:30:42 @agent_ppo2.py:185][0m |          -0.0005 |         144.2995 |         -71.8075 |
[32m[20221214 00:30:42 @agent_ppo2.py:185][0m |          -0.0006 |         144.1525 |         -71.4652 |
[32m[20221214 00:30:42 @agent_ppo2.py:185][0m |          -0.0006 |         144.8755 |         -72.1374 |
[32m[20221214 00:30:42 @agent_ppo2.py:185][0m |          -0.0009 |         143.6112 |         -71.5372 |
[32m[20221214 00:30:42 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:30:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 773.58
[32m[20221214 00:30:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 828.18
[32m[20221214 00:30:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 643.55
[32m[20221214 00:30:42 @agent_ppo2.py:143][0m Total time:      33.20 min
[32m[20221214 00:30:42 @agent_ppo2.py:145][0m 3018752 total steps have happened
[32m[20221214 00:30:42 @agent_ppo2.py:121][0m #------------------------ Iteration 5474 --------------------------#
[32m[20221214 00:30:42 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:30:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:30:43 @agent_ppo2.py:185][0m |          -0.0004 |          90.3873 |         -70.5380 |
[32m[20221214 00:30:43 @agent_ppo2.py:185][0m |          -0.0007 |          76.7969 |         -70.4363 |
[32m[20221214 00:30:43 @agent_ppo2.py:185][0m |          -0.0102 |          72.4839 |         -70.5668 |
[32m[20221214 00:30:43 @agent_ppo2.py:185][0m |          -0.0075 |          67.3953 |         -70.5351 |
[32m[20221214 00:30:43 @agent_ppo2.py:185][0m |          -0.0101 |          66.8729 |         -70.7447 |
[32m[20221214 00:30:43 @agent_ppo2.py:185][0m |          -0.0123 |          64.6812 |         -70.3064 |
[32m[20221214 00:30:43 @agent_ppo2.py:185][0m |          -0.0095 |          62.7075 |         -70.1081 |
[32m[20221214 00:30:43 @agent_ppo2.py:185][0m |          -0.0132 |          61.4380 |         -70.2471 |
[32m[20221214 00:30:43 @agent_ppo2.py:185][0m |          -0.0134 |          61.0933 |         -70.1366 |
[32m[20221214 00:30:44 @agent_ppo2.py:185][0m |          -0.0151 |          58.9489 |         -70.1834 |
[32m[20221214 00:30:44 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 00:30:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 550.68
[32m[20221214 00:30:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 700.23
[32m[20221214 00:30:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 626.57
[32m[20221214 00:30:44 @agent_ppo2.py:143][0m Total time:      33.22 min
[32m[20221214 00:30:44 @agent_ppo2.py:145][0m 3020800 total steps have happened
[32m[20221214 00:30:44 @agent_ppo2.py:121][0m #------------------------ Iteration 5475 --------------------------#
[32m[20221214 00:30:44 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:30:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:30:44 @agent_ppo2.py:185][0m |           0.0014 |          92.4353 |         -72.6693 |
[32m[20221214 00:30:44 @agent_ppo2.py:185][0m |          -0.0047 |          85.4699 |         -71.6815 |
[32m[20221214 00:30:44 @agent_ppo2.py:185][0m |           0.0026 |          86.7508 |         -72.4876 |
[32m[20221214 00:30:44 @agent_ppo2.py:185][0m |          -0.0136 |          79.8236 |         -72.6251 |
[32m[20221214 00:30:44 @agent_ppo2.py:185][0m |          -0.0141 |          78.6070 |         -72.6063 |
[32m[20221214 00:30:45 @agent_ppo2.py:185][0m |          -0.0134 |          77.7841 |         -72.6837 |
[32m[20221214 00:30:45 @agent_ppo2.py:185][0m |          -0.0176 |          77.4755 |         -72.7664 |
[32m[20221214 00:30:45 @agent_ppo2.py:185][0m |          -0.0172 |          77.2537 |         -72.7453 |
[32m[20221214 00:30:45 @agent_ppo2.py:185][0m |          -0.0123 |          76.6074 |         -72.5202 |
[32m[20221214 00:30:45 @agent_ppo2.py:185][0m |          -0.0163 |          75.7893 |         -72.7593 |
[32m[20221214 00:30:45 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:30:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 584.64
[32m[20221214 00:30:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 694.83
[32m[20221214 00:30:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 694.50
[32m[20221214 00:30:45 @agent_ppo2.py:143][0m Total time:      33.25 min
[32m[20221214 00:30:45 @agent_ppo2.py:145][0m 3022848 total steps have happened
[32m[20221214 00:30:45 @agent_ppo2.py:121][0m #------------------------ Iteration 5476 --------------------------#
[32m[20221214 00:30:45 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:30:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:30:45 @agent_ppo2.py:185][0m |           0.0069 |          73.9983 |         -70.6699 |
[32m[20221214 00:30:46 @agent_ppo2.py:185][0m |          -0.0034 |          61.5928 |         -70.1544 |
[32m[20221214 00:30:46 @agent_ppo2.py:185][0m |          -0.0122 |          57.4871 |         -70.2849 |
[32m[20221214 00:30:46 @agent_ppo2.py:185][0m |          -0.0122 |          54.9781 |         -70.3608 |
[32m[20221214 00:30:46 @agent_ppo2.py:185][0m |          -0.0207 |          53.4104 |         -70.2496 |
[32m[20221214 00:30:46 @agent_ppo2.py:185][0m |          -0.0192 |          51.7580 |         -70.1854 |
[32m[20221214 00:30:46 @agent_ppo2.py:185][0m |          -0.0223 |          51.5494 |         -70.1301 |
[32m[20221214 00:30:46 @agent_ppo2.py:185][0m |          -0.0258 |          50.8329 |         -69.9109 |
[32m[20221214 00:30:46 @agent_ppo2.py:185][0m |          -0.0271 |          49.5527 |         -69.7669 |
[32m[20221214 00:30:46 @agent_ppo2.py:185][0m |          -0.0220 |          48.9022 |         -69.8600 |
[32m[20221214 00:30:46 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:30:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 469.87
[32m[20221214 00:30:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 589.01
[32m[20221214 00:30:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 640.51
[32m[20221214 00:30:46 @agent_ppo2.py:143][0m Total time:      33.27 min
[32m[20221214 00:30:46 @agent_ppo2.py:145][0m 3024896 total steps have happened
[32m[20221214 00:30:46 @agent_ppo2.py:121][0m #------------------------ Iteration 5477 --------------------------#
[32m[20221214 00:30:47 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:30:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:30:47 @agent_ppo2.py:185][0m |          -0.0005 |         131.9009 |         -69.5467 |
[32m[20221214 00:30:47 @agent_ppo2.py:185][0m |          -0.0061 |         114.7120 |         -69.9423 |
[32m[20221214 00:30:47 @agent_ppo2.py:185][0m |          -0.0065 |         110.5914 |         -69.4901 |
[32m[20221214 00:30:47 @agent_ppo2.py:185][0m |          -0.0027 |         107.9201 |         -69.3861 |
[32m[20221214 00:30:47 @agent_ppo2.py:185][0m |          -0.0085 |         106.1322 |         -69.8133 |
[32m[20221214 00:30:47 @agent_ppo2.py:185][0m |          -0.0077 |         104.5778 |         -69.9027 |
[32m[20221214 00:30:47 @agent_ppo2.py:185][0m |          -0.0080 |         104.3687 |         -70.4579 |
[32m[20221214 00:30:48 @agent_ppo2.py:185][0m |          -0.0151 |         103.1460 |         -69.8239 |
[32m[20221214 00:30:48 @agent_ppo2.py:185][0m |          -0.0124 |         102.5136 |         -70.2699 |
[32m[20221214 00:30:48 @agent_ppo2.py:185][0m |          -0.0033 |         101.5835 |         -70.5916 |
[32m[20221214 00:30:48 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:30:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 691.05
[32m[20221214 00:30:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 835.40
[32m[20221214 00:30:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 850.35
[32m[20221214 00:30:48 @agent_ppo2.py:143][0m Total time:      33.29 min
[32m[20221214 00:30:48 @agent_ppo2.py:145][0m 3026944 total steps have happened
[32m[20221214 00:30:48 @agent_ppo2.py:121][0m #------------------------ Iteration 5478 --------------------------#
[32m[20221214 00:30:48 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:30:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:30:48 @agent_ppo2.py:185][0m |           0.0082 |         108.5270 |         -74.6260 |
[32m[20221214 00:30:48 @agent_ppo2.py:185][0m |          -0.0010 |          86.9577 |         -74.8875 |
[32m[20221214 00:30:48 @agent_ppo2.py:185][0m |          -0.0017 |          83.4802 |         -74.8733 |
[32m[20221214 00:30:49 @agent_ppo2.py:185][0m |          -0.0095 |          81.7154 |         -75.5164 |
[32m[20221214 00:30:49 @agent_ppo2.py:185][0m |          -0.0118 |          79.8012 |         -75.4612 |
[32m[20221214 00:30:49 @agent_ppo2.py:185][0m |          -0.0033 |          81.4277 |         -75.5850 |
[32m[20221214 00:30:49 @agent_ppo2.py:185][0m |          -0.0012 |          82.0400 |         -75.6577 |
[32m[20221214 00:30:49 @agent_ppo2.py:185][0m |          -0.0146 |          77.2678 |         -75.6065 |
[32m[20221214 00:30:49 @agent_ppo2.py:185][0m |          -0.0099 |          76.3200 |         -75.8202 |
[32m[20221214 00:30:49 @agent_ppo2.py:185][0m |          -0.0186 |          75.1968 |         -76.2501 |
[32m[20221214 00:30:49 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:30:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 574.89
[32m[20221214 00:30:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 740.26
[32m[20221214 00:30:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 843.94
[32m[20221214 00:30:49 @agent_ppo2.py:143][0m Total time:      33.31 min
[32m[20221214 00:30:49 @agent_ppo2.py:145][0m 3028992 total steps have happened
[32m[20221214 00:30:49 @agent_ppo2.py:121][0m #------------------------ Iteration 5479 --------------------------#
[32m[20221214 00:30:49 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:30:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:30:50 @agent_ppo2.py:185][0m |           0.0016 |         127.7329 |         -74.7035 |
[32m[20221214 00:30:50 @agent_ppo2.py:185][0m |          -0.0010 |         122.3063 |         -74.2830 |
[32m[20221214 00:30:50 @agent_ppo2.py:185][0m |           0.0005 |         120.0914 |         -74.4695 |
[32m[20221214 00:30:50 @agent_ppo2.py:185][0m |           0.0000 |         119.2067 |         -74.7654 |
[32m[20221214 00:30:50 @agent_ppo2.py:185][0m |          -0.0051 |         117.7271 |         -74.3128 |
[32m[20221214 00:30:50 @agent_ppo2.py:185][0m |          -0.0066 |         116.8291 |         -74.5643 |
[32m[20221214 00:30:50 @agent_ppo2.py:185][0m |          -0.0065 |         116.0875 |         -74.6532 |
[32m[20221214 00:30:50 @agent_ppo2.py:185][0m |          -0.0035 |         116.1179 |         -74.4580 |
[32m[20221214 00:30:50 @agent_ppo2.py:185][0m |          -0.0079 |         115.5614 |         -74.4644 |
[32m[20221214 00:30:50 @agent_ppo2.py:185][0m |          -0.0003 |         121.1656 |         -74.6733 |
[32m[20221214 00:30:50 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:30:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 674.45
[32m[20221214 00:30:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 794.15
[32m[20221214 00:30:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 598.08
[32m[20221214 00:30:51 @agent_ppo2.py:143][0m Total time:      33.34 min
[32m[20221214 00:30:51 @agent_ppo2.py:145][0m 3031040 total steps have happened
[32m[20221214 00:30:51 @agent_ppo2.py:121][0m #------------------------ Iteration 5480 --------------------------#
[32m[20221214 00:30:51 @agent_ppo2.py:127][0m Sampling time: 0.24 s by 5 slaves
[32m[20221214 00:30:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:30:51 @agent_ppo2.py:185][0m |           0.0077 |          81.6034 |         -76.6336 |
[32m[20221214 00:30:51 @agent_ppo2.py:185][0m |          -0.0058 |          69.3599 |         -76.1933 |
[32m[20221214 00:30:51 @agent_ppo2.py:185][0m |          -0.0165 |          67.1282 |         -75.9223 |
[32m[20221214 00:30:51 @agent_ppo2.py:185][0m |          -0.0122 |          64.6836 |         -75.3934 |
[32m[20221214 00:30:52 @agent_ppo2.py:185][0m |          -0.0100 |          63.9701 |         -75.6372 |
[32m[20221214 00:30:52 @agent_ppo2.py:185][0m |          -0.0175 |          62.6575 |         -74.9713 |
[32m[20221214 00:30:52 @agent_ppo2.py:185][0m |          -0.0080 |          69.0095 |         -74.9656 |
[32m[20221214 00:30:52 @agent_ppo2.py:185][0m |          -0.0173 |          61.5143 |         -75.1257 |
[32m[20221214 00:30:52 @agent_ppo2.py:185][0m |          -0.0142 |          60.9002 |         -74.8543 |
[32m[20221214 00:30:52 @agent_ppo2.py:185][0m |          -0.0220 |          59.9193 |         -74.9537 |
[32m[20221214 00:30:52 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221214 00:30:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 562.66
[32m[20221214 00:30:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 631.29
[32m[20221214 00:30:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 621.61
[32m[20221214 00:30:52 @agent_ppo2.py:143][0m Total time:      33.36 min
[32m[20221214 00:30:52 @agent_ppo2.py:145][0m 3033088 total steps have happened
[32m[20221214 00:30:52 @agent_ppo2.py:121][0m #------------------------ Iteration 5481 --------------------------#
[32m[20221214 00:30:52 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:30:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:30:53 @agent_ppo2.py:185][0m |          -0.0011 |         131.7521 |         -78.3248 |
[32m[20221214 00:30:53 @agent_ppo2.py:185][0m |          -0.0050 |         129.0511 |         -78.5005 |
[32m[20221214 00:30:53 @agent_ppo2.py:185][0m |          -0.0052 |         127.8919 |         -78.6054 |
[32m[20221214 00:30:53 @agent_ppo2.py:185][0m |          -0.0014 |         129.9766 |         -78.6899 |
[32m[20221214 00:30:53 @agent_ppo2.py:185][0m |          -0.0021 |         129.1430 |         -78.3488 |
[32m[20221214 00:30:53 @agent_ppo2.py:185][0m |          -0.0040 |         129.2297 |         -78.4412 |
[32m[20221214 00:30:53 @agent_ppo2.py:185][0m |          -0.0093 |         127.6825 |         -77.9394 |
[32m[20221214 00:30:53 @agent_ppo2.py:185][0m |          -0.0047 |         126.7164 |         -78.5630 |
[32m[20221214 00:30:53 @agent_ppo2.py:185][0m |          -0.0092 |         127.1916 |         -78.0746 |
[32m[20221214 00:30:53 @agent_ppo2.py:185][0m |          -0.0084 |         128.3589 |         -78.2634 |
[32m[20221214 00:30:53 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:30:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 797.37
[32m[20221214 00:30:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.99
[32m[20221214 00:30:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 787.53
[32m[20221214 00:30:54 @agent_ppo2.py:143][0m Total time:      33.39 min
[32m[20221214 00:30:54 @agent_ppo2.py:145][0m 3035136 total steps have happened
[32m[20221214 00:30:54 @agent_ppo2.py:121][0m #------------------------ Iteration 5482 --------------------------#
[32m[20221214 00:30:54 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:30:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:30:54 @agent_ppo2.py:185][0m |          -0.0016 |         156.7829 |         -77.4071 |
[32m[20221214 00:30:54 @agent_ppo2.py:185][0m |          -0.0037 |         150.8934 |         -77.0286 |
[32m[20221214 00:30:54 @agent_ppo2.py:185][0m |           0.0029 |         150.5583 |         -77.0205 |
[32m[20221214 00:30:54 @agent_ppo2.py:185][0m |          -0.0025 |         147.9774 |         -76.8416 |
[32m[20221214 00:30:54 @agent_ppo2.py:185][0m |          -0.0054 |         147.2457 |         -77.1836 |
[32m[20221214 00:30:54 @agent_ppo2.py:185][0m |           0.0041 |         165.5793 |         -77.0768 |
[32m[20221214 00:30:55 @agent_ppo2.py:185][0m |          -0.0048 |         146.8201 |         -77.6326 |
[32m[20221214 00:30:55 @agent_ppo2.py:185][0m |          -0.0072 |         145.7565 |         -77.4288 |
[32m[20221214 00:30:55 @agent_ppo2.py:185][0m |           0.0015 |         155.9637 |         -77.1601 |
[32m[20221214 00:30:55 @agent_ppo2.py:185][0m |          -0.0062 |         144.8975 |         -77.2568 |
[32m[20221214 00:30:55 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:30:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 704.21
[32m[20221214 00:30:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 836.16
[32m[20221214 00:30:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 600.23
[32m[20221214 00:30:55 @agent_ppo2.py:143][0m Total time:      33.41 min
[32m[20221214 00:30:55 @agent_ppo2.py:145][0m 3037184 total steps have happened
[32m[20221214 00:30:55 @agent_ppo2.py:121][0m #------------------------ Iteration 5483 --------------------------#
[32m[20221214 00:30:55 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:30:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:30:55 @agent_ppo2.py:185][0m |           0.0067 |         129.7541 |         -74.9970 |
[32m[20221214 00:30:55 @agent_ppo2.py:185][0m |          -0.0076 |         119.3498 |         -74.9482 |
[32m[20221214 00:30:56 @agent_ppo2.py:185][0m |          -0.0077 |         116.0837 |         -74.5810 |
[32m[20221214 00:30:56 @agent_ppo2.py:185][0m |          -0.0097 |         115.4556 |         -74.4331 |
[32m[20221214 00:30:56 @agent_ppo2.py:185][0m |          -0.0103 |         113.6741 |         -74.4066 |
[32m[20221214 00:30:56 @agent_ppo2.py:185][0m |          -0.0083 |         112.9171 |         -74.0868 |
[32m[20221214 00:30:56 @agent_ppo2.py:185][0m |          -0.0072 |         112.7522 |         -74.3503 |
[32m[20221214 00:30:56 @agent_ppo2.py:185][0m |          -0.0072 |         112.1040 |         -74.3615 |
[32m[20221214 00:30:56 @agent_ppo2.py:185][0m |          -0.0131 |         111.4231 |         -74.1572 |
[32m[20221214 00:30:56 @agent_ppo2.py:185][0m |          -0.0117 |         111.3829 |         -73.9269 |
[32m[20221214 00:30:56 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:30:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 633.58
[32m[20221214 00:30:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 675.65
[32m[20221214 00:30:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 230.11
[32m[20221214 00:30:57 @agent_ppo2.py:143][0m Total time:      33.44 min
[32m[20221214 00:30:57 @agent_ppo2.py:145][0m 3039232 total steps have happened
[32m[20221214 00:30:57 @agent_ppo2.py:121][0m #------------------------ Iteration 5484 --------------------------#
[32m[20221214 00:30:57 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:30:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:30:57 @agent_ppo2.py:185][0m |           0.0138 |         117.4540 |         -80.7198 |
[32m[20221214 00:30:57 @agent_ppo2.py:185][0m |          -0.0008 |         104.2711 |         -80.4658 |
[32m[20221214 00:30:57 @agent_ppo2.py:185][0m |          -0.0035 |          98.9196 |         -79.9680 |
[32m[20221214 00:30:57 @agent_ppo2.py:185][0m |           0.0064 |         108.5222 |         -80.0538 |
[32m[20221214 00:30:57 @agent_ppo2.py:185][0m |           0.0037 |         103.5218 |         -79.6310 |
[32m[20221214 00:30:57 @agent_ppo2.py:185][0m |          -0.0122 |          93.4825 |         -79.7635 |
[32m[20221214 00:30:57 @agent_ppo2.py:185][0m |          -0.0076 |          92.4323 |         -80.1794 |
[32m[20221214 00:30:58 @agent_ppo2.py:185][0m |          -0.0107 |          91.8396 |         -79.6580 |
[32m[20221214 00:30:58 @agent_ppo2.py:185][0m |          -0.0099 |          91.1013 |         -79.9738 |
[32m[20221214 00:30:58 @agent_ppo2.py:185][0m |          -0.0091 |          91.6991 |         -79.7653 |
[32m[20221214 00:30:58 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:30:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 446.96
[32m[20221214 00:30:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 633.52
[32m[20221214 00:30:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 743.86
[32m[20221214 00:30:58 @agent_ppo2.py:143][0m Total time:      33.46 min
[32m[20221214 00:30:58 @agent_ppo2.py:145][0m 3041280 total steps have happened
[32m[20221214 00:30:58 @agent_ppo2.py:121][0m #------------------------ Iteration 5485 --------------------------#
[32m[20221214 00:30:58 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:30:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:30:58 @agent_ppo2.py:185][0m |           0.0000 |          74.8854 |         -79.6546 |
[32m[20221214 00:30:58 @agent_ppo2.py:185][0m |          -0.0034 |          66.8041 |         -79.5664 |
[32m[20221214 00:30:58 @agent_ppo2.py:185][0m |           0.0006 |          65.7083 |         -79.7035 |
[32m[20221214 00:30:59 @agent_ppo2.py:185][0m |          -0.0081 |          63.6067 |         -78.9709 |
[32m[20221214 00:30:59 @agent_ppo2.py:185][0m |          -0.0094 |          61.5295 |         -79.8636 |
[32m[20221214 00:30:59 @agent_ppo2.py:185][0m |          -0.0133 |          61.0992 |         -79.7852 |
[32m[20221214 00:30:59 @agent_ppo2.py:185][0m |          -0.0183 |          59.9008 |         -79.4576 |
[32m[20221214 00:30:59 @agent_ppo2.py:185][0m |          -0.0153 |          59.1945 |         -79.3746 |
[32m[20221214 00:30:59 @agent_ppo2.py:185][0m |          -0.0109 |          59.3379 |         -79.5501 |
[32m[20221214 00:30:59 @agent_ppo2.py:185][0m |          -0.0215 |          58.1231 |         -79.8160 |
[32m[20221214 00:30:59 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 00:30:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 513.71
[32m[20221214 00:30:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 700.63
[32m[20221214 00:30:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 760.71
[32m[20221214 00:30:59 @agent_ppo2.py:143][0m Total time:      33.48 min
[32m[20221214 00:30:59 @agent_ppo2.py:145][0m 3043328 total steps have happened
[32m[20221214 00:30:59 @agent_ppo2.py:121][0m #------------------------ Iteration 5486 --------------------------#
[32m[20221214 00:31:00 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:31:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:31:00 @agent_ppo2.py:185][0m |          -0.0004 |         152.6102 |         -80.7434 |
[32m[20221214 00:31:00 @agent_ppo2.py:185][0m |           0.0016 |         151.0103 |         -80.9072 |
[32m[20221214 00:31:00 @agent_ppo2.py:185][0m |           0.0024 |         150.1352 |         -80.9490 |
[32m[20221214 00:31:00 @agent_ppo2.py:185][0m |          -0.0012 |         149.8792 |         -81.3716 |
[32m[20221214 00:31:00 @agent_ppo2.py:185][0m |           0.0116 |         154.9569 |         -80.5837 |
[32m[20221214 00:31:00 @agent_ppo2.py:185][0m |           0.0007 |         150.8034 |         -81.1659 |
[32m[20221214 00:31:00 @agent_ppo2.py:185][0m |          -0.0017 |         149.9577 |         -81.0388 |
[32m[20221214 00:31:00 @agent_ppo2.py:185][0m |          -0.0025 |         149.6200 |         -81.5934 |
[32m[20221214 00:31:01 @agent_ppo2.py:185][0m |          -0.0044 |         149.7779 |         -81.3774 |
[32m[20221214 00:31:01 @agent_ppo2.py:185][0m |           0.0023 |         151.8707 |         -81.4080 |
[32m[20221214 00:31:01 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:31:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 874.62
[32m[20221214 00:31:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 875.42
[32m[20221214 00:31:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 844.81
[32m[20221214 00:31:01 @agent_ppo2.py:143][0m Total time:      33.51 min
[32m[20221214 00:31:01 @agent_ppo2.py:145][0m 3045376 total steps have happened
[32m[20221214 00:31:01 @agent_ppo2.py:121][0m #------------------------ Iteration 5487 --------------------------#
[32m[20221214 00:31:01 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:31:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:31:01 @agent_ppo2.py:185][0m |          -0.0002 |          92.3818 |         -79.7683 |
[32m[20221214 00:31:01 @agent_ppo2.py:185][0m |          -0.0062 |          85.1679 |         -79.8702 |
[32m[20221214 00:31:01 @agent_ppo2.py:185][0m |          -0.0077 |          81.0464 |         -80.3555 |
[32m[20221214 00:31:01 @agent_ppo2.py:185][0m |          -0.0098 |          76.8233 |         -80.6580 |
[32m[20221214 00:31:02 @agent_ppo2.py:185][0m |          -0.0100 |          75.4296 |         -80.5493 |
[32m[20221214 00:31:02 @agent_ppo2.py:185][0m |          -0.0094 |          74.4114 |         -80.5317 |
[32m[20221214 00:31:02 @agent_ppo2.py:185][0m |          -0.0199 |          73.8178 |         -81.0405 |
[32m[20221214 00:31:02 @agent_ppo2.py:185][0m |          -0.0141 |          72.1003 |         -80.7079 |
[32m[20221214 00:31:02 @agent_ppo2.py:185][0m |          -0.0105 |          73.9382 |         -81.5914 |
[32m[20221214 00:31:02 @agent_ppo2.py:185][0m |          -0.0158 |          72.5662 |         -81.0330 |
[32m[20221214 00:31:02 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 00:31:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 556.68
[32m[20221214 00:31:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 650.16
[32m[20221214 00:31:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 632.92
[32m[20221214 00:31:02 @agent_ppo2.py:143][0m Total time:      33.53 min
[32m[20221214 00:31:02 @agent_ppo2.py:145][0m 3047424 total steps have happened
[32m[20221214 00:31:02 @agent_ppo2.py:121][0m #------------------------ Iteration 5488 --------------------------#
[32m[20221214 00:31:02 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:31:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:31:03 @agent_ppo2.py:185][0m |           0.0016 |         147.5186 |         -85.0903 |
[32m[20221214 00:31:03 @agent_ppo2.py:185][0m |           0.0010 |         139.9605 |         -84.9928 |
[32m[20221214 00:31:03 @agent_ppo2.py:185][0m |           0.0010 |         138.0560 |         -85.4612 |
[32m[20221214 00:31:03 @agent_ppo2.py:185][0m |          -0.0015 |         136.4173 |         -85.0473 |
[32m[20221214 00:31:03 @agent_ppo2.py:185][0m |          -0.0035 |         134.6593 |         -84.9151 |
[32m[20221214 00:31:03 @agent_ppo2.py:185][0m |          -0.0029 |         133.6435 |         -84.9006 |
[32m[20221214 00:31:03 @agent_ppo2.py:185][0m |           0.0036 |         137.9981 |         -85.1768 |
[32m[20221214 00:31:03 @agent_ppo2.py:185][0m |          -0.0044 |         132.1013 |         -84.7703 |
[32m[20221214 00:31:03 @agent_ppo2.py:185][0m |           0.0157 |         152.9967 |         -85.0675 |
[32m[20221214 00:31:04 @agent_ppo2.py:185][0m |          -0.0098 |         131.9807 |         -85.2205 |
[32m[20221214 00:31:04 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:31:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 596.63
[32m[20221214 00:31:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 738.91
[32m[20221214 00:31:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 891.87
[32m[20221214 00:31:04 @agent_ppo2.py:109][0m [4m[34mCRITICAL[0m Get the best episode reward: 891.87
[32m[20221214 00:31:04 @agent_ppo2.py:113][0m [4m[34mCRITICAL[0m Saving the best checkpoint with rewards 891.87
[32m[20221214 00:31:04 @agent_ppo2.py:143][0m Total time:      33.56 min
[32m[20221214 00:31:04 @agent_ppo2.py:145][0m 3049472 total steps have happened
[32m[20221214 00:31:04 @agent_ppo2.py:121][0m #------------------------ Iteration 5489 --------------------------#
[32m[20221214 00:31:04 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:31:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:31:04 @agent_ppo2.py:185][0m |           0.0040 |         137.8337 |         -86.6364 |
[32m[20221214 00:31:04 @agent_ppo2.py:185][0m |          -0.0044 |         126.0350 |         -86.5791 |
[32m[20221214 00:31:04 @agent_ppo2.py:185][0m |          -0.0086 |         121.7714 |         -86.0984 |
[32m[20221214 00:31:04 @agent_ppo2.py:185][0m |          -0.0116 |         119.7044 |         -86.6706 |
[32m[20221214 00:31:04 @agent_ppo2.py:185][0m |          -0.0102 |         118.0362 |         -86.4764 |
[32m[20221214 00:31:05 @agent_ppo2.py:185][0m |          -0.0123 |         117.0749 |         -86.1852 |
[32m[20221214 00:31:05 @agent_ppo2.py:185][0m |          -0.0112 |         116.5477 |         -85.9540 |
[32m[20221214 00:31:05 @agent_ppo2.py:185][0m |          -0.0125 |         115.6439 |         -86.0284 |
[32m[20221214 00:31:05 @agent_ppo2.py:185][0m |          -0.0116 |         114.3492 |         -85.7585 |
[32m[20221214 00:31:05 @agent_ppo2.py:185][0m |          -0.0069 |         119.1159 |         -85.8085 |
[32m[20221214 00:31:05 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221214 00:31:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 584.45
[32m[20221214 00:31:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 778.92
[32m[20221214 00:31:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 595.21
[32m[20221214 00:31:05 @agent_ppo2.py:143][0m Total time:      33.58 min
[32m[20221214 00:31:05 @agent_ppo2.py:145][0m 3051520 total steps have happened
[32m[20221214 00:31:05 @agent_ppo2.py:121][0m #------------------------ Iteration 5490 --------------------------#
[32m[20221214 00:31:05 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:31:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:31:06 @agent_ppo2.py:185][0m |           0.0036 |          82.5430 |         -84.0471 |
[32m[20221214 00:31:06 @agent_ppo2.py:185][0m |          -0.0002 |          72.8785 |         -83.7978 |
[32m[20221214 00:31:06 @agent_ppo2.py:185][0m |          -0.0028 |          70.3764 |         -83.8482 |
[32m[20221214 00:31:06 @agent_ppo2.py:185][0m |          -0.0049 |          70.5243 |         -83.6613 |
[32m[20221214 00:31:06 @agent_ppo2.py:185][0m |          -0.0085 |          68.4974 |         -83.4547 |
[32m[20221214 00:31:06 @agent_ppo2.py:185][0m |          -0.0118 |          67.8246 |         -83.1143 |
[32m[20221214 00:31:06 @agent_ppo2.py:185][0m |          -0.0073 |          69.0028 |         -82.6402 |
[32m[20221214 00:31:06 @agent_ppo2.py:185][0m |          -0.0110 |          68.3566 |         -82.8940 |
[32m[20221214 00:31:06 @agent_ppo2.py:185][0m |          -0.0156 |          66.2267 |         -82.7944 |
[32m[20221214 00:31:06 @agent_ppo2.py:185][0m |          -0.0103 |          65.9263 |         -82.6780 |
[32m[20221214 00:31:06 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 00:31:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 674.47
[32m[20221214 00:31:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 783.54
[32m[20221214 00:31:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 617.30
[32m[20221214 00:31:07 @agent_ppo2.py:143][0m Total time:      33.60 min
[32m[20221214 00:31:07 @agent_ppo2.py:145][0m 3053568 total steps have happened
[32m[20221214 00:31:07 @agent_ppo2.py:121][0m #------------------------ Iteration 5491 --------------------------#
[32m[20221214 00:31:07 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:31:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:31:07 @agent_ppo2.py:185][0m |           0.0107 |          94.0128 |         -82.7988 |
[32m[20221214 00:31:07 @agent_ppo2.py:185][0m |           0.0030 |          80.9650 |         -83.3855 |
[32m[20221214 00:31:07 @agent_ppo2.py:185][0m |          -0.0035 |          75.6169 |         -83.2380 |
[32m[20221214 00:31:07 @agent_ppo2.py:185][0m |          -0.0037 |          73.8992 |         -83.6166 |
[32m[20221214 00:31:07 @agent_ppo2.py:185][0m |          -0.0088 |          72.7082 |         -83.2144 |
[32m[20221214 00:31:08 @agent_ppo2.py:185][0m |          -0.0038 |          71.4175 |         -83.6935 |
[32m[20221214 00:31:08 @agent_ppo2.py:185][0m |          -0.0112 |          70.0368 |         -83.9219 |
[32m[20221214 00:31:08 @agent_ppo2.py:185][0m |          -0.0181 |          70.2611 |         -83.4523 |
[32m[20221214 00:31:08 @agent_ppo2.py:185][0m |          -0.0089 |          68.8190 |         -83.9272 |
[32m[20221214 00:31:08 @agent_ppo2.py:185][0m |          -0.0088 |          68.9993 |         -84.0799 |
[32m[20221214 00:31:08 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 00:31:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 605.11
[32m[20221214 00:31:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 644.11
[32m[20221214 00:31:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 800.71
[32m[20221214 00:31:08 @agent_ppo2.py:143][0m Total time:      33.63 min
[32m[20221214 00:31:08 @agent_ppo2.py:145][0m 3055616 total steps have happened
[32m[20221214 00:31:08 @agent_ppo2.py:121][0m #------------------------ Iteration 5492 --------------------------#
[32m[20221214 00:31:08 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:31:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:31:08 @agent_ppo2.py:185][0m |           0.0133 |         107.6612 |         -82.8231 |
[32m[20221214 00:31:09 @agent_ppo2.py:185][0m |          -0.0008 |          92.9413 |         -83.2857 |
[32m[20221214 00:31:09 @agent_ppo2.py:185][0m |          -0.0082 |          90.3828 |         -83.5018 |
[32m[20221214 00:31:09 @agent_ppo2.py:185][0m |          -0.0109 |          88.9936 |         -83.5842 |
[32m[20221214 00:31:09 @agent_ppo2.py:185][0m |          -0.0130 |          87.4547 |         -83.9071 |
[32m[20221214 00:31:09 @agent_ppo2.py:185][0m |          -0.0129 |          86.1282 |         -84.2084 |
[32m[20221214 00:31:09 @agent_ppo2.py:185][0m |          -0.0153 |          85.5955 |         -84.1804 |
[32m[20221214 00:31:09 @agent_ppo2.py:185][0m |          -0.0134 |          84.8440 |         -84.0807 |
[32m[20221214 00:31:09 @agent_ppo2.py:185][0m |           0.0001 |          92.2559 |         -84.3489 |
[32m[20221214 00:31:09 @agent_ppo2.py:185][0m |          -0.0125 |          85.4091 |         -84.3086 |
[32m[20221214 00:31:09 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:31:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 523.64
[32m[20221214 00:31:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 677.16
[32m[20221214 00:31:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 693.12
[32m[20221214 00:31:10 @agent_ppo2.py:143][0m Total time:      33.65 min
[32m[20221214 00:31:10 @agent_ppo2.py:145][0m 3057664 total steps have happened
[32m[20221214 00:31:10 @agent_ppo2.py:121][0m #------------------------ Iteration 5493 --------------------------#
[32m[20221214 00:31:10 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:31:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:31:10 @agent_ppo2.py:185][0m |          -0.0023 |          68.7386 |         -88.4375 |
[32m[20221214 00:31:10 @agent_ppo2.py:185][0m |           0.0091 |          67.0333 |         -88.4517 |
[32m[20221214 00:31:10 @agent_ppo2.py:185][0m |          -0.0145 |          56.0535 |         -88.4986 |
[32m[20221214 00:31:10 @agent_ppo2.py:185][0m |          -0.0086 |          54.3636 |         -88.3194 |
[32m[20221214 00:31:10 @agent_ppo2.py:185][0m |          -0.0166 |          52.9365 |         -88.0403 |
[32m[20221214 00:31:10 @agent_ppo2.py:185][0m |          -0.0126 |          53.9287 |         -87.6882 |
[32m[20221214 00:31:10 @agent_ppo2.py:185][0m |          -0.0198 |          51.1654 |         -87.6988 |
[32m[20221214 00:31:11 @agent_ppo2.py:185][0m |          -0.0148 |          50.9743 |         -88.3849 |
[32m[20221214 00:31:11 @agent_ppo2.py:185][0m |          -0.0192 |          50.1841 |         -87.9435 |
[32m[20221214 00:31:11 @agent_ppo2.py:185][0m |          -0.0196 |          49.5726 |         -87.9242 |
[32m[20221214 00:31:11 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:31:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 544.42
[32m[20221214 00:31:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 603.36
[32m[20221214 00:31:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 802.01
[32m[20221214 00:31:11 @agent_ppo2.py:143][0m Total time:      33.68 min
[32m[20221214 00:31:11 @agent_ppo2.py:145][0m 3059712 total steps have happened
[32m[20221214 00:31:11 @agent_ppo2.py:121][0m #------------------------ Iteration 5494 --------------------------#
[32m[20221214 00:31:11 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:31:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:31:11 @agent_ppo2.py:185][0m |          -0.0028 |         106.3707 |         -87.0724 |
[32m[20221214 00:31:11 @agent_ppo2.py:185][0m |          -0.0056 |          98.4265 |         -87.4813 |
[32m[20221214 00:31:12 @agent_ppo2.py:185][0m |          -0.0038 |          96.0281 |         -87.0924 |
[32m[20221214 00:31:12 @agent_ppo2.py:185][0m |          -0.0027 |          95.3315 |         -87.6017 |
[32m[20221214 00:31:12 @agent_ppo2.py:185][0m |          -0.0068 |          94.1234 |         -87.5915 |
[32m[20221214 00:31:12 @agent_ppo2.py:185][0m |          -0.0007 |          93.9213 |         -88.3182 |
[32m[20221214 00:31:12 @agent_ppo2.py:185][0m |          -0.0159 |          92.1945 |         -88.1072 |
[32m[20221214 00:31:12 @agent_ppo2.py:185][0m |          -0.0124 |          91.9954 |         -87.7144 |
[32m[20221214 00:31:12 @agent_ppo2.py:185][0m |          -0.0120 |          92.6648 |         -88.2733 |
[32m[20221214 00:31:12 @agent_ppo2.py:185][0m |          -0.0003 |          98.0988 |         -88.2913 |
[32m[20221214 00:31:12 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:31:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 497.31
[32m[20221214 00:31:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 584.78
[32m[20221214 00:31:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 603.55
[32m[20221214 00:31:12 @agent_ppo2.py:143][0m Total time:      33.70 min
[32m[20221214 00:31:12 @agent_ppo2.py:145][0m 3061760 total steps have happened
[32m[20221214 00:31:12 @agent_ppo2.py:121][0m #------------------------ Iteration 5495 --------------------------#
[32m[20221214 00:31:13 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:31:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:31:13 @agent_ppo2.py:185][0m |          -0.0017 |         128.6141 |         -87.1503 |
[32m[20221214 00:31:13 @agent_ppo2.py:185][0m |           0.0072 |         125.7922 |         -86.8495 |
[32m[20221214 00:31:13 @agent_ppo2.py:185][0m |          -0.0092 |         114.9962 |         -86.8208 |
[32m[20221214 00:31:13 @agent_ppo2.py:185][0m |          -0.0055 |         113.2423 |         -85.8391 |
[32m[20221214 00:31:13 @agent_ppo2.py:185][0m |          -0.0082 |         112.2758 |         -86.6630 |
[32m[20221214 00:31:13 @agent_ppo2.py:185][0m |          -0.0093 |         112.3369 |         -86.6814 |
[32m[20221214 00:31:13 @agent_ppo2.py:185][0m |          -0.0044 |         111.3416 |         -86.4570 |
[32m[20221214 00:31:13 @agent_ppo2.py:185][0m |          -0.0004 |         124.1293 |         -86.6603 |
[32m[20221214 00:31:14 @agent_ppo2.py:185][0m |          -0.0121 |         111.5271 |         -86.7107 |
[32m[20221214 00:31:14 @agent_ppo2.py:185][0m |          -0.0071 |         109.9330 |         -86.0998 |
[32m[20221214 00:31:14 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:31:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 650.28
[32m[20221214 00:31:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 821.26
[32m[20221214 00:31:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 581.86
[32m[20221214 00:31:14 @agent_ppo2.py:143][0m Total time:      33.72 min
[32m[20221214 00:31:14 @agent_ppo2.py:145][0m 3063808 total steps have happened
[32m[20221214 00:31:14 @agent_ppo2.py:121][0m #------------------------ Iteration 5496 --------------------------#
[32m[20221214 00:31:14 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:31:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:31:14 @agent_ppo2.py:185][0m |           0.0033 |         159.0000 |         -88.7593 |
[32m[20221214 00:31:14 @agent_ppo2.py:185][0m |           0.0030 |         158.8137 |         -89.1626 |
[32m[20221214 00:31:14 @agent_ppo2.py:185][0m |           0.0198 |         176.2359 |         -89.2489 |
[32m[20221214 00:31:15 @agent_ppo2.py:185][0m |           0.0067 |         160.5444 |         -88.5180 |
[32m[20221214 00:31:15 @agent_ppo2.py:185][0m |          -0.0007 |         155.7975 |         -90.3339 |
[32m[20221214 00:31:15 @agent_ppo2.py:185][0m |           0.0003 |         155.7903 |         -89.5865 |
[32m[20221214 00:31:15 @agent_ppo2.py:185][0m |          -0.0014 |         154.5219 |         -90.2441 |
[32m[20221214 00:31:15 @agent_ppo2.py:185][0m |          -0.0029 |         154.7351 |         -90.5582 |
[32m[20221214 00:31:15 @agent_ppo2.py:185][0m |          -0.0041 |         153.9171 |         -88.9676 |
[32m[20221214 00:31:15 @agent_ppo2.py:185][0m |          -0.0024 |         155.2236 |         -90.3773 |
[32m[20221214 00:31:15 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:31:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 790.00
[32m[20221214 00:31:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 827.05
[32m[20221214 00:31:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 676.03
[32m[20221214 00:31:15 @agent_ppo2.py:143][0m Total time:      33.75 min
[32m[20221214 00:31:15 @agent_ppo2.py:145][0m 3065856 total steps have happened
[32m[20221214 00:31:15 @agent_ppo2.py:121][0m #------------------------ Iteration 5497 --------------------------#
[32m[20221214 00:31:15 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:31:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:31:16 @agent_ppo2.py:185][0m |          -0.0004 |         166.9178 |         -91.9248 |
[32m[20221214 00:31:16 @agent_ppo2.py:185][0m |          -0.0012 |         158.5733 |         -91.5826 |
[32m[20221214 00:31:16 @agent_ppo2.py:185][0m |          -0.0049 |         156.3189 |         -91.8753 |
[32m[20221214 00:31:16 @agent_ppo2.py:185][0m |          -0.0069 |         156.1948 |         -91.9847 |
[32m[20221214 00:31:16 @agent_ppo2.py:185][0m |           0.0065 |         167.6932 |         -91.3155 |
[32m[20221214 00:31:16 @agent_ppo2.py:185][0m |          -0.0008 |         160.3542 |         -91.2072 |
[32m[20221214 00:31:16 @agent_ppo2.py:185][0m |          -0.0047 |         154.5850 |         -91.2483 |
[32m[20221214 00:31:16 @agent_ppo2.py:185][0m |          -0.0070 |         153.5525 |         -90.7705 |
[32m[20221214 00:31:16 @agent_ppo2.py:185][0m |          -0.0048 |         153.3056 |         -91.0235 |
[32m[20221214 00:31:17 @agent_ppo2.py:185][0m |          -0.0045 |         155.7591 |         -91.4468 |
[32m[20221214 00:31:17 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:31:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 734.04
[32m[20221214 00:31:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 838.82
[32m[20221214 00:31:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 761.84
[32m[20221214 00:31:17 @agent_ppo2.py:143][0m Total time:      33.77 min
[32m[20221214 00:31:17 @agent_ppo2.py:145][0m 3067904 total steps have happened
[32m[20221214 00:31:17 @agent_ppo2.py:121][0m #------------------------ Iteration 5498 --------------------------#
[32m[20221214 00:31:17 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:31:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:31:17 @agent_ppo2.py:185][0m |           0.0009 |         143.6167 |         -92.8399 |
[32m[20221214 00:31:17 @agent_ppo2.py:185][0m |          -0.0060 |         133.9922 |         -92.2203 |
[32m[20221214 00:31:17 @agent_ppo2.py:185][0m |          -0.0070 |         131.3103 |         -92.7003 |
[32m[20221214 00:31:17 @agent_ppo2.py:185][0m |          -0.0033 |         129.8998 |         -92.4910 |
[32m[20221214 00:31:18 @agent_ppo2.py:185][0m |          -0.0091 |         128.6774 |         -92.3525 |
[32m[20221214 00:31:18 @agent_ppo2.py:185][0m |          -0.0097 |         127.5967 |         -92.4033 |
[32m[20221214 00:31:18 @agent_ppo2.py:185][0m |           0.0232 |         163.1979 |         -91.9762 |
[32m[20221214 00:31:18 @agent_ppo2.py:185][0m |          -0.0102 |         130.0699 |         -92.2218 |
[32m[20221214 00:31:18 @agent_ppo2.py:185][0m |          -0.0112 |         125.9526 |         -91.8602 |
[32m[20221214 00:31:18 @agent_ppo2.py:185][0m |          -0.0094 |         125.3090 |         -91.7956 |
[32m[20221214 00:31:18 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:31:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 608.63
[32m[20221214 00:31:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 737.19
[32m[20221214 00:31:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 562.00
[32m[20221214 00:31:18 @agent_ppo2.py:143][0m Total time:      33.80 min
[32m[20221214 00:31:18 @agent_ppo2.py:145][0m 3069952 total steps have happened
[32m[20221214 00:31:18 @agent_ppo2.py:121][0m #------------------------ Iteration 5499 --------------------------#
[32m[20221214 00:31:18 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:31:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:31:19 @agent_ppo2.py:185][0m |          -0.0009 |          96.3915 |         -87.6732 |
[32m[20221214 00:31:19 @agent_ppo2.py:185][0m |           0.0011 |          86.2762 |         -86.9330 |
[32m[20221214 00:31:19 @agent_ppo2.py:185][0m |          -0.0074 |          83.0491 |         -87.3687 |
[32m[20221214 00:31:19 @agent_ppo2.py:185][0m |          -0.0114 |          82.2143 |         -87.1523 |
[32m[20221214 00:31:19 @agent_ppo2.py:185][0m |          -0.0077 |          81.3563 |         -86.9951 |
[32m[20221214 00:31:19 @agent_ppo2.py:185][0m |          -0.0091 |          88.5232 |         -86.8542 |
[32m[20221214 00:31:19 @agent_ppo2.py:185][0m |          -0.0067 |          79.8635 |         -86.5294 |
[32m[20221214 00:31:19 @agent_ppo2.py:185][0m |          -0.0136 |          78.9040 |         -86.5209 |
[32m[20221214 00:31:19 @agent_ppo2.py:185][0m |          -0.0179 |          78.4579 |         -86.5280 |
[32m[20221214 00:31:20 @agent_ppo2.py:185][0m |          -0.0177 |          78.1161 |         -86.8284 |
[32m[20221214 00:31:20 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 00:31:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 450.77
[32m[20221214 00:31:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 530.82
[32m[20221214 00:31:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 629.39
[32m[20221214 00:31:20 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Saving the interval checkpoint with rewards 891.87
[32m[20221214 00:31:20 @agent_ppo2.py:143][0m Total time:      33.82 min
[32m[20221214 00:31:20 @agent_ppo2.py:145][0m 3072000 total steps have happened
[32m[20221214 00:31:20 @agent_ppo2.py:121][0m #------------------------ Iteration 5500 --------------------------#
[32m[20221214 00:31:20 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:31:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:31:20 @agent_ppo2.py:185][0m |          -0.0045 |         120.0589 |         -91.6624 |
[32m[20221214 00:31:20 @agent_ppo2.py:185][0m |          -0.0057 |         111.0317 |         -91.5866 |
[32m[20221214 00:31:20 @agent_ppo2.py:185][0m |          -0.0071 |         108.4315 |         -91.4232 |
[32m[20221214 00:31:20 @agent_ppo2.py:185][0m |          -0.0100 |         108.0251 |         -91.4344 |
[32m[20221214 00:31:20 @agent_ppo2.py:185][0m |          -0.0095 |         106.7914 |         -91.7514 |
[32m[20221214 00:31:21 @agent_ppo2.py:185][0m |          -0.0113 |         105.6007 |         -91.8660 |
[32m[20221214 00:31:21 @agent_ppo2.py:185][0m |          -0.0122 |         105.2008 |         -92.0173 |
[32m[20221214 00:31:21 @agent_ppo2.py:185][0m |          -0.0131 |         104.8075 |         -91.5149 |
[32m[20221214 00:31:21 @agent_ppo2.py:185][0m |          -0.0136 |         103.5015 |         -92.0656 |
[32m[20221214 00:31:21 @agent_ppo2.py:185][0m |          -0.0177 |         103.1355 |         -92.1469 |
[32m[20221214 00:31:21 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:31:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 533.19
[32m[20221214 00:31:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 584.18
[32m[20221214 00:31:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 842.85
[32m[20221214 00:31:21 @agent_ppo2.py:143][0m Total time:      33.85 min
[32m[20221214 00:31:21 @agent_ppo2.py:145][0m 3074048 total steps have happened
[32m[20221214 00:31:21 @agent_ppo2.py:121][0m #------------------------ Iteration 5501 --------------------------#
[32m[20221214 00:31:21 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:31:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:31:21 @agent_ppo2.py:185][0m |           0.0050 |         109.4440 |         -88.6738 |
[32m[20221214 00:31:22 @agent_ppo2.py:185][0m |          -0.0025 |         101.1817 |         -88.8064 |
[32m[20221214 00:31:22 @agent_ppo2.py:185][0m |          -0.0001 |          98.2954 |         -88.7502 |
[32m[20221214 00:31:22 @agent_ppo2.py:185][0m |           0.0049 |         103.0075 |         -89.1286 |
[32m[20221214 00:31:22 @agent_ppo2.py:185][0m |          -0.0088 |          95.8709 |         -88.3758 |
[32m[20221214 00:31:22 @agent_ppo2.py:185][0m |          -0.0059 |          98.1747 |         -88.8295 |
[32m[20221214 00:31:22 @agent_ppo2.py:185][0m |          -0.0058 |          93.7690 |         -89.0472 |
[32m[20221214 00:31:22 @agent_ppo2.py:185][0m |          -0.0106 |          93.1126 |         -89.1553 |
[32m[20221214 00:31:22 @agent_ppo2.py:185][0m |          -0.0146 |          92.3811 |         -88.9902 |
[32m[20221214 00:31:22 @agent_ppo2.py:185][0m |          -0.0147 |          91.6655 |         -88.9235 |
[32m[20221214 00:31:22 @agent_ppo2.py:130][0m Policy update time: 1.21 s
[32m[20221214 00:31:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 463.98
[32m[20221214 00:31:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 588.71
[32m[20221214 00:31:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 588.28
[32m[20221214 00:31:23 @agent_ppo2.py:143][0m Total time:      33.87 min
[32m[20221214 00:31:23 @agent_ppo2.py:145][0m 3076096 total steps have happened
[32m[20221214 00:31:23 @agent_ppo2.py:121][0m #------------------------ Iteration 5502 --------------------------#
[32m[20221214 00:31:23 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:31:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:31:23 @agent_ppo2.py:185][0m |          -0.0033 |          84.8724 |         -89.3053 |
[32m[20221214 00:31:23 @agent_ppo2.py:185][0m |          -0.0032 |          80.3099 |         -89.1586 |
[32m[20221214 00:31:23 @agent_ppo2.py:185][0m |          -0.0088 |          79.0381 |         -88.9671 |
[32m[20221214 00:31:23 @agent_ppo2.py:185][0m |          -0.0056 |          77.5048 |         -89.3796 |
[32m[20221214 00:31:23 @agent_ppo2.py:185][0m |          -0.0092 |          76.4890 |         -89.0997 |
[32m[20221214 00:31:24 @agent_ppo2.py:185][0m |          -0.0122 |          75.9945 |         -89.4675 |
[32m[20221214 00:31:24 @agent_ppo2.py:185][0m |          -0.0021 |          78.8078 |         -88.7661 |
[32m[20221214 00:31:24 @agent_ppo2.py:185][0m |          -0.0096 |          76.4001 |         -89.0014 |
[32m[20221214 00:31:24 @agent_ppo2.py:185][0m |          -0.0120 |          75.5247 |         -89.1635 |
[32m[20221214 00:31:24 @agent_ppo2.py:185][0m |          -0.0111 |          74.7417 |         -88.7050 |
[32m[20221214 00:31:24 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221214 00:31:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 633.95
[32m[20221214 00:31:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 747.57
[32m[20221214 00:31:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 155.46
[32m[20221214 00:31:24 @agent_ppo2.py:143][0m Total time:      33.90 min
[32m[20221214 00:31:24 @agent_ppo2.py:145][0m 3078144 total steps have happened
[32m[20221214 00:31:24 @agent_ppo2.py:121][0m #------------------------ Iteration 5503 --------------------------#
[32m[20221214 00:31:24 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:31:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:31:24 @agent_ppo2.py:185][0m |           0.0001 |          58.6212 |         -88.8329 |
[32m[20221214 00:31:25 @agent_ppo2.py:185][0m |          -0.0031 |          51.1804 |         -88.5131 |
[32m[20221214 00:31:25 @agent_ppo2.py:185][0m |          -0.0107 |          47.6504 |         -88.8314 |
[32m[20221214 00:31:25 @agent_ppo2.py:185][0m |          -0.0119 |          46.0903 |         -88.4895 |
[32m[20221214 00:31:25 @agent_ppo2.py:185][0m |          -0.0079 |          45.3675 |         -88.6737 |
[32m[20221214 00:31:25 @agent_ppo2.py:185][0m |           0.0084 |          58.7641 |         -88.3295 |
[32m[20221214 00:31:25 @agent_ppo2.py:185][0m |          -0.0143 |          43.8954 |         -88.1728 |
[32m[20221214 00:31:25 @agent_ppo2.py:185][0m |          -0.0160 |          43.3649 |         -88.4005 |
[32m[20221214 00:31:25 @agent_ppo2.py:185][0m |          -0.0216 |          43.2368 |         -88.2972 |
[32m[20221214 00:31:25 @agent_ppo2.py:185][0m |          -0.0203 |          42.5534 |         -88.5012 |
[32m[20221214 00:31:25 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:31:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 492.38
[32m[20221214 00:31:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 594.90
[32m[20221214 00:31:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 743.54
[32m[20221214 00:31:26 @agent_ppo2.py:143][0m Total time:      33.92 min
[32m[20221214 00:31:26 @agent_ppo2.py:145][0m 3080192 total steps have happened
[32m[20221214 00:31:26 @agent_ppo2.py:121][0m #------------------------ Iteration 5504 --------------------------#
[32m[20221214 00:31:26 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:31:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:31:26 @agent_ppo2.py:185][0m |           0.0020 |          97.1907 |         -88.4423 |
[32m[20221214 00:31:26 @agent_ppo2.py:185][0m |          -0.0035 |          88.3905 |         -88.6743 |
[32m[20221214 00:31:26 @agent_ppo2.py:185][0m |          -0.0033 |          85.1901 |         -88.9949 |
[32m[20221214 00:31:26 @agent_ppo2.py:185][0m |          -0.0051 |          82.1460 |         -88.8961 |
[32m[20221214 00:31:26 @agent_ppo2.py:185][0m |          -0.0038 |          81.0523 |         -89.1125 |
[32m[20221214 00:31:27 @agent_ppo2.py:185][0m |          -0.0100 |          81.5290 |         -89.4876 |
[32m[20221214 00:31:27 @agent_ppo2.py:185][0m |          -0.0103 |          79.4984 |         -89.4837 |
[32m[20221214 00:31:27 @agent_ppo2.py:185][0m |          -0.0062 |          78.8123 |         -90.0703 |
[32m[20221214 00:31:27 @agent_ppo2.py:185][0m |          -0.0129 |          78.4843 |         -90.1581 |
[32m[20221214 00:31:27 @agent_ppo2.py:185][0m |          -0.0128 |          77.2347 |         -90.0760 |
[32m[20221214 00:31:27 @agent_ppo2.py:130][0m Policy update time: 1.24 s
[32m[20221214 00:31:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 390.81
[32m[20221214 00:31:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 699.78
[32m[20221214 00:31:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 752.19
[32m[20221214 00:31:27 @agent_ppo2.py:143][0m Total time:      33.95 min
[32m[20221214 00:31:27 @agent_ppo2.py:145][0m 3082240 total steps have happened
[32m[20221214 00:31:27 @agent_ppo2.py:121][0m #------------------------ Iteration 5505 --------------------------#
[32m[20221214 00:31:27 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:31:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:31:28 @agent_ppo2.py:185][0m |           0.0003 |         140.0663 |         -89.1285 |
[32m[20221214 00:31:28 @agent_ppo2.py:185][0m |           0.0036 |         137.0314 |         -89.3051 |
[32m[20221214 00:31:28 @agent_ppo2.py:185][0m |          -0.0006 |         134.9732 |         -89.7038 |
[32m[20221214 00:31:28 @agent_ppo2.py:185][0m |          -0.0012 |         134.4671 |         -89.4098 |
[32m[20221214 00:31:28 @agent_ppo2.py:185][0m |          -0.0018 |         134.4461 |         -88.8548 |
[32m[20221214 00:31:28 @agent_ppo2.py:185][0m |           0.0093 |         150.9513 |         -90.0752 |
[32m[20221214 00:31:28 @agent_ppo2.py:185][0m |           0.0037 |         135.1957 |         -89.8649 |
[32m[20221214 00:31:28 @agent_ppo2.py:185][0m |           0.0036 |         137.3085 |         -90.1171 |
[32m[20221214 00:31:28 @agent_ppo2.py:185][0m |          -0.0043 |         133.4493 |         -89.8456 |
[32m[20221214 00:31:28 @agent_ppo2.py:185][0m |          -0.0003 |         134.1216 |         -90.6054 |
[32m[20221214 00:31:28 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 00:31:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 795.67
[32m[20221214 00:31:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 823.87
[32m[20221214 00:31:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 629.02
[32m[20221214 00:31:29 @agent_ppo2.py:143][0m Total time:      33.97 min
[32m[20221214 00:31:29 @agent_ppo2.py:145][0m 3084288 total steps have happened
[32m[20221214 00:31:29 @agent_ppo2.py:121][0m #------------------------ Iteration 5506 --------------------------#
[32m[20221214 00:31:29 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:31:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:31:29 @agent_ppo2.py:185][0m |           0.0029 |          89.3522 |         -94.9041 |
[32m[20221214 00:31:29 @agent_ppo2.py:185][0m |          -0.0013 |          80.0731 |         -94.8834 |
[32m[20221214 00:31:29 @agent_ppo2.py:185][0m |          -0.0167 |          76.6840 |         -94.7161 |
[32m[20221214 00:31:29 @agent_ppo2.py:185][0m |          -0.0133 |          74.9147 |         -94.3761 |
[32m[20221214 00:31:29 @agent_ppo2.py:185][0m |          -0.0164 |          73.5521 |         -94.3830 |
[32m[20221214 00:31:30 @agent_ppo2.py:185][0m |          -0.0153 |          73.0730 |         -94.1960 |
[32m[20221214 00:31:30 @agent_ppo2.py:185][0m |          -0.0218 |          71.5752 |         -94.1650 |
[32m[20221214 00:31:30 @agent_ppo2.py:185][0m |          -0.0216 |          71.0529 |         -94.0749 |
[32m[20221214 00:31:30 @agent_ppo2.py:185][0m |          -0.0255 |          70.5419 |         -94.0022 |
[32m[20221214 00:31:30 @agent_ppo2.py:185][0m |          -0.0100 |          85.9424 |         -93.9936 |
[32m[20221214 00:31:30 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 00:31:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 471.65
[32m[20221214 00:31:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 656.55
[32m[20221214 00:31:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 842.87
[32m[20221214 00:31:30 @agent_ppo2.py:143][0m Total time:      34.00 min
[32m[20221214 00:31:30 @agent_ppo2.py:145][0m 3086336 total steps have happened
[32m[20221214 00:31:30 @agent_ppo2.py:121][0m #------------------------ Iteration 5507 --------------------------#
[32m[20221214 00:31:30 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:31:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:31:30 @agent_ppo2.py:185][0m |           0.0009 |          90.0237 |         -98.1042 |
[32m[20221214 00:31:31 @agent_ppo2.py:185][0m |          -0.0023 |          76.2439 |         -98.0143 |
[32m[20221214 00:31:31 @agent_ppo2.py:185][0m |          -0.0057 |          73.2057 |         -98.1754 |
[32m[20221214 00:31:31 @agent_ppo2.py:185][0m |          -0.0078 |          69.9008 |         -97.8031 |
[32m[20221214 00:31:31 @agent_ppo2.py:185][0m |          -0.0123 |          68.3418 |         -98.1805 |
[32m[20221214 00:31:31 @agent_ppo2.py:185][0m |          -0.0161 |          66.9735 |         -98.4365 |
[32m[20221214 00:31:31 @agent_ppo2.py:185][0m |          -0.0149 |          65.9897 |         -98.2270 |
[32m[20221214 00:31:31 @agent_ppo2.py:185][0m |          -0.0031 |          66.7963 |         -98.3984 |
[32m[20221214 00:31:31 @agent_ppo2.py:185][0m |          -0.0159 |          65.2343 |         -98.2863 |
[32m[20221214 00:31:31 @agent_ppo2.py:185][0m |          -0.0073 |          64.3429 |         -98.4527 |
[32m[20221214 00:31:31 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 00:31:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 389.91
[32m[20221214 00:31:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 567.73
[32m[20221214 00:31:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 661.42
[32m[20221214 00:31:32 @agent_ppo2.py:143][0m Total time:      34.02 min
[32m[20221214 00:31:32 @agent_ppo2.py:145][0m 3088384 total steps have happened
[32m[20221214 00:31:32 @agent_ppo2.py:121][0m #------------------------ Iteration 5508 --------------------------#
[32m[20221214 00:31:32 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:31:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:31:32 @agent_ppo2.py:185][0m |          -0.0010 |         101.0595 |         -97.0775 |
[32m[20221214 00:31:32 @agent_ppo2.py:185][0m |           0.0011 |          96.7008 |         -97.8392 |
[32m[20221214 00:31:32 @agent_ppo2.py:185][0m |           0.0009 |          96.6016 |         -97.6675 |
[32m[20221214 00:31:32 @agent_ppo2.py:185][0m |          -0.0017 |          95.0957 |         -97.5375 |
[32m[20221214 00:31:32 @agent_ppo2.py:185][0m |           0.0001 |          94.9519 |         -97.7225 |
[32m[20221214 00:31:32 @agent_ppo2.py:185][0m |          -0.0017 |          93.4662 |         -98.0921 |
[32m[20221214 00:31:33 @agent_ppo2.py:185][0m |          -0.0013 |          93.6206 |         -97.6372 |
[32m[20221214 00:31:33 @agent_ppo2.py:185][0m |          -0.0051 |          95.1791 |         -97.7445 |
[32m[20221214 00:31:33 @agent_ppo2.py:185][0m |          -0.0022 |          92.5894 |         -96.4459 |
[32m[20221214 00:31:33 @agent_ppo2.py:185][0m |          -0.0048 |          92.5543 |         -98.1377 |
[32m[20221214 00:31:33 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:31:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 802.31
[32m[20221214 00:31:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.59
[32m[20221214 00:31:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 772.54
[32m[20221214 00:31:33 @agent_ppo2.py:143][0m Total time:      34.04 min
[32m[20221214 00:31:33 @agent_ppo2.py:145][0m 3090432 total steps have happened
[32m[20221214 00:31:33 @agent_ppo2.py:121][0m #------------------------ Iteration 5509 --------------------------#
[32m[20221214 00:31:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:31:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:31:33 @agent_ppo2.py:185][0m |           0.0007 |          93.4150 |         -95.6526 |
[32m[20221214 00:31:33 @agent_ppo2.py:185][0m |          -0.0043 |          89.5142 |         -95.5102 |
[32m[20221214 00:31:34 @agent_ppo2.py:185][0m |           0.0030 |          91.2572 |         -95.3724 |
[32m[20221214 00:31:34 @agent_ppo2.py:185][0m |          -0.0086 |          87.8313 |         -95.7343 |
[32m[20221214 00:31:34 @agent_ppo2.py:185][0m |          -0.0085 |          87.2849 |         -94.3708 |
[32m[20221214 00:31:34 @agent_ppo2.py:185][0m |          -0.0053 |          87.6095 |         -95.2060 |
[32m[20221214 00:31:34 @agent_ppo2.py:185][0m |          -0.0044 |          87.9557 |         -94.9367 |
[32m[20221214 00:31:34 @agent_ppo2.py:185][0m |          -0.0020 |          86.6310 |         -94.1427 |
[32m[20221214 00:31:34 @agent_ppo2.py:185][0m |          -0.0138 |          85.6273 |         -95.3112 |
[32m[20221214 00:31:34 @agent_ppo2.py:185][0m |          -0.0081 |          85.7194 |         -95.1935 |
[32m[20221214 00:31:34 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:31:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 742.80
[32m[20221214 00:31:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 820.36
[32m[20221214 00:31:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 756.20
[32m[20221214 00:31:34 @agent_ppo2.py:143][0m Total time:      34.07 min
[32m[20221214 00:31:34 @agent_ppo2.py:145][0m 3092480 total steps have happened
[32m[20221214 00:31:34 @agent_ppo2.py:121][0m #------------------------ Iteration 5510 --------------------------#
[32m[20221214 00:31:35 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:31:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:31:35 @agent_ppo2.py:185][0m |           0.0005 |         136.2591 |         -99.7965 |
[32m[20221214 00:31:35 @agent_ppo2.py:185][0m |          -0.0010 |         132.5863 |         -99.9099 |
[32m[20221214 00:31:35 @agent_ppo2.py:185][0m |          -0.0026 |         129.8871 |         -99.9612 |
[32m[20221214 00:31:35 @agent_ppo2.py:185][0m |          -0.0031 |         129.0534 |         -99.8059 |
[32m[20221214 00:31:35 @agent_ppo2.py:185][0m |          -0.0087 |         129.4695 |         -99.7532 |
[32m[20221214 00:31:35 @agent_ppo2.py:185][0m |           0.0051 |         138.4356 |        -100.1034 |
[32m[20221214 00:31:35 @agent_ppo2.py:185][0m |          -0.0050 |         130.2491 |         -99.8395 |
[32m[20221214 00:31:36 @agent_ppo2.py:185][0m |          -0.0064 |         128.1010 |         -99.0623 |
[32m[20221214 00:31:36 @agent_ppo2.py:185][0m |          -0.0092 |         125.9946 |         -99.7959 |
[32m[20221214 00:31:36 @agent_ppo2.py:185][0m |          -0.0028 |         127.2851 |         -99.5453 |
[32m[20221214 00:31:36 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221214 00:31:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 726.73
[32m[20221214 00:31:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.89
[32m[20221214 00:31:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 763.11
[32m[20221214 00:31:36 @agent_ppo2.py:143][0m Total time:      34.09 min
[32m[20221214 00:31:36 @agent_ppo2.py:145][0m 3094528 total steps have happened
[32m[20221214 00:31:36 @agent_ppo2.py:121][0m #------------------------ Iteration 5511 --------------------------#
[32m[20221214 00:31:36 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:31:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:31:36 @agent_ppo2.py:185][0m |           0.0354 |         103.4668 |         -94.1467 |
[32m[20221214 00:31:36 @agent_ppo2.py:185][0m |          -0.0046 |          87.0601 |         -93.6245 |
[32m[20221214 00:31:36 @agent_ppo2.py:185][0m |          -0.0054 |          71.5770 |         -93.5181 |
[32m[20221214 00:31:37 @agent_ppo2.py:185][0m |          -0.0068 |          70.8613 |         -93.0791 |
[32m[20221214 00:31:37 @agent_ppo2.py:185][0m |          -0.0069 |          69.2365 |         -93.5519 |
[32m[20221214 00:31:37 @agent_ppo2.py:185][0m |          -0.0101 |          69.2491 |         -93.3535 |
[32m[20221214 00:31:37 @agent_ppo2.py:185][0m |          -0.0084 |          68.3264 |         -93.2572 |
[32m[20221214 00:31:37 @agent_ppo2.py:185][0m |          -0.0150 |          68.1539 |         -93.3215 |
[32m[20221214 00:31:37 @agent_ppo2.py:185][0m |          -0.0161 |          66.9439 |         -93.0942 |
[32m[20221214 00:31:37 @agent_ppo2.py:185][0m |           0.0004 |          74.0154 |         -93.0197 |
[32m[20221214 00:31:37 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221214 00:31:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 517.83
[32m[20221214 00:31:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 635.00
[32m[20221214 00:31:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 745.98
[32m[20221214 00:31:37 @agent_ppo2.py:143][0m Total time:      34.12 min
[32m[20221214 00:31:37 @agent_ppo2.py:145][0m 3096576 total steps have happened
[32m[20221214 00:31:37 @agent_ppo2.py:121][0m #------------------------ Iteration 5512 --------------------------#
[32m[20221214 00:31:38 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:31:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:31:38 @agent_ppo2.py:185][0m |          -0.0012 |          62.9391 |         -94.5700 |
[32m[20221214 00:31:38 @agent_ppo2.py:185][0m |          -0.0022 |          52.4703 |         -95.0791 |
[32m[20221214 00:31:38 @agent_ppo2.py:185][0m |          -0.0104 |          49.7938 |         -95.2343 |
[32m[20221214 00:31:38 @agent_ppo2.py:185][0m |          -0.0091 |          48.0070 |         -95.1793 |
[32m[20221214 00:31:38 @agent_ppo2.py:185][0m |          -0.0096 |          47.0243 |         -95.1126 |
[32m[20221214 00:31:38 @agent_ppo2.py:185][0m |          -0.0145 |          46.0822 |         -95.4906 |
[32m[20221214 00:31:38 @agent_ppo2.py:185][0m |          -0.0175 |          45.0375 |         -95.2962 |
[32m[20221214 00:31:39 @agent_ppo2.py:185][0m |          -0.0069 |          44.8945 |         -95.2598 |
[32m[20221214 00:31:39 @agent_ppo2.py:185][0m |          -0.0129 |          44.5109 |         -95.3625 |
[32m[20221214 00:31:39 @agent_ppo2.py:185][0m |          -0.0170 |          43.6301 |         -95.2219 |
[32m[20221214 00:31:39 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221214 00:31:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 511.62
[32m[20221214 00:31:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 587.92
[32m[20221214 00:31:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 698.27
[32m[20221214 00:31:39 @agent_ppo2.py:143][0m Total time:      34.14 min
[32m[20221214 00:31:39 @agent_ppo2.py:145][0m 3098624 total steps have happened
[32m[20221214 00:31:39 @agent_ppo2.py:121][0m #------------------------ Iteration 5513 --------------------------#
[32m[20221214 00:31:39 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:31:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:31:39 @agent_ppo2.py:185][0m |           0.0017 |         101.6381 |         -97.4921 |
[32m[20221214 00:31:39 @agent_ppo2.py:185][0m |          -0.0015 |          73.9574 |         -96.9497 |
[32m[20221214 00:31:39 @agent_ppo2.py:185][0m |          -0.0073 |          66.5302 |         -96.7121 |
[32m[20221214 00:31:40 @agent_ppo2.py:185][0m |          -0.0085 |          63.9988 |         -96.2613 |
[32m[20221214 00:31:40 @agent_ppo2.py:185][0m |          -0.0099 |          62.0476 |         -96.7562 |
[32m[20221214 00:31:40 @agent_ppo2.py:185][0m |          -0.0144 |          60.8374 |         -95.9439 |
[32m[20221214 00:31:40 @agent_ppo2.py:185][0m |          -0.0140 |          60.4737 |         -96.4309 |
[32m[20221214 00:31:40 @agent_ppo2.py:185][0m |          -0.0109 |          59.3951 |         -95.5900 |
[32m[20221214 00:31:40 @agent_ppo2.py:185][0m |          -0.0157 |          58.0066 |         -96.2725 |
[32m[20221214 00:31:40 @agent_ppo2.py:185][0m |          -0.0087 |          62.7230 |         -96.1182 |
[32m[20221214 00:31:40 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221214 00:31:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 626.54
[32m[20221214 00:31:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 768.02
[32m[20221214 00:31:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 631.92
[32m[20221214 00:31:40 @agent_ppo2.py:143][0m Total time:      34.17 min
[32m[20221214 00:31:40 @agent_ppo2.py:145][0m 3100672 total steps have happened
[32m[20221214 00:31:40 @agent_ppo2.py:121][0m #------------------------ Iteration 5514 --------------------------#
[32m[20221214 00:31:41 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:31:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:31:41 @agent_ppo2.py:185][0m |          -0.0011 |          85.0985 |         -98.1778 |
[32m[20221214 00:31:41 @agent_ppo2.py:185][0m |          -0.0063 |          65.6327 |         -98.1232 |
[32m[20221214 00:31:41 @agent_ppo2.py:185][0m |          -0.0065 |          60.5388 |         -98.4976 |
[32m[20221214 00:31:41 @agent_ppo2.py:185][0m |          -0.0098 |          59.8068 |         -97.8235 |
[32m[20221214 00:31:41 @agent_ppo2.py:185][0m |          -0.0083 |          56.9305 |         -97.8040 |
[32m[20221214 00:31:41 @agent_ppo2.py:185][0m |          -0.0113 |          60.0867 |         -97.5919 |
[32m[20221214 00:31:41 @agent_ppo2.py:185][0m |          -0.0149 |          57.2897 |         -97.4017 |
[32m[20221214 00:31:41 @agent_ppo2.py:185][0m |          -0.0101 |          54.2196 |         -97.4571 |
[32m[20221214 00:31:42 @agent_ppo2.py:185][0m |          -0.0149 |          52.8069 |         -97.4953 |
[32m[20221214 00:31:42 @agent_ppo2.py:185][0m |          -0.0213 |          53.7013 |         -97.4296 |
[32m[20221214 00:31:42 @agent_ppo2.py:130][0m Policy update time: 1.17 s
[32m[20221214 00:31:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 573.85
[32m[20221214 00:31:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 684.39
[32m[20221214 00:31:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 645.99
[32m[20221214 00:31:42 @agent_ppo2.py:143][0m Total time:      34.19 min
[32m[20221214 00:31:42 @agent_ppo2.py:145][0m 3102720 total steps have happened
[32m[20221214 00:31:42 @agent_ppo2.py:121][0m #------------------------ Iteration 5515 --------------------------#
[32m[20221214 00:31:42 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:31:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:31:42 @agent_ppo2.py:185][0m |           0.0006 |         101.2841 |         -99.2929 |
[32m[20221214 00:31:42 @agent_ppo2.py:185][0m |          -0.0055 |          95.9157 |         -98.5493 |
[32m[20221214 00:31:42 @agent_ppo2.py:185][0m |          -0.0013 |          94.4649 |         -98.4320 |
[32m[20221214 00:31:43 @agent_ppo2.py:185][0m |          -0.0019 |          93.0259 |         -98.4920 |
[32m[20221214 00:31:43 @agent_ppo2.py:185][0m |          -0.0041 |          91.7059 |         -98.4895 |
[32m[20221214 00:31:43 @agent_ppo2.py:185][0m |          -0.0040 |          90.7237 |         -98.1310 |
[32m[20221214 00:31:43 @agent_ppo2.py:185][0m |          -0.0116 |          90.5386 |         -97.4453 |
[32m[20221214 00:31:43 @agent_ppo2.py:185][0m |          -0.0127 |          89.8154 |         -97.6163 |
[32m[20221214 00:31:43 @agent_ppo2.py:185][0m |          -0.0098 |          89.3109 |         -97.7154 |
[32m[20221214 00:31:43 @agent_ppo2.py:185][0m |          -0.0104 |          88.7200 |         -97.7340 |
[32m[20221214 00:31:43 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221214 00:31:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 555.90
[32m[20221214 00:31:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 667.46
[32m[20221214 00:31:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 116.51
[32m[20221214 00:31:43 @agent_ppo2.py:143][0m Total time:      34.22 min
[32m[20221214 00:31:43 @agent_ppo2.py:145][0m 3104768 total steps have happened
[32m[20221214 00:31:43 @agent_ppo2.py:121][0m #------------------------ Iteration 5516 --------------------------#
[32m[20221214 00:31:43 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:31:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:31:44 @agent_ppo2.py:185][0m |          -0.0014 |          76.2798 |         -92.8654 |
[32m[20221214 00:31:44 @agent_ppo2.py:185][0m |          -0.0029 |          70.0902 |         -92.7616 |
[32m[20221214 00:31:44 @agent_ppo2.py:185][0m |           0.0014 |          70.1472 |         -92.6435 |
[32m[20221214 00:31:44 @agent_ppo2.py:185][0m |          -0.0164 |          66.8589 |         -92.3507 |
[32m[20221214 00:31:44 @agent_ppo2.py:185][0m |          -0.0104 |          66.6802 |         -92.6343 |
[32m[20221214 00:31:44 @agent_ppo2.py:185][0m |          -0.0124 |          65.8166 |         -92.8436 |
[32m[20221214 00:31:44 @agent_ppo2.py:185][0m |          -0.0119 |          64.9977 |         -92.9113 |
[32m[20221214 00:31:44 @agent_ppo2.py:185][0m |          -0.0080 |          64.8441 |         -93.2759 |
[32m[20221214 00:31:45 @agent_ppo2.py:185][0m |          -0.0170 |          64.6128 |         -93.1036 |
[32m[20221214 00:31:45 @agent_ppo2.py:185][0m |          -0.0103 |          63.9508 |         -93.0948 |
[32m[20221214 00:31:45 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221214 00:31:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 523.28
[32m[20221214 00:31:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 561.94
[32m[20221214 00:31:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 795.04
[32m[20221214 00:31:45 @agent_ppo2.py:143][0m Total time:      34.24 min
[32m[20221214 00:31:45 @agent_ppo2.py:145][0m 3106816 total steps have happened
[32m[20221214 00:31:45 @agent_ppo2.py:121][0m #------------------------ Iteration 5517 --------------------------#
[32m[20221214 00:31:45 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:31:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:31:45 @agent_ppo2.py:185][0m |           0.0058 |          87.6734 |         -92.9366 |
[32m[20221214 00:31:45 @agent_ppo2.py:185][0m |          -0.0021 |          81.3283 |         -92.7107 |
[32m[20221214 00:31:45 @agent_ppo2.py:185][0m |           0.0103 |          86.5954 |         -92.7275 |
[32m[20221214 00:31:45 @agent_ppo2.py:185][0m |          -0.0099 |          76.5569 |         -92.7151 |
[32m[20221214 00:31:46 @agent_ppo2.py:185][0m |          -0.0090 |          75.3270 |         -93.0440 |
[32m[20221214 00:31:46 @agent_ppo2.py:185][0m |          -0.0114 |          74.2497 |         -93.2327 |
[32m[20221214 00:31:46 @agent_ppo2.py:185][0m |          -0.0128 |          73.4536 |         -93.0225 |
[32m[20221214 00:31:46 @agent_ppo2.py:185][0m |          -0.0146 |          72.8634 |         -93.1028 |
[32m[20221214 00:31:46 @agent_ppo2.py:185][0m |          -0.0118 |          72.7642 |         -92.9087 |
[32m[20221214 00:31:46 @agent_ppo2.py:185][0m |          -0.0133 |          72.3099 |         -93.1660 |
[32m[20221214 00:31:46 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221214 00:31:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 602.09
[32m[20221214 00:31:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 651.14
[32m[20221214 00:31:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 760.48
[32m[20221214 00:31:46 @agent_ppo2.py:143][0m Total time:      34.26 min
[32m[20221214 00:31:46 @agent_ppo2.py:145][0m 3108864 total steps have happened
[32m[20221214 00:31:46 @agent_ppo2.py:121][0m #------------------------ Iteration 5518 --------------------------#
[32m[20221214 00:31:46 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:31:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:31:47 @agent_ppo2.py:185][0m |           0.0101 |         109.3999 |         -95.9454 |
[32m[20221214 00:31:47 @agent_ppo2.py:185][0m |          -0.0009 |          94.6240 |         -96.0392 |
[32m[20221214 00:31:47 @agent_ppo2.py:185][0m |           0.0043 |          99.8114 |         -96.0792 |
[32m[20221214 00:31:47 @agent_ppo2.py:185][0m |          -0.0038 |          93.7687 |         -96.9125 |
[32m[20221214 00:31:47 @agent_ppo2.py:185][0m |          -0.0022 |          87.2851 |         -96.4018 |
[32m[20221214 00:31:47 @agent_ppo2.py:185][0m |          -0.0015 |          91.0480 |         -96.5955 |
[32m[20221214 00:31:47 @agent_ppo2.py:185][0m |          -0.0083 |          86.0179 |         -96.8297 |
[32m[20221214 00:31:47 @agent_ppo2.py:185][0m |          -0.0081 |          85.3936 |         -97.0188 |
[32m[20221214 00:31:47 @agent_ppo2.py:185][0m |          -0.0042 |          84.1127 |         -96.8031 |
[32m[20221214 00:31:48 @agent_ppo2.py:185][0m |          -0.0096 |          83.7590 |         -97.7777 |
[32m[20221214 00:31:48 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221214 00:31:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 656.47
[32m[20221214 00:31:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 701.71
[32m[20221214 00:31:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 675.62
[32m[20221214 00:31:48 @agent_ppo2.py:143][0m Total time:      34.29 min
[32m[20221214 00:31:48 @agent_ppo2.py:145][0m 3110912 total steps have happened
[32m[20221214 00:31:48 @agent_ppo2.py:121][0m #------------------------ Iteration 5519 --------------------------#
[32m[20221214 00:31:48 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:31:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:31:48 @agent_ppo2.py:185][0m |           0.0033 |          76.1567 |         -99.9725 |
[32m[20221214 00:31:48 @agent_ppo2.py:185][0m |          -0.0057 |          61.1910 |         -99.8579 |
[32m[20221214 00:31:48 @agent_ppo2.py:185][0m |          -0.0109 |          55.3808 |         -99.5810 |
[32m[20221214 00:31:48 @agent_ppo2.py:185][0m |          -0.0106 |          53.0717 |        -100.2064 |
[32m[20221214 00:31:49 @agent_ppo2.py:185][0m |          -0.0147 |          52.1823 |         -99.3130 |
[32m[20221214 00:31:49 @agent_ppo2.py:185][0m |          -0.0128 |          51.2255 |         -99.2315 |
[32m[20221214 00:31:49 @agent_ppo2.py:185][0m |          -0.0171 |          50.7378 |         -99.2365 |
[32m[20221214 00:31:49 @agent_ppo2.py:185][0m |          -0.0053 |          53.9023 |         -99.0640 |
[32m[20221214 00:31:49 @agent_ppo2.py:185][0m |          -0.0169 |          50.0394 |         -99.1418 |
[32m[20221214 00:31:49 @agent_ppo2.py:185][0m |          -0.0179 |          49.1077 |         -99.0218 |
[32m[20221214 00:31:49 @agent_ppo2.py:130][0m Policy update time: 1.17 s
[32m[20221214 00:31:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 596.36
[32m[20221214 00:31:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 694.82
[32m[20221214 00:31:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 764.20
[32m[20221214 00:31:49 @agent_ppo2.py:143][0m Total time:      34.31 min
[32m[20221214 00:31:49 @agent_ppo2.py:145][0m 3112960 total steps have happened
[32m[20221214 00:31:49 @agent_ppo2.py:121][0m #------------------------ Iteration 5520 --------------------------#
[32m[20221214 00:31:49 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 00:31:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:31:50 @agent_ppo2.py:185][0m |           0.0024 |          80.0642 |         -98.3219 |
[32m[20221214 00:31:50 @agent_ppo2.py:185][0m |          -0.0014 |          66.1865 |         -98.3240 |
[32m[20221214 00:31:50 @agent_ppo2.py:185][0m |          -0.0128 |          60.8022 |         -98.2504 |
[32m[20221214 00:31:50 @agent_ppo2.py:185][0m |          -0.0096 |          58.3935 |         -97.9370 |
[32m[20221214 00:31:50 @agent_ppo2.py:185][0m |          -0.0087 |          56.7066 |         -98.4175 |
[32m[20221214 00:31:50 @agent_ppo2.py:185][0m |          -0.0134 |          56.1031 |         -98.3628 |
[32m[20221214 00:31:50 @agent_ppo2.py:185][0m |          -0.0101 |          54.9809 |         -98.2401 |
[32m[20221214 00:31:50 @agent_ppo2.py:185][0m |          -0.0142 |          53.8062 |         -98.5007 |
[32m[20221214 00:31:50 @agent_ppo2.py:185][0m |          -0.0106 |          53.3979 |         -98.5001 |
[32m[20221214 00:31:51 @agent_ppo2.py:185][0m |          -0.0150 |          52.7268 |         -98.4808 |
[32m[20221214 00:31:51 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221214 00:31:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 495.11
[32m[20221214 00:31:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 602.43
[32m[20221214 00:31:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 549.24
[32m[20221214 00:31:51 @agent_ppo2.py:143][0m Total time:      34.34 min
[32m[20221214 00:31:51 @agent_ppo2.py:145][0m 3115008 total steps have happened
[32m[20221214 00:31:51 @agent_ppo2.py:121][0m #------------------------ Iteration 5521 --------------------------#
[32m[20221214 00:31:51 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:31:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:31:51 @agent_ppo2.py:185][0m |          -0.0041 |          53.3942 |        -100.8716 |
[32m[20221214 00:31:51 @agent_ppo2.py:185][0m |          -0.0043 |          44.1357 |        -100.4512 |
[32m[20221214 00:31:51 @agent_ppo2.py:185][0m |          -0.0082 |          40.6971 |        -100.3319 |
[32m[20221214 00:31:51 @agent_ppo2.py:185][0m |          -0.0086 |          39.4803 |        -100.0101 |
[32m[20221214 00:31:52 @agent_ppo2.py:185][0m |          -0.0093 |          39.2568 |        -100.5402 |
[32m[20221214 00:31:52 @agent_ppo2.py:185][0m |          -0.0133 |          40.1484 |         -99.8906 |
[32m[20221214 00:31:52 @agent_ppo2.py:185][0m |          -0.0085 |          37.6285 |         -99.6453 |
[32m[20221214 00:31:52 @agent_ppo2.py:185][0m |          -0.0164 |          37.8009 |         -99.6308 |
[32m[20221214 00:31:52 @agent_ppo2.py:185][0m |          -0.0193 |          37.4432 |         -99.6837 |
[32m[20221214 00:31:52 @agent_ppo2.py:185][0m |          -0.0166 |          36.4711 |         -99.2449 |
[32m[20221214 00:31:52 @agent_ppo2.py:130][0m Policy update time: 1.17 s
[32m[20221214 00:31:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 621.89
[32m[20221214 00:31:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 716.43
[32m[20221214 00:31:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 719.72
[32m[20221214 00:31:52 @agent_ppo2.py:143][0m Total time:      34.36 min
[32m[20221214 00:31:52 @agent_ppo2.py:145][0m 3117056 total steps have happened
[32m[20221214 00:31:52 @agent_ppo2.py:121][0m #------------------------ Iteration 5522 --------------------------#
[32m[20221214 00:31:52 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:31:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:31:53 @agent_ppo2.py:185][0m |           0.0123 |         127.2274 |         -98.8668 |
[32m[20221214 00:31:53 @agent_ppo2.py:185][0m |           0.0062 |         124.7466 |         -98.9000 |
[32m[20221214 00:31:53 @agent_ppo2.py:185][0m |          -0.0020 |         119.4487 |         -98.8873 |
[32m[20221214 00:31:53 @agent_ppo2.py:185][0m |          -0.0029 |         119.7515 |         -99.4408 |
[32m[20221214 00:31:53 @agent_ppo2.py:185][0m |          -0.0025 |         119.3417 |         -98.9614 |
[32m[20221214 00:31:53 @agent_ppo2.py:185][0m |           0.0163 |         123.2746 |        -100.1277 |
[32m[20221214 00:31:53 @agent_ppo2.py:185][0m |          -0.0028 |         119.3594 |         -99.8788 |
[32m[20221214 00:31:53 @agent_ppo2.py:185][0m |          -0.0021 |         118.0108 |        -100.4545 |
[32m[20221214 00:31:53 @agent_ppo2.py:185][0m |          -0.0043 |         117.5856 |         -99.7027 |
[32m[20221214 00:31:54 @agent_ppo2.py:185][0m |          -0.0026 |         117.4787 |        -100.7420 |
[32m[20221214 00:31:54 @agent_ppo2.py:130][0m Policy update time: 1.18 s
[32m[20221214 00:31:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 811.79
[32m[20221214 00:31:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.52
[32m[20221214 00:31:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 760.08
[32m[20221214 00:31:54 @agent_ppo2.py:143][0m Total time:      34.39 min
[32m[20221214 00:31:54 @agent_ppo2.py:145][0m 3119104 total steps have happened
[32m[20221214 00:31:54 @agent_ppo2.py:121][0m #------------------------ Iteration 5523 --------------------------#
[32m[20221214 00:31:54 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:31:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:31:54 @agent_ppo2.py:185][0m |           0.0011 |          98.3957 |        -102.1409 |
[32m[20221214 00:31:54 @agent_ppo2.py:185][0m |          -0.0009 |          91.7676 |        -101.7138 |
[32m[20221214 00:31:54 @agent_ppo2.py:185][0m |          -0.0044 |          89.5755 |        -101.5280 |
[32m[20221214 00:31:54 @agent_ppo2.py:185][0m |          -0.0063 |          88.8094 |        -101.3958 |
[32m[20221214 00:31:55 @agent_ppo2.py:185][0m |          -0.0094 |          87.1578 |        -101.5736 |
[32m[20221214 00:31:55 @agent_ppo2.py:185][0m |          -0.0055 |          86.2455 |        -101.4739 |
[32m[20221214 00:31:55 @agent_ppo2.py:185][0m |          -0.0066 |          85.8840 |        -101.3107 |
[32m[20221214 00:31:55 @agent_ppo2.py:185][0m |          -0.0106 |          84.8629 |        -101.4462 |
[32m[20221214 00:31:55 @agent_ppo2.py:185][0m |          -0.0118 |          84.3967 |        -101.0352 |
[32m[20221214 00:31:55 @agent_ppo2.py:185][0m |          -0.0073 |          84.9821 |        -100.6987 |
[32m[20221214 00:31:55 @agent_ppo2.py:130][0m Policy update time: 1.17 s
[32m[20221214 00:31:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 512.89
[32m[20221214 00:31:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 655.62
[32m[20221214 00:31:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 758.14
[32m[20221214 00:31:55 @agent_ppo2.py:143][0m Total time:      34.42 min
[32m[20221214 00:31:55 @agent_ppo2.py:145][0m 3121152 total steps have happened
[32m[20221214 00:31:55 @agent_ppo2.py:121][0m #------------------------ Iteration 5524 --------------------------#
[32m[20221214 00:31:55 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:31:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:31:56 @agent_ppo2.py:185][0m |          -0.0003 |          51.6321 |         -99.4073 |
[32m[20221214 00:31:56 @agent_ppo2.py:185][0m |          -0.0064 |          43.2203 |         -99.3326 |
[32m[20221214 00:31:56 @agent_ppo2.py:185][0m |          -0.0087 |          40.7782 |         -99.5003 |
[32m[20221214 00:31:56 @agent_ppo2.py:185][0m |          -0.0180 |          39.9243 |         -99.1692 |
[32m[20221214 00:31:56 @agent_ppo2.py:185][0m |          -0.0123 |          39.7089 |         -98.7265 |
[32m[20221214 00:31:56 @agent_ppo2.py:185][0m |          -0.0041 |          38.5706 |         -98.9120 |
[32m[20221214 00:31:56 @agent_ppo2.py:185][0m |          -0.0127 |          37.9586 |         -98.6776 |
[32m[20221214 00:31:56 @agent_ppo2.py:185][0m |          -0.0150 |          37.8090 |         -98.6356 |
[32m[20221214 00:31:57 @agent_ppo2.py:185][0m |          -0.0192 |          37.2673 |         -98.2893 |
[32m[20221214 00:31:57 @agent_ppo2.py:185][0m |          -0.0164 |          36.9002 |         -98.3825 |
[32m[20221214 00:31:57 @agent_ppo2.py:130][0m Policy update time: 1.20 s
[32m[20221214 00:31:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 513.91
[32m[20221214 00:31:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 580.00
[32m[20221214 00:31:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 766.69
[32m[20221214 00:31:57 @agent_ppo2.py:143][0m Total time:      34.44 min
[32m[20221214 00:31:57 @agent_ppo2.py:145][0m 3123200 total steps have happened
[32m[20221214 00:31:57 @agent_ppo2.py:121][0m #------------------------ Iteration 5525 --------------------------#
[32m[20221214 00:31:57 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:31:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:31:57 @agent_ppo2.py:185][0m |           0.0106 |          68.2112 |         -95.2929 |
[32m[20221214 00:31:57 @agent_ppo2.py:185][0m |           0.0141 |          61.9285 |         -95.1453 |
[32m[20221214 00:31:57 @agent_ppo2.py:185][0m |          -0.0106 |          53.1225 |         -95.1694 |
[32m[20221214 00:31:58 @agent_ppo2.py:185][0m |          -0.0053 |          52.7005 |         -95.4332 |
[32m[20221214 00:31:58 @agent_ppo2.py:185][0m |          -0.0104 |          50.4756 |         -95.2588 |
[32m[20221214 00:31:58 @agent_ppo2.py:185][0m |          -0.0139 |          48.8520 |         -95.3938 |
[32m[20221214 00:31:58 @agent_ppo2.py:185][0m |          -0.0135 |          47.1971 |         -95.1806 |
[32m[20221214 00:31:58 @agent_ppo2.py:185][0m |          -0.0114 |          48.9840 |         -95.7945 |
[32m[20221214 00:31:58 @agent_ppo2.py:185][0m |          -0.0123 |          46.6833 |         -95.2399 |
[32m[20221214 00:31:58 @agent_ppo2.py:185][0m |          -0.0209 |          44.6329 |         -95.6551 |
[32m[20221214 00:31:58 @agent_ppo2.py:130][0m Policy update time: 1.18 s
[32m[20221214 00:31:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 636.84
[32m[20221214 00:31:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 679.71
[32m[20221214 00:31:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 420.54
[32m[20221214 00:31:58 @agent_ppo2.py:143][0m Total time:      34.47 min
[32m[20221214 00:31:58 @agent_ppo2.py:145][0m 3125248 total steps have happened
[32m[20221214 00:31:58 @agent_ppo2.py:121][0m #------------------------ Iteration 5526 --------------------------#
[32m[20221214 00:31:59 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:31:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:31:59 @agent_ppo2.py:185][0m |           0.0006 |         148.7739 |        -100.2378 |
[32m[20221214 00:31:59 @agent_ppo2.py:185][0m |           0.0156 |         160.3305 |        -100.5298 |
[32m[20221214 00:31:59 @agent_ppo2.py:185][0m |           0.0034 |         141.7922 |        -100.2766 |
[32m[20221214 00:31:59 @agent_ppo2.py:185][0m |          -0.0034 |         140.9597 |        -100.5720 |
[32m[20221214 00:31:59 @agent_ppo2.py:185][0m |          -0.0026 |         141.0508 |        -100.6061 |
[32m[20221214 00:31:59 @agent_ppo2.py:185][0m |           0.0005 |         139.2472 |        -100.9662 |
[32m[20221214 00:31:59 @agent_ppo2.py:185][0m |          -0.0023 |         138.8608 |        -100.9197 |
[32m[20221214 00:31:59 @agent_ppo2.py:185][0m |          -0.0038 |         138.6211 |        -101.0452 |
[32m[20221214 00:32:00 @agent_ppo2.py:185][0m |           0.0035 |         146.3156 |        -100.8861 |
[32m[20221214 00:32:00 @agent_ppo2.py:185][0m |          -0.0010 |         138.2696 |        -100.4615 |
[32m[20221214 00:32:00 @agent_ppo2.py:130][0m Policy update time: 1.19 s
[32m[20221214 00:32:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 719.06
[32m[20221214 00:32:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 782.61
[32m[20221214 00:32:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 640.21
[32m[20221214 00:32:00 @agent_ppo2.py:143][0m Total time:      34.49 min
[32m[20221214 00:32:00 @agent_ppo2.py:145][0m 3127296 total steps have happened
[32m[20221214 00:32:00 @agent_ppo2.py:121][0m #------------------------ Iteration 5527 --------------------------#
[32m[20221214 00:32:00 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:32:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:32:00 @agent_ppo2.py:185][0m |          -0.0020 |          67.4457 |        -105.3368 |
[32m[20221214 00:32:00 @agent_ppo2.py:185][0m |          -0.0030 |          55.3273 |        -105.4999 |
[32m[20221214 00:32:00 @agent_ppo2.py:185][0m |          -0.0114 |          52.0812 |        -105.0577 |
[32m[20221214 00:32:01 @agent_ppo2.py:185][0m |          -0.0034 |          52.6450 |        -104.8864 |
[32m[20221214 00:32:01 @agent_ppo2.py:185][0m |          -0.0244 |          49.3803 |        -104.6174 |
[32m[20221214 00:32:01 @agent_ppo2.py:185][0m |          -0.0135 |          48.3691 |        -104.4533 |
[32m[20221214 00:32:01 @agent_ppo2.py:185][0m |          -0.0151 |          48.1993 |        -104.0705 |
[32m[20221214 00:32:01 @agent_ppo2.py:185][0m |          -0.0153 |          47.9927 |        -103.4745 |
[32m[20221214 00:32:01 @agent_ppo2.py:185][0m |          -0.0196 |          46.8166 |        -103.7316 |
[32m[20221214 00:32:01 @agent_ppo2.py:185][0m |          -0.0167 |          47.6898 |        -103.1953 |
[32m[20221214 00:32:01 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221214 00:32:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 515.40
[32m[20221214 00:32:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 637.76
[32m[20221214 00:32:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 588.10
[32m[20221214 00:32:01 @agent_ppo2.py:143][0m Total time:      34.52 min
[32m[20221214 00:32:01 @agent_ppo2.py:145][0m 3129344 total steps have happened
[32m[20221214 00:32:01 @agent_ppo2.py:121][0m #------------------------ Iteration 5528 --------------------------#
[32m[20221214 00:32:01 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:32:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:32:02 @agent_ppo2.py:185][0m |           0.0016 |         127.5578 |        -101.1448 |
[32m[20221214 00:32:02 @agent_ppo2.py:185][0m |          -0.0028 |         120.2783 |        -100.9787 |
[32m[20221214 00:32:02 @agent_ppo2.py:185][0m |          -0.0035 |         117.3416 |        -100.5738 |
[32m[20221214 00:32:02 @agent_ppo2.py:185][0m |          -0.0005 |         117.6758 |        -100.7523 |
[32m[20221214 00:32:02 @agent_ppo2.py:185][0m |          -0.0028 |         114.9814 |        -100.2915 |
[32m[20221214 00:32:02 @agent_ppo2.py:185][0m |          -0.0107 |         113.1606 |        -100.6712 |
[32m[20221214 00:32:02 @agent_ppo2.py:185][0m |          -0.0097 |         113.1069 |        -100.7083 |
[32m[20221214 00:32:02 @agent_ppo2.py:185][0m |          -0.0130 |         112.3436 |        -100.4288 |
[32m[20221214 00:32:03 @agent_ppo2.py:185][0m |           0.0076 |         124.5940 |        -100.5626 |
[32m[20221214 00:32:03 @agent_ppo2.py:185][0m |          -0.0127 |         110.5215 |        -100.6145 |
[32m[20221214 00:32:03 @agent_ppo2.py:130][0m Policy update time: 1.19 s
[32m[20221214 00:32:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 602.77
[32m[20221214 00:32:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 797.64
[32m[20221214 00:32:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 678.02
[32m[20221214 00:32:03 @agent_ppo2.py:143][0m Total time:      34.54 min
[32m[20221214 00:32:03 @agent_ppo2.py:145][0m 3131392 total steps have happened
[32m[20221214 00:32:03 @agent_ppo2.py:121][0m #------------------------ Iteration 5529 --------------------------#
[32m[20221214 00:32:03 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:32:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:32:03 @agent_ppo2.py:185][0m |          -0.0027 |          98.4888 |        -101.8540 |
[32m[20221214 00:32:03 @agent_ppo2.py:185][0m |          -0.0006 |          89.2436 |        -101.8337 |
[32m[20221214 00:32:03 @agent_ppo2.py:185][0m |          -0.0042 |          84.9200 |        -101.8979 |
[32m[20221214 00:32:04 @agent_ppo2.py:185][0m |          -0.0042 |          86.1460 |        -102.2785 |
[32m[20221214 00:32:04 @agent_ppo2.py:185][0m |          -0.0111 |          82.4945 |        -102.5033 |
[32m[20221214 00:32:04 @agent_ppo2.py:185][0m |          -0.0116 |          80.8575 |        -102.4412 |
[32m[20221214 00:32:04 @agent_ppo2.py:185][0m |          -0.0126 |          79.7523 |        -103.0428 |
[32m[20221214 00:32:04 @agent_ppo2.py:185][0m |          -0.0113 |          79.8872 |        -103.3595 |
[32m[20221214 00:32:04 @agent_ppo2.py:185][0m |          -0.0078 |          81.6366 |        -103.0695 |
[32m[20221214 00:32:04 @agent_ppo2.py:185][0m |          -0.0088 |          78.8928 |        -103.0241 |
[32m[20221214 00:32:04 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221214 00:32:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 608.31
[32m[20221214 00:32:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 750.61
[32m[20221214 00:32:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 782.21
[32m[20221214 00:32:04 @agent_ppo2.py:143][0m Total time:      34.57 min
[32m[20221214 00:32:04 @agent_ppo2.py:145][0m 3133440 total steps have happened
[32m[20221214 00:32:04 @agent_ppo2.py:121][0m #------------------------ Iteration 5530 --------------------------#
[32m[20221214 00:32:05 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:32:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:32:05 @agent_ppo2.py:185][0m |           0.0001 |         128.0458 |        -100.8959 |
[32m[20221214 00:32:05 @agent_ppo2.py:185][0m |           0.0181 |         139.9990 |         -99.9005 |
[32m[20221214 00:32:05 @agent_ppo2.py:185][0m |          -0.0012 |         123.7948 |        -100.4410 |
[32m[20221214 00:32:05 @agent_ppo2.py:185][0m |           0.0089 |         123.8332 |        -100.8513 |
[32m[20221214 00:32:05 @agent_ppo2.py:185][0m |           0.0128 |         131.6050 |        -100.9065 |
[32m[20221214 00:32:05 @agent_ppo2.py:185][0m |           0.0033 |         126.0621 |        -100.4808 |
[32m[20221214 00:32:05 @agent_ppo2.py:185][0m |          -0.0025 |         121.2478 |        -101.3159 |
[32m[20221214 00:32:05 @agent_ppo2.py:185][0m |          -0.0031 |         119.2443 |        -101.6406 |
[32m[20221214 00:32:06 @agent_ppo2.py:185][0m |          -0.0067 |         119.1347 |        -100.6524 |
[32m[20221214 00:32:06 @agent_ppo2.py:185][0m |           0.0041 |         137.2119 |        -101.1622 |
[32m[20221214 00:32:06 @agent_ppo2.py:130][0m Policy update time: 1.18 s
[32m[20221214 00:32:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 741.78
[32m[20221214 00:32:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 804.85
[32m[20221214 00:32:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 737.85
[32m[20221214 00:32:06 @agent_ppo2.py:143][0m Total time:      34.59 min
[32m[20221214 00:32:06 @agent_ppo2.py:145][0m 3135488 total steps have happened
[32m[20221214 00:32:06 @agent_ppo2.py:121][0m #------------------------ Iteration 5531 --------------------------#
[32m[20221214 00:32:06 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:32:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:32:06 @agent_ppo2.py:185][0m |           0.0005 |          64.1087 |        -103.4687 |
[32m[20221214 00:32:06 @agent_ppo2.py:185][0m |          -0.0013 |          56.7334 |        -103.3492 |
[32m[20221214 00:32:06 @agent_ppo2.py:185][0m |           0.0019 |          55.8066 |        -103.8716 |
[32m[20221214 00:32:07 @agent_ppo2.py:185][0m |          -0.0062 |          53.6620 |        -103.2142 |
[32m[20221214 00:32:07 @agent_ppo2.py:185][0m |          -0.0096 |          52.8873 |        -104.2389 |
[32m[20221214 00:32:07 @agent_ppo2.py:185][0m |          -0.0083 |          52.3292 |        -104.0241 |
[32m[20221214 00:32:07 @agent_ppo2.py:185][0m |          -0.0084 |          52.0947 |        -103.7835 |
[32m[20221214 00:32:07 @agent_ppo2.py:185][0m |          -0.0064 |          51.4931 |        -104.2662 |
[32m[20221214 00:32:07 @agent_ppo2.py:185][0m |          -0.0138 |          51.3685 |        -104.7772 |
[32m[20221214 00:32:07 @agent_ppo2.py:185][0m |          -0.0152 |          51.7242 |        -104.6680 |
[32m[20221214 00:32:07 @agent_ppo2.py:130][0m Policy update time: 1.22 s
[32m[20221214 00:32:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 588.13
[32m[20221214 00:32:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 719.25
[32m[20221214 00:32:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 588.48
[32m[20221214 00:32:07 @agent_ppo2.py:143][0m Total time:      34.62 min
[32m[20221214 00:32:07 @agent_ppo2.py:145][0m 3137536 total steps have happened
[32m[20221214 00:32:07 @agent_ppo2.py:121][0m #------------------------ Iteration 5532 --------------------------#
[32m[20221214 00:32:08 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:32:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:32:08 @agent_ppo2.py:185][0m |           0.0021 |          68.5554 |        -106.9885 |
[32m[20221214 00:32:08 @agent_ppo2.py:185][0m |          -0.0059 |          62.7943 |        -106.5509 |
[32m[20221214 00:32:08 @agent_ppo2.py:185][0m |          -0.0040 |          62.5937 |        -106.3606 |
[32m[20221214 00:32:08 @agent_ppo2.py:185][0m |          -0.0074 |          61.9308 |        -105.9080 |
[32m[20221214 00:32:08 @agent_ppo2.py:185][0m |          -0.0066 |          60.1873 |        -105.8433 |
[32m[20221214 00:32:08 @agent_ppo2.py:185][0m |          -0.0102 |          60.7010 |        -105.4602 |
[32m[20221214 00:32:08 @agent_ppo2.py:185][0m |          -0.0117 |          61.6531 |        -105.6346 |
[32m[20221214 00:32:09 @agent_ppo2.py:185][0m |          -0.0045 |          63.4705 |        -105.3480 |
[32m[20221214 00:32:09 @agent_ppo2.py:185][0m |          -0.0157 |          60.5302 |        -105.3631 |
[32m[20221214 00:32:09 @agent_ppo2.py:185][0m |          -0.0113 |          61.2693 |        -105.0305 |
[32m[20221214 00:32:09 @agent_ppo2.py:130][0m Policy update time: 1.17 s
[32m[20221214 00:32:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 687.92
[32m[20221214 00:32:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 724.61
[32m[20221214 00:32:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 688.67
[32m[20221214 00:32:09 @agent_ppo2.py:143][0m Total time:      34.64 min
[32m[20221214 00:32:09 @agent_ppo2.py:145][0m 3139584 total steps have happened
[32m[20221214 00:32:09 @agent_ppo2.py:121][0m #------------------------ Iteration 5533 --------------------------#
[32m[20221214 00:32:09 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:32:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:32:09 @agent_ppo2.py:185][0m |           0.0002 |          64.5755 |        -105.6972 |
[32m[20221214 00:32:09 @agent_ppo2.py:185][0m |          -0.0080 |          56.9610 |        -104.7912 |
[32m[20221214 00:32:10 @agent_ppo2.py:185][0m |          -0.0114 |          55.0866 |        -104.3871 |
[32m[20221214 00:32:10 @agent_ppo2.py:185][0m |          -0.0163 |          53.8310 |        -104.5065 |
[32m[20221214 00:32:10 @agent_ppo2.py:185][0m |          -0.0129 |          53.3147 |        -103.6698 |
[32m[20221214 00:32:10 @agent_ppo2.py:185][0m |          -0.0127 |          52.6472 |        -104.1294 |
[32m[20221214 00:32:10 @agent_ppo2.py:185][0m |          -0.0080 |          54.1697 |        -104.3288 |
[32m[20221214 00:32:10 @agent_ppo2.py:185][0m |          -0.0163 |          51.6178 |        -104.0402 |
[32m[20221214 00:32:10 @agent_ppo2.py:185][0m |          -0.0172 |          51.1214 |        -104.3888 |
[32m[20221214 00:32:10 @agent_ppo2.py:185][0m |          -0.0218 |          50.7592 |        -104.3449 |
[32m[20221214 00:32:10 @agent_ppo2.py:130][0m Policy update time: 1.19 s
[32m[20221214 00:32:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 510.50
[32m[20221214 00:32:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 621.54
[32m[20221214 00:32:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 847.43
[32m[20221214 00:32:10 @agent_ppo2.py:143][0m Total time:      34.67 min
[32m[20221214 00:32:10 @agent_ppo2.py:145][0m 3141632 total steps have happened
[32m[20221214 00:32:10 @agent_ppo2.py:121][0m #------------------------ Iteration 5534 --------------------------#
[32m[20221214 00:32:11 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:32:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:32:11 @agent_ppo2.py:185][0m |           0.0012 |          94.2396 |        -106.6269 |
[32m[20221214 00:32:11 @agent_ppo2.py:185][0m |          -0.0018 |          87.3110 |        -105.8088 |
[32m[20221214 00:32:11 @agent_ppo2.py:185][0m |          -0.0035 |          86.3095 |        -106.9372 |
[32m[20221214 00:32:11 @agent_ppo2.py:185][0m |           0.0008 |          85.2237 |        -105.9830 |
[32m[20221214 00:32:11 @agent_ppo2.py:185][0m |          -0.0071 |          83.8364 |        -106.8668 |
[32m[20221214 00:32:11 @agent_ppo2.py:185][0m |          -0.0041 |          83.3466 |        -106.2129 |
[32m[20221214 00:32:12 @agent_ppo2.py:185][0m |           0.0125 |          89.2781 |        -106.1316 |
[32m[20221214 00:32:12 @agent_ppo2.py:185][0m |          -0.0042 |          84.9667 |        -106.8205 |
[32m[20221214 00:32:12 @agent_ppo2.py:185][0m |          -0.0007 |          82.7193 |        -105.7011 |
[32m[20221214 00:32:12 @agent_ppo2.py:185][0m |          -0.0015 |          81.3512 |        -106.5538 |
[32m[20221214 00:32:12 @agent_ppo2.py:130][0m Policy update time: 1.19 s
[32m[20221214 00:32:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 630.06
[32m[20221214 00:32:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 745.37
[32m[20221214 00:32:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 591.30
[32m[20221214 00:32:12 @agent_ppo2.py:143][0m Total time:      34.69 min
[32m[20221214 00:32:12 @agent_ppo2.py:145][0m 3143680 total steps have happened
[32m[20221214 00:32:12 @agent_ppo2.py:121][0m #------------------------ Iteration 5535 --------------------------#
[32m[20221214 00:32:12 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:32:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:32:12 @agent_ppo2.py:185][0m |           0.0055 |         186.5561 |        -102.9668 |
[32m[20221214 00:32:12 @agent_ppo2.py:185][0m |           0.0005 |         182.5688 |        -102.5620 |
[32m[20221214 00:32:13 @agent_ppo2.py:185][0m |           0.0124 |         199.2737 |        -104.0578 |
[32m[20221214 00:32:13 @agent_ppo2.py:185][0m |          -0.0039 |         182.1490 |        -102.7806 |
[32m[20221214 00:32:13 @agent_ppo2.py:185][0m |          -0.0019 |         181.0950 |        -103.5716 |
[32m[20221214 00:32:13 @agent_ppo2.py:185][0m |          -0.0005 |         180.3583 |        -103.1309 |
[32m[20221214 00:32:13 @agent_ppo2.py:185][0m |          -0.0033 |         180.4247 |        -104.2395 |
[32m[20221214 00:32:13 @agent_ppo2.py:185][0m |          -0.0036 |         180.0570 |        -103.6067 |
[32m[20221214 00:32:13 @agent_ppo2.py:185][0m |          -0.0041 |         180.0216 |        -104.2021 |
[32m[20221214 00:32:13 @agent_ppo2.py:185][0m |          -0.0009 |         181.9457 |        -103.9242 |
[32m[20221214 00:32:13 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 00:32:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 781.32
[32m[20221214 00:32:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 836.90
[32m[20221214 00:32:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 641.52
[32m[20221214 00:32:13 @agent_ppo2.py:143][0m Total time:      34.72 min
[32m[20221214 00:32:13 @agent_ppo2.py:145][0m 3145728 total steps have happened
[32m[20221214 00:32:13 @agent_ppo2.py:121][0m #------------------------ Iteration 5536 --------------------------#
[32m[20221214 00:32:14 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:32:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:32:14 @agent_ppo2.py:185][0m |           0.0095 |          56.4788 |        -102.9768 |
[32m[20221214 00:32:14 @agent_ppo2.py:185][0m |           0.0007 |          53.1821 |        -102.5358 |
[32m[20221214 00:32:14 @agent_ppo2.py:185][0m |          -0.0093 |          51.1600 |        -102.9074 |
[32m[20221214 00:32:14 @agent_ppo2.py:185][0m |          -0.0022 |          50.7274 |        -102.5614 |
[32m[20221214 00:32:14 @agent_ppo2.py:185][0m |          -0.0073 |          49.7738 |        -103.0187 |
[32m[20221214 00:32:14 @agent_ppo2.py:185][0m |          -0.0082 |          48.7404 |        -102.6196 |
[32m[20221214 00:32:14 @agent_ppo2.py:185][0m |          -0.0059 |          48.6527 |        -102.2258 |
[32m[20221214 00:32:15 @agent_ppo2.py:185][0m |          -0.0152 |          47.8965 |        -102.0368 |
[32m[20221214 00:32:15 @agent_ppo2.py:185][0m |          -0.0137 |          48.1985 |        -102.3564 |
[32m[20221214 00:32:15 @agent_ppo2.py:185][0m |          -0.0127 |          48.0350 |        -102.8611 |
[32m[20221214 00:32:15 @agent_ppo2.py:130][0m Policy update time: 1.19 s
[32m[20221214 00:32:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 606.80
[32m[20221214 00:32:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 783.04
[32m[20221214 00:32:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 796.56
[32m[20221214 00:32:15 @agent_ppo2.py:143][0m Total time:      34.74 min
[32m[20221214 00:32:15 @agent_ppo2.py:145][0m 3147776 total steps have happened
[32m[20221214 00:32:15 @agent_ppo2.py:121][0m #------------------------ Iteration 5537 --------------------------#
[32m[20221214 00:32:15 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:32:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:32:15 @agent_ppo2.py:185][0m |           0.0017 |          86.6024 |        -103.8749 |
[32m[20221214 00:32:15 @agent_ppo2.py:185][0m |          -0.0013 |          80.4561 |        -103.5028 |
[32m[20221214 00:32:16 @agent_ppo2.py:185][0m |          -0.0106 |          77.4449 |        -103.5408 |
[32m[20221214 00:32:16 @agent_ppo2.py:185][0m |          -0.0056 |          75.9025 |        -102.7344 |
[32m[20221214 00:32:16 @agent_ppo2.py:185][0m |          -0.0109 |          73.9820 |        -103.5503 |
[32m[20221214 00:32:16 @agent_ppo2.py:185][0m |          -0.0095 |          72.8133 |        -102.8294 |
[32m[20221214 00:32:16 @agent_ppo2.py:185][0m |          -0.0113 |          71.6169 |        -103.1238 |
[32m[20221214 00:32:16 @agent_ppo2.py:185][0m |          -0.0135 |          70.9575 |        -102.8315 |
[32m[20221214 00:32:16 @agent_ppo2.py:185][0m |          -0.0134 |          71.7617 |        -102.8102 |
[32m[20221214 00:32:16 @agent_ppo2.py:185][0m |          -0.0147 |          70.2221 |        -101.9913 |
[32m[20221214 00:32:16 @agent_ppo2.py:130][0m Policy update time: 1.25 s
[32m[20221214 00:32:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 465.88
[32m[20221214 00:32:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 603.85
[32m[20221214 00:32:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 706.49
[32m[20221214 00:32:17 @agent_ppo2.py:143][0m Total time:      34.77 min
[32m[20221214 00:32:17 @agent_ppo2.py:145][0m 3149824 total steps have happened
[32m[20221214 00:32:17 @agent_ppo2.py:121][0m #------------------------ Iteration 5538 --------------------------#
[32m[20221214 00:32:17 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:32:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:32:17 @agent_ppo2.py:185][0m |           0.0066 |         116.6704 |        -104.0310 |
[32m[20221214 00:32:17 @agent_ppo2.py:185][0m |          -0.0047 |         108.1548 |        -103.6675 |
[32m[20221214 00:32:17 @agent_ppo2.py:185][0m |          -0.0116 |         105.3826 |        -103.5519 |
[32m[20221214 00:32:17 @agent_ppo2.py:185][0m |          -0.0043 |         105.8707 |        -103.3382 |
[32m[20221214 00:32:18 @agent_ppo2.py:185][0m |          -0.0140 |         102.6911 |        -103.6048 |
[32m[20221214 00:32:18 @agent_ppo2.py:185][0m |          -0.0152 |         100.6674 |        -103.6437 |
[32m[20221214 00:32:18 @agent_ppo2.py:185][0m |          -0.0075 |         106.8283 |        -103.5098 |
[32m[20221214 00:32:18 @agent_ppo2.py:185][0m |          -0.0220 |          99.4356 |        -102.9897 |
[32m[20221214 00:32:18 @agent_ppo2.py:185][0m |          -0.0184 |          98.5734 |        -102.8510 |
[32m[20221214 00:32:18 @agent_ppo2.py:185][0m |          -0.0169 |         100.7343 |        -103.3892 |
[32m[20221214 00:32:18 @agent_ppo2.py:130][0m Policy update time: 1.36 s
[32m[20221214 00:32:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 493.33
[32m[20221214 00:32:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 560.76
[32m[20221214 00:32:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 766.03
[32m[20221214 00:32:18 @agent_ppo2.py:143][0m Total time:      34.80 min
[32m[20221214 00:32:18 @agent_ppo2.py:145][0m 3151872 total steps have happened
[32m[20221214 00:32:18 @agent_ppo2.py:121][0m #------------------------ Iteration 5539 --------------------------#
[32m[20221214 00:32:19 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:32:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:32:19 @agent_ppo2.py:185][0m |           0.0015 |          93.9014 |        -107.9163 |
[32m[20221214 00:32:19 @agent_ppo2.py:185][0m |          -0.0044 |          86.3567 |        -107.5132 |
[32m[20221214 00:32:19 @agent_ppo2.py:185][0m |          -0.0066 |          83.8105 |        -106.5525 |
[32m[20221214 00:32:19 @agent_ppo2.py:185][0m |          -0.0067 |          81.7546 |        -107.1932 |
[32m[20221214 00:32:19 @agent_ppo2.py:185][0m |          -0.0138 |          81.0850 |        -107.1619 |
[32m[20221214 00:32:19 @agent_ppo2.py:185][0m |          -0.0159 |          79.9742 |        -107.1035 |
[32m[20221214 00:32:19 @agent_ppo2.py:185][0m |          -0.0088 |          79.3392 |        -107.2852 |
[32m[20221214 00:32:19 @agent_ppo2.py:185][0m |          -0.0062 |          78.5923 |        -107.1838 |
[32m[20221214 00:32:20 @agent_ppo2.py:185][0m |          -0.0111 |          79.0038 |        -107.1147 |
[32m[20221214 00:32:20 @agent_ppo2.py:185][0m |          -0.0150 |          76.9815 |        -106.9888 |
[32m[20221214 00:32:20 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 00:32:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 584.17
[32m[20221214 00:32:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 730.09
[32m[20221214 00:32:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 673.53
[32m[20221214 00:32:20 @agent_ppo2.py:143][0m Total time:      34.82 min
[32m[20221214 00:32:20 @agent_ppo2.py:145][0m 3153920 total steps have happened
[32m[20221214 00:32:20 @agent_ppo2.py:121][0m #------------------------ Iteration 5540 --------------------------#
[32m[20221214 00:32:20 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:32:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:32:20 @agent_ppo2.py:185][0m |           0.0045 |         120.1929 |        -104.2659 |
[32m[20221214 00:32:20 @agent_ppo2.py:185][0m |           0.0002 |         113.2097 |        -105.2156 |
[32m[20221214 00:32:20 @agent_ppo2.py:185][0m |          -0.0058 |         111.1879 |        -105.6430 |
[32m[20221214 00:32:20 @agent_ppo2.py:185][0m |           0.0021 |         116.6451 |        -105.0280 |
[32m[20221214 00:32:21 @agent_ppo2.py:185][0m |          -0.0017 |         109.5042 |        -105.0002 |
[32m[20221214 00:32:21 @agent_ppo2.py:185][0m |          -0.0029 |         108.6330 |        -106.0021 |
[32m[20221214 00:32:21 @agent_ppo2.py:185][0m |          -0.0060 |         108.1128 |        -106.0475 |
[32m[20221214 00:32:21 @agent_ppo2.py:185][0m |          -0.0096 |         108.0826 |        -106.1810 |
[32m[20221214 00:32:21 @agent_ppo2.py:185][0m |          -0.0144 |         107.4797 |        -105.9440 |
[32m[20221214 00:32:21 @agent_ppo2.py:185][0m |          -0.0081 |         108.1471 |        -106.5586 |
[32m[20221214 00:32:21 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221214 00:32:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 618.58
[32m[20221214 00:32:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 722.48
[32m[20221214 00:32:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 581.61
[32m[20221214 00:32:21 @agent_ppo2.py:143][0m Total time:      34.85 min
[32m[20221214 00:32:21 @agent_ppo2.py:145][0m 3155968 total steps have happened
[32m[20221214 00:32:21 @agent_ppo2.py:121][0m #------------------------ Iteration 5541 --------------------------#
[32m[20221214 00:32:21 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:32:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:32:22 @agent_ppo2.py:185][0m |           0.0125 |         195.0717 |        -109.2662 |
[32m[20221214 00:32:22 @agent_ppo2.py:185][0m |           0.0105 |         192.3439 |        -108.9622 |
[32m[20221214 00:32:22 @agent_ppo2.py:185][0m |           0.0040 |         173.9681 |        -108.6350 |
[32m[20221214 00:32:22 @agent_ppo2.py:185][0m |           0.0002 |         172.7555 |        -109.1918 |
[32m[20221214 00:32:22 @agent_ppo2.py:185][0m |           0.0008 |         172.6447 |        -109.1133 |
[32m[20221214 00:32:22 @agent_ppo2.py:185][0m |          -0.0032 |         171.4393 |        -109.2225 |
[32m[20221214 00:32:22 @agent_ppo2.py:185][0m |          -0.0054 |         172.4275 |        -109.7425 |
[32m[20221214 00:32:22 @agent_ppo2.py:185][0m |          -0.0024 |         171.5920 |        -109.7536 |
[32m[20221214 00:32:23 @agent_ppo2.py:185][0m |          -0.0015 |         172.8202 |        -109.4413 |
[32m[20221214 00:32:23 @agent_ppo2.py:185][0m |          -0.0022 |         171.2172 |        -109.2294 |
[32m[20221214 00:32:23 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 00:32:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 786.94
[32m[20221214 00:32:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.13
[32m[20221214 00:32:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 686.79
[32m[20221214 00:32:23 @agent_ppo2.py:143][0m Total time:      34.87 min
[32m[20221214 00:32:23 @agent_ppo2.py:145][0m 3158016 total steps have happened
[32m[20221214 00:32:23 @agent_ppo2.py:121][0m #------------------------ Iteration 5542 --------------------------#
[32m[20221214 00:32:23 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:32:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:32:23 @agent_ppo2.py:185][0m |           0.0025 |         107.9338 |        -111.9830 |
[32m[20221214 00:32:23 @agent_ppo2.py:185][0m |          -0.0067 |         101.9691 |        -112.4359 |
[32m[20221214 00:32:23 @agent_ppo2.py:185][0m |           0.0001 |          98.3682 |        -112.2415 |
[32m[20221214 00:32:23 @agent_ppo2.py:185][0m |           0.0001 |          98.1742 |        -112.5076 |
[32m[20221214 00:32:24 @agent_ppo2.py:185][0m |          -0.0002 |          99.6059 |        -112.2466 |
[32m[20221214 00:32:24 @agent_ppo2.py:185][0m |          -0.0094 |          93.6624 |        -112.1968 |
[32m[20221214 00:32:24 @agent_ppo2.py:185][0m |          -0.0142 |          93.4090 |        -111.9943 |
[32m[20221214 00:32:24 @agent_ppo2.py:185][0m |          -0.0100 |          91.5762 |        -112.1756 |
[32m[20221214 00:32:24 @agent_ppo2.py:185][0m |          -0.0123 |          91.5573 |        -111.5925 |
[32m[20221214 00:32:24 @agent_ppo2.py:185][0m |          -0.0179 |          91.1284 |        -112.7136 |
[32m[20221214 00:32:24 @agent_ppo2.py:130][0m Policy update time: 1.17 s
[32m[20221214 00:32:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 535.53
[32m[20221214 00:32:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 655.90
[32m[20221214 00:32:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 533.00
[32m[20221214 00:32:24 @agent_ppo2.py:143][0m Total time:      34.90 min
[32m[20221214 00:32:24 @agent_ppo2.py:145][0m 3160064 total steps have happened
[32m[20221214 00:32:24 @agent_ppo2.py:121][0m #------------------------ Iteration 5543 --------------------------#
[32m[20221214 00:32:24 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:32:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:32:25 @agent_ppo2.py:185][0m |          -0.0023 |         150.2932 |        -115.1015 |
[32m[20221214 00:32:25 @agent_ppo2.py:185][0m |           0.0046 |         151.2028 |        -114.5766 |
[32m[20221214 00:32:25 @agent_ppo2.py:185][0m |          -0.0069 |         136.3767 |        -114.8281 |
[32m[20221214 00:32:25 @agent_ppo2.py:185][0m |           0.0002 |         136.5600 |        -114.9916 |
[32m[20221214 00:32:25 @agent_ppo2.py:185][0m |          -0.0076 |         133.8603 |        -115.2046 |
[32m[20221214 00:32:25 @agent_ppo2.py:185][0m |          -0.0090 |         132.0707 |        -115.2464 |
[32m[20221214 00:32:25 @agent_ppo2.py:185][0m |          -0.0086 |         132.4327 |        -116.0946 |
[32m[20221214 00:32:25 @agent_ppo2.py:185][0m |          -0.0084 |         130.4912 |        -115.5327 |
[32m[20221214 00:32:25 @agent_ppo2.py:185][0m |          -0.0116 |         130.7059 |        -116.0323 |
[32m[20221214 00:32:26 @agent_ppo2.py:185][0m |          -0.0100 |         130.3262 |        -116.2927 |
[32m[20221214 00:32:26 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221214 00:32:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 587.58
[32m[20221214 00:32:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 701.62
[32m[20221214 00:32:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 834.69
[32m[20221214 00:32:26 @agent_ppo2.py:143][0m Total time:      34.92 min
[32m[20221214 00:32:26 @agent_ppo2.py:145][0m 3162112 total steps have happened
[32m[20221214 00:32:26 @agent_ppo2.py:121][0m #------------------------ Iteration 5544 --------------------------#
[32m[20221214 00:32:26 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:32:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:32:26 @agent_ppo2.py:185][0m |           0.0126 |         170.7583 |        -118.0410 |
[32m[20221214 00:32:26 @agent_ppo2.py:185][0m |          -0.0024 |         149.3735 |        -117.1180 |
[32m[20221214 00:32:26 @agent_ppo2.py:185][0m |          -0.0037 |         145.3056 |        -116.9196 |
[32m[20221214 00:32:26 @agent_ppo2.py:185][0m |          -0.0004 |         144.2176 |        -117.0550 |
[32m[20221214 00:32:27 @agent_ppo2.py:185][0m |          -0.0053 |         142.0894 |        -116.7845 |
[32m[20221214 00:32:27 @agent_ppo2.py:185][0m |          -0.0075 |         141.0654 |        -116.8386 |
[32m[20221214 00:32:27 @agent_ppo2.py:185][0m |          -0.0054 |         139.8388 |        -116.3935 |
[32m[20221214 00:32:27 @agent_ppo2.py:185][0m |          -0.0067 |         139.0561 |        -116.4077 |
[32m[20221214 00:32:27 @agent_ppo2.py:185][0m |          -0.0049 |         137.1122 |        -116.7165 |
[32m[20221214 00:32:27 @agent_ppo2.py:185][0m |          -0.0094 |         135.4440 |        -116.0687 |
[32m[20221214 00:32:27 @agent_ppo2.py:130][0m Policy update time: 1.18 s
[32m[20221214 00:32:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 723.92
[32m[20221214 00:32:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.39
[32m[20221214 00:32:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 809.98
[32m[20221214 00:32:27 @agent_ppo2.py:143][0m Total time:      34.95 min
[32m[20221214 00:32:27 @agent_ppo2.py:145][0m 3164160 total steps have happened
[32m[20221214 00:32:27 @agent_ppo2.py:121][0m #------------------------ Iteration 5545 --------------------------#
[32m[20221214 00:32:27 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:32:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:32:28 @agent_ppo2.py:185][0m |           0.0089 |          82.3383 |        -116.3882 |
[32m[20221214 00:32:28 @agent_ppo2.py:185][0m |          -0.0014 |          60.8178 |        -116.0738 |
[32m[20221214 00:32:28 @agent_ppo2.py:185][0m |          -0.0120 |          55.9079 |        -115.2800 |
[32m[20221214 00:32:28 @agent_ppo2.py:185][0m |          -0.0178 |          52.7071 |        -115.9617 |
[32m[20221214 00:32:28 @agent_ppo2.py:185][0m |          -0.0056 |          50.3008 |        -115.6911 |
[32m[20221214 00:32:28 @agent_ppo2.py:185][0m |          -0.0136 |          49.2687 |        -116.2450 |
[32m[20221214 00:32:28 @agent_ppo2.py:185][0m |          -0.0130 |          49.2336 |        -116.3944 |
[32m[20221214 00:32:28 @agent_ppo2.py:185][0m |           0.0033 |          60.8336 |        -116.4016 |
[32m[20221214 00:32:28 @agent_ppo2.py:185][0m |          -0.0148 |          50.3216 |        -115.8714 |
[32m[20221214 00:32:29 @agent_ppo2.py:185][0m |          -0.0221 |          47.4849 |        -116.1931 |
[32m[20221214 00:32:29 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221214 00:32:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 506.64
[32m[20221214 00:32:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 696.64
[32m[20221214 00:32:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 846.70
[32m[20221214 00:32:29 @agent_ppo2.py:143][0m Total time:      34.97 min
[32m[20221214 00:32:29 @agent_ppo2.py:145][0m 3166208 total steps have happened
[32m[20221214 00:32:29 @agent_ppo2.py:121][0m #------------------------ Iteration 5546 --------------------------#
[32m[20221214 00:32:29 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:32:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:32:29 @agent_ppo2.py:185][0m |           0.0025 |         147.5832 |        -112.3410 |
[32m[20221214 00:32:29 @agent_ppo2.py:185][0m |          -0.0034 |         133.9110 |        -111.7307 |
[32m[20221214 00:32:29 @agent_ppo2.py:185][0m |          -0.0073 |         131.5642 |        -112.0275 |
[32m[20221214 00:32:29 @agent_ppo2.py:185][0m |          -0.0032 |         128.4073 |        -112.4842 |
[32m[20221214 00:32:29 @agent_ppo2.py:185][0m |          -0.0087 |         126.2174 |        -112.2119 |
[32m[20221214 00:32:30 @agent_ppo2.py:185][0m |          -0.0098 |         125.7714 |        -112.0201 |
[32m[20221214 00:32:30 @agent_ppo2.py:185][0m |          -0.0080 |         124.8956 |        -111.5472 |
[32m[20221214 00:32:30 @agent_ppo2.py:185][0m |          -0.0094 |         123.5769 |        -111.2713 |
[32m[20221214 00:32:30 @agent_ppo2.py:185][0m |          -0.0059 |         122.6088 |        -112.0330 |
[32m[20221214 00:32:30 @agent_ppo2.py:185][0m |          -0.0089 |         122.2543 |        -111.4186 |
[32m[20221214 00:32:30 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221214 00:32:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 692.59
[32m[20221214 00:32:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 784.80
[32m[20221214 00:32:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 676.71
[32m[20221214 00:32:30 @agent_ppo2.py:143][0m Total time:      35.00 min
[32m[20221214 00:32:30 @agent_ppo2.py:145][0m 3168256 total steps have happened
[32m[20221214 00:32:30 @agent_ppo2.py:121][0m #------------------------ Iteration 5547 --------------------------#
[32m[20221214 00:32:30 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:32:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:32:31 @agent_ppo2.py:185][0m |           0.0040 |          55.1082 |        -115.6004 |
[32m[20221214 00:32:31 @agent_ppo2.py:185][0m |          -0.0094 |          45.6370 |        -115.8188 |
[32m[20221214 00:32:31 @agent_ppo2.py:185][0m |          -0.0059 |          43.3356 |        -115.2132 |
[32m[20221214 00:32:31 @agent_ppo2.py:185][0m |          -0.0041 |          41.1546 |        -115.2835 |
[32m[20221214 00:32:31 @agent_ppo2.py:185][0m |          -0.0044 |          42.1929 |        -115.3372 |
[32m[20221214 00:32:31 @agent_ppo2.py:185][0m |          -0.0055 |          41.7330 |        -115.6262 |
[32m[20221214 00:32:31 @agent_ppo2.py:185][0m |          -0.0125 |          40.1498 |        -115.4998 |
[32m[20221214 00:32:31 @agent_ppo2.py:185][0m |          -0.0134 |          38.4691 |        -115.5716 |
[32m[20221214 00:32:31 @agent_ppo2.py:185][0m |          -0.0114 |          37.8939 |        -115.4077 |
[32m[20221214 00:32:32 @agent_ppo2.py:185][0m |          -0.0192 |          36.8791 |        -115.3510 |
[32m[20221214 00:32:32 @agent_ppo2.py:130][0m Policy update time: 1.20 s
[32m[20221214 00:32:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 552.08
[32m[20221214 00:32:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 747.38
[32m[20221214 00:32:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 543.87
[32m[20221214 00:32:32 @agent_ppo2.py:143][0m Total time:      35.02 min
[32m[20221214 00:32:32 @agent_ppo2.py:145][0m 3170304 total steps have happened
[32m[20221214 00:32:32 @agent_ppo2.py:121][0m #------------------------ Iteration 5548 --------------------------#
[32m[20221214 00:32:32 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:32:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:32:32 @agent_ppo2.py:185][0m |          -0.0001 |         139.6539 |        -116.0772 |
[32m[20221214 00:32:32 @agent_ppo2.py:185][0m |          -0.0003 |         134.0482 |        -115.9320 |
[32m[20221214 00:32:32 @agent_ppo2.py:185][0m |          -0.0048 |         131.9704 |        -116.0416 |
[32m[20221214 00:32:32 @agent_ppo2.py:185][0m |          -0.0035 |         129.8987 |        -115.5206 |
[32m[20221214 00:32:33 @agent_ppo2.py:185][0m |          -0.0046 |         128.4513 |        -115.9625 |
[32m[20221214 00:32:33 @agent_ppo2.py:185][0m |           0.0060 |         133.6686 |        -115.7651 |
[32m[20221214 00:32:33 @agent_ppo2.py:185][0m |          -0.0021 |         128.0304 |        -116.0728 |
[32m[20221214 00:32:33 @agent_ppo2.py:185][0m |          -0.0043 |         125.5757 |        -116.2167 |
[32m[20221214 00:32:33 @agent_ppo2.py:185][0m |          -0.0079 |         125.1303 |        -115.4564 |
[32m[20221214 00:32:33 @agent_ppo2.py:185][0m |          -0.0077 |         124.4117 |        -115.3229 |
[32m[20221214 00:32:33 @agent_ppo2.py:130][0m Policy update time: 1.19 s
[32m[20221214 00:32:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 659.93
[32m[20221214 00:32:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 760.12
[32m[20221214 00:32:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 614.65
[32m[20221214 00:32:33 @agent_ppo2.py:143][0m Total time:      35.05 min
[32m[20221214 00:32:33 @agent_ppo2.py:145][0m 3172352 total steps have happened
[32m[20221214 00:32:33 @agent_ppo2.py:121][0m #------------------------ Iteration 5549 --------------------------#
[32m[20221214 00:32:33 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:32:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:32:34 @agent_ppo2.py:185][0m |          -0.0008 |          96.4279 |        -117.8658 |
[32m[20221214 00:32:34 @agent_ppo2.py:185][0m |          -0.0023 |          85.1971 |        -117.0941 |
[32m[20221214 00:32:34 @agent_ppo2.py:185][0m |          -0.0066 |          82.5582 |        -118.1144 |
[32m[20221214 00:32:34 @agent_ppo2.py:185][0m |          -0.0094 |          80.6480 |        -118.0034 |
[32m[20221214 00:32:34 @agent_ppo2.py:185][0m |          -0.0165 |          79.1298 |        -118.3891 |
[32m[20221214 00:32:34 @agent_ppo2.py:185][0m |          -0.0125 |          78.0676 |        -117.7913 |
[32m[20221214 00:32:34 @agent_ppo2.py:185][0m |          -0.0072 |          77.4514 |        -118.4659 |
[32m[20221214 00:32:34 @agent_ppo2.py:185][0m |          -0.0111 |          77.7086 |        -118.4955 |
[32m[20221214 00:32:34 @agent_ppo2.py:185][0m |          -0.0132 |          75.9351 |        -117.9969 |
[32m[20221214 00:32:35 @agent_ppo2.py:185][0m |          -0.0074 |          79.7459 |        -118.1304 |
[32m[20221214 00:32:35 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:32:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 623.76
[32m[20221214 00:32:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 709.91
[32m[20221214 00:32:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 800.70
[32m[20221214 00:32:35 @agent_ppo2.py:143][0m Total time:      35.07 min
[32m[20221214 00:32:35 @agent_ppo2.py:145][0m 3174400 total steps have happened
[32m[20221214 00:32:35 @agent_ppo2.py:121][0m #------------------------ Iteration 5550 --------------------------#
[32m[20221214 00:32:35 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:32:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:32:35 @agent_ppo2.py:185][0m |           0.0068 |         100.0252 |        -117.4262 |
[32m[20221214 00:32:35 @agent_ppo2.py:185][0m |          -0.0054 |          89.5311 |        -117.2018 |
[32m[20221214 00:32:35 @agent_ppo2.py:185][0m |          -0.0129 |          87.2465 |        -117.6300 |
[32m[20221214 00:32:35 @agent_ppo2.py:185][0m |          -0.0108 |          84.4643 |        -117.3463 |
[32m[20221214 00:32:36 @agent_ppo2.py:185][0m |          -0.0053 |          83.5497 |        -117.4914 |
[32m[20221214 00:32:36 @agent_ppo2.py:185][0m |          -0.0140 |          82.2309 |        -117.3576 |
[32m[20221214 00:32:36 @agent_ppo2.py:185][0m |          -0.0066 |          85.1532 |        -117.4617 |
[32m[20221214 00:32:36 @agent_ppo2.py:185][0m |          -0.0163 |          80.8296 |        -117.6205 |
[32m[20221214 00:32:36 @agent_ppo2.py:185][0m |          -0.0172 |          81.1636 |        -117.4869 |
[32m[20221214 00:32:36 @agent_ppo2.py:185][0m |          -0.0089 |          81.2895 |        -117.9336 |
[32m[20221214 00:32:36 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:32:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 558.83
[32m[20221214 00:32:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 702.93
[32m[20221214 00:32:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 636.89
[32m[20221214 00:32:36 @agent_ppo2.py:143][0m Total time:      35.10 min
[32m[20221214 00:32:36 @agent_ppo2.py:145][0m 3176448 total steps have happened
[32m[20221214 00:32:36 @agent_ppo2.py:121][0m #------------------------ Iteration 5551 --------------------------#
[32m[20221214 00:32:36 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:32:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:32:37 @agent_ppo2.py:185][0m |           0.0034 |         128.6838 |        -119.7096 |
[32m[20221214 00:32:37 @agent_ppo2.py:185][0m |          -0.0022 |         120.8302 |        -119.7684 |
[32m[20221214 00:32:37 @agent_ppo2.py:185][0m |          -0.0043 |         119.2572 |        -119.0495 |
[32m[20221214 00:32:37 @agent_ppo2.py:185][0m |          -0.0070 |         117.6963 |        -119.6173 |
[32m[20221214 00:32:37 @agent_ppo2.py:185][0m |          -0.0078 |         117.6821 |        -118.9280 |
[32m[20221214 00:32:37 @agent_ppo2.py:185][0m |           0.0003 |         123.6467 |        -119.2161 |
[32m[20221214 00:32:37 @agent_ppo2.py:185][0m |          -0.0049 |         118.0531 |        -118.8913 |
[32m[20221214 00:32:37 @agent_ppo2.py:185][0m |          -0.0060 |         115.1550 |        -117.9536 |
[32m[20221214 00:32:37 @agent_ppo2.py:185][0m |          -0.0097 |         115.0202 |        -118.2666 |
[32m[20221214 00:32:38 @agent_ppo2.py:185][0m |          -0.0122 |         115.4622 |        -118.2258 |
[32m[20221214 00:32:38 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221214 00:32:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 596.93
[32m[20221214 00:32:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 683.00
[32m[20221214 00:32:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 817.82
[32m[20221214 00:32:38 @agent_ppo2.py:143][0m Total time:      35.12 min
[32m[20221214 00:32:38 @agent_ppo2.py:145][0m 3178496 total steps have happened
[32m[20221214 00:32:38 @agent_ppo2.py:121][0m #------------------------ Iteration 5552 --------------------------#
[32m[20221214 00:32:38 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:32:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:32:38 @agent_ppo2.py:185][0m |           0.0158 |         140.8168 |        -122.7802 |
[32m[20221214 00:32:38 @agent_ppo2.py:185][0m |          -0.0000 |         133.1244 |        -121.2777 |
[32m[20221214 00:32:38 @agent_ppo2.py:185][0m |          -0.0014 |         130.5941 |        -121.7472 |
[32m[20221214 00:32:38 @agent_ppo2.py:185][0m |           0.0130 |         153.1663 |        -121.3437 |
[32m[20221214 00:32:38 @agent_ppo2.py:185][0m |          -0.0072 |         124.9658 |        -120.2879 |
[32m[20221214 00:32:39 @agent_ppo2.py:185][0m |          -0.0101 |         123.4272 |        -120.9887 |
[32m[20221214 00:32:39 @agent_ppo2.py:185][0m |          -0.0063 |         122.9553 |        -121.2852 |
[32m[20221214 00:32:39 @agent_ppo2.py:185][0m |          -0.0136 |         121.4435 |        -120.3290 |
[32m[20221214 00:32:39 @agent_ppo2.py:185][0m |          -0.0094 |         121.0149 |        -120.6754 |
[32m[20221214 00:32:39 @agent_ppo2.py:185][0m |          -0.0148 |         120.7293 |        -120.1598 |
[32m[20221214 00:32:39 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:32:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 555.44
[32m[20221214 00:32:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 626.21
[32m[20221214 00:32:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 693.31
[32m[20221214 00:32:39 @agent_ppo2.py:143][0m Total time:      35.15 min
[32m[20221214 00:32:39 @agent_ppo2.py:145][0m 3180544 total steps have happened
[32m[20221214 00:32:39 @agent_ppo2.py:121][0m #------------------------ Iteration 5553 --------------------------#
[32m[20221214 00:32:39 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:32:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:32:39 @agent_ppo2.py:185][0m |           0.0008 |          85.2131 |        -120.1725 |
[32m[20221214 00:32:40 @agent_ppo2.py:185][0m |          -0.0016 |          78.6922 |        -119.1421 |
[32m[20221214 00:32:40 @agent_ppo2.py:185][0m |          -0.0039 |          73.9616 |        -118.6814 |
[32m[20221214 00:32:40 @agent_ppo2.py:185][0m |          -0.0103 |          74.3401 |        -118.4495 |
[32m[20221214 00:32:40 @agent_ppo2.py:185][0m |          -0.0168 |          70.4060 |        -118.6241 |
[32m[20221214 00:32:40 @agent_ppo2.py:185][0m |          -0.0124 |          70.0493 |        -117.8041 |
[32m[20221214 00:32:40 @agent_ppo2.py:185][0m |          -0.0175 |          70.1787 |        -117.9627 |
[32m[20221214 00:32:40 @agent_ppo2.py:185][0m |          -0.0150 |          69.1468 |        -117.5614 |
[32m[20221214 00:32:40 @agent_ppo2.py:185][0m |          -0.0208 |          66.9641 |        -116.9478 |
[32m[20221214 00:32:40 @agent_ppo2.py:185][0m |          -0.0093 |          70.1484 |        -117.3217 |
[32m[20221214 00:32:40 @agent_ppo2.py:130][0m Policy update time: 1.19 s
[32m[20221214 00:32:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 425.54
[32m[20221214 00:32:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 672.36
[32m[20221214 00:32:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 327.77
[32m[20221214 00:32:41 @agent_ppo2.py:143][0m Total time:      35.17 min
[32m[20221214 00:32:41 @agent_ppo2.py:145][0m 3182592 total steps have happened
[32m[20221214 00:32:41 @agent_ppo2.py:121][0m #------------------------ Iteration 5554 --------------------------#
[32m[20221214 00:32:41 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:32:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:32:41 @agent_ppo2.py:185][0m |          -0.0013 |         163.6328 |        -119.4614 |
[32m[20221214 00:32:41 @agent_ppo2.py:185][0m |          -0.0010 |         159.5061 |        -119.6370 |
[32m[20221214 00:32:41 @agent_ppo2.py:185][0m |          -0.0020 |         158.6840 |        -119.1369 |
[32m[20221214 00:32:41 @agent_ppo2.py:185][0m |          -0.0033 |         157.7299 |        -118.8447 |
[32m[20221214 00:32:41 @agent_ppo2.py:185][0m |           0.0046 |         166.0371 |        -118.1481 |
[32m[20221214 00:32:41 @agent_ppo2.py:185][0m |          -0.0013 |         157.8272 |        -118.2844 |
[32m[20221214 00:32:42 @agent_ppo2.py:185][0m |          -0.0030 |         157.0420 |        -118.4069 |
[32m[20221214 00:32:42 @agent_ppo2.py:185][0m |          -0.0031 |         156.4462 |        -117.7693 |
[32m[20221214 00:32:42 @agent_ppo2.py:185][0m |          -0.0044 |         156.4981 |        -118.4431 |
[32m[20221214 00:32:42 @agent_ppo2.py:185][0m |           0.0009 |         157.5271 |        -119.1546 |
[32m[20221214 00:32:42 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:32:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 737.81
[32m[20221214 00:32:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 817.05
[32m[20221214 00:32:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 600.99
[32m[20221214 00:32:42 @agent_ppo2.py:143][0m Total time:      35.19 min
[32m[20221214 00:32:42 @agent_ppo2.py:145][0m 3184640 total steps have happened
[32m[20221214 00:32:42 @agent_ppo2.py:121][0m #------------------------ Iteration 5555 --------------------------#
[32m[20221214 00:32:42 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:32:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:32:42 @agent_ppo2.py:185][0m |           0.0011 |         113.4079 |        -115.3066 |
[32m[20221214 00:32:43 @agent_ppo2.py:185][0m |          -0.0036 |         107.1645 |        -113.9845 |
[32m[20221214 00:32:43 @agent_ppo2.py:185][0m |          -0.0048 |         105.2294 |        -115.1183 |
[32m[20221214 00:32:43 @agent_ppo2.py:185][0m |          -0.0031 |         104.4518 |        -114.8131 |
[32m[20221214 00:32:43 @agent_ppo2.py:185][0m |          -0.0084 |         102.6777 |        -115.4236 |
[32m[20221214 00:32:43 @agent_ppo2.py:185][0m |          -0.0096 |         103.5535 |        -114.7759 |
[32m[20221214 00:32:43 @agent_ppo2.py:185][0m |          -0.0070 |         102.2305 |        -115.0081 |
[32m[20221214 00:32:43 @agent_ppo2.py:185][0m |          -0.0049 |         102.0226 |        -114.8493 |
[32m[20221214 00:32:43 @agent_ppo2.py:185][0m |          -0.0092 |         101.7046 |        -114.9475 |
[32m[20221214 00:32:43 @agent_ppo2.py:185][0m |          -0.0096 |         101.1512 |        -115.2257 |
[32m[20221214 00:32:43 @agent_ppo2.py:130][0m Policy update time: 1.19 s
[32m[20221214 00:32:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 568.78
[32m[20221214 00:32:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 714.64
[32m[20221214 00:32:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 835.99
[32m[20221214 00:32:44 @agent_ppo2.py:143][0m Total time:      35.22 min
[32m[20221214 00:32:44 @agent_ppo2.py:145][0m 3186688 total steps have happened
[32m[20221214 00:32:44 @agent_ppo2.py:121][0m #------------------------ Iteration 5556 --------------------------#
[32m[20221214 00:32:44 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:32:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:32:44 @agent_ppo2.py:185][0m |           0.0033 |         133.2413 |        -110.3255 |
[32m[20221214 00:32:44 @agent_ppo2.py:185][0m |          -0.0016 |         127.7526 |        -110.0049 |
[32m[20221214 00:32:44 @agent_ppo2.py:185][0m |          -0.0085 |         122.6216 |        -110.1831 |
[32m[20221214 00:32:44 @agent_ppo2.py:185][0m |          -0.0090 |         120.8964 |        -110.3677 |
[32m[20221214 00:32:44 @agent_ppo2.py:185][0m |          -0.0118 |         119.3465 |        -110.1505 |
[32m[20221214 00:32:44 @agent_ppo2.py:185][0m |          -0.0120 |         118.1682 |        -110.2296 |
[32m[20221214 00:32:45 @agent_ppo2.py:185][0m |          -0.0142 |         117.5195 |        -110.6036 |
[32m[20221214 00:32:45 @agent_ppo2.py:185][0m |          -0.0115 |         116.6581 |        -110.9850 |
[32m[20221214 00:32:45 @agent_ppo2.py:185][0m |          -0.0021 |         132.4387 |        -110.9720 |
[32m[20221214 00:32:45 @agent_ppo2.py:185][0m |          -0.0114 |         115.1105 |        -110.7579 |
[32m[20221214 00:32:45 @agent_ppo2.py:130][0m Policy update time: 1.17 s
[32m[20221214 00:32:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 651.95
[32m[20221214 00:32:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 737.76
[32m[20221214 00:32:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 664.48
[32m[20221214 00:32:45 @agent_ppo2.py:143][0m Total time:      35.25 min
[32m[20221214 00:32:45 @agent_ppo2.py:145][0m 3188736 total steps have happened
[32m[20221214 00:32:45 @agent_ppo2.py:121][0m #------------------------ Iteration 5557 --------------------------#
[32m[20221214 00:32:45 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:32:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:32:46 @agent_ppo2.py:185][0m |           0.0060 |         132.4312 |        -117.5101 |
[32m[20221214 00:32:46 @agent_ppo2.py:185][0m |          -0.0027 |         127.1049 |        -117.0375 |
[32m[20221214 00:32:46 @agent_ppo2.py:185][0m |          -0.0016 |         125.3783 |        -117.4278 |
[32m[20221214 00:32:46 @agent_ppo2.py:185][0m |          -0.0007 |         129.5021 |        -116.8480 |
[32m[20221214 00:32:46 @agent_ppo2.py:185][0m |           0.0118 |         148.2053 |        -116.5957 |
[32m[20221214 00:32:46 @agent_ppo2.py:185][0m |          -0.0061 |         126.7025 |        -116.1440 |
[32m[20221214 00:32:46 @agent_ppo2.py:185][0m |          -0.0110 |         119.4693 |        -116.8218 |
[32m[20221214 00:32:46 @agent_ppo2.py:185][0m |           0.0056 |         133.2615 |        -116.7448 |
[32m[20221214 00:32:46 @agent_ppo2.py:185][0m |           0.0030 |         138.1959 |        -116.3623 |
[32m[20221214 00:32:46 @agent_ppo2.py:185][0m |           0.0009 |         121.3670 |        -116.2737 |
[32m[20221214 00:32:46 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221214 00:32:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 627.12
[32m[20221214 00:32:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 731.23
[32m[20221214 00:32:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 796.53
[32m[20221214 00:32:47 @agent_ppo2.py:143][0m Total time:      35.27 min
[32m[20221214 00:32:47 @agent_ppo2.py:145][0m 3190784 total steps have happened
[32m[20221214 00:32:47 @agent_ppo2.py:121][0m #------------------------ Iteration 5558 --------------------------#
[32m[20221214 00:32:47 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:32:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:32:47 @agent_ppo2.py:185][0m |           0.0253 |         130.3891 |        -115.1420 |
[32m[20221214 00:32:47 @agent_ppo2.py:185][0m |          -0.0027 |         106.0489 |        -114.7848 |
[32m[20221214 00:32:47 @agent_ppo2.py:185][0m |          -0.0030 |         103.3718 |        -114.1687 |
[32m[20221214 00:32:47 @agent_ppo2.py:185][0m |          -0.0098 |         101.2969 |        -114.9356 |
[32m[20221214 00:32:47 @agent_ppo2.py:185][0m |          -0.0017 |          99.9340 |        -113.8420 |
[32m[20221214 00:32:48 @agent_ppo2.py:185][0m |          -0.0066 |          98.7492 |        -114.5150 |
[32m[20221214 00:32:48 @agent_ppo2.py:185][0m |          -0.0101 |          97.4298 |        -114.5359 |
[32m[20221214 00:32:48 @agent_ppo2.py:185][0m |          -0.0127 |          96.8545 |        -114.3160 |
[32m[20221214 00:32:48 @agent_ppo2.py:185][0m |          -0.0130 |          96.5007 |        -114.1009 |
[32m[20221214 00:32:48 @agent_ppo2.py:185][0m |          -0.0065 |          94.9226 |        -114.1984 |
[32m[20221214 00:32:48 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221214 00:32:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 566.45
[32m[20221214 00:32:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 702.28
[32m[20221214 00:32:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 704.26
[32m[20221214 00:32:48 @agent_ppo2.py:143][0m Total time:      35.30 min
[32m[20221214 00:32:48 @agent_ppo2.py:145][0m 3192832 total steps have happened
[32m[20221214 00:32:48 @agent_ppo2.py:121][0m #------------------------ Iteration 5559 --------------------------#
[32m[20221214 00:32:48 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:32:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:32:48 @agent_ppo2.py:185][0m |           0.0142 |         122.3872 |        -117.2322 |
[32m[20221214 00:32:49 @agent_ppo2.py:185][0m |          -0.0007 |         105.4514 |        -117.2482 |
[32m[20221214 00:32:49 @agent_ppo2.py:185][0m |          -0.0080 |         103.2097 |        -117.5637 |
[32m[20221214 00:32:49 @agent_ppo2.py:185][0m |          -0.0110 |         100.9176 |        -117.9097 |
[32m[20221214 00:32:49 @agent_ppo2.py:185][0m |          -0.0085 |         100.8298 |        -117.8117 |
[32m[20221214 00:32:49 @agent_ppo2.py:185][0m |          -0.0100 |          99.3213 |        -118.1162 |
[32m[20221214 00:32:49 @agent_ppo2.py:185][0m |          -0.0084 |          98.8699 |        -117.8200 |
[32m[20221214 00:32:49 @agent_ppo2.py:185][0m |          -0.0109 |          98.4605 |        -118.0345 |
[32m[20221214 00:32:49 @agent_ppo2.py:185][0m |          -0.0077 |          98.2132 |        -118.7119 |
[32m[20221214 00:32:49 @agent_ppo2.py:185][0m |          -0.0121 |          97.1846 |        -118.1886 |
[32m[20221214 00:32:49 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221214 00:32:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 609.79
[32m[20221214 00:32:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 663.62
[32m[20221214 00:32:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 832.45
[32m[20221214 00:32:50 @agent_ppo2.py:143][0m Total time:      35.32 min
[32m[20221214 00:32:50 @agent_ppo2.py:145][0m 3194880 total steps have happened
[32m[20221214 00:32:50 @agent_ppo2.py:121][0m #------------------------ Iteration 5560 --------------------------#
[32m[20221214 00:32:50 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 00:32:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:32:50 @agent_ppo2.py:185][0m |           0.0062 |          84.2259 |        -117.2848 |
[32m[20221214 00:32:50 @agent_ppo2.py:185][0m |          -0.0055 |          71.6493 |        -116.8403 |
[32m[20221214 00:32:50 @agent_ppo2.py:185][0m |          -0.0034 |          65.2698 |        -116.4009 |
[32m[20221214 00:32:50 @agent_ppo2.py:185][0m |          -0.0062 |          64.0645 |        -116.2302 |
[32m[20221214 00:32:50 @agent_ppo2.py:185][0m |          -0.0083 |          61.5387 |        -116.2955 |
[32m[20221214 00:32:51 @agent_ppo2.py:185][0m |          -0.0105 |          60.3241 |        -116.0848 |
[32m[20221214 00:32:51 @agent_ppo2.py:185][0m |          -0.0083 |          60.2605 |        -115.5735 |
[32m[20221214 00:32:51 @agent_ppo2.py:185][0m |          -0.0103 |          58.9895 |        -115.7561 |
[32m[20221214 00:32:51 @agent_ppo2.py:185][0m |          -0.0135 |          59.7064 |        -115.6196 |
[32m[20221214 00:32:51 @agent_ppo2.py:185][0m |          -0.0181 |          58.2059 |        -114.9383 |
[32m[20221214 00:32:51 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221214 00:32:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 563.84
[32m[20221214 00:32:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 654.29
[32m[20221214 00:32:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 490.49
[32m[20221214 00:32:51 @agent_ppo2.py:143][0m Total time:      35.35 min
[32m[20221214 00:32:51 @agent_ppo2.py:145][0m 3196928 total steps have happened
[32m[20221214 00:32:51 @agent_ppo2.py:121][0m #------------------------ Iteration 5561 --------------------------#
[32m[20221214 00:32:51 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:32:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:32:51 @agent_ppo2.py:185][0m |           0.0129 |         104.4324 |        -120.5676 |
[32m[20221214 00:32:52 @agent_ppo2.py:185][0m |          -0.0020 |          90.3303 |        -120.6722 |
[32m[20221214 00:32:52 @agent_ppo2.py:185][0m |          -0.0025 |          88.5019 |        -120.1582 |
[32m[20221214 00:32:52 @agent_ppo2.py:185][0m |           0.0091 |          93.3729 |        -120.1876 |
[32m[20221214 00:32:52 @agent_ppo2.py:185][0m |          -0.0077 |          85.6352 |        -120.0138 |
[32m[20221214 00:32:52 @agent_ppo2.py:185][0m |          -0.0099 |          84.1953 |        -120.3183 |
[32m[20221214 00:32:52 @agent_ppo2.py:185][0m |          -0.0039 |          83.9947 |        -120.0315 |
[32m[20221214 00:32:52 @agent_ppo2.py:185][0m |          -0.0117 |          83.6027 |        -119.7941 |
[32m[20221214 00:32:52 @agent_ppo2.py:185][0m |          -0.0083 |          82.5110 |        -119.7356 |
[32m[20221214 00:32:52 @agent_ppo2.py:185][0m |          -0.0083 |          82.8468 |        -119.3981 |
[32m[20221214 00:32:52 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:32:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 477.96
[32m[20221214 00:32:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 633.41
[32m[20221214 00:32:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 706.77
[32m[20221214 00:32:53 @agent_ppo2.py:143][0m Total time:      35.37 min
[32m[20221214 00:32:53 @agent_ppo2.py:145][0m 3198976 total steps have happened
[32m[20221214 00:32:53 @agent_ppo2.py:121][0m #------------------------ Iteration 5562 --------------------------#
[32m[20221214 00:32:53 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:32:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:32:53 @agent_ppo2.py:185][0m |          -0.0004 |         109.7082 |        -117.6717 |
[32m[20221214 00:32:53 @agent_ppo2.py:185][0m |          -0.0047 |         100.4523 |        -116.2902 |
[32m[20221214 00:32:53 @agent_ppo2.py:185][0m |          -0.0109 |          97.6778 |        -116.3456 |
[32m[20221214 00:32:53 @agent_ppo2.py:185][0m |          -0.0137 |          95.6194 |        -116.7453 |
[32m[20221214 00:32:53 @agent_ppo2.py:185][0m |          -0.0185 |          93.8853 |        -116.7931 |
[32m[20221214 00:32:53 @agent_ppo2.py:185][0m |          -0.0139 |          91.7629 |        -116.2085 |
[32m[20221214 00:32:54 @agent_ppo2.py:185][0m |          -0.0147 |          92.8473 |        -116.9834 |
[32m[20221214 00:32:54 @agent_ppo2.py:185][0m |          -0.0147 |          90.4716 |        -116.5960 |
[32m[20221214 00:32:54 @agent_ppo2.py:185][0m |          -0.0131 |          89.1011 |        -116.6030 |
[32m[20221214 00:32:54 @agent_ppo2.py:185][0m |          -0.0192 |          89.1339 |        -116.8081 |
[32m[20221214 00:32:54 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221214 00:32:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 483.91
[32m[20221214 00:32:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 654.59
[32m[20221214 00:32:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 302.48
[32m[20221214 00:32:54 @agent_ppo2.py:143][0m Total time:      35.39 min
[32m[20221214 00:32:54 @agent_ppo2.py:145][0m 3201024 total steps have happened
[32m[20221214 00:32:54 @agent_ppo2.py:121][0m #------------------------ Iteration 5563 --------------------------#
[32m[20221214 00:32:54 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:32:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:32:54 @agent_ppo2.py:185][0m |           0.0003 |          92.3384 |        -117.1927 |
[32m[20221214 00:32:54 @agent_ppo2.py:185][0m |          -0.0053 |          84.1931 |        -117.1817 |
[32m[20221214 00:32:55 @agent_ppo2.py:185][0m |          -0.0133 |          80.9276 |        -116.8039 |
[32m[20221214 00:32:55 @agent_ppo2.py:185][0m |          -0.0116 |          79.0897 |        -116.9861 |
[32m[20221214 00:32:55 @agent_ppo2.py:185][0m |          -0.0122 |          78.1054 |        -117.0617 |
[32m[20221214 00:32:55 @agent_ppo2.py:185][0m |          -0.0168 |          76.5761 |        -116.5237 |
[32m[20221214 00:32:55 @agent_ppo2.py:185][0m |          -0.0136 |          76.3179 |        -117.1173 |
[32m[20221214 00:32:55 @agent_ppo2.py:185][0m |          -0.0228 |          75.4477 |        -116.5480 |
[32m[20221214 00:32:55 @agent_ppo2.py:185][0m |          -0.0190 |          74.2807 |        -117.0405 |
[32m[20221214 00:32:55 @agent_ppo2.py:185][0m |          -0.0168 |          73.4415 |        -116.8334 |
[32m[20221214 00:32:55 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:32:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 554.47
[32m[20221214 00:32:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 684.07
[32m[20221214 00:32:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 552.00
[32m[20221214 00:32:55 @agent_ppo2.py:143][0m Total time:      35.42 min
[32m[20221214 00:32:55 @agent_ppo2.py:145][0m 3203072 total steps have happened
[32m[20221214 00:32:55 @agent_ppo2.py:121][0m #------------------------ Iteration 5564 --------------------------#
[32m[20221214 00:32:56 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:32:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:32:56 @agent_ppo2.py:185][0m |          -0.0016 |         143.3970 |        -120.0721 |
[32m[20221214 00:32:56 @agent_ppo2.py:185][0m |           0.0035 |         132.6069 |        -119.3729 |
[32m[20221214 00:32:56 @agent_ppo2.py:185][0m |          -0.0054 |         128.2895 |        -119.6782 |
[32m[20221214 00:32:56 @agent_ppo2.py:185][0m |          -0.0044 |         124.6174 |        -116.9057 |
[32m[20221214 00:32:56 @agent_ppo2.py:185][0m |          -0.0058 |         125.6767 |        -119.1766 |
[32m[20221214 00:32:56 @agent_ppo2.py:185][0m |          -0.0087 |         122.7654 |        -119.8120 |
[32m[20221214 00:32:56 @agent_ppo2.py:185][0m |          -0.0127 |         121.9028 |        -119.5360 |
[32m[20221214 00:32:57 @agent_ppo2.py:185][0m |          -0.0143 |         122.8754 |        -119.9635 |
[32m[20221214 00:32:57 @agent_ppo2.py:185][0m |          -0.0005 |         125.2256 |        -119.1391 |
[32m[20221214 00:32:57 @agent_ppo2.py:185][0m |          -0.0136 |         122.3276 |        -118.5315 |
[32m[20221214 00:32:57 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221214 00:32:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 659.52
[32m[20221214 00:32:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 748.96
[32m[20221214 00:32:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 655.85
[32m[20221214 00:32:57 @agent_ppo2.py:143][0m Total time:      35.44 min
[32m[20221214 00:32:57 @agent_ppo2.py:145][0m 3205120 total steps have happened
[32m[20221214 00:32:57 @agent_ppo2.py:121][0m #------------------------ Iteration 5565 --------------------------#
[32m[20221214 00:32:57 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:32:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:32:57 @agent_ppo2.py:185][0m |           0.0048 |          84.1638 |        -119.5986 |
[32m[20221214 00:32:57 @agent_ppo2.py:185][0m |          -0.0064 |          75.1314 |        -118.4332 |
[32m[20221214 00:32:57 @agent_ppo2.py:185][0m |          -0.0092 |          73.9515 |        -119.0150 |
[32m[20221214 00:32:58 @agent_ppo2.py:185][0m |          -0.0096 |          73.5756 |        -118.5336 |
[32m[20221214 00:32:58 @agent_ppo2.py:185][0m |          -0.0132 |          70.7891 |        -117.8342 |
[32m[20221214 00:32:58 @agent_ppo2.py:185][0m |          -0.0168 |          70.0976 |        -118.1378 |
[32m[20221214 00:32:58 @agent_ppo2.py:185][0m |          -0.0153 |          70.3991 |        -118.1717 |
[32m[20221214 00:32:58 @agent_ppo2.py:185][0m |          -0.0170 |          69.2669 |        -118.2627 |
[32m[20221214 00:32:58 @agent_ppo2.py:185][0m |          -0.0149 |          69.0366 |        -118.0883 |
[32m[20221214 00:32:58 @agent_ppo2.py:185][0m |          -0.0182 |          68.7561 |        -117.8760 |
[32m[20221214 00:32:58 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:32:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 531.08
[32m[20221214 00:32:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 613.90
[32m[20221214 00:32:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 724.75
[32m[20221214 00:32:58 @agent_ppo2.py:143][0m Total time:      35.47 min
[32m[20221214 00:32:58 @agent_ppo2.py:145][0m 3207168 total steps have happened
[32m[20221214 00:32:58 @agent_ppo2.py:121][0m #------------------------ Iteration 5566 --------------------------#
[32m[20221214 00:32:58 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:32:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:32:59 @agent_ppo2.py:185][0m |           0.0026 |         124.5014 |        -109.2984 |
[32m[20221214 00:32:59 @agent_ppo2.py:185][0m |          -0.0044 |         115.8749 |        -109.9596 |
[32m[20221214 00:32:59 @agent_ppo2.py:185][0m |           0.0049 |         120.4827 |        -110.2993 |
[32m[20221214 00:32:59 @agent_ppo2.py:185][0m |          -0.0050 |         112.3040 |        -110.3898 |
[32m[20221214 00:32:59 @agent_ppo2.py:185][0m |           0.0009 |         122.6810 |        -110.2437 |
[32m[20221214 00:32:59 @agent_ppo2.py:185][0m |          -0.0131 |         111.5666 |        -110.5972 |
[32m[20221214 00:32:59 @agent_ppo2.py:185][0m |          -0.0103 |         108.7555 |        -109.5778 |
[32m[20221214 00:32:59 @agent_ppo2.py:185][0m |          -0.0129 |         108.4490 |        -109.9964 |
[32m[20221214 00:33:00 @agent_ppo2.py:185][0m |          -0.0117 |         108.3121 |        -111.0323 |
[32m[20221214 00:33:00 @agent_ppo2.py:185][0m |          -0.0105 |         108.3319 |        -111.0245 |
[32m[20221214 00:33:00 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221214 00:33:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 530.28
[32m[20221214 00:33:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 691.99
[32m[20221214 00:33:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 815.78
[32m[20221214 00:33:00 @agent_ppo2.py:143][0m Total time:      35.49 min
[32m[20221214 00:33:00 @agent_ppo2.py:145][0m 3209216 total steps have happened
[32m[20221214 00:33:00 @agent_ppo2.py:121][0m #------------------------ Iteration 5567 --------------------------#
[32m[20221214 00:33:00 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:33:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:33:00 @agent_ppo2.py:185][0m |          -0.0022 |         138.0208 |        -119.1739 |
[32m[20221214 00:33:00 @agent_ppo2.py:185][0m |          -0.0064 |         126.3351 |        -118.2175 |
[32m[20221214 00:33:00 @agent_ppo2.py:185][0m |          -0.0088 |         121.9449 |        -118.1132 |
[32m[20221214 00:33:00 @agent_ppo2.py:185][0m |           0.0052 |         126.4555 |        -116.8890 |
[32m[20221214 00:33:01 @agent_ppo2.py:185][0m |          -0.0102 |         118.1584 |        -117.8193 |
[32m[20221214 00:33:01 @agent_ppo2.py:185][0m |          -0.0104 |         116.0114 |        -117.5069 |
[32m[20221214 00:33:01 @agent_ppo2.py:185][0m |          -0.0118 |         116.8555 |        -117.3303 |
[32m[20221214 00:33:01 @agent_ppo2.py:185][0m |          -0.0141 |         114.2705 |        -117.2821 |
[32m[20221214 00:33:01 @agent_ppo2.py:185][0m |          -0.0153 |         112.8599 |        -117.0894 |
[32m[20221214 00:33:01 @agent_ppo2.py:185][0m |          -0.0105 |         111.6648 |        -117.2963 |
[32m[20221214 00:33:01 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:33:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 665.26
[32m[20221214 00:33:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.46
[32m[20221214 00:33:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 567.17
[32m[20221214 00:33:01 @agent_ppo2.py:143][0m Total time:      35.51 min
[32m[20221214 00:33:01 @agent_ppo2.py:145][0m 3211264 total steps have happened
[32m[20221214 00:33:01 @agent_ppo2.py:121][0m #------------------------ Iteration 5568 --------------------------#
[32m[20221214 00:33:01 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:33:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:33:02 @agent_ppo2.py:185][0m |          -0.0027 |         128.1832 |        -117.3201 |
[32m[20221214 00:33:02 @agent_ppo2.py:185][0m |          -0.0016 |         113.4542 |        -117.0824 |
[32m[20221214 00:33:02 @agent_ppo2.py:185][0m |          -0.0123 |         108.5771 |        -116.4839 |
[32m[20221214 00:33:02 @agent_ppo2.py:185][0m |          -0.0129 |         105.9549 |        -116.9627 |
[32m[20221214 00:33:02 @agent_ppo2.py:185][0m |          -0.0115 |         105.2790 |        -116.6083 |
[32m[20221214 00:33:02 @agent_ppo2.py:185][0m |          -0.0137 |         102.5336 |        -116.4346 |
[32m[20221214 00:33:02 @agent_ppo2.py:185][0m |          -0.0177 |         101.4005 |        -115.9313 |
[32m[20221214 00:33:02 @agent_ppo2.py:185][0m |          -0.0168 |         100.7592 |        -116.7996 |
[32m[20221214 00:33:02 @agent_ppo2.py:185][0m |          -0.0171 |          99.7149 |        -115.7457 |
[32m[20221214 00:33:03 @agent_ppo2.py:185][0m |          -0.0132 |         101.7531 |        -115.7483 |
[32m[20221214 00:33:03 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221214 00:33:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 523.84
[32m[20221214 00:33:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 686.79
[32m[20221214 00:33:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 673.85
[32m[20221214 00:33:03 @agent_ppo2.py:143][0m Total time:      35.54 min
[32m[20221214 00:33:03 @agent_ppo2.py:145][0m 3213312 total steps have happened
[32m[20221214 00:33:03 @agent_ppo2.py:121][0m #------------------------ Iteration 5569 --------------------------#
[32m[20221214 00:33:03 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:33:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:33:03 @agent_ppo2.py:185][0m |          -0.0030 |         153.0214 |        -121.4667 |
[32m[20221214 00:33:03 @agent_ppo2.py:185][0m |          -0.0030 |         146.9929 |        -120.7223 |
[32m[20221214 00:33:03 @agent_ppo2.py:185][0m |          -0.0076 |         144.4247 |        -121.1219 |
[32m[20221214 00:33:03 @agent_ppo2.py:185][0m |          -0.0058 |         142.3098 |        -120.4926 |
[32m[20221214 00:33:04 @agent_ppo2.py:185][0m |          -0.0060 |         140.6067 |        -120.5510 |
[32m[20221214 00:33:04 @agent_ppo2.py:185][0m |          -0.0087 |         141.2170 |        -120.5820 |
[32m[20221214 00:33:04 @agent_ppo2.py:185][0m |          -0.0063 |         139.6184 |        -121.3705 |
[32m[20221214 00:33:04 @agent_ppo2.py:185][0m |          -0.0015 |         140.5204 |        -121.4500 |
[32m[20221214 00:33:04 @agent_ppo2.py:185][0m |          -0.0070 |         139.5460 |        -120.7659 |
[32m[20221214 00:33:04 @agent_ppo2.py:185][0m |          -0.0119 |         138.9010 |        -120.9786 |
[32m[20221214 00:33:04 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221214 00:33:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 702.33
[32m[20221214 00:33:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 788.13
[32m[20221214 00:33:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 596.54
[32m[20221214 00:33:04 @agent_ppo2.py:143][0m Total time:      35.56 min
[32m[20221214 00:33:04 @agent_ppo2.py:145][0m 3215360 total steps have happened
[32m[20221214 00:33:04 @agent_ppo2.py:121][0m #------------------------ Iteration 5570 --------------------------#
[32m[20221214 00:33:04 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:33:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:33:05 @agent_ppo2.py:185][0m |          -0.0007 |         124.3384 |        -120.1386 |
[32m[20221214 00:33:05 @agent_ppo2.py:185][0m |          -0.0054 |         115.5637 |        -119.6203 |
[32m[20221214 00:33:05 @agent_ppo2.py:185][0m |          -0.0028 |         116.4611 |        -120.4935 |
[32m[20221214 00:33:05 @agent_ppo2.py:185][0m |          -0.0052 |         112.1743 |        -120.2426 |
[32m[20221214 00:33:05 @agent_ppo2.py:185][0m |          -0.0013 |         113.4750 |        -119.5177 |
[32m[20221214 00:33:05 @agent_ppo2.py:185][0m |          -0.0103 |         110.3577 |        -120.0816 |
[32m[20221214 00:33:05 @agent_ppo2.py:185][0m |          -0.0102 |         109.9530 |        -120.3205 |
[32m[20221214 00:33:05 @agent_ppo2.py:185][0m |          -0.0147 |         110.1261 |        -120.7258 |
[32m[20221214 00:33:05 @agent_ppo2.py:185][0m |          -0.0128 |         109.1696 |        -120.4835 |
[32m[20221214 00:33:06 @agent_ppo2.py:185][0m |          -0.0074 |         109.0042 |        -120.8362 |
[32m[20221214 00:33:06 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 00:33:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 583.36
[32m[20221214 00:33:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 716.61
[32m[20221214 00:33:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 844.71
[32m[20221214 00:33:06 @agent_ppo2.py:143][0m Total time:      35.59 min
[32m[20221214 00:33:06 @agent_ppo2.py:145][0m 3217408 total steps have happened
[32m[20221214 00:33:06 @agent_ppo2.py:121][0m #------------------------ Iteration 5571 --------------------------#
[32m[20221214 00:33:06 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:33:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:33:06 @agent_ppo2.py:185][0m |          -0.0006 |         167.5071 |        -120.1151 |
[32m[20221214 00:33:06 @agent_ppo2.py:185][0m |          -0.0015 |         165.5311 |        -120.5423 |
[32m[20221214 00:33:06 @agent_ppo2.py:185][0m |          -0.0048 |         165.3908 |        -118.6038 |
[32m[20221214 00:33:06 @agent_ppo2.py:185][0m |          -0.0016 |         163.9530 |        -119.4892 |
[32m[20221214 00:33:06 @agent_ppo2.py:185][0m |          -0.0029 |         164.5430 |        -119.9923 |
[32m[20221214 00:33:07 @agent_ppo2.py:185][0m |           0.0053 |         168.7007 |        -119.6685 |
[32m[20221214 00:33:07 @agent_ppo2.py:185][0m |           0.0026 |         165.8229 |        -119.7083 |
[32m[20221214 00:33:07 @agent_ppo2.py:185][0m |          -0.0002 |         163.9253 |        -119.5579 |
[32m[20221214 00:33:07 @agent_ppo2.py:185][0m |          -0.0026 |         162.5495 |        -120.2151 |
[32m[20221214 00:33:07 @agent_ppo2.py:185][0m |           0.0036 |         171.2202 |        -120.3034 |
[32m[20221214 00:33:07 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221214 00:33:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 805.82
[32m[20221214 00:33:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.68
[32m[20221214 00:33:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 802.25
[32m[20221214 00:33:07 @agent_ppo2.py:143][0m Total time:      35.61 min
[32m[20221214 00:33:07 @agent_ppo2.py:145][0m 3219456 total steps have happened
[32m[20221214 00:33:07 @agent_ppo2.py:121][0m #------------------------ Iteration 5572 --------------------------#
[32m[20221214 00:33:07 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:33:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:33:08 @agent_ppo2.py:185][0m |           0.0051 |         132.5829 |        -118.9853 |
[32m[20221214 00:33:08 @agent_ppo2.py:185][0m |          -0.0055 |         124.7712 |        -119.1234 |
[32m[20221214 00:33:08 @agent_ppo2.py:185][0m |          -0.0018 |         123.1152 |        -119.0180 |
[32m[20221214 00:33:08 @agent_ppo2.py:185][0m |          -0.0065 |         121.9780 |        -118.3738 |
[32m[20221214 00:33:08 @agent_ppo2.py:185][0m |          -0.0054 |         122.0714 |        -119.0064 |
[32m[20221214 00:33:08 @agent_ppo2.py:185][0m |          -0.0072 |         122.8044 |        -118.3713 |
[32m[20221214 00:33:08 @agent_ppo2.py:185][0m |           0.0061 |         131.3355 |        -118.4943 |
[32m[20221214 00:33:08 @agent_ppo2.py:185][0m |           0.0145 |         131.5497 |        -118.5917 |
[32m[20221214 00:33:08 @agent_ppo2.py:185][0m |          -0.0081 |         120.9871 |        -119.2423 |
[32m[20221214 00:33:08 @agent_ppo2.py:185][0m |          -0.0086 |         120.8574 |        -119.1319 |
[32m[20221214 00:33:08 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:33:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 577.75
[32m[20221214 00:33:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 655.33
[32m[20221214 00:33:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 804.67
[32m[20221214 00:33:09 @agent_ppo2.py:143][0m Total time:      35.64 min
[32m[20221214 00:33:09 @agent_ppo2.py:145][0m 3221504 total steps have happened
[32m[20221214 00:33:09 @agent_ppo2.py:121][0m #------------------------ Iteration 5573 --------------------------#
[32m[20221214 00:33:09 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:33:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:33:09 @agent_ppo2.py:185][0m |           0.0013 |         139.4005 |        -120.7490 |
[32m[20221214 00:33:09 @agent_ppo2.py:185][0m |          -0.0020 |         132.6622 |        -121.6431 |
[32m[20221214 00:33:09 @agent_ppo2.py:185][0m |          -0.0051 |         130.8688 |        -122.0950 |
[32m[20221214 00:33:09 @agent_ppo2.py:185][0m |          -0.0044 |         129.0571 |        -121.8635 |
[32m[20221214 00:33:09 @agent_ppo2.py:185][0m |           0.0047 |         136.6573 |        -121.2901 |
[32m[20221214 00:33:10 @agent_ppo2.py:185][0m |          -0.0060 |         127.6041 |        -121.7891 |
[32m[20221214 00:33:10 @agent_ppo2.py:185][0m |          -0.0056 |         125.6324 |        -121.0145 |
[32m[20221214 00:33:10 @agent_ppo2.py:185][0m |          -0.0096 |         125.7438 |        -122.4762 |
[32m[20221214 00:33:10 @agent_ppo2.py:185][0m |          -0.0103 |         124.6379 |        -121.4482 |
[32m[20221214 00:33:10 @agent_ppo2.py:185][0m |          -0.0046 |         124.9148 |        -121.3028 |
[32m[20221214 00:33:10 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221214 00:33:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 665.55
[32m[20221214 00:33:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 749.23
[32m[20221214 00:33:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 501.29
[32m[20221214 00:33:10 @agent_ppo2.py:143][0m Total time:      35.66 min
[32m[20221214 00:33:10 @agent_ppo2.py:145][0m 3223552 total steps have happened
[32m[20221214 00:33:10 @agent_ppo2.py:121][0m #------------------------ Iteration 5574 --------------------------#
[32m[20221214 00:33:10 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:33:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:33:10 @agent_ppo2.py:185][0m |           0.0052 |         120.2775 |        -118.1393 |
[32m[20221214 00:33:11 @agent_ppo2.py:185][0m |          -0.0053 |         105.0860 |        -117.7346 |
[32m[20221214 00:33:11 @agent_ppo2.py:185][0m |          -0.0087 |         101.0960 |        -117.9152 |
[32m[20221214 00:33:11 @agent_ppo2.py:185][0m |          -0.0151 |          99.0506 |        -118.2656 |
[32m[20221214 00:33:11 @agent_ppo2.py:185][0m |          -0.0094 |          97.4386 |        -118.2639 |
[32m[20221214 00:33:11 @agent_ppo2.py:185][0m |          -0.0135 |          96.8543 |        -117.2205 |
[32m[20221214 00:33:11 @agent_ppo2.py:185][0m |          -0.0097 |         102.5438 |        -117.9734 |
[32m[20221214 00:33:11 @agent_ppo2.py:185][0m |          -0.0087 |          98.2117 |        -117.0126 |
[32m[20221214 00:33:11 @agent_ppo2.py:185][0m |          -0.0206 |          94.9808 |        -117.0209 |
[32m[20221214 00:33:11 @agent_ppo2.py:185][0m |          -0.0212 |          93.9934 |        -117.5828 |
[32m[20221214 00:33:11 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:33:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 517.77
[32m[20221214 00:33:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 718.77
[32m[20221214 00:33:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 732.14
[32m[20221214 00:33:12 @agent_ppo2.py:143][0m Total time:      35.69 min
[32m[20221214 00:33:12 @agent_ppo2.py:145][0m 3225600 total steps have happened
[32m[20221214 00:33:12 @agent_ppo2.py:121][0m #------------------------ Iteration 5575 --------------------------#
[32m[20221214 00:33:12 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:33:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:33:12 @agent_ppo2.py:185][0m |           0.0008 |         139.4022 |        -128.2485 |
[32m[20221214 00:33:12 @agent_ppo2.py:185][0m |          -0.0061 |         128.2083 |        -128.6395 |
[32m[20221214 00:33:12 @agent_ppo2.py:185][0m |          -0.0078 |         123.4572 |        -128.3525 |
[32m[20221214 00:33:12 @agent_ppo2.py:185][0m |          -0.0098 |         120.7914 |        -127.7167 |
[32m[20221214 00:33:12 @agent_ppo2.py:185][0m |          -0.0081 |         119.1941 |        -127.6651 |
[32m[20221214 00:33:12 @agent_ppo2.py:185][0m |          -0.0114 |         118.0995 |        -127.9492 |
[32m[20221214 00:33:13 @agent_ppo2.py:185][0m |          -0.0118 |         117.4918 |        -127.5812 |
[32m[20221214 00:33:13 @agent_ppo2.py:185][0m |          -0.0138 |         116.6369 |        -127.8791 |
[32m[20221214 00:33:13 @agent_ppo2.py:185][0m |          -0.0115 |         117.0136 |        -127.9488 |
[32m[20221214 00:33:13 @agent_ppo2.py:185][0m |          -0.0155 |         115.1560 |        -127.4253 |
[32m[20221214 00:33:13 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221214 00:33:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 703.76
[32m[20221214 00:33:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 776.70
[32m[20221214 00:33:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 631.81
[32m[20221214 00:33:13 @agent_ppo2.py:143][0m Total time:      35.71 min
[32m[20221214 00:33:13 @agent_ppo2.py:145][0m 3227648 total steps have happened
[32m[20221214 00:33:13 @agent_ppo2.py:121][0m #------------------------ Iteration 5576 --------------------------#
[32m[20221214 00:33:13 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:33:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:33:13 @agent_ppo2.py:185][0m |           0.0014 |         107.9894 |        -115.7514 |
[32m[20221214 00:33:13 @agent_ppo2.py:185][0m |          -0.0028 |          98.2257 |        -115.4843 |
[32m[20221214 00:33:14 @agent_ppo2.py:185][0m |          -0.0096 |          95.9613 |        -115.5428 |
[32m[20221214 00:33:14 @agent_ppo2.py:185][0m |          -0.0079 |          93.6414 |        -115.3445 |
[32m[20221214 00:33:14 @agent_ppo2.py:185][0m |          -0.0090 |          92.0947 |        -115.2616 |
[32m[20221214 00:33:14 @agent_ppo2.py:185][0m |          -0.0045 |          95.0151 |        -115.0644 |
[32m[20221214 00:33:14 @agent_ppo2.py:185][0m |          -0.0150 |          90.6889 |        -115.6338 |
[32m[20221214 00:33:14 @agent_ppo2.py:185][0m |          -0.0085 |          90.5026 |        -115.5858 |
[32m[20221214 00:33:14 @agent_ppo2.py:185][0m |          -0.0076 |          91.2454 |        -114.7137 |
[32m[20221214 00:33:14 @agent_ppo2.py:185][0m |          -0.0138 |          88.9557 |        -114.7009 |
[32m[20221214 00:33:14 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:33:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 501.23
[32m[20221214 00:33:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 595.38
[32m[20221214 00:33:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 588.62
[32m[20221214 00:33:14 @agent_ppo2.py:143][0m Total time:      35.74 min
[32m[20221214 00:33:14 @agent_ppo2.py:145][0m 3229696 total steps have happened
[32m[20221214 00:33:14 @agent_ppo2.py:121][0m #------------------------ Iteration 5577 --------------------------#
[32m[20221214 00:33:15 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:33:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:33:15 @agent_ppo2.py:185][0m |           0.0003 |         117.4803 |        -120.5043 |
[32m[20221214 00:33:15 @agent_ppo2.py:185][0m |          -0.0043 |         108.7571 |        -119.2004 |
[32m[20221214 00:33:15 @agent_ppo2.py:185][0m |          -0.0077 |         106.2470 |        -119.1891 |
[32m[20221214 00:33:15 @agent_ppo2.py:185][0m |          -0.0056 |         110.0289 |        -119.5219 |
[32m[20221214 00:33:15 @agent_ppo2.py:185][0m |          -0.0118 |         104.9376 |        -118.8403 |
[32m[20221214 00:33:15 @agent_ppo2.py:185][0m |          -0.0104 |         103.6126 |        -118.5950 |
[32m[20221214 00:33:16 @agent_ppo2.py:185][0m |          -0.0105 |         102.9375 |        -118.6108 |
[32m[20221214 00:33:16 @agent_ppo2.py:185][0m |          -0.0162 |         102.5998 |        -118.5299 |
[32m[20221214 00:33:16 @agent_ppo2.py:185][0m |          -0.0130 |         102.5158 |        -117.8849 |
[32m[20221214 00:33:16 @agent_ppo2.py:185][0m |          -0.0147 |         102.1135 |        -117.9565 |
[32m[20221214 00:33:16 @agent_ppo2.py:130][0m Policy update time: 1.17 s
[32m[20221214 00:33:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 609.59
[32m[20221214 00:33:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 723.95
[32m[20221214 00:33:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 531.95
[32m[20221214 00:33:16 @agent_ppo2.py:143][0m Total time:      35.76 min
[32m[20221214 00:33:16 @agent_ppo2.py:145][0m 3231744 total steps have happened
[32m[20221214 00:33:16 @agent_ppo2.py:121][0m #------------------------ Iteration 5578 --------------------------#
[32m[20221214 00:33:16 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:33:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:33:16 @agent_ppo2.py:185][0m |          -0.0017 |         151.7249 |        -120.4552 |
[32m[20221214 00:33:16 @agent_ppo2.py:185][0m |          -0.0001 |         143.0750 |        -119.9653 |
[32m[20221214 00:33:17 @agent_ppo2.py:185][0m |           0.0027 |         155.8358 |        -120.8217 |
[32m[20221214 00:33:17 @agent_ppo2.py:185][0m |          -0.0057 |         142.0337 |        -120.3202 |
[32m[20221214 00:33:17 @agent_ppo2.py:185][0m |          -0.0059 |         138.7066 |        -119.5539 |
[32m[20221214 00:33:17 @agent_ppo2.py:185][0m |          -0.0105 |         137.9008 |        -120.0128 |
[32m[20221214 00:33:17 @agent_ppo2.py:185][0m |          -0.0080 |         137.6399 |        -119.3535 |
[32m[20221214 00:33:17 @agent_ppo2.py:185][0m |          -0.0100 |         136.2984 |        -119.7323 |
[32m[20221214 00:33:17 @agent_ppo2.py:185][0m |          -0.0134 |         135.9210 |        -120.2839 |
[32m[20221214 00:33:17 @agent_ppo2.py:185][0m |          -0.0093 |         134.5278 |        -119.7873 |
[32m[20221214 00:33:17 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 00:33:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 524.79
[32m[20221214 00:33:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 629.06
[32m[20221214 00:33:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 563.66
[32m[20221214 00:33:17 @agent_ppo2.py:143][0m Total time:      35.78 min
[32m[20221214 00:33:17 @agent_ppo2.py:145][0m 3233792 total steps have happened
[32m[20221214 00:33:17 @agent_ppo2.py:121][0m #------------------------ Iteration 5579 --------------------------#
[32m[20221214 00:33:18 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:33:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:33:18 @agent_ppo2.py:185][0m |           0.0070 |         143.8727 |        -114.6155 |
[32m[20221214 00:33:18 @agent_ppo2.py:185][0m |           0.0011 |         136.0548 |        -114.6758 |
[32m[20221214 00:33:18 @agent_ppo2.py:185][0m |          -0.0028 |         130.1781 |        -112.6623 |
[32m[20221214 00:33:18 @agent_ppo2.py:185][0m |          -0.0063 |         129.3641 |        -114.4151 |
[32m[20221214 00:33:18 @agent_ppo2.py:185][0m |           0.0006 |         134.3134 |        -114.3413 |
[32m[20221214 00:33:18 @agent_ppo2.py:185][0m |           0.0001 |         138.4679 |        -113.8541 |
[32m[20221214 00:33:18 @agent_ppo2.py:185][0m |          -0.0051 |         128.1029 |        -112.8742 |
[32m[20221214 00:33:19 @agent_ppo2.py:185][0m |          -0.0086 |         126.8779 |        -114.1743 |
[32m[20221214 00:33:19 @agent_ppo2.py:185][0m |          -0.0120 |         126.2963 |        -114.0345 |
[32m[20221214 00:33:19 @agent_ppo2.py:185][0m |          -0.0114 |         126.3940 |        -113.6174 |
[32m[20221214 00:33:19 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:33:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 694.38
[32m[20221214 00:33:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 753.16
[32m[20221214 00:33:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 795.07
[32m[20221214 00:33:19 @agent_ppo2.py:143][0m Total time:      35.81 min
[32m[20221214 00:33:19 @agent_ppo2.py:145][0m 3235840 total steps have happened
[32m[20221214 00:33:19 @agent_ppo2.py:121][0m #------------------------ Iteration 5580 --------------------------#
[32m[20221214 00:33:19 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:33:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:33:19 @agent_ppo2.py:185][0m |           0.0033 |         146.2939 |        -113.6441 |
[32m[20221214 00:33:19 @agent_ppo2.py:185][0m |          -0.0023 |         139.3287 |        -113.0676 |
[32m[20221214 00:33:19 @agent_ppo2.py:185][0m |          -0.0021 |         137.3755 |        -113.5291 |
[32m[20221214 00:33:20 @agent_ppo2.py:185][0m |          -0.0133 |         136.6971 |        -113.7967 |
[32m[20221214 00:33:20 @agent_ppo2.py:185][0m |          -0.0068 |         134.8270 |        -112.8792 |
[32m[20221214 00:33:20 @agent_ppo2.py:185][0m |          -0.0089 |         134.4753 |        -114.1014 |
[32m[20221214 00:33:20 @agent_ppo2.py:185][0m |          -0.0055 |         136.7813 |        -113.1307 |
[32m[20221214 00:33:20 @agent_ppo2.py:185][0m |          -0.0100 |         133.5540 |        -113.8614 |
[32m[20221214 00:33:20 @agent_ppo2.py:185][0m |          -0.0091 |         132.5788 |        -112.9732 |
[32m[20221214 00:33:20 @agent_ppo2.py:185][0m |           0.0038 |         142.2987 |        -112.9517 |
[32m[20221214 00:33:20 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221214 00:33:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 719.72
[32m[20221214 00:33:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 776.49
[32m[20221214 00:33:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 827.96
[32m[20221214 00:33:20 @agent_ppo2.py:143][0m Total time:      35.83 min
[32m[20221214 00:33:20 @agent_ppo2.py:145][0m 3237888 total steps have happened
[32m[20221214 00:33:20 @agent_ppo2.py:121][0m #------------------------ Iteration 5581 --------------------------#
[32m[20221214 00:33:21 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:33:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:33:21 @agent_ppo2.py:185][0m |           0.0019 |         179.0590 |        -117.4348 |
[32m[20221214 00:33:21 @agent_ppo2.py:185][0m |          -0.0055 |         167.1419 |        -117.7462 |
[32m[20221214 00:33:21 @agent_ppo2.py:185][0m |          -0.0043 |         161.6325 |        -117.7588 |
[32m[20221214 00:33:21 @agent_ppo2.py:185][0m |          -0.0040 |         158.4308 |        -117.9285 |
[32m[20221214 00:33:21 @agent_ppo2.py:185][0m |           0.0005 |         162.5971 |        -117.8021 |
[32m[20221214 00:33:21 @agent_ppo2.py:185][0m |          -0.0044 |         156.5173 |        -118.0191 |
[32m[20221214 00:33:21 @agent_ppo2.py:185][0m |           0.0002 |         170.2646 |        -117.9404 |
[32m[20221214 00:33:21 @agent_ppo2.py:185][0m |          -0.0053 |         158.2325 |        -117.6207 |
[32m[20221214 00:33:22 @agent_ppo2.py:185][0m |          -0.0096 |         151.7499 |        -117.8706 |
[32m[20221214 00:33:22 @agent_ppo2.py:185][0m |          -0.0027 |         155.3275 |        -118.2899 |
[32m[20221214 00:33:22 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221214 00:33:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 487.18
[32m[20221214 00:33:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 577.05
[32m[20221214 00:33:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 767.73
[32m[20221214 00:33:22 @agent_ppo2.py:143][0m Total time:      35.86 min
[32m[20221214 00:33:22 @agent_ppo2.py:145][0m 3239936 total steps have happened
[32m[20221214 00:33:22 @agent_ppo2.py:121][0m #------------------------ Iteration 5582 --------------------------#
[32m[20221214 00:33:22 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:33:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:33:22 @agent_ppo2.py:185][0m |           0.0038 |          88.7194 |        -117.6017 |
[32m[20221214 00:33:22 @agent_ppo2.py:185][0m |          -0.0068 |          82.6642 |        -116.8446 |
[32m[20221214 00:33:22 @agent_ppo2.py:185][0m |          -0.0033 |          81.1482 |        -116.8636 |
[32m[20221214 00:33:23 @agent_ppo2.py:185][0m |          -0.0063 |          78.9473 |        -116.6028 |
[32m[20221214 00:33:23 @agent_ppo2.py:185][0m |          -0.0128 |          77.6795 |        -116.1041 |
[32m[20221214 00:33:23 @agent_ppo2.py:185][0m |          -0.0101 |          77.2051 |        -115.9678 |
[32m[20221214 00:33:23 @agent_ppo2.py:185][0m |          -0.0136 |          76.3624 |        -116.2149 |
[32m[20221214 00:33:23 @agent_ppo2.py:185][0m |          -0.0178 |          75.8357 |        -115.3272 |
[32m[20221214 00:33:23 @agent_ppo2.py:185][0m |          -0.0110 |          74.7550 |        -114.9759 |
[32m[20221214 00:33:23 @agent_ppo2.py:185][0m |          -0.0126 |          74.3137 |        -114.6262 |
[32m[20221214 00:33:23 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:33:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 605.94
[32m[20221214 00:33:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 733.19
[32m[20221214 00:33:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 716.58
[32m[20221214 00:33:23 @agent_ppo2.py:143][0m Total time:      35.88 min
[32m[20221214 00:33:23 @agent_ppo2.py:145][0m 3241984 total steps have happened
[32m[20221214 00:33:23 @agent_ppo2.py:121][0m #------------------------ Iteration 5583 --------------------------#
[32m[20221214 00:33:23 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:33:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:33:24 @agent_ppo2.py:185][0m |           0.0085 |          86.5747 |        -111.9176 |
[32m[20221214 00:33:24 @agent_ppo2.py:185][0m |          -0.0038 |          79.2738 |        -111.5354 |
[32m[20221214 00:33:24 @agent_ppo2.py:185][0m |          -0.0113 |          76.6182 |        -111.2763 |
[32m[20221214 00:33:24 @agent_ppo2.py:185][0m |          -0.0115 |          75.0267 |        -110.9852 |
[32m[20221214 00:33:24 @agent_ppo2.py:185][0m |          -0.0141 |          73.9821 |        -110.4663 |
[32m[20221214 00:33:24 @agent_ppo2.py:185][0m |          -0.0057 |          74.9496 |        -110.5106 |
[32m[20221214 00:33:24 @agent_ppo2.py:185][0m |          -0.0082 |          71.9834 |        -110.2477 |
[32m[20221214 00:33:24 @agent_ppo2.py:185][0m |          -0.0140 |          72.7095 |        -109.7591 |
[32m[20221214 00:33:24 @agent_ppo2.py:185][0m |          -0.0192 |          70.9918 |        -110.3453 |
[32m[20221214 00:33:25 @agent_ppo2.py:185][0m |          -0.0149 |          70.6985 |        -109.6285 |
[32m[20221214 00:33:25 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:33:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 542.49
[32m[20221214 00:33:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 710.93
[32m[20221214 00:33:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 717.99
[32m[20221214 00:33:25 @agent_ppo2.py:143][0m Total time:      35.91 min
[32m[20221214 00:33:25 @agent_ppo2.py:145][0m 3244032 total steps have happened
[32m[20221214 00:33:25 @agent_ppo2.py:121][0m #------------------------ Iteration 5584 --------------------------#
[32m[20221214 00:33:25 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:33:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:33:25 @agent_ppo2.py:185][0m |          -0.0012 |         122.3519 |        -108.5533 |
[32m[20221214 00:33:25 @agent_ppo2.py:185][0m |           0.0022 |         118.4143 |        -108.4929 |
[32m[20221214 00:33:25 @agent_ppo2.py:185][0m |          -0.0104 |         115.9520 |        -108.3440 |
[32m[20221214 00:33:25 @agent_ppo2.py:185][0m |           0.0070 |         118.6983 |        -107.7388 |
[32m[20221214 00:33:25 @agent_ppo2.py:185][0m |          -0.0050 |         115.7732 |        -107.7077 |
[32m[20221214 00:33:26 @agent_ppo2.py:185][0m |          -0.0075 |         113.3287 |        -107.1896 |
[32m[20221214 00:33:26 @agent_ppo2.py:185][0m |          -0.0138 |         112.8498 |        -107.2039 |
[32m[20221214 00:33:26 @agent_ppo2.py:185][0m |          -0.0079 |         112.1249 |        -107.0188 |
[32m[20221214 00:33:26 @agent_ppo2.py:185][0m |          -0.0102 |         112.3983 |        -106.7882 |
[32m[20221214 00:33:26 @agent_ppo2.py:185][0m |          -0.0119 |         111.3139 |        -106.9811 |
[32m[20221214 00:33:26 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:33:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 746.39
[32m[20221214 00:33:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.76
[32m[20221214 00:33:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 817.26
[32m[20221214 00:33:26 @agent_ppo2.py:143][0m Total time:      35.93 min
[32m[20221214 00:33:26 @agent_ppo2.py:145][0m 3246080 total steps have happened
[32m[20221214 00:33:26 @agent_ppo2.py:121][0m #------------------------ Iteration 5585 --------------------------#
[32m[20221214 00:33:26 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:33:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:33:26 @agent_ppo2.py:185][0m |           0.0002 |          40.8236 |        -108.5900 |
[32m[20221214 00:33:27 @agent_ppo2.py:185][0m |          -0.0028 |          32.4222 |        -107.9837 |
[32m[20221214 00:33:27 @agent_ppo2.py:185][0m |          -0.0097 |          29.4197 |        -107.7106 |
[32m[20221214 00:33:27 @agent_ppo2.py:185][0m |          -0.0190 |          27.4796 |        -107.7769 |
[32m[20221214 00:33:27 @agent_ppo2.py:185][0m |          -0.0164 |          25.7295 |        -107.6608 |
[32m[20221214 00:33:27 @agent_ppo2.py:185][0m |          -0.0237 |          24.7807 |        -107.5090 |
[32m[20221214 00:33:27 @agent_ppo2.py:185][0m |          -0.0175 |          23.8685 |        -107.2353 |
[32m[20221214 00:33:27 @agent_ppo2.py:185][0m |          -0.0208 |          23.0567 |        -107.5061 |
[32m[20221214 00:33:27 @agent_ppo2.py:185][0m |          -0.0285 |          22.3196 |        -107.3360 |
[32m[20221214 00:33:27 @agent_ppo2.py:185][0m |          -0.0245 |          21.4988 |        -107.1219 |
[32m[20221214 00:33:27 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:33:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 445.98
[32m[20221214 00:33:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 516.97
[32m[20221214 00:33:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 835.62
[32m[20221214 00:33:28 @agent_ppo2.py:143][0m Total time:      35.95 min
[32m[20221214 00:33:28 @agent_ppo2.py:145][0m 3248128 total steps have happened
[32m[20221214 00:33:28 @agent_ppo2.py:121][0m #------------------------ Iteration 5586 --------------------------#
[32m[20221214 00:33:28 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:33:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:33:28 @agent_ppo2.py:185][0m |           0.0002 |         114.0708 |        -110.4714 |
[32m[20221214 00:33:28 @agent_ppo2.py:185][0m |           0.0136 |         126.9125 |        -110.9563 |
[32m[20221214 00:33:28 @agent_ppo2.py:185][0m |          -0.0020 |         109.3977 |        -111.0009 |
[32m[20221214 00:33:28 @agent_ppo2.py:185][0m |          -0.0073 |         107.1805 |        -111.3512 |
[32m[20221214 00:33:28 @agent_ppo2.py:185][0m |          -0.0061 |         108.4765 |        -110.4053 |
[32m[20221214 00:33:28 @agent_ppo2.py:185][0m |          -0.0054 |         104.7930 |        -110.7661 |
[32m[20221214 00:33:29 @agent_ppo2.py:185][0m |           0.0004 |         111.9190 |        -111.8086 |
[32m[20221214 00:33:29 @agent_ppo2.py:185][0m |          -0.0053 |         104.3146 |        -111.4715 |
[32m[20221214 00:33:29 @agent_ppo2.py:185][0m |          -0.0090 |         103.3727 |        -111.1027 |
[32m[20221214 00:33:29 @agent_ppo2.py:185][0m |          -0.0001 |         107.5379 |        -112.0274 |
[32m[20221214 00:33:29 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:33:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 782.09
[32m[20221214 00:33:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.44
[32m[20221214 00:33:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 716.16
[32m[20221214 00:33:29 @agent_ppo2.py:143][0m Total time:      35.98 min
[32m[20221214 00:33:29 @agent_ppo2.py:145][0m 3250176 total steps have happened
[32m[20221214 00:33:29 @agent_ppo2.py:121][0m #------------------------ Iteration 5587 --------------------------#
[32m[20221214 00:33:29 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:33:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:33:29 @agent_ppo2.py:185][0m |           0.0021 |          58.5210 |        -111.1096 |
[32m[20221214 00:33:29 @agent_ppo2.py:185][0m |          -0.0120 |          49.9097 |        -111.2942 |
[32m[20221214 00:33:30 @agent_ppo2.py:185][0m |          -0.0063 |          47.8992 |        -111.5628 |
[32m[20221214 00:33:30 @agent_ppo2.py:185][0m |          -0.0111 |          47.0886 |        -111.6023 |
[32m[20221214 00:33:30 @agent_ppo2.py:185][0m |          -0.0152 |          45.8194 |        -111.8257 |
[32m[20221214 00:33:30 @agent_ppo2.py:185][0m |          -0.0150 |          44.8413 |        -111.7521 |
[32m[20221214 00:33:30 @agent_ppo2.py:185][0m |          -0.0182 |          44.8316 |        -111.9352 |
[32m[20221214 00:33:30 @agent_ppo2.py:185][0m |          -0.0176 |          44.8283 |        -111.9971 |
[32m[20221214 00:33:30 @agent_ppo2.py:185][0m |          -0.0089 |          44.0241 |        -111.7469 |
[32m[20221214 00:33:30 @agent_ppo2.py:185][0m |          -0.0149 |          43.5042 |        -111.8840 |
[32m[20221214 00:33:30 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:33:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 537.32
[32m[20221214 00:33:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 774.74
[32m[20221214 00:33:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 798.35
[32m[20221214 00:33:30 @agent_ppo2.py:143][0m Total time:      36.00 min
[32m[20221214 00:33:30 @agent_ppo2.py:145][0m 3252224 total steps have happened
[32m[20221214 00:33:30 @agent_ppo2.py:121][0m #------------------------ Iteration 5588 --------------------------#
[32m[20221214 00:33:31 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:33:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:33:31 @agent_ppo2.py:185][0m |           0.0074 |         126.9522 |        -115.2955 |
[32m[20221214 00:33:31 @agent_ppo2.py:185][0m |          -0.0004 |         119.8312 |        -114.0602 |
[32m[20221214 00:33:31 @agent_ppo2.py:185][0m |          -0.0055 |         116.1678 |        -115.4113 |
[32m[20221214 00:33:31 @agent_ppo2.py:185][0m |          -0.0043 |         114.6451 |        -114.9044 |
[32m[20221214 00:33:31 @agent_ppo2.py:185][0m |          -0.0062 |         112.7200 |        -114.5422 |
[32m[20221214 00:33:31 @agent_ppo2.py:185][0m |          -0.0063 |         110.3599 |        -115.7019 |
[32m[20221214 00:33:31 @agent_ppo2.py:185][0m |          -0.0116 |         110.4189 |        -114.6403 |
[32m[20221214 00:33:31 @agent_ppo2.py:185][0m |          -0.0086 |         109.3190 |        -115.6096 |
[32m[20221214 00:33:32 @agent_ppo2.py:185][0m |          -0.0099 |         109.9327 |        -114.3284 |
[32m[20221214 00:33:32 @agent_ppo2.py:185][0m |          -0.0070 |         109.0398 |        -114.9220 |
[32m[20221214 00:33:32 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:33:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 460.79
[32m[20221214 00:33:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 717.61
[32m[20221214 00:33:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 835.67
[32m[20221214 00:33:32 @agent_ppo2.py:143][0m Total time:      36.02 min
[32m[20221214 00:33:32 @agent_ppo2.py:145][0m 3254272 total steps have happened
[32m[20221214 00:33:32 @agent_ppo2.py:121][0m #------------------------ Iteration 5589 --------------------------#
[32m[20221214 00:33:32 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:33:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:33:32 @agent_ppo2.py:185][0m |          -0.0004 |         121.9049 |        -111.7790 |
[32m[20221214 00:33:32 @agent_ppo2.py:185][0m |          -0.0051 |         115.7513 |        -111.8186 |
[32m[20221214 00:33:32 @agent_ppo2.py:185][0m |           0.0082 |         127.5524 |        -111.3662 |
[32m[20221214 00:33:32 @agent_ppo2.py:185][0m |          -0.0061 |         113.0065 |        -111.0599 |
[32m[20221214 00:33:33 @agent_ppo2.py:185][0m |          -0.0059 |         110.4424 |        -111.0061 |
[32m[20221214 00:33:33 @agent_ppo2.py:185][0m |          -0.0080 |         109.1526 |        -110.9630 |
[32m[20221214 00:33:33 @agent_ppo2.py:185][0m |          -0.0100 |         109.0350 |        -110.3419 |
[32m[20221214 00:33:33 @agent_ppo2.py:185][0m |          -0.0116 |         108.1180 |        -110.4381 |
[32m[20221214 00:33:33 @agent_ppo2.py:185][0m |          -0.0099 |         107.2751 |        -110.1495 |
[32m[20221214 00:33:33 @agent_ppo2.py:185][0m |          -0.0114 |         106.2420 |        -109.0284 |
[32m[20221214 00:33:33 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:33:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 684.57
[32m[20221214 00:33:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 777.47
[32m[20221214 00:33:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 841.41
[32m[20221214 00:33:33 @agent_ppo2.py:143][0m Total time:      36.05 min
[32m[20221214 00:33:33 @agent_ppo2.py:145][0m 3256320 total steps have happened
[32m[20221214 00:33:33 @agent_ppo2.py:121][0m #------------------------ Iteration 5590 --------------------------#
[32m[20221214 00:33:33 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:33:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:33:34 @agent_ppo2.py:185][0m |           0.0100 |         191.3380 |        -111.9642 |
[32m[20221214 00:33:34 @agent_ppo2.py:185][0m |           0.0009 |         174.9542 |        -112.0734 |
[32m[20221214 00:33:34 @agent_ppo2.py:185][0m |          -0.0059 |         171.6326 |        -111.8455 |
[32m[20221214 00:33:34 @agent_ppo2.py:185][0m |          -0.0037 |         169.8429 |        -111.2459 |
[32m[20221214 00:33:34 @agent_ppo2.py:185][0m |          -0.0070 |         170.3837 |        -111.1091 |
[32m[20221214 00:33:34 @agent_ppo2.py:185][0m |          -0.0084 |         167.6720 |        -111.5713 |
[32m[20221214 00:33:34 @agent_ppo2.py:185][0m |          -0.0127 |         167.9696 |        -111.2061 |
[32m[20221214 00:33:34 @agent_ppo2.py:185][0m |          -0.0119 |         166.0194 |        -110.7507 |
[32m[20221214 00:33:34 @agent_ppo2.py:185][0m |          -0.0113 |         165.7679 |        -110.4270 |
[32m[20221214 00:33:34 @agent_ppo2.py:185][0m |          -0.0092 |         164.9415 |        -110.5239 |
[32m[20221214 00:33:34 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:33:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 618.36
[32m[20221214 00:33:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 724.02
[32m[20221214 00:33:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 760.30
[32m[20221214 00:33:35 @agent_ppo2.py:143][0m Total time:      36.07 min
[32m[20221214 00:33:35 @agent_ppo2.py:145][0m 3258368 total steps have happened
[32m[20221214 00:33:35 @agent_ppo2.py:121][0m #------------------------ Iteration 5591 --------------------------#
[32m[20221214 00:33:35 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:33:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:33:35 @agent_ppo2.py:185][0m |           0.0036 |         118.3259 |        -107.8170 |
[32m[20221214 00:33:35 @agent_ppo2.py:185][0m |          -0.0071 |         109.5970 |        -107.3430 |
[32m[20221214 00:33:35 @agent_ppo2.py:185][0m |          -0.0042 |         107.2479 |        -107.1410 |
[32m[20221214 00:33:35 @agent_ppo2.py:185][0m |          -0.0097 |         104.8235 |        -106.9681 |
[32m[20221214 00:33:35 @agent_ppo2.py:185][0m |          -0.0103 |         104.3045 |        -107.8230 |
[32m[20221214 00:33:35 @agent_ppo2.py:185][0m |          -0.0073 |         101.8617 |        -106.8812 |
[32m[20221214 00:33:36 @agent_ppo2.py:185][0m |          -0.0140 |         101.3982 |        -106.6757 |
[32m[20221214 00:33:36 @agent_ppo2.py:185][0m |          -0.0121 |         100.5329 |        -107.0240 |
[32m[20221214 00:33:36 @agent_ppo2.py:185][0m |          -0.0125 |          99.2006 |        -107.0992 |
[32m[20221214 00:33:36 @agent_ppo2.py:185][0m |          -0.0146 |          98.4734 |        -106.9952 |
[32m[20221214 00:33:36 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:33:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 455.98
[32m[20221214 00:33:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 511.64
[32m[20221214 00:33:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 724.11
[32m[20221214 00:33:36 @agent_ppo2.py:143][0m Total time:      36.09 min
[32m[20221214 00:33:36 @agent_ppo2.py:145][0m 3260416 total steps have happened
[32m[20221214 00:33:36 @agent_ppo2.py:121][0m #------------------------ Iteration 5592 --------------------------#
[32m[20221214 00:33:36 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:33:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:33:36 @agent_ppo2.py:185][0m |           0.0042 |         121.6355 |        -107.0426 |
[32m[20221214 00:33:36 @agent_ppo2.py:185][0m |          -0.0064 |         115.2825 |        -106.4088 |
[32m[20221214 00:33:37 @agent_ppo2.py:185][0m |          -0.0071 |         113.6432 |        -106.5765 |
[32m[20221214 00:33:37 @agent_ppo2.py:185][0m |          -0.0046 |         115.4423 |        -107.1503 |
[32m[20221214 00:33:37 @agent_ppo2.py:185][0m |          -0.0112 |         111.5209 |        -106.9225 |
[32m[20221214 00:33:37 @agent_ppo2.py:185][0m |          -0.0127 |         110.8644 |        -107.0982 |
[32m[20221214 00:33:37 @agent_ppo2.py:185][0m |          -0.0099 |         111.9338 |        -107.1191 |
[32m[20221214 00:33:37 @agent_ppo2.py:185][0m |          -0.0136 |         110.4352 |        -107.3486 |
[32m[20221214 00:33:37 @agent_ppo2.py:185][0m |          -0.0149 |         110.3211 |        -107.2958 |
[32m[20221214 00:33:37 @agent_ppo2.py:185][0m |          -0.0054 |         113.1512 |        -106.8867 |
[32m[20221214 00:33:37 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:33:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 746.62
[32m[20221214 00:33:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.63
[32m[20221214 00:33:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 699.91
[32m[20221214 00:33:37 @agent_ppo2.py:143][0m Total time:      36.12 min
[32m[20221214 00:33:37 @agent_ppo2.py:145][0m 3262464 total steps have happened
[32m[20221214 00:33:37 @agent_ppo2.py:121][0m #------------------------ Iteration 5593 --------------------------#
[32m[20221214 00:33:38 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:33:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:33:38 @agent_ppo2.py:185][0m |           0.0084 |         131.7050 |        -108.0575 |
[32m[20221214 00:33:38 @agent_ppo2.py:185][0m |          -0.0018 |         116.1865 |        -108.5186 |
[32m[20221214 00:33:38 @agent_ppo2.py:185][0m |          -0.0088 |         111.8078 |        -108.0282 |
[32m[20221214 00:33:38 @agent_ppo2.py:185][0m |          -0.0188 |         111.2242 |        -108.2193 |
[32m[20221214 00:33:38 @agent_ppo2.py:185][0m |          -0.0130 |         109.9650 |        -108.0493 |
[32m[20221214 00:33:38 @agent_ppo2.py:185][0m |          -0.0131 |         108.4424 |        -108.2773 |
[32m[20221214 00:33:38 @agent_ppo2.py:185][0m |          -0.0148 |         106.1370 |        -108.3304 |
[32m[20221214 00:33:38 @agent_ppo2.py:185][0m |          -0.0123 |         105.3511 |        -108.1835 |
[32m[20221214 00:33:39 @agent_ppo2.py:185][0m |          -0.0163 |         104.3499 |        -108.1965 |
[32m[20221214 00:33:39 @agent_ppo2.py:185][0m |          -0.0167 |         104.1317 |        -108.1943 |
[32m[20221214 00:33:39 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:33:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 565.54
[32m[20221214 00:33:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 703.62
[32m[20221214 00:33:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 813.63
[32m[20221214 00:33:39 @agent_ppo2.py:143][0m Total time:      36.14 min
[32m[20221214 00:33:39 @agent_ppo2.py:145][0m 3264512 total steps have happened
[32m[20221214 00:33:39 @agent_ppo2.py:121][0m #------------------------ Iteration 5594 --------------------------#
[32m[20221214 00:33:39 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:33:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:33:39 @agent_ppo2.py:185][0m |           0.0044 |         168.7445 |        -109.4975 |
[32m[20221214 00:33:39 @agent_ppo2.py:185][0m |           0.0003 |         161.7224 |        -109.6008 |
[32m[20221214 00:33:39 @agent_ppo2.py:185][0m |           0.0014 |         159.4414 |        -109.8483 |
[32m[20221214 00:33:39 @agent_ppo2.py:185][0m |           0.0101 |         178.2342 |        -109.5051 |
[32m[20221214 00:33:40 @agent_ppo2.py:185][0m |          -0.0062 |         159.9497 |        -109.4962 |
[32m[20221214 00:33:40 @agent_ppo2.py:185][0m |          -0.0053 |         158.3848 |        -109.5313 |
[32m[20221214 00:33:40 @agent_ppo2.py:185][0m |           0.0249 |         195.2081 |        -109.6283 |
[32m[20221214 00:33:40 @agent_ppo2.py:185][0m |           0.0053 |         170.4833 |        -109.5162 |
[32m[20221214 00:33:40 @agent_ppo2.py:185][0m |          -0.0077 |         157.9159 |        -109.1682 |
[32m[20221214 00:33:40 @agent_ppo2.py:185][0m |           0.0041 |         168.0466 |        -109.6305 |
[32m[20221214 00:33:40 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:33:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 689.79
[32m[20221214 00:33:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 796.22
[32m[20221214 00:33:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 731.77
[32m[20221214 00:33:40 @agent_ppo2.py:143][0m Total time:      36.16 min
[32m[20221214 00:33:40 @agent_ppo2.py:145][0m 3266560 total steps have happened
[32m[20221214 00:33:40 @agent_ppo2.py:121][0m #------------------------ Iteration 5595 --------------------------#
[32m[20221214 00:33:40 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:33:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:33:41 @agent_ppo2.py:185][0m |          -0.0028 |         109.4177 |        -109.3500 |
[32m[20221214 00:33:41 @agent_ppo2.py:185][0m |          -0.0059 |         101.9705 |        -108.6995 |
[32m[20221214 00:33:41 @agent_ppo2.py:185][0m |          -0.0096 |          99.3343 |        -108.5442 |
[32m[20221214 00:33:41 @agent_ppo2.py:185][0m |          -0.0136 |          98.4254 |        -108.4230 |
[32m[20221214 00:33:41 @agent_ppo2.py:185][0m |          -0.0051 |         100.4137 |        -108.0602 |
[32m[20221214 00:33:41 @agent_ppo2.py:185][0m |          -0.0131 |          96.7643 |        -108.3776 |
[32m[20221214 00:33:41 @agent_ppo2.py:185][0m |          -0.0131 |          95.8369 |        -107.3611 |
[32m[20221214 00:33:41 @agent_ppo2.py:185][0m |          -0.0153 |          94.3718 |        -107.6463 |
[32m[20221214 00:33:41 @agent_ppo2.py:185][0m |          -0.0024 |         102.7561 |        -107.9162 |
[32m[20221214 00:33:42 @agent_ppo2.py:185][0m |          -0.0172 |          93.5348 |        -107.9413 |
[32m[20221214 00:33:42 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:33:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 491.94
[32m[20221214 00:33:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 739.36
[32m[20221214 00:33:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 844.50
[32m[20221214 00:33:42 @agent_ppo2.py:143][0m Total time:      36.19 min
[32m[20221214 00:33:42 @agent_ppo2.py:145][0m 3268608 total steps have happened
[32m[20221214 00:33:42 @agent_ppo2.py:121][0m #------------------------ Iteration 5596 --------------------------#
[32m[20221214 00:33:42 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:33:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:33:42 @agent_ppo2.py:185][0m |           0.0034 |         165.4247 |        -109.7809 |
[32m[20221214 00:33:42 @agent_ppo2.py:185][0m |           0.0037 |         162.6508 |        -108.2647 |
[32m[20221214 00:33:42 @agent_ppo2.py:185][0m |          -0.0006 |         160.7580 |        -109.2561 |
[32m[20221214 00:33:42 @agent_ppo2.py:185][0m |          -0.0063 |         159.0923 |        -108.5544 |
[32m[20221214 00:33:42 @agent_ppo2.py:185][0m |          -0.0063 |         158.6094 |        -109.4215 |
[32m[20221214 00:33:43 @agent_ppo2.py:185][0m |           0.0029 |         167.5387 |        -109.0220 |
[32m[20221214 00:33:43 @agent_ppo2.py:185][0m |          -0.0020 |         158.5058 |        -107.9121 |
[32m[20221214 00:33:43 @agent_ppo2.py:185][0m |          -0.0051 |         157.1543 |        -108.1954 |
[32m[20221214 00:33:43 @agent_ppo2.py:185][0m |          -0.0058 |         157.5566 |        -109.0583 |
[32m[20221214 00:33:43 @agent_ppo2.py:185][0m |          -0.0063 |         157.0262 |        -108.5037 |
[32m[20221214 00:33:43 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:33:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 569.35
[32m[20221214 00:33:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 717.35
[32m[20221214 00:33:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 731.69
[32m[20221214 00:33:43 @agent_ppo2.py:143][0m Total time:      36.21 min
[32m[20221214 00:33:43 @agent_ppo2.py:145][0m 3270656 total steps have happened
[32m[20221214 00:33:43 @agent_ppo2.py:121][0m #------------------------ Iteration 5597 --------------------------#
[32m[20221214 00:33:43 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:33:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:33:43 @agent_ppo2.py:185][0m |           0.0023 |         123.4525 |        -108.3607 |
[32m[20221214 00:33:44 @agent_ppo2.py:185][0m |          -0.0032 |         119.6500 |        -107.6225 |
[32m[20221214 00:33:44 @agent_ppo2.py:185][0m |           0.0029 |         118.4460 |        -108.2327 |
[32m[20221214 00:33:44 @agent_ppo2.py:185][0m |          -0.0075 |         117.5309 |        -108.5618 |
[32m[20221214 00:33:44 @agent_ppo2.py:185][0m |          -0.0079 |         116.7062 |        -107.8447 |
[32m[20221214 00:33:44 @agent_ppo2.py:185][0m |          -0.0071 |         116.1514 |        -108.1577 |
[32m[20221214 00:33:44 @agent_ppo2.py:185][0m |          -0.0071 |         115.4686 |        -108.3372 |
[32m[20221214 00:33:44 @agent_ppo2.py:185][0m |          -0.0063 |         115.1820 |        -108.0136 |
[32m[20221214 00:33:44 @agent_ppo2.py:185][0m |          -0.0125 |         114.3454 |        -108.6010 |
[32m[20221214 00:33:44 @agent_ppo2.py:185][0m |          -0.0079 |         114.9428 |        -108.5750 |
[32m[20221214 00:33:44 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 00:33:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 560.41
[32m[20221214 00:33:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 651.68
[32m[20221214 00:33:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.67
[32m[20221214 00:33:45 @agent_ppo2.py:143][0m Total time:      36.24 min
[32m[20221214 00:33:45 @agent_ppo2.py:145][0m 3272704 total steps have happened
[32m[20221214 00:33:45 @agent_ppo2.py:121][0m #------------------------ Iteration 5598 --------------------------#
[32m[20221214 00:33:45 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:33:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:33:45 @agent_ppo2.py:185][0m |           0.0036 |         130.5952 |        -105.6952 |
[32m[20221214 00:33:45 @agent_ppo2.py:185][0m |          -0.0003 |         117.0742 |        -105.8433 |
[32m[20221214 00:33:45 @agent_ppo2.py:185][0m |           0.0029 |         126.7871 |        -105.9586 |
[32m[20221214 00:33:45 @agent_ppo2.py:185][0m |          -0.0078 |         114.1817 |        -106.0861 |
[32m[20221214 00:33:45 @agent_ppo2.py:185][0m |          -0.0148 |         108.2133 |        -105.8712 |
[32m[20221214 00:33:45 @agent_ppo2.py:185][0m |          -0.0153 |         107.5265 |        -106.2327 |
[32m[20221214 00:33:46 @agent_ppo2.py:185][0m |          -0.0152 |         106.3137 |        -105.7794 |
[32m[20221214 00:33:46 @agent_ppo2.py:185][0m |          -0.0108 |         104.3322 |        -106.2457 |
[32m[20221214 00:33:46 @agent_ppo2.py:185][0m |          -0.0187 |         103.4426 |        -105.9955 |
[32m[20221214 00:33:46 @agent_ppo2.py:185][0m |          -0.0152 |         103.4185 |        -106.0633 |
[32m[20221214 00:33:46 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 00:33:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 515.18
[32m[20221214 00:33:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 613.26
[32m[20221214 00:33:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 676.76
[32m[20221214 00:33:46 @agent_ppo2.py:143][0m Total time:      36.26 min
[32m[20221214 00:33:46 @agent_ppo2.py:145][0m 3274752 total steps have happened
[32m[20221214 00:33:46 @agent_ppo2.py:121][0m #------------------------ Iteration 5599 --------------------------#
[32m[20221214 00:33:46 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:33:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:33:46 @agent_ppo2.py:185][0m |           0.0074 |         166.5124 |        -108.5831 |
[32m[20221214 00:33:46 @agent_ppo2.py:185][0m |           0.0047 |         158.8365 |        -107.7336 |
[32m[20221214 00:33:47 @agent_ppo2.py:185][0m |          -0.0033 |         155.1201 |        -109.2529 |
[32m[20221214 00:33:47 @agent_ppo2.py:185][0m |          -0.0074 |         153.6719 |        -108.7866 |
[32m[20221214 00:33:47 @agent_ppo2.py:185][0m |          -0.0051 |         153.5805 |        -109.9500 |
[32m[20221214 00:33:47 @agent_ppo2.py:185][0m |          -0.0079 |         153.6838 |        -110.2707 |
[32m[20221214 00:33:47 @agent_ppo2.py:185][0m |          -0.0055 |         152.4898 |        -109.7308 |
[32m[20221214 00:33:47 @agent_ppo2.py:185][0m |          -0.0073 |         152.2064 |        -110.4682 |
[32m[20221214 00:33:47 @agent_ppo2.py:185][0m |          -0.0082 |         151.4171 |        -110.7443 |
[32m[20221214 00:33:47 @agent_ppo2.py:185][0m |           0.0114 |         174.5825 |        -110.9769 |
[32m[20221214 00:33:47 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:33:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 747.75
[32m[20221214 00:33:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 831.02
[32m[20221214 00:33:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.16
[32m[20221214 00:33:47 @agent_ppo2.py:143][0m Total time:      36.28 min
[32m[20221214 00:33:47 @agent_ppo2.py:145][0m 3276800 total steps have happened
[32m[20221214 00:33:47 @agent_ppo2.py:121][0m #------------------------ Iteration 5600 --------------------------#
[32m[20221214 00:33:48 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:33:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:33:48 @agent_ppo2.py:185][0m |           0.0037 |         144.1550 |        -112.9654 |
[32m[20221214 00:33:48 @agent_ppo2.py:185][0m |           0.0004 |         129.7322 |        -113.1050 |
[32m[20221214 00:33:48 @agent_ppo2.py:185][0m |          -0.0001 |         127.2885 |        -111.9027 |
[32m[20221214 00:33:48 @agent_ppo2.py:185][0m |          -0.0058 |         121.1312 |        -112.8192 |
[32m[20221214 00:33:48 @agent_ppo2.py:185][0m |           0.0035 |         129.2930 |        -112.8812 |
[32m[20221214 00:33:48 @agent_ppo2.py:185][0m |          -0.0018 |         120.2964 |        -112.3813 |
[32m[20221214 00:33:48 @agent_ppo2.py:185][0m |          -0.0107 |         116.7829 |        -112.4586 |
[32m[20221214 00:33:49 @agent_ppo2.py:185][0m |          -0.0005 |         124.4191 |        -112.6790 |
[32m[20221214 00:33:49 @agent_ppo2.py:185][0m |          -0.0140 |         116.0115 |        -112.1210 |
[32m[20221214 00:33:49 @agent_ppo2.py:185][0m |          -0.0144 |         112.8665 |        -111.9491 |
[32m[20221214 00:33:49 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:33:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 521.72
[32m[20221214 00:33:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 625.70
[32m[20221214 00:33:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 518.67
[32m[20221214 00:33:49 @agent_ppo2.py:143][0m Total time:      36.31 min
[32m[20221214 00:33:49 @agent_ppo2.py:145][0m 3278848 total steps have happened
[32m[20221214 00:33:49 @agent_ppo2.py:121][0m #------------------------ Iteration 5601 --------------------------#
[32m[20221214 00:33:49 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:33:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:33:49 @agent_ppo2.py:185][0m |           0.0028 |         132.1827 |        -106.8694 |
[32m[20221214 00:33:49 @agent_ppo2.py:185][0m |           0.0046 |         124.3893 |        -107.7898 |
[32m[20221214 00:33:49 @agent_ppo2.py:185][0m |          -0.0052 |         120.0972 |        -107.6017 |
[32m[20221214 00:33:50 @agent_ppo2.py:185][0m |          -0.0024 |         127.4473 |        -107.3692 |
[32m[20221214 00:33:50 @agent_ppo2.py:185][0m |          -0.0067 |         117.3196 |        -107.4443 |
[32m[20221214 00:33:50 @agent_ppo2.py:185][0m |          -0.0083 |         116.1149 |        -106.9693 |
[32m[20221214 00:33:50 @agent_ppo2.py:185][0m |          -0.0128 |         114.8059 |        -107.2210 |
[32m[20221214 00:33:50 @agent_ppo2.py:185][0m |          -0.0127 |         114.0784 |        -106.9083 |
[32m[20221214 00:33:50 @agent_ppo2.py:185][0m |          -0.0130 |         112.7576 |        -106.7698 |
[32m[20221214 00:33:50 @agent_ppo2.py:185][0m |          -0.0141 |         111.6152 |        -106.7534 |
[32m[20221214 00:33:50 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:33:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 527.28
[32m[20221214 00:33:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 628.83
[32m[20221214 00:33:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 773.00
[32m[20221214 00:33:50 @agent_ppo2.py:143][0m Total time:      36.33 min
[32m[20221214 00:33:50 @agent_ppo2.py:145][0m 3280896 total steps have happened
[32m[20221214 00:33:50 @agent_ppo2.py:121][0m #------------------------ Iteration 5602 --------------------------#
[32m[20221214 00:33:50 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:33:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:33:51 @agent_ppo2.py:185][0m |           0.0086 |         158.2055 |        -102.8594 |
[32m[20221214 00:33:51 @agent_ppo2.py:185][0m |           0.0038 |         156.1020 |        -103.8776 |
[32m[20221214 00:33:51 @agent_ppo2.py:185][0m |           0.0024 |         155.6623 |        -103.5582 |
[32m[20221214 00:33:51 @agent_ppo2.py:185][0m |           0.0041 |         160.0191 |        -104.3278 |
[32m[20221214 00:33:51 @agent_ppo2.py:185][0m |           0.0110 |         166.2695 |        -103.7374 |
[32m[20221214 00:33:51 @agent_ppo2.py:185][0m |           0.0183 |         174.2838 |        -103.6335 |
[32m[20221214 00:33:51 @agent_ppo2.py:185][0m |           0.0023 |         156.2697 |        -103.9129 |
[32m[20221214 00:33:51 @agent_ppo2.py:185][0m |           0.0005 |         156.6720 |        -103.5759 |
[32m[20221214 00:33:51 @agent_ppo2.py:185][0m |           0.0018 |         154.4526 |        -104.0954 |
[32m[20221214 00:33:52 @agent_ppo2.py:185][0m |          -0.0027 |         154.4961 |        -104.0027 |
[32m[20221214 00:33:52 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:33:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 781.97
[32m[20221214 00:33:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 835.04
[32m[20221214 00:33:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 716.05
[32m[20221214 00:33:52 @agent_ppo2.py:143][0m Total time:      36.35 min
[32m[20221214 00:33:52 @agent_ppo2.py:145][0m 3282944 total steps have happened
[32m[20221214 00:33:52 @agent_ppo2.py:121][0m #------------------------ Iteration 5603 --------------------------#
[32m[20221214 00:33:52 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:33:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:33:52 @agent_ppo2.py:185][0m |           0.0097 |         167.2490 |        -107.7552 |
[32m[20221214 00:33:52 @agent_ppo2.py:185][0m |           0.0007 |         159.7012 |        -107.7845 |
[32m[20221214 00:33:52 @agent_ppo2.py:185][0m |          -0.0007 |         158.4537 |        -108.7440 |
[32m[20221214 00:33:52 @agent_ppo2.py:185][0m |          -0.0052 |         159.2643 |        -108.2656 |
[32m[20221214 00:33:52 @agent_ppo2.py:185][0m |           0.0026 |         158.6842 |        -107.8780 |
[32m[20221214 00:33:53 @agent_ppo2.py:185][0m |           0.0046 |         160.3566 |        -107.8795 |
[32m[20221214 00:33:53 @agent_ppo2.py:185][0m |           0.0024 |         160.4182 |        -108.1159 |
[32m[20221214 00:33:53 @agent_ppo2.py:185][0m |           0.0042 |         157.1377 |        -107.7152 |
[32m[20221214 00:33:53 @agent_ppo2.py:185][0m |          -0.0057 |         156.9738 |        -107.9002 |
[32m[20221214 00:33:53 @agent_ppo2.py:185][0m |           0.0018 |         171.4330 |        -108.7295 |
[32m[20221214 00:33:53 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:33:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 780.24
[32m[20221214 00:33:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 833.76
[32m[20221214 00:33:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 680.83
[32m[20221214 00:33:53 @agent_ppo2.py:143][0m Total time:      36.38 min
[32m[20221214 00:33:53 @agent_ppo2.py:145][0m 3284992 total steps have happened
[32m[20221214 00:33:53 @agent_ppo2.py:121][0m #------------------------ Iteration 5604 --------------------------#
[32m[20221214 00:33:53 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:33:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:33:53 @agent_ppo2.py:185][0m |           0.0011 |         125.2171 |        -112.6552 |
[32m[20221214 00:33:54 @agent_ppo2.py:185][0m |          -0.0006 |         116.6587 |        -112.0218 |
[32m[20221214 00:33:54 @agent_ppo2.py:185][0m |           0.0026 |         125.9478 |        -111.5155 |
[32m[20221214 00:33:54 @agent_ppo2.py:185][0m |          -0.0069 |         112.8610 |        -111.8225 |
[32m[20221214 00:33:54 @agent_ppo2.py:185][0m |          -0.0087 |         111.3469 |        -112.2405 |
[32m[20221214 00:33:54 @agent_ppo2.py:185][0m |          -0.0115 |         110.3783 |        -111.8800 |
[32m[20221214 00:33:54 @agent_ppo2.py:185][0m |          -0.0069 |         111.8620 |        -111.2881 |
[32m[20221214 00:33:54 @agent_ppo2.py:185][0m |          -0.0142 |         108.9466 |        -111.8207 |
[32m[20221214 00:33:54 @agent_ppo2.py:185][0m |          -0.0143 |         108.5471 |        -111.4828 |
[32m[20221214 00:33:54 @agent_ppo2.py:185][0m |          -0.0140 |         107.9179 |        -111.0870 |
[32m[20221214 00:33:54 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:33:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 547.98
[32m[20221214 00:33:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 675.50
[32m[20221214 00:33:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 604.33
[32m[20221214 00:33:55 @agent_ppo2.py:143][0m Total time:      36.40 min
[32m[20221214 00:33:55 @agent_ppo2.py:145][0m 3287040 total steps have happened
[32m[20221214 00:33:55 @agent_ppo2.py:121][0m #------------------------ Iteration 5605 --------------------------#
[32m[20221214 00:33:55 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:33:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:33:55 @agent_ppo2.py:185][0m |          -0.0010 |         144.4836 |        -107.5802 |
[32m[20221214 00:33:55 @agent_ppo2.py:185][0m |           0.0037 |         139.9094 |        -107.9937 |
[32m[20221214 00:33:55 @agent_ppo2.py:185][0m |          -0.0067 |         138.1398 |        -108.1827 |
[32m[20221214 00:33:55 @agent_ppo2.py:185][0m |          -0.0072 |         135.7508 |        -107.8944 |
[32m[20221214 00:33:55 @agent_ppo2.py:185][0m |          -0.0008 |         136.3425 |        -108.0829 |
[32m[20221214 00:33:55 @agent_ppo2.py:185][0m |          -0.0104 |         134.4381 |        -108.4535 |
[32m[20221214 00:33:55 @agent_ppo2.py:185][0m |          -0.0002 |         137.7020 |        -108.3677 |
[32m[20221214 00:33:56 @agent_ppo2.py:185][0m |          -0.0092 |         135.2993 |        -108.5845 |
[32m[20221214 00:33:56 @agent_ppo2.py:185][0m |          -0.0034 |         132.5461 |        -108.2244 |
[32m[20221214 00:33:56 @agent_ppo2.py:185][0m |          -0.0122 |         132.6715 |        -108.7330 |
[32m[20221214 00:33:56 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:33:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 644.86
[32m[20221214 00:33:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 750.20
[32m[20221214 00:33:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 594.09
[32m[20221214 00:33:56 @agent_ppo2.py:143][0m Total time:      36.43 min
[32m[20221214 00:33:56 @agent_ppo2.py:145][0m 3289088 total steps have happened
[32m[20221214 00:33:56 @agent_ppo2.py:121][0m #------------------------ Iteration 5606 --------------------------#
[32m[20221214 00:33:56 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:33:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:33:56 @agent_ppo2.py:185][0m |           0.0004 |         150.3786 |        -110.9314 |
[32m[20221214 00:33:56 @agent_ppo2.py:185][0m |          -0.0056 |         142.9364 |        -112.0063 |
[32m[20221214 00:33:57 @agent_ppo2.py:185][0m |          -0.0054 |         139.3637 |        -111.6955 |
[32m[20221214 00:33:57 @agent_ppo2.py:185][0m |          -0.0046 |         137.8754 |        -111.7150 |
[32m[20221214 00:33:57 @agent_ppo2.py:185][0m |          -0.0079 |         136.4897 |        -111.5761 |
[32m[20221214 00:33:57 @agent_ppo2.py:185][0m |          -0.0089 |         135.3439 |        -111.8626 |
[32m[20221214 00:33:57 @agent_ppo2.py:185][0m |          -0.0087 |         134.9800 |        -111.1963 |
[32m[20221214 00:33:57 @agent_ppo2.py:185][0m |          -0.0085 |         133.6387 |        -111.5883 |
[32m[20221214 00:33:57 @agent_ppo2.py:185][0m |          -0.0107 |         133.9263 |        -110.9626 |
[32m[20221214 00:33:57 @agent_ppo2.py:185][0m |           0.0004 |         138.2936 |        -111.2894 |
[32m[20221214 00:33:57 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221214 00:33:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 683.96
[32m[20221214 00:33:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.38
[32m[20221214 00:33:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 800.74
[32m[20221214 00:33:57 @agent_ppo2.py:143][0m Total time:      36.45 min
[32m[20221214 00:33:57 @agent_ppo2.py:145][0m 3291136 total steps have happened
[32m[20221214 00:33:57 @agent_ppo2.py:121][0m #------------------------ Iteration 5607 --------------------------#
[32m[20221214 00:33:58 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:33:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:33:58 @agent_ppo2.py:185][0m |           0.0014 |          67.4692 |        -110.0885 |
[32m[20221214 00:33:58 @agent_ppo2.py:185][0m |          -0.0025 |          58.3302 |        -108.7458 |
[32m[20221214 00:33:58 @agent_ppo2.py:185][0m |          -0.0088 |          55.8778 |        -108.5961 |
[32m[20221214 00:33:58 @agent_ppo2.py:185][0m |          -0.0074 |          54.4083 |        -108.7604 |
[32m[20221214 00:33:58 @agent_ppo2.py:185][0m |          -0.0103 |          54.4096 |        -108.4273 |
[32m[20221214 00:33:58 @agent_ppo2.py:185][0m |          -0.0056 |          52.8155 |        -108.3637 |
[32m[20221214 00:33:58 @agent_ppo2.py:185][0m |          -0.0106 |          52.4621 |        -108.1069 |
[32m[20221214 00:33:59 @agent_ppo2.py:185][0m |          -0.0139 |          52.5654 |        -108.3744 |
[32m[20221214 00:33:59 @agent_ppo2.py:185][0m |          -0.0169 |          50.9397 |        -107.4218 |
[32m[20221214 00:33:59 @agent_ppo2.py:185][0m |          -0.0128 |          50.6094 |        -107.4408 |
[32m[20221214 00:33:59 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221214 00:33:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 565.98
[32m[20221214 00:33:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 699.08
[32m[20221214 00:33:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 807.93
[32m[20221214 00:33:59 @agent_ppo2.py:143][0m Total time:      36.48 min
[32m[20221214 00:33:59 @agent_ppo2.py:145][0m 3293184 total steps have happened
[32m[20221214 00:33:59 @agent_ppo2.py:121][0m #------------------------ Iteration 5608 --------------------------#
[32m[20221214 00:33:59 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:33:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:33:59 @agent_ppo2.py:185][0m |           0.0009 |         153.3992 |        -107.6825 |
[32m[20221214 00:33:59 @agent_ppo2.py:185][0m |           0.0014 |         139.8702 |        -106.9702 |
[32m[20221214 00:33:59 @agent_ppo2.py:185][0m |          -0.0079 |         134.2422 |        -107.2027 |
[32m[20221214 00:34:00 @agent_ppo2.py:185][0m |          -0.0002 |         138.9061 |        -107.4424 |
[32m[20221214 00:34:00 @agent_ppo2.py:185][0m |          -0.0098 |         130.8386 |        -106.9164 |
[32m[20221214 00:34:00 @agent_ppo2.py:185][0m |          -0.0111 |         129.8527 |        -106.9988 |
[32m[20221214 00:34:00 @agent_ppo2.py:185][0m |          -0.0123 |         127.9183 |        -107.1214 |
[32m[20221214 00:34:00 @agent_ppo2.py:185][0m |          -0.0107 |         128.5638 |        -107.1426 |
[32m[20221214 00:34:00 @agent_ppo2.py:185][0m |          -0.0122 |         125.4566 |        -107.2729 |
[32m[20221214 00:34:00 @agent_ppo2.py:185][0m |          -0.0142 |         125.3921 |        -107.4863 |
[32m[20221214 00:34:00 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221214 00:34:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 538.24
[32m[20221214 00:34:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 671.90
[32m[20221214 00:34:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 837.53
[32m[20221214 00:34:00 @agent_ppo2.py:143][0m Total time:      36.50 min
[32m[20221214 00:34:00 @agent_ppo2.py:145][0m 3295232 total steps have happened
[32m[20221214 00:34:00 @agent_ppo2.py:121][0m #------------------------ Iteration 5609 --------------------------#
[32m[20221214 00:34:01 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:34:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:34:01 @agent_ppo2.py:185][0m |           0.0032 |          78.4042 |        -111.4153 |
[32m[20221214 00:34:01 @agent_ppo2.py:185][0m |          -0.0036 |          69.2233 |        -111.0639 |
[32m[20221214 00:34:01 @agent_ppo2.py:185][0m |          -0.0070 |          66.9692 |        -111.6594 |
[32m[20221214 00:34:01 @agent_ppo2.py:185][0m |          -0.0081 |          64.5325 |        -111.5561 |
[32m[20221214 00:34:01 @agent_ppo2.py:185][0m |          -0.0049 |          63.8809 |        -110.9539 |
[32m[20221214 00:34:01 @agent_ppo2.py:185][0m |          -0.0124 |          63.9976 |        -111.4051 |
[32m[20221214 00:34:01 @agent_ppo2.py:185][0m |          -0.0170 |          62.4794 |        -111.5463 |
[32m[20221214 00:34:01 @agent_ppo2.py:185][0m |          -0.0144 |          62.0606 |        -111.9486 |
[32m[20221214 00:34:02 @agent_ppo2.py:185][0m |          -0.0126 |          61.0981 |        -112.1928 |
[32m[20221214 00:34:02 @agent_ppo2.py:185][0m |          -0.0171 |          60.7263 |        -111.4992 |
[32m[20221214 00:34:02 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:34:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 519.49
[32m[20221214 00:34:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 624.43
[32m[20221214 00:34:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 371.82
[32m[20221214 00:34:02 @agent_ppo2.py:143][0m Total time:      36.52 min
[32m[20221214 00:34:02 @agent_ppo2.py:145][0m 3297280 total steps have happened
[32m[20221214 00:34:02 @agent_ppo2.py:121][0m #------------------------ Iteration 5610 --------------------------#
[32m[20221214 00:34:02 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:34:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:34:02 @agent_ppo2.py:185][0m |          -0.0067 |          28.1848 |        -110.0322 |
[32m[20221214 00:34:02 @agent_ppo2.py:185][0m |          -0.0051 |          20.2418 |        -109.9451 |
[32m[20221214 00:34:02 @agent_ppo2.py:185][0m |          -0.0100 |          18.7468 |        -109.9393 |
[32m[20221214 00:34:02 @agent_ppo2.py:185][0m |          -0.0164 |          17.7155 |        -110.1574 |
[32m[20221214 00:34:03 @agent_ppo2.py:185][0m |          -0.0189 |          17.0627 |        -110.0699 |
[32m[20221214 00:34:03 @agent_ppo2.py:185][0m |          -0.0185 |          16.6305 |        -109.5058 |
[32m[20221214 00:34:03 @agent_ppo2.py:185][0m |          -0.0235 |          15.9822 |        -110.1434 |
[32m[20221214 00:34:03 @agent_ppo2.py:185][0m |          -0.0232 |          15.6231 |        -109.9085 |
[32m[20221214 00:34:03 @agent_ppo2.py:185][0m |          -0.0237 |          15.3022 |        -110.0009 |
[32m[20221214 00:34:03 @agent_ppo2.py:185][0m |          -0.0165 |          14.9662 |        -109.6415 |
[32m[20221214 00:34:03 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:34:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 491.75
[32m[20221214 00:34:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 563.16
[32m[20221214 00:34:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 792.76
[32m[20221214 00:34:03 @agent_ppo2.py:143][0m Total time:      36.55 min
[32m[20221214 00:34:03 @agent_ppo2.py:145][0m 3299328 total steps have happened
[32m[20221214 00:34:03 @agent_ppo2.py:121][0m #------------------------ Iteration 5611 --------------------------#
[32m[20221214 00:34:03 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:34:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:34:04 @agent_ppo2.py:185][0m |           0.0051 |         137.3727 |        -109.4495 |
[32m[20221214 00:34:04 @agent_ppo2.py:185][0m |          -0.0002 |         126.0464 |        -108.7049 |
[32m[20221214 00:34:04 @agent_ppo2.py:185][0m |           0.0014 |         126.4027 |        -108.6267 |
[32m[20221214 00:34:04 @agent_ppo2.py:185][0m |          -0.0027 |         125.2344 |        -108.4555 |
[32m[20221214 00:34:04 @agent_ppo2.py:185][0m |          -0.0073 |         119.7246 |        -108.1668 |
[32m[20221214 00:34:04 @agent_ppo2.py:185][0m |          -0.0064 |         120.7249 |        -109.0736 |
[32m[20221214 00:34:04 @agent_ppo2.py:185][0m |           0.0044 |         121.7977 |        -108.0740 |
[32m[20221214 00:34:04 @agent_ppo2.py:185][0m |          -0.0043 |         120.7289 |        -108.1010 |
[32m[20221214 00:34:04 @agent_ppo2.py:185][0m |          -0.0084 |         117.5334 |        -108.1442 |
[32m[20221214 00:34:05 @agent_ppo2.py:185][0m |          -0.0089 |         116.0530 |        -108.4007 |
[32m[20221214 00:34:05 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:34:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 683.89
[32m[20221214 00:34:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 792.05
[32m[20221214 00:34:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 576.87
[32m[20221214 00:34:05 @agent_ppo2.py:143][0m Total time:      36.57 min
[32m[20221214 00:34:05 @agent_ppo2.py:145][0m 3301376 total steps have happened
[32m[20221214 00:34:05 @agent_ppo2.py:121][0m #------------------------ Iteration 5612 --------------------------#
[32m[20221214 00:34:05 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:34:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:34:05 @agent_ppo2.py:185][0m |          -0.0016 |         150.5059 |        -113.1359 |
[32m[20221214 00:34:05 @agent_ppo2.py:185][0m |           0.0031 |         146.9002 |        -112.0148 |
[32m[20221214 00:34:05 @agent_ppo2.py:185][0m |           0.0036 |         159.3022 |        -113.2656 |
[32m[20221214 00:34:05 @agent_ppo2.py:185][0m |           0.0053 |         160.3111 |        -112.7643 |
[32m[20221214 00:34:05 @agent_ppo2.py:185][0m |          -0.0054 |         136.4276 |        -112.3506 |
[32m[20221214 00:34:06 @agent_ppo2.py:185][0m |          -0.0077 |         134.5766 |        -112.6707 |
[32m[20221214 00:34:06 @agent_ppo2.py:185][0m |          -0.0050 |         134.0919 |        -112.1984 |
[32m[20221214 00:34:06 @agent_ppo2.py:185][0m |          -0.0101 |         133.5017 |        -112.5988 |
[32m[20221214 00:34:06 @agent_ppo2.py:185][0m |          -0.0108 |         132.4763 |        -112.9631 |
[32m[20221214 00:34:06 @agent_ppo2.py:185][0m |          -0.0143 |         132.7542 |        -112.5316 |
[32m[20221214 00:34:06 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:34:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 666.26
[32m[20221214 00:34:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 799.87
[32m[20221214 00:34:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 627.96
[32m[20221214 00:34:06 @agent_ppo2.py:143][0m Total time:      36.60 min
[32m[20221214 00:34:06 @agent_ppo2.py:145][0m 3303424 total steps have happened
[32m[20221214 00:34:06 @agent_ppo2.py:121][0m #------------------------ Iteration 5613 --------------------------#
[32m[20221214 00:34:06 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:34:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:34:06 @agent_ppo2.py:185][0m |          -0.0004 |         186.2771 |        -117.9770 |
[32m[20221214 00:34:07 @agent_ppo2.py:185][0m |          -0.0004 |         181.6073 |        -116.2028 |
[32m[20221214 00:34:07 @agent_ppo2.py:185][0m |          -0.0020 |         180.1585 |        -117.9178 |
[32m[20221214 00:34:07 @agent_ppo2.py:185][0m |           0.0087 |         185.3201 |        -117.5693 |
[32m[20221214 00:34:07 @agent_ppo2.py:185][0m |          -0.0036 |         178.4473 |        -117.0342 |
[32m[20221214 00:34:07 @agent_ppo2.py:185][0m |           0.0120 |         194.3880 |        -117.9072 |
[32m[20221214 00:34:07 @agent_ppo2.py:185][0m |           0.0010 |         177.4815 |        -117.6835 |
[32m[20221214 00:34:07 @agent_ppo2.py:185][0m |          -0.0047 |         177.1521 |        -117.5464 |
[32m[20221214 00:34:07 @agent_ppo2.py:185][0m |          -0.0028 |         176.7846 |        -117.6696 |
[32m[20221214 00:34:07 @agent_ppo2.py:185][0m |          -0.0024 |         176.5047 |        -118.0950 |
[32m[20221214 00:34:07 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:34:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 742.26
[32m[20221214 00:34:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 812.06
[32m[20221214 00:34:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 844.39
[32m[20221214 00:34:08 @agent_ppo2.py:143][0m Total time:      36.62 min
[32m[20221214 00:34:08 @agent_ppo2.py:145][0m 3305472 total steps have happened
[32m[20221214 00:34:08 @agent_ppo2.py:121][0m #------------------------ Iteration 5614 --------------------------#
[32m[20221214 00:34:08 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:34:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:34:08 @agent_ppo2.py:185][0m |           0.0018 |          33.4879 |        -113.7813 |
[32m[20221214 00:34:08 @agent_ppo2.py:185][0m |          -0.0079 |          27.4220 |        -113.5722 |
[32m[20221214 00:34:08 @agent_ppo2.py:185][0m |          -0.0081 |          24.9092 |        -113.1601 |
[32m[20221214 00:34:08 @agent_ppo2.py:185][0m |          -0.0239 |          23.6103 |        -113.1376 |
[32m[20221214 00:34:08 @agent_ppo2.py:185][0m |          -0.0137 |          22.9614 |        -113.1991 |
[32m[20221214 00:34:08 @agent_ppo2.py:185][0m |          -0.0208 |          21.7993 |        -113.8695 |
[32m[20221214 00:34:09 @agent_ppo2.py:185][0m |          -0.0173 |          21.4910 |        -114.0572 |
[32m[20221214 00:34:09 @agent_ppo2.py:185][0m |          -0.0178 |          20.8665 |        -114.0137 |
[32m[20221214 00:34:09 @agent_ppo2.py:185][0m |          -0.0232 |          20.2400 |        -113.9089 |
[32m[20221214 00:34:09 @agent_ppo2.py:185][0m |          -0.0212 |          20.4336 |        -114.4072 |
[32m[20221214 00:34:09 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:34:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 460.58
[32m[20221214 00:34:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 524.36
[32m[20221214 00:34:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 845.79
[32m[20221214 00:34:09 @agent_ppo2.py:143][0m Total time:      36.64 min
[32m[20221214 00:34:09 @agent_ppo2.py:145][0m 3307520 total steps have happened
[32m[20221214 00:34:09 @agent_ppo2.py:121][0m #------------------------ Iteration 5615 --------------------------#
[32m[20221214 00:34:09 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:34:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:34:09 @agent_ppo2.py:185][0m |          -0.0000 |          80.9391 |        -121.0451 |
[32m[20221214 00:34:09 @agent_ppo2.py:185][0m |           0.0007 |          76.6533 |        -121.5127 |
[32m[20221214 00:34:09 @agent_ppo2.py:185][0m |           0.0002 |          75.0830 |        -120.1530 |
[32m[20221214 00:34:10 @agent_ppo2.py:185][0m |          -0.0063 |          73.6961 |        -121.1462 |
[32m[20221214 00:34:10 @agent_ppo2.py:185][0m |           0.0021 |          80.6052 |        -120.7000 |
[32m[20221214 00:34:10 @agent_ppo2.py:185][0m |          -0.0046 |          72.8426 |        -121.3461 |
[32m[20221214 00:34:10 @agent_ppo2.py:185][0m |          -0.0032 |          72.2265 |        -121.6943 |
[32m[20221214 00:34:10 @agent_ppo2.py:185][0m |          -0.0107 |          72.4498 |        -120.9121 |
[32m[20221214 00:34:10 @agent_ppo2.py:185][0m |          -0.0096 |          70.4268 |        -121.3240 |
[32m[20221214 00:34:10 @agent_ppo2.py:185][0m |          -0.0089 |          70.5189 |        -121.3291 |
[32m[20221214 00:34:10 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:34:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 592.40
[32m[20221214 00:34:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 736.01
[32m[20221214 00:34:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 804.44
[32m[20221214 00:34:10 @agent_ppo2.py:143][0m Total time:      36.67 min
[32m[20221214 00:34:10 @agent_ppo2.py:145][0m 3309568 total steps have happened
[32m[20221214 00:34:10 @agent_ppo2.py:121][0m #------------------------ Iteration 5616 --------------------------#
[32m[20221214 00:34:11 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:34:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:34:11 @agent_ppo2.py:185][0m |          -0.0022 |          62.8774 |        -118.2862 |
[32m[20221214 00:34:11 @agent_ppo2.py:185][0m |          -0.0098 |          57.7449 |        -117.8029 |
[32m[20221214 00:34:11 @agent_ppo2.py:185][0m |          -0.0140 |          56.3729 |        -117.3420 |
[32m[20221214 00:34:11 @agent_ppo2.py:185][0m |          -0.0117 |          54.8024 |        -117.4434 |
[32m[20221214 00:34:11 @agent_ppo2.py:185][0m |          -0.0181 |          53.2980 |        -117.6693 |
[32m[20221214 00:34:11 @agent_ppo2.py:185][0m |          -0.0122 |          53.2735 |        -117.3115 |
[32m[20221214 00:34:11 @agent_ppo2.py:185][0m |          -0.0158 |          52.6548 |        -117.3328 |
[32m[20221214 00:34:11 @agent_ppo2.py:185][0m |          -0.0237 |          51.6398 |        -117.6441 |
[32m[20221214 00:34:12 @agent_ppo2.py:185][0m |          -0.0198 |          51.3794 |        -117.2316 |
[32m[20221214 00:34:12 @agent_ppo2.py:185][0m |          -0.0051 |          56.1104 |        -117.4564 |
[32m[20221214 00:34:12 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:34:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 533.58
[32m[20221214 00:34:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 779.25
[32m[20221214 00:34:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 576.30
[32m[20221214 00:34:12 @agent_ppo2.py:143][0m Total time:      36.69 min
[32m[20221214 00:34:12 @agent_ppo2.py:145][0m 3311616 total steps have happened
[32m[20221214 00:34:12 @agent_ppo2.py:121][0m #------------------------ Iteration 5617 --------------------------#
[32m[20221214 00:34:12 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:34:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:34:12 @agent_ppo2.py:185][0m |           0.0014 |         147.6347 |        -120.5396 |
[32m[20221214 00:34:12 @agent_ppo2.py:185][0m |          -0.0021 |         141.3783 |        -120.1486 |
[32m[20221214 00:34:12 @agent_ppo2.py:185][0m |          -0.0040 |         139.8974 |        -119.6803 |
[32m[20221214 00:34:12 @agent_ppo2.py:185][0m |          -0.0003 |         141.1674 |        -120.7020 |
[32m[20221214 00:34:13 @agent_ppo2.py:185][0m |          -0.0044 |         137.9204 |        -119.9079 |
[32m[20221214 00:34:13 @agent_ppo2.py:185][0m |           0.0052 |         147.3702 |        -120.9338 |
[32m[20221214 00:34:13 @agent_ppo2.py:185][0m |          -0.0059 |         137.4215 |        -120.5218 |
[32m[20221214 00:34:13 @agent_ppo2.py:185][0m |          -0.0072 |         136.4715 |        -120.4224 |
[32m[20221214 00:34:13 @agent_ppo2.py:185][0m |          -0.0082 |         136.5299 |        -121.0150 |
[32m[20221214 00:34:13 @agent_ppo2.py:185][0m |          -0.0055 |         136.2261 |        -120.9846 |
[32m[20221214 00:34:13 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:34:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 672.87
[32m[20221214 00:34:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 771.43
[32m[20221214 00:34:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 692.43
[32m[20221214 00:34:13 @agent_ppo2.py:143][0m Total time:      36.71 min
[32m[20221214 00:34:13 @agent_ppo2.py:145][0m 3313664 total steps have happened
[32m[20221214 00:34:13 @agent_ppo2.py:121][0m #------------------------ Iteration 5618 --------------------------#
[32m[20221214 00:34:13 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:34:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:34:14 @agent_ppo2.py:185][0m |           0.0010 |         157.7557 |        -119.8431 |
[32m[20221214 00:34:14 @agent_ppo2.py:185][0m |          -0.0035 |         152.8032 |        -119.7235 |
[32m[20221214 00:34:14 @agent_ppo2.py:185][0m |          -0.0009 |         153.6874 |        -119.6199 |
[32m[20221214 00:34:14 @agent_ppo2.py:185][0m |          -0.0104 |         147.8004 |        -119.9772 |
[32m[20221214 00:34:14 @agent_ppo2.py:185][0m |          -0.0127 |         146.9344 |        -119.9046 |
[32m[20221214 00:34:14 @agent_ppo2.py:185][0m |          -0.0133 |         146.3915 |        -119.8982 |
[32m[20221214 00:34:14 @agent_ppo2.py:185][0m |          -0.0100 |         145.5220 |        -120.3978 |
[32m[20221214 00:34:14 @agent_ppo2.py:185][0m |          -0.0146 |         145.2080 |        -119.6785 |
[32m[20221214 00:34:14 @agent_ppo2.py:185][0m |          -0.0097 |         144.0236 |        -119.8014 |
[32m[20221214 00:34:15 @agent_ppo2.py:185][0m |          -0.0102 |         143.2942 |        -120.4407 |
[32m[20221214 00:34:15 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:34:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 671.45
[32m[20221214 00:34:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 742.86
[32m[20221214 00:34:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 661.58
[32m[20221214 00:34:15 @agent_ppo2.py:143][0m Total time:      36.74 min
[32m[20221214 00:34:15 @agent_ppo2.py:145][0m 3315712 total steps have happened
[32m[20221214 00:34:15 @agent_ppo2.py:121][0m #------------------------ Iteration 5619 --------------------------#
[32m[20221214 00:34:15 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:34:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:34:15 @agent_ppo2.py:185][0m |           0.0155 |         190.5459 |        -128.0605 |
[32m[20221214 00:34:15 @agent_ppo2.py:185][0m |           0.0004 |         171.7858 |        -127.8181 |
[32m[20221214 00:34:15 @agent_ppo2.py:185][0m |           0.0007 |         169.2836 |        -127.2202 |
[32m[20221214 00:34:15 @agent_ppo2.py:185][0m |          -0.0026 |         169.1443 |        -128.2266 |
[32m[20221214 00:34:15 @agent_ppo2.py:185][0m |          -0.0015 |         169.5351 |        -128.1564 |
[32m[20221214 00:34:15 @agent_ppo2.py:185][0m |          -0.0024 |         168.4182 |        -127.6295 |
[32m[20221214 00:34:16 @agent_ppo2.py:185][0m |           0.0028 |         170.3970 |        -127.6722 |
[32m[20221214 00:34:16 @agent_ppo2.py:185][0m |           0.0021 |         167.3843 |        -126.6030 |
[32m[20221214 00:34:16 @agent_ppo2.py:185][0m |           0.0023 |         175.6619 |        -127.1124 |
[32m[20221214 00:34:16 @agent_ppo2.py:185][0m |          -0.0014 |         168.0468 |        -126.9686 |
[32m[20221214 00:34:16 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:34:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 612.05
[32m[20221214 00:34:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 727.84
[32m[20221214 00:34:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 830.27
[32m[20221214 00:34:16 @agent_ppo2.py:143][0m Total time:      36.76 min
[32m[20221214 00:34:16 @agent_ppo2.py:145][0m 3317760 total steps have happened
[32m[20221214 00:34:16 @agent_ppo2.py:121][0m #------------------------ Iteration 5620 --------------------------#
[32m[20221214 00:34:16 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:34:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:34:16 @agent_ppo2.py:185][0m |           0.0026 |         146.1864 |        -123.5682 |
[32m[20221214 00:34:17 @agent_ppo2.py:185][0m |           0.0032 |         140.8259 |        -122.1839 |
[32m[20221214 00:34:17 @agent_ppo2.py:185][0m |           0.0011 |         136.8085 |        -123.4054 |
[32m[20221214 00:34:17 @agent_ppo2.py:185][0m |          -0.0060 |         135.1538 |        -123.0662 |
[32m[20221214 00:34:17 @agent_ppo2.py:185][0m |          -0.0062 |         133.2873 |        -123.5643 |
[32m[20221214 00:34:17 @agent_ppo2.py:185][0m |           0.0041 |         144.1079 |        -122.8929 |
[32m[20221214 00:34:17 @agent_ppo2.py:185][0m |          -0.0051 |         133.1980 |        -122.2859 |
[32m[20221214 00:34:17 @agent_ppo2.py:185][0m |          -0.0091 |         132.0918 |        -123.1281 |
[32m[20221214 00:34:17 @agent_ppo2.py:185][0m |          -0.0043 |         131.0387 |        -122.6343 |
[32m[20221214 00:34:17 @agent_ppo2.py:185][0m |          -0.0131 |         132.8047 |        -123.2319 |
[32m[20221214 00:34:17 @agent_ppo2.py:130][0m Policy update time: 1.17 s
[32m[20221214 00:34:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 582.66
[32m[20221214 00:34:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 681.73
[32m[20221214 00:34:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 811.59
[32m[20221214 00:34:18 @agent_ppo2.py:143][0m Total time:      36.79 min
[32m[20221214 00:34:18 @agent_ppo2.py:145][0m 3319808 total steps have happened
[32m[20221214 00:34:18 @agent_ppo2.py:121][0m #------------------------ Iteration 5621 --------------------------#
[32m[20221214 00:34:18 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:34:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:34:18 @agent_ppo2.py:185][0m |           0.0015 |         149.0748 |        -120.6982 |
[32m[20221214 00:34:18 @agent_ppo2.py:185][0m |          -0.0051 |         139.2751 |        -120.5612 |
[32m[20221214 00:34:18 @agent_ppo2.py:185][0m |          -0.0052 |         135.7042 |        -121.3659 |
[32m[20221214 00:34:18 @agent_ppo2.py:185][0m |          -0.0046 |         132.9024 |        -121.8611 |
[32m[20221214 00:34:18 @agent_ppo2.py:185][0m |          -0.0080 |         131.9287 |        -120.4391 |
[32m[20221214 00:34:19 @agent_ppo2.py:185][0m |          -0.0062 |         129.1991 |        -121.2073 |
[32m[20221214 00:34:19 @agent_ppo2.py:185][0m |          -0.0118 |         132.1638 |        -122.0892 |
[32m[20221214 00:34:19 @agent_ppo2.py:185][0m |          -0.0078 |         128.6573 |        -121.9605 |
[32m[20221214 00:34:19 @agent_ppo2.py:185][0m |          -0.0134 |         126.1518 |        -121.5660 |
[32m[20221214 00:34:19 @agent_ppo2.py:185][0m |          -0.0142 |         124.7951 |        -122.1767 |
[32m[20221214 00:34:19 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221214 00:34:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 670.57
[32m[20221214 00:34:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 837.75
[32m[20221214 00:34:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 785.03
[32m[20221214 00:34:19 @agent_ppo2.py:143][0m Total time:      36.81 min
[32m[20221214 00:34:19 @agent_ppo2.py:145][0m 3321856 total steps have happened
[32m[20221214 00:34:19 @agent_ppo2.py:121][0m #------------------------ Iteration 5622 --------------------------#
[32m[20221214 00:34:19 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:34:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:34:19 @agent_ppo2.py:185][0m |           0.0016 |         175.0755 |        -131.2740 |
[32m[20221214 00:34:20 @agent_ppo2.py:185][0m |           0.0011 |         170.7497 |        -131.3611 |
[32m[20221214 00:34:20 @agent_ppo2.py:185][0m |           0.0092 |         186.6876 |        -131.3234 |
[32m[20221214 00:34:20 @agent_ppo2.py:185][0m |          -0.0033 |         169.0729 |        -129.6392 |
[32m[20221214 00:34:20 @agent_ppo2.py:185][0m |           0.0026 |         170.7510 |        -131.2783 |
[32m[20221214 00:34:20 @agent_ppo2.py:185][0m |          -0.0054 |         167.6402 |        -130.9692 |
[32m[20221214 00:34:20 @agent_ppo2.py:185][0m |          -0.0058 |         167.4411 |        -130.4488 |
[32m[20221214 00:34:20 @agent_ppo2.py:185][0m |          -0.0033 |         166.0070 |        -131.5866 |
[32m[20221214 00:34:20 @agent_ppo2.py:185][0m |          -0.0002 |         167.9852 |        -130.6206 |
[32m[20221214 00:34:20 @agent_ppo2.py:185][0m |          -0.0031 |         165.9127 |        -130.6055 |
[32m[20221214 00:34:20 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:34:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 666.04
[32m[20221214 00:34:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 722.02
[32m[20221214 00:34:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 843.81
[32m[20221214 00:34:21 @agent_ppo2.py:143][0m Total time:      36.84 min
[32m[20221214 00:34:21 @agent_ppo2.py:145][0m 3323904 total steps have happened
[32m[20221214 00:34:21 @agent_ppo2.py:121][0m #------------------------ Iteration 5623 --------------------------#
[32m[20221214 00:34:21 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:34:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:34:21 @agent_ppo2.py:185][0m |           0.0102 |          25.4471 |        -124.7939 |
[32m[20221214 00:34:21 @agent_ppo2.py:185][0m |          -0.0097 |          20.9213 |        -124.2072 |
[32m[20221214 00:34:21 @agent_ppo2.py:185][0m |          -0.0021 |          20.2741 |        -123.9610 |
[32m[20221214 00:34:21 @agent_ppo2.py:185][0m |          -0.0070 |          18.1121 |        -124.4013 |
[32m[20221214 00:34:21 @agent_ppo2.py:185][0m |          -0.0195 |          17.2894 |        -124.4294 |
[32m[20221214 00:34:21 @agent_ppo2.py:185][0m |          -0.0115 |          16.8438 |        -124.6249 |
[32m[20221214 00:34:21 @agent_ppo2.py:185][0m |          -0.0203 |          16.2379 |        -124.6992 |
[32m[20221214 00:34:22 @agent_ppo2.py:185][0m |          -0.0128 |          15.8370 |        -125.1874 |
[32m[20221214 00:34:22 @agent_ppo2.py:185][0m |          -0.0183 |          15.5661 |        -124.3554 |
[32m[20221214 00:34:22 @agent_ppo2.py:185][0m |          -0.0243 |          15.4039 |        -124.6754 |
[32m[20221214 00:34:22 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:34:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 575.72
[32m[20221214 00:34:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 702.93
[32m[20221214 00:34:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 806.95
[32m[20221214 00:34:22 @agent_ppo2.py:143][0m Total time:      36.86 min
[32m[20221214 00:34:22 @agent_ppo2.py:145][0m 3325952 total steps have happened
[32m[20221214 00:34:22 @agent_ppo2.py:121][0m #------------------------ Iteration 5624 --------------------------#
[32m[20221214 00:34:22 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:34:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:34:22 @agent_ppo2.py:185][0m |          -0.0028 |          23.6685 |        -132.8650 |
[32m[20221214 00:34:22 @agent_ppo2.py:185][0m |          -0.0119 |          21.2003 |        -133.1359 |
[32m[20221214 00:34:23 @agent_ppo2.py:185][0m |          -0.0194 |          20.4692 |        -133.2052 |
[32m[20221214 00:34:23 @agent_ppo2.py:185][0m |          -0.0141 |          19.5718 |        -133.1697 |
[32m[20221214 00:34:23 @agent_ppo2.py:185][0m |          -0.0204 |          19.1003 |        -133.5885 |
[32m[20221214 00:34:23 @agent_ppo2.py:185][0m |          -0.0207 |          18.6025 |        -133.5286 |
[32m[20221214 00:34:23 @agent_ppo2.py:185][0m |          -0.0196 |          18.4318 |        -133.5092 |
[32m[20221214 00:34:23 @agent_ppo2.py:185][0m |          -0.0271 |          18.4312 |        -134.0366 |
[32m[20221214 00:34:23 @agent_ppo2.py:185][0m |          -0.0272 |          17.7586 |        -133.9568 |
[32m[20221214 00:34:23 @agent_ppo2.py:185][0m |          -0.0273 |          17.5314 |        -133.9740 |
[32m[20221214 00:34:23 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:34:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 657.41
[32m[20221214 00:34:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 763.81
[32m[20221214 00:34:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 716.57
[32m[20221214 00:34:23 @agent_ppo2.py:143][0m Total time:      36.88 min
[32m[20221214 00:34:23 @agent_ppo2.py:145][0m 3328000 total steps have happened
[32m[20221214 00:34:23 @agent_ppo2.py:121][0m #------------------------ Iteration 5625 --------------------------#
[32m[20221214 00:34:24 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:34:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:34:24 @agent_ppo2.py:185][0m |           0.0011 |         171.8197 |        -132.5009 |
[32m[20221214 00:34:24 @agent_ppo2.py:185][0m |           0.0024 |         165.5356 |        -132.4312 |
[32m[20221214 00:34:24 @agent_ppo2.py:185][0m |           0.0119 |         169.4745 |        -131.7521 |
[32m[20221214 00:34:24 @agent_ppo2.py:185][0m |          -0.0030 |         162.0511 |        -132.3873 |
[32m[20221214 00:34:24 @agent_ppo2.py:185][0m |           0.0015 |         168.2300 |        -133.1478 |
[32m[20221214 00:34:24 @agent_ppo2.py:185][0m |           0.0029 |         161.9672 |        -133.8977 |
[32m[20221214 00:34:24 @agent_ppo2.py:185][0m |          -0.0055 |         163.0216 |        -134.0016 |
[32m[20221214 00:34:24 @agent_ppo2.py:185][0m |          -0.0086 |         159.9535 |        -133.6299 |
[32m[20221214 00:34:25 @agent_ppo2.py:185][0m |           0.0018 |         160.4927 |        -133.2507 |
[32m[20221214 00:34:25 @agent_ppo2.py:185][0m |          -0.0028 |         164.9254 |        -134.4899 |
[32m[20221214 00:34:25 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:34:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 796.84
[32m[20221214 00:34:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 839.38
[32m[20221214 00:34:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 303.85
[32m[20221214 00:34:25 @agent_ppo2.py:143][0m Total time:      36.91 min
[32m[20221214 00:34:25 @agent_ppo2.py:145][0m 3330048 total steps have happened
[32m[20221214 00:34:25 @agent_ppo2.py:121][0m #------------------------ Iteration 5626 --------------------------#
[32m[20221214 00:34:25 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:34:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:34:25 @agent_ppo2.py:185][0m |          -0.0009 |          94.9322 |        -138.3491 |
[32m[20221214 00:34:25 @agent_ppo2.py:185][0m |          -0.0066 |          89.8572 |        -137.7478 |
[32m[20221214 00:34:25 @agent_ppo2.py:185][0m |           0.0018 |          88.8552 |        -137.8827 |
[32m[20221214 00:34:25 @agent_ppo2.py:185][0m |           0.0012 |          87.6522 |        -137.6818 |
[32m[20221214 00:34:26 @agent_ppo2.py:185][0m |          -0.0031 |          86.7769 |        -137.3922 |
[32m[20221214 00:34:26 @agent_ppo2.py:185][0m |          -0.0112 |          86.3349 |        -138.0917 |
[32m[20221214 00:34:26 @agent_ppo2.py:185][0m |          -0.0070 |          86.6894 |        -137.9294 |
[32m[20221214 00:34:26 @agent_ppo2.py:185][0m |          -0.0107 |          86.3507 |        -138.5235 |
[32m[20221214 00:34:26 @agent_ppo2.py:185][0m |          -0.0091 |          85.3512 |        -138.1171 |
[32m[20221214 00:34:26 @agent_ppo2.py:185][0m |          -0.0118 |          84.8538 |        -138.7813 |
[32m[20221214 00:34:26 @agent_ppo2.py:130][0m Policy update time: 1.17 s
[32m[20221214 00:34:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 670.15
[32m[20221214 00:34:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 739.31
[32m[20221214 00:34:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 843.25
[32m[20221214 00:34:26 @agent_ppo2.py:143][0m Total time:      36.93 min
[32m[20221214 00:34:26 @agent_ppo2.py:145][0m 3332096 total steps have happened
[32m[20221214 00:34:26 @agent_ppo2.py:121][0m #------------------------ Iteration 5627 --------------------------#
[32m[20221214 00:34:27 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:34:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:34:27 @agent_ppo2.py:185][0m |          -0.0020 |         107.8310 |        -139.7325 |
[32m[20221214 00:34:27 @agent_ppo2.py:185][0m |          -0.0056 |          94.2436 |        -139.8535 |
[32m[20221214 00:34:27 @agent_ppo2.py:185][0m |          -0.0105 |          89.5301 |        -139.4440 |
[32m[20221214 00:34:27 @agent_ppo2.py:185][0m |          -0.0143 |          87.5151 |        -138.9005 |
[32m[20221214 00:34:27 @agent_ppo2.py:185][0m |          -0.0076 |          86.8741 |        -139.9064 |
[32m[20221214 00:34:27 @agent_ppo2.py:185][0m |          -0.0109 |          84.0745 |        -139.3850 |
[32m[20221214 00:34:27 @agent_ppo2.py:185][0m |          -0.0143 |          82.0119 |        -139.2038 |
[32m[20221214 00:34:27 @agent_ppo2.py:185][0m |          -0.0187 |          81.1898 |        -139.4494 |
[32m[20221214 00:34:28 @agent_ppo2.py:185][0m |          -0.0241 |          79.9043 |        -139.2908 |
[32m[20221214 00:34:28 @agent_ppo2.py:185][0m |          -0.0230 |          80.3040 |        -139.3532 |
[32m[20221214 00:34:28 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:34:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 529.37
[32m[20221214 00:34:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 759.00
[32m[20221214 00:34:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 779.00
[32m[20221214 00:34:28 @agent_ppo2.py:143][0m Total time:      36.96 min
[32m[20221214 00:34:28 @agent_ppo2.py:145][0m 3334144 total steps have happened
[32m[20221214 00:34:28 @agent_ppo2.py:121][0m #------------------------ Iteration 5628 --------------------------#
[32m[20221214 00:34:28 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:34:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:34:28 @agent_ppo2.py:185][0m |          -0.0020 |         140.7122 |        -141.5123 |
[32m[20221214 00:34:28 @agent_ppo2.py:185][0m |          -0.0006 |         128.3659 |        -140.5317 |
[32m[20221214 00:34:28 @agent_ppo2.py:185][0m |          -0.0061 |         122.1423 |        -141.6062 |
[32m[20221214 00:34:28 @agent_ppo2.py:185][0m |          -0.0120 |         118.8072 |        -141.4610 |
[32m[20221214 00:34:29 @agent_ppo2.py:185][0m |          -0.0110 |         116.9290 |        -141.2152 |
[32m[20221214 00:34:29 @agent_ppo2.py:185][0m |          -0.0112 |         114.6118 |        -142.0227 |
[32m[20221214 00:34:29 @agent_ppo2.py:185][0m |          -0.0150 |         112.9858 |        -141.5479 |
[32m[20221214 00:34:29 @agent_ppo2.py:185][0m |          -0.0175 |         111.8102 |        -140.8643 |
[32m[20221214 00:34:29 @agent_ppo2.py:185][0m |          -0.0184 |         110.3300 |        -141.8074 |
[32m[20221214 00:34:29 @agent_ppo2.py:185][0m |          -0.0144 |         109.6924 |        -141.1674 |
[32m[20221214 00:34:29 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:34:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 616.45
[32m[20221214 00:34:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 782.03
[32m[20221214 00:34:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 664.21
[32m[20221214 00:34:29 @agent_ppo2.py:143][0m Total time:      36.98 min
[32m[20221214 00:34:29 @agent_ppo2.py:145][0m 3336192 total steps have happened
[32m[20221214 00:34:29 @agent_ppo2.py:121][0m #------------------------ Iteration 5629 --------------------------#
[32m[20221214 00:34:29 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:34:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:34:30 @agent_ppo2.py:185][0m |           0.0020 |         187.6972 |        -135.9238 |
[32m[20221214 00:34:30 @agent_ppo2.py:185][0m |           0.0023 |         182.7437 |        -133.8766 |
[32m[20221214 00:34:30 @agent_ppo2.py:185][0m |          -0.0011 |         181.6104 |        -134.9674 |
[32m[20221214 00:34:30 @agent_ppo2.py:185][0m |           0.0024 |         181.0508 |        -133.8675 |
[32m[20221214 00:34:30 @agent_ppo2.py:185][0m |          -0.0012 |         179.9152 |        -134.8197 |
[32m[20221214 00:34:30 @agent_ppo2.py:185][0m |          -0.0017 |         179.6280 |        -135.5042 |
[32m[20221214 00:34:30 @agent_ppo2.py:185][0m |          -0.0010 |         179.6206 |        -135.6030 |
[32m[20221214 00:34:30 @agent_ppo2.py:185][0m |          -0.0011 |         179.1882 |        -135.0873 |
[32m[20221214 00:34:30 @agent_ppo2.py:185][0m |          -0.0000 |         179.1494 |        -134.7355 |
[32m[20221214 00:34:30 @agent_ppo2.py:185][0m |          -0.0019 |         178.6580 |        -134.7737 |
[32m[20221214 00:34:30 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:34:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 728.48
[32m[20221214 00:34:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 805.86
[32m[20221214 00:34:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 841.93
[32m[20221214 00:34:31 @agent_ppo2.py:143][0m Total time:      37.00 min
[32m[20221214 00:34:31 @agent_ppo2.py:145][0m 3338240 total steps have happened
[32m[20221214 00:34:31 @agent_ppo2.py:121][0m #------------------------ Iteration 5630 --------------------------#
[32m[20221214 00:34:31 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:34:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:34:31 @agent_ppo2.py:185][0m |           0.0097 |         184.4717 |        -135.7981 |
[32m[20221214 00:34:31 @agent_ppo2.py:185][0m |          -0.0003 |         176.9768 |        -135.5813 |
[32m[20221214 00:34:31 @agent_ppo2.py:185][0m |           0.0130 |         195.3711 |        -135.5082 |
[32m[20221214 00:34:31 @agent_ppo2.py:185][0m |          -0.0034 |         174.3364 |        -135.9022 |
[32m[20221214 00:34:31 @agent_ppo2.py:185][0m |           0.0145 |         196.1953 |        -135.8335 |
[32m[20221214 00:34:32 @agent_ppo2.py:185][0m |          -0.0020 |         173.2766 |        -136.7305 |
[32m[20221214 00:34:32 @agent_ppo2.py:185][0m |          -0.0047 |         172.5425 |        -135.7137 |
[32m[20221214 00:34:32 @agent_ppo2.py:185][0m |           0.0064 |         195.7433 |        -136.3749 |
[32m[20221214 00:34:32 @agent_ppo2.py:185][0m |          -0.0053 |         172.3753 |        -135.3896 |
[32m[20221214 00:34:32 @agent_ppo2.py:185][0m |          -0.0081 |         172.3477 |        -135.8912 |
[32m[20221214 00:34:32 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:34:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 747.47
[32m[20221214 00:34:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 836.02
[32m[20221214 00:34:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.90
[32m[20221214 00:34:32 @agent_ppo2.py:143][0m Total time:      37.03 min
[32m[20221214 00:34:32 @agent_ppo2.py:145][0m 3340288 total steps have happened
[32m[20221214 00:34:32 @agent_ppo2.py:121][0m #------------------------ Iteration 5631 --------------------------#
[32m[20221214 00:34:32 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:34:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:34:32 @agent_ppo2.py:185][0m |          -0.0017 |         185.2675 |        -144.6344 |
[32m[20221214 00:34:33 @agent_ppo2.py:185][0m |          -0.0017 |         173.0457 |        -144.7087 |
[32m[20221214 00:34:33 @agent_ppo2.py:185][0m |          -0.0057 |         162.0452 |        -144.2495 |
[32m[20221214 00:34:33 @agent_ppo2.py:185][0m |          -0.0058 |         154.6110 |        -144.9608 |
[32m[20221214 00:34:33 @agent_ppo2.py:185][0m |           0.0077 |         154.9274 |        -144.6300 |
[32m[20221214 00:34:33 @agent_ppo2.py:185][0m |          -0.0088 |         151.4577 |        -144.5020 |
[32m[20221214 00:34:33 @agent_ppo2.py:185][0m |          -0.0048 |         151.3802 |        -144.3780 |
[32m[20221214 00:34:33 @agent_ppo2.py:185][0m |          -0.0098 |         150.4201 |        -144.6106 |
[32m[20221214 00:34:33 @agent_ppo2.py:185][0m |          -0.0048 |         152.7132 |        -144.5996 |
[32m[20221214 00:34:33 @agent_ppo2.py:185][0m |          -0.0066 |         149.4323 |        -143.7410 |
[32m[20221214 00:34:33 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:34:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 710.08
[32m[20221214 00:34:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 789.91
[32m[20221214 00:34:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 772.82
[32m[20221214 00:34:34 @agent_ppo2.py:143][0m Total time:      37.05 min
[32m[20221214 00:34:34 @agent_ppo2.py:145][0m 3342336 total steps have happened
[32m[20221214 00:34:34 @agent_ppo2.py:121][0m #------------------------ Iteration 5632 --------------------------#
[32m[20221214 00:34:34 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:34:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:34:34 @agent_ppo2.py:185][0m |           0.0010 |          96.7019 |        -142.3706 |
[32m[20221214 00:34:34 @agent_ppo2.py:185][0m |          -0.0084 |          77.2579 |        -141.6279 |
[32m[20221214 00:34:34 @agent_ppo2.py:185][0m |           0.0019 |          82.7930 |        -141.6099 |
[32m[20221214 00:34:34 @agent_ppo2.py:185][0m |          -0.0057 |          75.0455 |        -140.9130 |
[32m[20221214 00:34:34 @agent_ppo2.py:185][0m |          -0.0079 |          72.5188 |        -142.3494 |
[32m[20221214 00:34:34 @agent_ppo2.py:185][0m |          -0.0160 |          72.3271 |        -142.2949 |
[32m[20221214 00:34:35 @agent_ppo2.py:185][0m |          -0.0060 |          72.1111 |        -142.1275 |
[32m[20221214 00:34:35 @agent_ppo2.py:185][0m |          -0.0087 |          72.2359 |        -142.4604 |
[32m[20221214 00:34:35 @agent_ppo2.py:185][0m |          -0.0033 |          74.1475 |        -142.5302 |
[32m[20221214 00:34:35 @agent_ppo2.py:185][0m |          -0.0077 |          71.7508 |        -143.0440 |
[32m[20221214 00:34:35 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:34:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 711.81
[32m[20221214 00:34:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 770.29
[32m[20221214 00:34:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 730.49
[32m[20221214 00:34:35 @agent_ppo2.py:143][0m Total time:      37.08 min
[32m[20221214 00:34:35 @agent_ppo2.py:145][0m 3344384 total steps have happened
[32m[20221214 00:34:35 @agent_ppo2.py:121][0m #------------------------ Iteration 5633 --------------------------#
[32m[20221214 00:34:35 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:34:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:34:35 @agent_ppo2.py:185][0m |           0.0043 |          98.0122 |        -141.2488 |
[32m[20221214 00:34:35 @agent_ppo2.py:185][0m |          -0.0057 |          84.1060 |        -141.7491 |
[32m[20221214 00:34:36 @agent_ppo2.py:185][0m |          -0.0050 |          81.6964 |        -141.2086 |
[32m[20221214 00:34:36 @agent_ppo2.py:185][0m |          -0.0090 |          79.8361 |        -142.1913 |
[32m[20221214 00:34:36 @agent_ppo2.py:185][0m |          -0.0128 |          78.3909 |        -141.9000 |
[32m[20221214 00:34:36 @agent_ppo2.py:185][0m |          -0.0069 |          78.1181 |        -141.8049 |
[32m[20221214 00:34:36 @agent_ppo2.py:185][0m |          -0.0107 |          77.3789 |        -141.8311 |
[32m[20221214 00:34:36 @agent_ppo2.py:185][0m |          -0.0088 |          77.4916 |        -142.1314 |
[32m[20221214 00:34:36 @agent_ppo2.py:185][0m |          -0.0056 |          77.2484 |        -141.6043 |
[32m[20221214 00:34:36 @agent_ppo2.py:185][0m |          -0.0105 |          76.6249 |        -142.3802 |
[32m[20221214 00:34:36 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:34:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 476.72
[32m[20221214 00:34:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 796.65
[32m[20221214 00:34:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 853.63
[32m[20221214 00:34:36 @agent_ppo2.py:143][0m Total time:      37.10 min
[32m[20221214 00:34:36 @agent_ppo2.py:145][0m 3346432 total steps have happened
[32m[20221214 00:34:36 @agent_ppo2.py:121][0m #------------------------ Iteration 5634 --------------------------#
[32m[20221214 00:34:37 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:34:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:34:37 @agent_ppo2.py:185][0m |           0.0062 |         179.3299 |        -143.8829 |
[32m[20221214 00:34:37 @agent_ppo2.py:185][0m |           0.0048 |         189.3998 |        -143.7249 |
[32m[20221214 00:34:37 @agent_ppo2.py:185][0m |           0.0037 |         177.2461 |        -142.0658 |
[32m[20221214 00:34:37 @agent_ppo2.py:185][0m |          -0.0011 |         175.0669 |        -144.0461 |
[32m[20221214 00:34:37 @agent_ppo2.py:185][0m |           0.0012 |         175.2251 |        -144.1989 |
[32m[20221214 00:34:37 @agent_ppo2.py:185][0m |          -0.0029 |         174.7299 |        -144.0042 |
[32m[20221214 00:34:37 @agent_ppo2.py:185][0m |          -0.0064 |         174.6896 |        -143.4278 |
[32m[20221214 00:34:37 @agent_ppo2.py:185][0m |           0.0007 |         174.1782 |        -143.4041 |
[32m[20221214 00:34:38 @agent_ppo2.py:185][0m |          -0.0054 |         174.6867 |        -143.6683 |
[32m[20221214 00:34:38 @agent_ppo2.py:185][0m |          -0.0025 |         174.5879 |        -144.1390 |
[32m[20221214 00:34:38 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:34:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 703.42
[32m[20221214 00:34:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 792.09
[32m[20221214 00:34:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 663.65
[32m[20221214 00:34:38 @agent_ppo2.py:143][0m Total time:      37.12 min
[32m[20221214 00:34:38 @agent_ppo2.py:145][0m 3348480 total steps have happened
[32m[20221214 00:34:38 @agent_ppo2.py:121][0m #------------------------ Iteration 5635 --------------------------#
[32m[20221214 00:34:38 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:34:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:34:38 @agent_ppo2.py:185][0m |           0.0007 |         106.5828 |        -139.4301 |
[32m[20221214 00:34:38 @agent_ppo2.py:185][0m |          -0.0018 |          99.0910 |        -140.0875 |
[32m[20221214 00:34:38 @agent_ppo2.py:185][0m |          -0.0070 |          96.4223 |        -139.9768 |
[32m[20221214 00:34:38 @agent_ppo2.py:185][0m |          -0.0108 |          94.7330 |        -139.4964 |
[32m[20221214 00:34:39 @agent_ppo2.py:185][0m |          -0.0085 |          93.5954 |        -139.1288 |
[32m[20221214 00:34:39 @agent_ppo2.py:185][0m |          -0.0045 |          93.2169 |        -139.4480 |
[32m[20221214 00:34:39 @agent_ppo2.py:185][0m |          -0.0094 |          91.2639 |        -139.8563 |
[32m[20221214 00:34:39 @agent_ppo2.py:185][0m |          -0.0123 |          90.2688 |        -138.5157 |
[32m[20221214 00:34:39 @agent_ppo2.py:185][0m |          -0.0039 |          97.2892 |        -139.1392 |
[32m[20221214 00:34:39 @agent_ppo2.py:185][0m |          -0.0118 |          89.6657 |        -138.6881 |
[32m[20221214 00:34:39 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 00:34:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 565.82
[32m[20221214 00:34:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 787.87
[32m[20221214 00:34:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 827.19
[32m[20221214 00:34:39 @agent_ppo2.py:143][0m Total time:      37.15 min
[32m[20221214 00:34:39 @agent_ppo2.py:145][0m 3350528 total steps have happened
[32m[20221214 00:34:39 @agent_ppo2.py:121][0m #------------------------ Iteration 5636 --------------------------#
[32m[20221214 00:34:39 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:34:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:34:40 @agent_ppo2.py:185][0m |           0.0159 |         107.1518 |        -141.2240 |
[32m[20221214 00:34:40 @agent_ppo2.py:185][0m |           0.0046 |          90.5482 |        -141.4345 |
[32m[20221214 00:34:40 @agent_ppo2.py:185][0m |          -0.0076 |          73.6317 |        -141.2501 |
[32m[20221214 00:34:40 @agent_ppo2.py:185][0m |          -0.0102 |          67.7554 |        -141.4849 |
[32m[20221214 00:34:40 @agent_ppo2.py:185][0m |          -0.0052 |          68.0404 |        -141.5374 |
[32m[20221214 00:34:40 @agent_ppo2.py:185][0m |           0.0012 |          73.0194 |        -141.8995 |
[32m[20221214 00:34:40 @agent_ppo2.py:185][0m |          -0.0109 |          64.8529 |        -141.5801 |
[32m[20221214 00:34:40 @agent_ppo2.py:185][0m |          -0.0168 |          60.5474 |        -141.6433 |
[32m[20221214 00:34:40 @agent_ppo2.py:185][0m |          -0.0185 |          59.8639 |        -142.0496 |
[32m[20221214 00:34:41 @agent_ppo2.py:185][0m |          -0.0151 |          58.4923 |        -141.8461 |
[32m[20221214 00:34:41 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:34:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 677.91
[32m[20221214 00:34:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 818.89
[32m[20221214 00:34:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 720.88
[32m[20221214 00:34:41 @agent_ppo2.py:143][0m Total time:      37.17 min
[32m[20221214 00:34:41 @agent_ppo2.py:145][0m 3352576 total steps have happened
[32m[20221214 00:34:41 @agent_ppo2.py:121][0m #------------------------ Iteration 5637 --------------------------#
[32m[20221214 00:34:41 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:34:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:34:41 @agent_ppo2.py:185][0m |           0.0043 |         167.4407 |        -144.7049 |
[32m[20221214 00:34:41 @agent_ppo2.py:185][0m |           0.0085 |         164.4509 |        -143.6143 |
[32m[20221214 00:34:41 @agent_ppo2.py:185][0m |          -0.0007 |         155.7075 |        -143.9944 |
[32m[20221214 00:34:41 @agent_ppo2.py:185][0m |          -0.0023 |         156.4622 |        -143.9596 |
[32m[20221214 00:34:41 @agent_ppo2.py:185][0m |          -0.0054 |         151.8237 |        -144.0139 |
[32m[20221214 00:34:42 @agent_ppo2.py:185][0m |           0.0012 |         159.4202 |        -144.2696 |
[32m[20221214 00:34:42 @agent_ppo2.py:185][0m |          -0.0048 |         149.6274 |        -141.9584 |
[32m[20221214 00:34:42 @agent_ppo2.py:185][0m |          -0.0038 |         149.2312 |        -143.2723 |
[32m[20221214 00:34:42 @agent_ppo2.py:185][0m |          -0.0067 |         147.7157 |        -143.5373 |
[32m[20221214 00:34:42 @agent_ppo2.py:185][0m |          -0.0107 |         147.1501 |        -143.7196 |
[32m[20221214 00:34:42 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:34:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 683.93
[32m[20221214 00:34:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 828.92
[32m[20221214 00:34:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 850.09
[32m[20221214 00:34:42 @agent_ppo2.py:143][0m Total time:      37.20 min
[32m[20221214 00:34:42 @agent_ppo2.py:145][0m 3354624 total steps have happened
[32m[20221214 00:34:42 @agent_ppo2.py:121][0m #------------------------ Iteration 5638 --------------------------#
[32m[20221214 00:34:42 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:34:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:34:42 @agent_ppo2.py:185][0m |          -0.0018 |         170.5922 |        -144.4530 |
[32m[20221214 00:34:43 @agent_ppo2.py:185][0m |          -0.0009 |         160.7992 |        -145.4619 |
[32m[20221214 00:34:43 @agent_ppo2.py:185][0m |          -0.0042 |         157.3151 |        -144.5590 |
[32m[20221214 00:34:43 @agent_ppo2.py:185][0m |          -0.0052 |         154.4243 |        -145.0292 |
[32m[20221214 00:34:43 @agent_ppo2.py:185][0m |          -0.0078 |         151.4395 |        -144.6361 |
[32m[20221214 00:34:43 @agent_ppo2.py:185][0m |          -0.0010 |         152.3296 |        -145.9894 |
[32m[20221214 00:34:43 @agent_ppo2.py:185][0m |          -0.0049 |         151.7475 |        -146.1529 |
[32m[20221214 00:34:43 @agent_ppo2.py:185][0m |          -0.0078 |         149.3095 |        -146.5373 |
[32m[20221214 00:34:43 @agent_ppo2.py:185][0m |          -0.0113 |         147.9616 |        -145.9988 |
[32m[20221214 00:34:43 @agent_ppo2.py:185][0m |          -0.0045 |         148.5578 |        -146.6695 |
[32m[20221214 00:34:43 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 00:34:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 605.44
[32m[20221214 00:34:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 668.13
[32m[20221214 00:34:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 836.25
[32m[20221214 00:34:44 @agent_ppo2.py:143][0m Total time:      37.22 min
[32m[20221214 00:34:44 @agent_ppo2.py:145][0m 3356672 total steps have happened
[32m[20221214 00:34:44 @agent_ppo2.py:121][0m #------------------------ Iteration 5639 --------------------------#
[32m[20221214 00:34:44 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:34:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:34:44 @agent_ppo2.py:185][0m |          -0.0019 |          83.9408 |        -147.0591 |
[32m[20221214 00:34:44 @agent_ppo2.py:185][0m |          -0.0058 |          72.2796 |        -147.0213 |
[32m[20221214 00:34:44 @agent_ppo2.py:185][0m |          -0.0110 |          68.3019 |        -146.7479 |
[32m[20221214 00:34:44 @agent_ppo2.py:185][0m |          -0.0138 |          65.6625 |        -146.6913 |
[32m[20221214 00:34:44 @agent_ppo2.py:185][0m |          -0.0137 |          64.0535 |        -146.0158 |
[32m[20221214 00:34:44 @agent_ppo2.py:185][0m |          -0.0050 |          65.2927 |        -146.5516 |
[32m[20221214 00:34:45 @agent_ppo2.py:185][0m |          -0.0181 |          61.4274 |        -145.8601 |
[32m[20221214 00:34:45 @agent_ppo2.py:185][0m |          -0.0102 |          60.3842 |        -146.3040 |
[32m[20221214 00:34:45 @agent_ppo2.py:185][0m |          -0.0245 |          58.5570 |        -145.8783 |
[32m[20221214 00:34:45 @agent_ppo2.py:185][0m |          -0.0269 |          57.7306 |        -145.3776 |
[32m[20221214 00:34:45 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:34:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 468.58
[32m[20221214 00:34:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 615.98
[32m[20221214 00:34:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 806.37
[32m[20221214 00:34:45 @agent_ppo2.py:143][0m Total time:      37.24 min
[32m[20221214 00:34:45 @agent_ppo2.py:145][0m 3358720 total steps have happened
[32m[20221214 00:34:45 @agent_ppo2.py:121][0m #------------------------ Iteration 5640 --------------------------#
[32m[20221214 00:34:45 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:34:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:34:45 @agent_ppo2.py:185][0m |          -0.0009 |         131.0437 |        -141.9012 |
[32m[20221214 00:34:45 @agent_ppo2.py:185][0m |           0.0016 |         119.2430 |        -142.8600 |
[32m[20221214 00:34:46 @agent_ppo2.py:185][0m |          -0.0049 |         115.1513 |        -142.4476 |
[32m[20221214 00:34:46 @agent_ppo2.py:185][0m |          -0.0031 |         113.5979 |        -143.4284 |
[32m[20221214 00:34:46 @agent_ppo2.py:185][0m |          -0.0084 |         112.9788 |        -142.5243 |
[32m[20221214 00:34:46 @agent_ppo2.py:185][0m |          -0.0052 |         112.4319 |        -142.7256 |
[32m[20221214 00:34:46 @agent_ppo2.py:185][0m |          -0.0067 |         111.2907 |        -142.9080 |
[32m[20221214 00:34:46 @agent_ppo2.py:185][0m |          -0.0076 |         111.7602 |        -143.0245 |
[32m[20221214 00:34:46 @agent_ppo2.py:185][0m |          -0.0003 |         110.0809 |        -142.7231 |
[32m[20221214 00:34:46 @agent_ppo2.py:185][0m |          -0.0043 |         108.4531 |        -142.7643 |
[32m[20221214 00:34:46 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221214 00:34:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 604.36
[32m[20221214 00:34:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 757.48
[32m[20221214 00:34:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 155.42
[32m[20221214 00:34:46 @agent_ppo2.py:143][0m Total time:      37.27 min
[32m[20221214 00:34:46 @agent_ppo2.py:145][0m 3360768 total steps have happened
[32m[20221214 00:34:46 @agent_ppo2.py:121][0m #------------------------ Iteration 5641 --------------------------#
[32m[20221214 00:34:47 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:34:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:34:47 @agent_ppo2.py:185][0m |           0.0003 |         138.8744 |        -149.1417 |
[32m[20221214 00:34:47 @agent_ppo2.py:185][0m |          -0.0040 |         128.5534 |        -148.9994 |
[32m[20221214 00:34:47 @agent_ppo2.py:185][0m |           0.0079 |         134.0575 |        -148.8424 |
[32m[20221214 00:34:47 @agent_ppo2.py:185][0m |          -0.0082 |         123.8090 |        -148.7405 |
[32m[20221214 00:34:47 @agent_ppo2.py:185][0m |          -0.0055 |         122.8000 |        -148.6334 |
[32m[20221214 00:34:47 @agent_ppo2.py:185][0m |          -0.0098 |         121.8188 |        -147.7987 |
[32m[20221214 00:34:47 @agent_ppo2.py:185][0m |          -0.0081 |         120.8253 |        -148.3534 |
[32m[20221214 00:34:48 @agent_ppo2.py:185][0m |          -0.0132 |         119.4531 |        -148.2046 |
[32m[20221214 00:34:48 @agent_ppo2.py:185][0m |           0.0010 |         128.6236 |        -148.5472 |
[32m[20221214 00:34:48 @agent_ppo2.py:185][0m |          -0.0106 |         119.1922 |        -148.2908 |
[32m[20221214 00:34:48 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:34:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 675.10
[32m[20221214 00:34:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 831.86
[32m[20221214 00:34:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 799.14
[32m[20221214 00:34:48 @agent_ppo2.py:143][0m Total time:      37.29 min
[32m[20221214 00:34:48 @agent_ppo2.py:145][0m 3362816 total steps have happened
[32m[20221214 00:34:48 @agent_ppo2.py:121][0m #------------------------ Iteration 5642 --------------------------#
[32m[20221214 00:34:48 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:34:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:34:48 @agent_ppo2.py:185][0m |          -0.0002 |         171.1245 |        -155.2803 |
[32m[20221214 00:34:48 @agent_ppo2.py:185][0m |           0.0014 |         160.2934 |        -154.7235 |
[32m[20221214 00:34:48 @agent_ppo2.py:185][0m |          -0.0026 |         157.8408 |        -155.5342 |
[32m[20221214 00:34:49 @agent_ppo2.py:185][0m |           0.0092 |         171.4612 |        -155.4478 |
[32m[20221214 00:34:49 @agent_ppo2.py:185][0m |          -0.0091 |         155.2583 |        -155.8568 |
[32m[20221214 00:34:49 @agent_ppo2.py:185][0m |          -0.0049 |         153.7225 |        -155.0089 |
[32m[20221214 00:34:49 @agent_ppo2.py:185][0m |          -0.0016 |         153.1289 |        -155.2816 |
[32m[20221214 00:34:49 @agent_ppo2.py:185][0m |          -0.0064 |         151.9009 |        -155.6396 |
[32m[20221214 00:34:49 @agent_ppo2.py:185][0m |          -0.0085 |         152.0916 |        -155.2265 |
[32m[20221214 00:34:49 @agent_ppo2.py:185][0m |          -0.0118 |         152.5358 |        -155.8452 |
[32m[20221214 00:34:49 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:34:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 735.38
[32m[20221214 00:34:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.12
[32m[20221214 00:34:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 587.20
[32m[20221214 00:34:49 @agent_ppo2.py:143][0m Total time:      37.32 min
[32m[20221214 00:34:49 @agent_ppo2.py:145][0m 3364864 total steps have happened
[32m[20221214 00:34:49 @agent_ppo2.py:121][0m #------------------------ Iteration 5643 --------------------------#
[32m[20221214 00:34:49 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:34:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:34:50 @agent_ppo2.py:185][0m |          -0.0001 |          84.7658 |        -153.5382 |
[32m[20221214 00:34:50 @agent_ppo2.py:185][0m |          -0.0090 |          78.2374 |        -153.8089 |
[32m[20221214 00:34:50 @agent_ppo2.py:185][0m |          -0.0088 |          76.8334 |        -153.9445 |
[32m[20221214 00:34:50 @agent_ppo2.py:185][0m |          -0.0163 |          74.8204 |        -154.1366 |
[32m[20221214 00:34:50 @agent_ppo2.py:185][0m |          -0.0166 |          73.8493 |        -153.7656 |
[32m[20221214 00:34:50 @agent_ppo2.py:185][0m |          -0.0148 |          73.3363 |        -153.8807 |
[32m[20221214 00:34:50 @agent_ppo2.py:185][0m |          -0.0220 |          72.6767 |        -153.3141 |
[32m[20221214 00:34:50 @agent_ppo2.py:185][0m |          -0.0159 |          71.5290 |        -152.9677 |
[32m[20221214 00:34:51 @agent_ppo2.py:185][0m |          -0.0212 |          71.4059 |        -153.4566 |
[32m[20221214 00:34:51 @agent_ppo2.py:185][0m |          -0.0224 |          70.6486 |        -153.6627 |
[32m[20221214 00:34:51 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:34:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 604.46
[32m[20221214 00:34:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 776.24
[32m[20221214 00:34:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 742.12
[32m[20221214 00:34:51 @agent_ppo2.py:143][0m Total time:      37.34 min
[32m[20221214 00:34:51 @agent_ppo2.py:145][0m 3366912 total steps have happened
[32m[20221214 00:34:51 @agent_ppo2.py:121][0m #------------------------ Iteration 5644 --------------------------#
[32m[20221214 00:34:51 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:34:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:34:51 @agent_ppo2.py:185][0m |           0.0029 |         109.8305 |        -152.2899 |
[32m[20221214 00:34:51 @agent_ppo2.py:185][0m |          -0.0059 |         102.9998 |        -151.3800 |
[32m[20221214 00:34:51 @agent_ppo2.py:185][0m |          -0.0019 |         104.1804 |        -151.5040 |
[32m[20221214 00:34:51 @agent_ppo2.py:185][0m |          -0.0073 |          99.8540 |        -151.3723 |
[32m[20221214 00:34:52 @agent_ppo2.py:185][0m |          -0.0108 |          98.5588 |        -150.8784 |
[32m[20221214 00:34:52 @agent_ppo2.py:185][0m |          -0.0147 |          96.9925 |        -151.4851 |
[32m[20221214 00:34:52 @agent_ppo2.py:185][0m |          -0.0115 |          96.1399 |        -150.5097 |
[32m[20221214 00:34:52 @agent_ppo2.py:185][0m |          -0.0148 |          95.7578 |        -150.9051 |
[32m[20221214 00:34:52 @agent_ppo2.py:185][0m |          -0.0161 |          94.7581 |        -150.8918 |
[32m[20221214 00:34:52 @agent_ppo2.py:185][0m |          -0.0132 |          94.5998 |        -150.9341 |
[32m[20221214 00:34:52 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 00:34:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 553.29
[32m[20221214 00:34:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 713.35
[32m[20221214 00:34:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 518.46
[32m[20221214 00:34:52 @agent_ppo2.py:143][0m Total time:      37.36 min
[32m[20221214 00:34:52 @agent_ppo2.py:145][0m 3368960 total steps have happened
[32m[20221214 00:34:52 @agent_ppo2.py:121][0m #------------------------ Iteration 5645 --------------------------#
[32m[20221214 00:34:52 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:34:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:34:53 @agent_ppo2.py:185][0m |           0.0045 |         140.7011 |        -149.9845 |
[32m[20221214 00:34:53 @agent_ppo2.py:185][0m |          -0.0011 |         130.1939 |        -149.8745 |
[32m[20221214 00:34:53 @agent_ppo2.py:185][0m |          -0.0035 |         126.7850 |        -150.2108 |
[32m[20221214 00:34:53 @agent_ppo2.py:185][0m |          -0.0077 |         125.5903 |        -149.3353 |
[32m[20221214 00:34:53 @agent_ppo2.py:185][0m |          -0.0047 |         125.6794 |        -148.9270 |
[32m[20221214 00:34:53 @agent_ppo2.py:185][0m |          -0.0055 |         124.0471 |        -149.4628 |
[32m[20221214 00:34:53 @agent_ppo2.py:185][0m |          -0.0100 |         124.1430 |        -149.2026 |
[32m[20221214 00:34:53 @agent_ppo2.py:185][0m |           0.0008 |         124.6194 |        -149.1102 |
[32m[20221214 00:34:53 @agent_ppo2.py:185][0m |          -0.0117 |         123.2784 |        -149.1201 |
[32m[20221214 00:34:54 @agent_ppo2.py:185][0m |          -0.0084 |         122.6936 |        -149.1387 |
[32m[20221214 00:34:54 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:34:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 592.00
[32m[20221214 00:34:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 672.72
[32m[20221214 00:34:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 841.38
[32m[20221214 00:34:54 @agent_ppo2.py:143][0m Total time:      37.39 min
[32m[20221214 00:34:54 @agent_ppo2.py:145][0m 3371008 total steps have happened
[32m[20221214 00:34:54 @agent_ppo2.py:121][0m #------------------------ Iteration 5646 --------------------------#
[32m[20221214 00:34:54 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:34:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:34:54 @agent_ppo2.py:185][0m |           0.0130 |         170.9057 |        -148.2798 |
[32m[20221214 00:34:54 @agent_ppo2.py:185][0m |          -0.0081 |         145.2494 |        -148.0044 |
[32m[20221214 00:34:54 @agent_ppo2.py:185][0m |          -0.0051 |         141.0228 |        -147.9637 |
[32m[20221214 00:34:54 @agent_ppo2.py:185][0m |          -0.0081 |         138.9147 |        -148.3856 |
[32m[20221214 00:34:54 @agent_ppo2.py:185][0m |          -0.0128 |         138.8332 |        -149.3191 |
[32m[20221214 00:34:55 @agent_ppo2.py:185][0m |          -0.0127 |         136.9312 |        -150.2255 |
[32m[20221214 00:34:55 @agent_ppo2.py:185][0m |          -0.0111 |         137.2139 |        -149.7402 |
[32m[20221214 00:34:55 @agent_ppo2.py:185][0m |          -0.0124 |         136.4816 |        -149.7259 |
[32m[20221214 00:34:55 @agent_ppo2.py:185][0m |          -0.0109 |         135.8491 |        -148.9651 |
[32m[20221214 00:34:55 @agent_ppo2.py:185][0m |          -0.0121 |         135.4338 |        -149.4494 |
[32m[20221214 00:34:55 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:34:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 692.92
[32m[20221214 00:34:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 789.54
[32m[20221214 00:34:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 740.75
[32m[20221214 00:34:55 @agent_ppo2.py:143][0m Total time:      37.41 min
[32m[20221214 00:34:55 @agent_ppo2.py:145][0m 3373056 total steps have happened
[32m[20221214 00:34:55 @agent_ppo2.py:121][0m #------------------------ Iteration 5647 --------------------------#
[32m[20221214 00:34:55 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:34:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:34:55 @agent_ppo2.py:185][0m |           0.0049 |         119.3350 |        -147.5610 |
[32m[20221214 00:34:56 @agent_ppo2.py:185][0m |          -0.0055 |         110.9423 |        -146.8557 |
[32m[20221214 00:34:56 @agent_ppo2.py:185][0m |          -0.0034 |         109.4527 |        -147.1155 |
[32m[20221214 00:34:56 @agent_ppo2.py:185][0m |          -0.0066 |         108.8257 |        -146.9777 |
[32m[20221214 00:34:56 @agent_ppo2.py:185][0m |          -0.0023 |         109.1656 |        -146.3610 |
[32m[20221214 00:34:56 @agent_ppo2.py:185][0m |          -0.0090 |         107.6619 |        -146.0353 |
[32m[20221214 00:34:56 @agent_ppo2.py:185][0m |          -0.0060 |         106.4275 |        -146.1480 |
[32m[20221214 00:34:56 @agent_ppo2.py:185][0m |          -0.0094 |         106.0013 |        -146.0424 |
[32m[20221214 00:34:56 @agent_ppo2.py:185][0m |          -0.0115 |         106.3712 |        -145.9284 |
[32m[20221214 00:34:56 @agent_ppo2.py:185][0m |          -0.0125 |         106.1389 |        -146.5224 |
[32m[20221214 00:34:56 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:34:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 543.71
[32m[20221214 00:34:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 697.36
[32m[20221214 00:34:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 820.33
[32m[20221214 00:34:57 @agent_ppo2.py:143][0m Total time:      37.44 min
[32m[20221214 00:34:57 @agent_ppo2.py:145][0m 3375104 total steps have happened
[32m[20221214 00:34:57 @agent_ppo2.py:121][0m #------------------------ Iteration 5648 --------------------------#
[32m[20221214 00:34:57 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:34:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:34:57 @agent_ppo2.py:185][0m |           0.0051 |         188.6033 |        -146.9925 |
[32m[20221214 00:34:57 @agent_ppo2.py:185][0m |           0.0071 |         193.6279 |        -146.8274 |
[32m[20221214 00:34:57 @agent_ppo2.py:185][0m |           0.0050 |         187.2657 |        -146.3457 |
[32m[20221214 00:34:57 @agent_ppo2.py:185][0m |           0.0131 |         205.4746 |        -147.2325 |
[32m[20221214 00:34:57 @agent_ppo2.py:185][0m |           0.0052 |         184.5057 |        -145.7796 |
[32m[20221214 00:34:57 @agent_ppo2.py:185][0m |          -0.0051 |         184.0010 |        -146.8252 |
[32m[20221214 00:34:57 @agent_ppo2.py:185][0m |           0.0065 |         191.5330 |        -145.8503 |
[32m[20221214 00:34:58 @agent_ppo2.py:185][0m |          -0.0028 |         183.5041 |        -145.6198 |
[32m[20221214 00:34:58 @agent_ppo2.py:185][0m |          -0.0050 |         183.3885 |        -145.5930 |
[32m[20221214 00:34:58 @agent_ppo2.py:185][0m |           0.0020 |         185.1111 |        -145.8385 |
[32m[20221214 00:34:58 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:34:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 741.22
[32m[20221214 00:34:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 835.14
[32m[20221214 00:34:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 731.61
[32m[20221214 00:34:58 @agent_ppo2.py:143][0m Total time:      37.46 min
[32m[20221214 00:34:58 @agent_ppo2.py:145][0m 3377152 total steps have happened
[32m[20221214 00:34:58 @agent_ppo2.py:121][0m #------------------------ Iteration 5649 --------------------------#
[32m[20221214 00:34:58 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:34:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:34:58 @agent_ppo2.py:185][0m |           0.0047 |         130.9376 |        -147.7654 |
[32m[20221214 00:34:58 @agent_ppo2.py:185][0m |          -0.0009 |         127.2838 |        -147.9010 |
[32m[20221214 00:34:58 @agent_ppo2.py:185][0m |          -0.0004 |         126.0455 |        -147.9699 |
[32m[20221214 00:34:59 @agent_ppo2.py:185][0m |          -0.0012 |         123.5377 |        -147.3722 |
[32m[20221214 00:34:59 @agent_ppo2.py:185][0m |          -0.0079 |         120.2706 |        -147.2565 |
[32m[20221214 00:34:59 @agent_ppo2.py:185][0m |          -0.0097 |         119.1045 |        -147.0935 |
[32m[20221214 00:34:59 @agent_ppo2.py:185][0m |          -0.0005 |         122.9307 |        -146.5820 |
[32m[20221214 00:34:59 @agent_ppo2.py:185][0m |          -0.0065 |         117.8899 |        -147.2344 |
[32m[20221214 00:34:59 @agent_ppo2.py:185][0m |          -0.0112 |         116.9855 |        -146.2385 |
[32m[20221214 00:34:59 @agent_ppo2.py:185][0m |          -0.0077 |         116.4387 |        -146.7264 |
[32m[20221214 00:34:59 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:34:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 630.09
[32m[20221214 00:34:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 741.17
[32m[20221214 00:34:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 720.39
[32m[20221214 00:34:59 @agent_ppo2.py:143][0m Total time:      37.48 min
[32m[20221214 00:34:59 @agent_ppo2.py:145][0m 3379200 total steps have happened
[32m[20221214 00:34:59 @agent_ppo2.py:121][0m #------------------------ Iteration 5650 --------------------------#
[32m[20221214 00:35:00 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:35:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:35:00 @agent_ppo2.py:185][0m |           0.0042 |          97.1929 |        -151.9419 |
[32m[20221214 00:35:00 @agent_ppo2.py:185][0m |          -0.0042 |          92.6584 |        -151.8151 |
[32m[20221214 00:35:00 @agent_ppo2.py:185][0m |          -0.0028 |          89.3544 |        -151.4595 |
[32m[20221214 00:35:00 @agent_ppo2.py:185][0m |          -0.0082 |          87.3036 |        -151.1486 |
[32m[20221214 00:35:00 @agent_ppo2.py:185][0m |          -0.0069 |          85.9703 |        -150.9759 |
[32m[20221214 00:35:00 @agent_ppo2.py:185][0m |          -0.0146 |          84.8902 |        -151.2314 |
[32m[20221214 00:35:00 @agent_ppo2.py:185][0m |          -0.0015 |          92.1659 |        -151.3563 |
[32m[20221214 00:35:00 @agent_ppo2.py:185][0m |          -0.0082 |          85.2124 |        -151.2057 |
[32m[20221214 00:35:01 @agent_ppo2.py:185][0m |          -0.0136 |          82.5943 |        -151.0853 |
[32m[20221214 00:35:01 @agent_ppo2.py:185][0m |          -0.0138 |          81.8070 |        -151.8711 |
[32m[20221214 00:35:01 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:35:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 532.86
[32m[20221214 00:35:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 619.26
[32m[20221214 00:35:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 700.75
[32m[20221214 00:35:01 @agent_ppo2.py:143][0m Total time:      37.51 min
[32m[20221214 00:35:01 @agent_ppo2.py:145][0m 3381248 total steps have happened
[32m[20221214 00:35:01 @agent_ppo2.py:121][0m #------------------------ Iteration 5651 --------------------------#
[32m[20221214 00:35:01 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:35:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:35:01 @agent_ppo2.py:185][0m |           0.0013 |         179.9405 |        -144.9488 |
[32m[20221214 00:35:01 @agent_ppo2.py:185][0m |          -0.0042 |         174.8404 |        -144.5924 |
[32m[20221214 00:35:01 @agent_ppo2.py:185][0m |          -0.0024 |         172.0992 |        -144.5308 |
[32m[20221214 00:35:01 @agent_ppo2.py:185][0m |          -0.0053 |         170.3013 |        -144.6317 |
[32m[20221214 00:35:02 @agent_ppo2.py:185][0m |          -0.0036 |         169.5477 |        -145.0333 |
[32m[20221214 00:35:02 @agent_ppo2.py:185][0m |          -0.0070 |         168.4018 |        -144.5586 |
[32m[20221214 00:35:02 @agent_ppo2.py:185][0m |          -0.0058 |         167.7561 |        -144.2430 |
[32m[20221214 00:35:02 @agent_ppo2.py:185][0m |           0.0043 |         178.5346 |        -144.4955 |
[32m[20221214 00:35:02 @agent_ppo2.py:185][0m |          -0.0067 |         167.2017 |        -144.0987 |
[32m[20221214 00:35:02 @agent_ppo2.py:185][0m |          -0.0072 |         166.0414 |        -144.2858 |
[32m[20221214 00:35:02 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:35:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 716.50
[32m[20221214 00:35:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 809.17
[32m[20221214 00:35:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 804.34
[32m[20221214 00:35:02 @agent_ppo2.py:143][0m Total time:      37.53 min
[32m[20221214 00:35:02 @agent_ppo2.py:145][0m 3383296 total steps have happened
[32m[20221214 00:35:02 @agent_ppo2.py:121][0m #------------------------ Iteration 5652 --------------------------#
[32m[20221214 00:35:02 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:35:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:35:03 @agent_ppo2.py:185][0m |          -0.0042 |         171.7914 |        -151.2879 |
[32m[20221214 00:35:03 @agent_ppo2.py:185][0m |           0.0018 |         163.2715 |        -151.1740 |
[32m[20221214 00:35:03 @agent_ppo2.py:185][0m |          -0.0051 |         163.0923 |        -152.1649 |
[32m[20221214 00:35:03 @agent_ppo2.py:185][0m |          -0.0015 |         158.2330 |        -152.1170 |
[32m[20221214 00:35:03 @agent_ppo2.py:185][0m |          -0.0045 |         156.4044 |        -152.6246 |
[32m[20221214 00:35:03 @agent_ppo2.py:185][0m |          -0.0036 |         156.3436 |        -153.4741 |
[32m[20221214 00:35:03 @agent_ppo2.py:185][0m |          -0.0066 |         159.0546 |        -153.6601 |
[32m[20221214 00:35:03 @agent_ppo2.py:185][0m |           0.0045 |         161.4508 |        -154.1103 |
[32m[20221214 00:35:03 @agent_ppo2.py:185][0m |          -0.0016 |         154.2024 |        -153.3672 |
[32m[20221214 00:35:03 @agent_ppo2.py:185][0m |          -0.0027 |         154.1241 |        -154.3855 |
[32m[20221214 00:35:03 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:35:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 566.96
[32m[20221214 00:35:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 712.78
[32m[20221214 00:35:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 551.00
[32m[20221214 00:35:04 @agent_ppo2.py:143][0m Total time:      37.55 min
[32m[20221214 00:35:04 @agent_ppo2.py:145][0m 3385344 total steps have happened
[32m[20221214 00:35:04 @agent_ppo2.py:121][0m #------------------------ Iteration 5653 --------------------------#
[32m[20221214 00:35:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:35:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:35:04 @agent_ppo2.py:185][0m |          -0.0014 |         141.6012 |        -153.5076 |
[32m[20221214 00:35:04 @agent_ppo2.py:185][0m |          -0.0080 |         134.8943 |        -153.0093 |
[32m[20221214 00:35:04 @agent_ppo2.py:185][0m |           0.0100 |         141.3266 |        -152.7275 |
[32m[20221214 00:35:04 @agent_ppo2.py:185][0m |          -0.0070 |         131.7158 |        -153.3150 |
[32m[20221214 00:35:04 @agent_ppo2.py:185][0m |          -0.0079 |         130.9892 |        -152.3619 |
[32m[20221214 00:35:04 @agent_ppo2.py:185][0m |          -0.0098 |         129.8279 |        -152.8348 |
[32m[20221214 00:35:05 @agent_ppo2.py:185][0m |          -0.0049 |         130.2661 |        -152.4731 |
[32m[20221214 00:35:05 @agent_ppo2.py:185][0m |          -0.0060 |         129.4493 |        -152.6631 |
[32m[20221214 00:35:05 @agent_ppo2.py:185][0m |          -0.0116 |         128.6290 |        -153.0806 |
[32m[20221214 00:35:05 @agent_ppo2.py:185][0m |           0.0013 |         140.2213 |        -152.5641 |
[32m[20221214 00:35:05 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221214 00:35:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 560.67
[32m[20221214 00:35:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 700.60
[32m[20221214 00:35:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 729.04
[32m[20221214 00:35:05 @agent_ppo2.py:143][0m Total time:      37.58 min
[32m[20221214 00:35:05 @agent_ppo2.py:145][0m 3387392 total steps have happened
[32m[20221214 00:35:05 @agent_ppo2.py:121][0m #------------------------ Iteration 5654 --------------------------#
[32m[20221214 00:35:05 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:35:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:35:05 @agent_ppo2.py:185][0m |           0.0055 |         106.4204 |        -152.6676 |
[32m[20221214 00:35:06 @agent_ppo2.py:185][0m |          -0.0027 |          98.2594 |        -153.4294 |
[32m[20221214 00:35:06 @agent_ppo2.py:185][0m |           0.0001 |          95.0913 |        -153.5517 |
[32m[20221214 00:35:06 @agent_ppo2.py:185][0m |          -0.0074 |          91.4895 |        -153.8074 |
[32m[20221214 00:35:06 @agent_ppo2.py:185][0m |          -0.0069 |          90.4835 |        -153.0260 |
[32m[20221214 00:35:06 @agent_ppo2.py:185][0m |           0.0097 |         109.7187 |        -153.2685 |
[32m[20221214 00:35:06 @agent_ppo2.py:185][0m |          -0.0086 |          91.3097 |        -152.9836 |
[32m[20221214 00:35:06 @agent_ppo2.py:185][0m |          -0.0115 |          89.3454 |        -153.2409 |
[32m[20221214 00:35:06 @agent_ppo2.py:185][0m |          -0.0102 |          88.6278 |        -153.0326 |
[32m[20221214 00:35:06 @agent_ppo2.py:185][0m |          -0.0090 |          88.4011 |        -153.0830 |
[32m[20221214 00:35:06 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:35:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 554.37
[32m[20221214 00:35:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 714.77
[32m[20221214 00:35:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 711.98
[32m[20221214 00:35:06 @agent_ppo2.py:143][0m Total time:      37.60 min
[32m[20221214 00:35:06 @agent_ppo2.py:145][0m 3389440 total steps have happened
[32m[20221214 00:35:06 @agent_ppo2.py:121][0m #------------------------ Iteration 5655 --------------------------#
[32m[20221214 00:35:07 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:35:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:35:07 @agent_ppo2.py:185][0m |           0.0185 |         159.4449 |        -157.3161 |
[32m[20221214 00:35:07 @agent_ppo2.py:185][0m |          -0.0035 |         125.6656 |        -157.9803 |
[32m[20221214 00:35:07 @agent_ppo2.py:185][0m |          -0.0071 |         122.3035 |        -157.8798 |
[32m[20221214 00:35:07 @agent_ppo2.py:185][0m |          -0.0046 |         120.9376 |        -157.7789 |
[32m[20221214 00:35:07 @agent_ppo2.py:185][0m |          -0.0075 |         119.5863 |        -158.0188 |
[32m[20221214 00:35:07 @agent_ppo2.py:185][0m |          -0.0110 |         119.1863 |        -157.1217 |
[32m[20221214 00:35:07 @agent_ppo2.py:185][0m |          -0.0077 |         118.2794 |        -158.0154 |
[32m[20221214 00:35:08 @agent_ppo2.py:185][0m |          -0.0081 |         117.3834 |        -157.7076 |
[32m[20221214 00:35:08 @agent_ppo2.py:185][0m |          -0.0121 |         117.4460 |        -157.9891 |
[32m[20221214 00:35:08 @agent_ppo2.py:185][0m |          -0.0088 |         117.3251 |        -157.9188 |
[32m[20221214 00:35:08 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:35:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 540.09
[32m[20221214 00:35:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 708.54
[32m[20221214 00:35:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 479.64
[32m[20221214 00:35:08 @agent_ppo2.py:143][0m Total time:      37.62 min
[32m[20221214 00:35:08 @agent_ppo2.py:145][0m 3391488 total steps have happened
[32m[20221214 00:35:08 @agent_ppo2.py:121][0m #------------------------ Iteration 5656 --------------------------#
[32m[20221214 00:35:08 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:35:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:35:08 @agent_ppo2.py:185][0m |          -0.0023 |         140.4021 |        -156.3440 |
[32m[20221214 00:35:08 @agent_ppo2.py:185][0m |          -0.0044 |         130.7988 |        -156.3784 |
[32m[20221214 00:35:08 @agent_ppo2.py:185][0m |          -0.0056 |         128.5989 |        -156.4569 |
[32m[20221214 00:35:09 @agent_ppo2.py:185][0m |          -0.0065 |         127.4589 |        -156.8225 |
[32m[20221214 00:35:09 @agent_ppo2.py:185][0m |           0.0031 |         140.6700 |        -155.7571 |
[32m[20221214 00:35:09 @agent_ppo2.py:185][0m |          -0.0077 |         126.3679 |        -156.0826 |
[32m[20221214 00:35:09 @agent_ppo2.py:185][0m |          -0.0114 |         124.5984 |        -156.3073 |
[32m[20221214 00:35:09 @agent_ppo2.py:185][0m |          -0.0140 |         123.4236 |        -156.4372 |
[32m[20221214 00:35:09 @agent_ppo2.py:185][0m |          -0.0104 |         122.6048 |        -156.5128 |
[32m[20221214 00:35:09 @agent_ppo2.py:185][0m |          -0.0177 |         123.0215 |        -156.4465 |
[32m[20221214 00:35:09 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:35:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 489.99
[32m[20221214 00:35:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 666.22
[32m[20221214 00:35:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 619.23
[32m[20221214 00:35:09 @agent_ppo2.py:143][0m Total time:      37.65 min
[32m[20221214 00:35:09 @agent_ppo2.py:145][0m 3393536 total steps have happened
[32m[20221214 00:35:09 @agent_ppo2.py:121][0m #------------------------ Iteration 5657 --------------------------#
[32m[20221214 00:35:09 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:35:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:35:10 @agent_ppo2.py:185][0m |           0.0019 |         159.8673 |        -153.0602 |
[32m[20221214 00:35:10 @agent_ppo2.py:185][0m |           0.0002 |         154.2380 |        -152.0053 |
[32m[20221214 00:35:10 @agent_ppo2.py:185][0m |          -0.0010 |         149.4201 |        -150.7454 |
[32m[20221214 00:35:10 @agent_ppo2.py:185][0m |          -0.0064 |         147.7506 |        -151.6533 |
[32m[20221214 00:35:10 @agent_ppo2.py:185][0m |          -0.0050 |         145.1111 |        -151.8288 |
[32m[20221214 00:35:10 @agent_ppo2.py:185][0m |          -0.0040 |         144.5507 |        -151.4803 |
[32m[20221214 00:35:10 @agent_ppo2.py:185][0m |           0.0038 |         144.2425 |        -150.3730 |
[32m[20221214 00:35:10 @agent_ppo2.py:185][0m |          -0.0088 |         142.7901 |        -151.0806 |
[32m[20221214 00:35:10 @agent_ppo2.py:185][0m |          -0.0048 |         143.2180 |        -151.6395 |
[32m[20221214 00:35:11 @agent_ppo2.py:185][0m |          -0.0070 |         141.8917 |        -151.4543 |
[32m[20221214 00:35:11 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:35:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 716.39
[32m[20221214 00:35:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 808.39
[32m[20221214 00:35:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 768.63
[32m[20221214 00:35:11 @agent_ppo2.py:143][0m Total time:      37.67 min
[32m[20221214 00:35:11 @agent_ppo2.py:145][0m 3395584 total steps have happened
[32m[20221214 00:35:11 @agent_ppo2.py:121][0m #------------------------ Iteration 5658 --------------------------#
[32m[20221214 00:35:11 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:35:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:35:11 @agent_ppo2.py:185][0m |           0.0036 |          38.0864 |        -151.7668 |
[32m[20221214 00:35:11 @agent_ppo2.py:185][0m |           0.0002 |          31.9898 |        -153.2600 |
[32m[20221214 00:35:11 @agent_ppo2.py:185][0m |           0.0014 |          32.1128 |        -153.2751 |
[32m[20221214 00:35:11 @agent_ppo2.py:185][0m |          -0.0035 |          31.6106 |        -153.5597 |
[32m[20221214 00:35:11 @agent_ppo2.py:185][0m |          -0.0004 |          31.5639 |        -152.5278 |
[32m[20221214 00:35:12 @agent_ppo2.py:185][0m |          -0.0054 |          31.6570 |        -152.2118 |
[32m[20221214 00:35:12 @agent_ppo2.py:185][0m |           0.0000 |          31.5832 |        -152.9530 |
[32m[20221214 00:35:12 @agent_ppo2.py:185][0m |           0.0022 |          32.0014 |        -152.9111 |
[32m[20221214 00:35:12 @agent_ppo2.py:185][0m |           0.0027 |          33.7660 |        -152.2871 |
[32m[20221214 00:35:12 @agent_ppo2.py:185][0m |          -0.0050 |          31.4877 |        -152.7847 |
[32m[20221214 00:35:12 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:35:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221214 00:35:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221214 00:35:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221214 00:35:12 @agent_ppo2.py:143][0m Total time:      37.70 min
[32m[20221214 00:35:12 @agent_ppo2.py:145][0m 3397632 total steps have happened
[32m[20221214 00:35:12 @agent_ppo2.py:121][0m #------------------------ Iteration 5659 --------------------------#
[32m[20221214 00:35:12 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:35:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:35:13 @agent_ppo2.py:185][0m |           0.0050 |         144.7750 |        -149.4166 |
[32m[20221214 00:35:13 @agent_ppo2.py:185][0m |           0.0035 |         133.9629 |        -149.2048 |
[32m[20221214 00:35:13 @agent_ppo2.py:185][0m |          -0.0071 |         127.2601 |        -148.8264 |
[32m[20221214 00:35:13 @agent_ppo2.py:185][0m |           0.0021 |         124.8269 |        -149.0652 |
[32m[20221214 00:35:13 @agent_ppo2.py:185][0m |          -0.0075 |         123.4364 |        -147.9593 |
[32m[20221214 00:35:13 @agent_ppo2.py:185][0m |          -0.0093 |         121.4877 |        -149.0415 |
[32m[20221214 00:35:13 @agent_ppo2.py:185][0m |          -0.0079 |         120.6534 |        -148.5109 |
[32m[20221214 00:35:13 @agent_ppo2.py:185][0m |          -0.0020 |         138.0016 |        -148.5077 |
[32m[20221214 00:35:13 @agent_ppo2.py:185][0m |          -0.0152 |         120.5999 |        -148.3971 |
[32m[20221214 00:35:13 @agent_ppo2.py:185][0m |          -0.0146 |         118.2898 |        -148.9931 |
[32m[20221214 00:35:13 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221214 00:35:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 478.80
[32m[20221214 00:35:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 672.46
[32m[20221214 00:35:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 652.24
[32m[20221214 00:35:14 @agent_ppo2.py:143][0m Total time:      37.72 min
[32m[20221214 00:35:14 @agent_ppo2.py:145][0m 3399680 total steps have happened
[32m[20221214 00:35:14 @agent_ppo2.py:121][0m #------------------------ Iteration 5660 --------------------------#
[32m[20221214 00:35:14 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:35:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:35:14 @agent_ppo2.py:185][0m |           0.0039 |         116.1163 |        -158.1806 |
[32m[20221214 00:35:14 @agent_ppo2.py:185][0m |          -0.0027 |          99.6408 |        -158.7102 |
[32m[20221214 00:35:14 @agent_ppo2.py:185][0m |          -0.0071 |          96.5759 |        -158.0771 |
[32m[20221214 00:35:14 @agent_ppo2.py:185][0m |          -0.0064 |          94.7451 |        -158.8468 |
[32m[20221214 00:35:15 @agent_ppo2.py:185][0m |          -0.0050 |          93.9647 |        -157.2246 |
[32m[20221214 00:35:15 @agent_ppo2.py:185][0m |          -0.0131 |          95.1557 |        -157.9575 |
[32m[20221214 00:35:15 @agent_ppo2.py:185][0m |          -0.0110 |          92.4894 |        -157.4061 |
[32m[20221214 00:35:15 @agent_ppo2.py:185][0m |          -0.0120 |          91.0383 |        -158.1563 |
[32m[20221214 00:35:15 @agent_ppo2.py:185][0m |          -0.0122 |          91.6533 |        -158.2505 |
[32m[20221214 00:35:15 @agent_ppo2.py:185][0m |          -0.0110 |          90.8619 |        -157.9688 |
[32m[20221214 00:35:15 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 00:35:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 681.14
[32m[20221214 00:35:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 769.67
[32m[20221214 00:35:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 397.11
[32m[20221214 00:35:15 @agent_ppo2.py:143][0m Total time:      37.75 min
[32m[20221214 00:35:15 @agent_ppo2.py:145][0m 3401728 total steps have happened
[32m[20221214 00:35:15 @agent_ppo2.py:121][0m #------------------------ Iteration 5661 --------------------------#
[32m[20221214 00:35:15 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:35:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:35:16 @agent_ppo2.py:185][0m |          -0.0002 |         139.7173 |        -148.5535 |
[32m[20221214 00:35:16 @agent_ppo2.py:185][0m |           0.0008 |         131.1283 |        -147.8829 |
[32m[20221214 00:35:16 @agent_ppo2.py:185][0m |          -0.0016 |         128.2352 |        -148.2846 |
[32m[20221214 00:35:16 @agent_ppo2.py:185][0m |          -0.0077 |         125.7938 |        -148.1395 |
[32m[20221214 00:35:16 @agent_ppo2.py:185][0m |          -0.0037 |         123.8638 |        -147.8732 |
[32m[20221214 00:35:16 @agent_ppo2.py:185][0m |          -0.0034 |         122.9557 |        -148.0198 |
[32m[20221214 00:35:16 @agent_ppo2.py:185][0m |          -0.0053 |         122.1199 |        -147.9799 |
[32m[20221214 00:35:16 @agent_ppo2.py:185][0m |          -0.0078 |         120.8343 |        -148.7722 |
[32m[20221214 00:35:16 @agent_ppo2.py:185][0m |          -0.0036 |         124.6335 |        -148.6806 |
[32m[20221214 00:35:17 @agent_ppo2.py:185][0m |          -0.0093 |         120.3936 |        -148.2094 |
[32m[20221214 00:35:17 @agent_ppo2.py:130][0m Policy update time: 1.18 s
[32m[20221214 00:35:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 618.21
[32m[20221214 00:35:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 779.68
[32m[20221214 00:35:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 594.44
[32m[20221214 00:35:17 @agent_ppo2.py:143][0m Total time:      37.77 min
[32m[20221214 00:35:17 @agent_ppo2.py:145][0m 3403776 total steps have happened
[32m[20221214 00:35:17 @agent_ppo2.py:121][0m #------------------------ Iteration 5662 --------------------------#
[32m[20221214 00:35:17 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:35:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:35:17 @agent_ppo2.py:185][0m |           0.0082 |         123.7634 |        -157.6796 |
[32m[20221214 00:35:17 @agent_ppo2.py:185][0m |          -0.0030 |         109.8369 |        -158.3886 |
[32m[20221214 00:35:17 @agent_ppo2.py:185][0m |          -0.0036 |         104.8259 |        -158.5224 |
[32m[20221214 00:35:17 @agent_ppo2.py:185][0m |          -0.0075 |         102.4596 |        -158.6106 |
[32m[20221214 00:35:18 @agent_ppo2.py:185][0m |          -0.0009 |         101.3396 |        -159.1119 |
[32m[20221214 00:35:18 @agent_ppo2.py:185][0m |          -0.0063 |          99.3821 |        -158.6349 |
[32m[20221214 00:35:18 @agent_ppo2.py:185][0m |          -0.0128 |          98.4956 |        -159.6288 |
[32m[20221214 00:35:18 @agent_ppo2.py:185][0m |          -0.0116 |          97.0293 |        -159.0278 |
[32m[20221214 00:35:18 @agent_ppo2.py:185][0m |          -0.0080 |          96.6721 |        -159.8688 |
[32m[20221214 00:35:18 @agent_ppo2.py:185][0m |          -0.0134 |          95.6545 |        -160.3443 |
[32m[20221214 00:35:18 @agent_ppo2.py:130][0m Policy update time: 1.23 s
[32m[20221214 00:35:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 684.52
[32m[20221214 00:35:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 730.90
[32m[20221214 00:35:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 809.71
[32m[20221214 00:35:18 @agent_ppo2.py:143][0m Total time:      37.80 min
[32m[20221214 00:35:18 @agent_ppo2.py:145][0m 3405824 total steps have happened
[32m[20221214 00:35:18 @agent_ppo2.py:121][0m #------------------------ Iteration 5663 --------------------------#
[32m[20221214 00:35:18 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:35:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:35:19 @agent_ppo2.py:185][0m |           0.0036 |         154.2105 |        -160.5135 |
[32m[20221214 00:35:19 @agent_ppo2.py:185][0m |           0.0080 |         159.6993 |        -159.7803 |
[32m[20221214 00:35:19 @agent_ppo2.py:185][0m |          -0.0047 |         144.5434 |        -161.0139 |
[32m[20221214 00:35:19 @agent_ppo2.py:185][0m |          -0.0007 |         140.5483 |        -159.1576 |
[32m[20221214 00:35:19 @agent_ppo2.py:185][0m |          -0.0008 |         141.0124 |        -159.4482 |
[32m[20221214 00:35:19 @agent_ppo2.py:185][0m |          -0.0103 |         137.8153 |        -160.3199 |
[32m[20221214 00:35:19 @agent_ppo2.py:185][0m |          -0.0066 |         138.0731 |        -160.5883 |
[32m[20221214 00:35:19 @agent_ppo2.py:185][0m |          -0.0109 |         136.2036 |        -159.6173 |
[32m[20221214 00:35:19 @agent_ppo2.py:185][0m |          -0.0094 |         136.3257 |        -160.1575 |
[32m[20221214 00:35:20 @agent_ppo2.py:185][0m |          -0.0043 |         137.9062 |        -160.2153 |
[32m[20221214 00:35:20 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221214 00:35:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 596.99
[32m[20221214 00:35:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 678.31
[32m[20221214 00:35:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 604.82
[32m[20221214 00:35:20 @agent_ppo2.py:143][0m Total time:      37.82 min
[32m[20221214 00:35:20 @agent_ppo2.py:145][0m 3407872 total steps have happened
[32m[20221214 00:35:20 @agent_ppo2.py:121][0m #------------------------ Iteration 5664 --------------------------#
[32m[20221214 00:35:20 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:35:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:35:20 @agent_ppo2.py:185][0m |           0.0004 |         145.4369 |        -165.5063 |
[32m[20221214 00:35:20 @agent_ppo2.py:185][0m |          -0.0041 |         134.2987 |        -164.0664 |
[32m[20221214 00:35:20 @agent_ppo2.py:185][0m |          -0.0073 |         127.9758 |        -164.8013 |
[32m[20221214 00:35:20 @agent_ppo2.py:185][0m |          -0.0068 |         125.1548 |        -165.0263 |
[32m[20221214 00:35:21 @agent_ppo2.py:185][0m |          -0.0040 |         125.5406 |        -165.2761 |
[32m[20221214 00:35:21 @agent_ppo2.py:185][0m |          -0.0124 |         121.2846 |        -164.5338 |
[32m[20221214 00:35:21 @agent_ppo2.py:185][0m |          -0.0119 |         120.7241 |        -165.1916 |
[32m[20221214 00:35:21 @agent_ppo2.py:185][0m |          -0.0051 |         122.7987 |        -164.9532 |
[32m[20221214 00:35:21 @agent_ppo2.py:185][0m |          -0.0139 |         118.2175 |        -165.6277 |
[32m[20221214 00:35:21 @agent_ppo2.py:185][0m |          -0.0166 |         117.9702 |        -164.1043 |
[32m[20221214 00:35:21 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221214 00:35:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 593.27
[32m[20221214 00:35:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 625.10
[32m[20221214 00:35:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 510.03
[32m[20221214 00:35:21 @agent_ppo2.py:143][0m Total time:      37.85 min
[32m[20221214 00:35:21 @agent_ppo2.py:145][0m 3409920 total steps have happened
[32m[20221214 00:35:21 @agent_ppo2.py:121][0m #------------------------ Iteration 5665 --------------------------#
[32m[20221214 00:35:21 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:35:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:35:22 @agent_ppo2.py:185][0m |           0.0033 |         128.5196 |        -159.9501 |
[32m[20221214 00:35:22 @agent_ppo2.py:185][0m |          -0.0044 |         114.2327 |        -159.6624 |
[32m[20221214 00:35:22 @agent_ppo2.py:185][0m |           0.0042 |         122.7515 |        -159.6760 |
[32m[20221214 00:35:22 @agent_ppo2.py:185][0m |          -0.0049 |         110.7263 |        -160.0347 |
[32m[20221214 00:35:22 @agent_ppo2.py:185][0m |          -0.0043 |         111.5900 |        -160.9432 |
[32m[20221214 00:35:22 @agent_ppo2.py:185][0m |          -0.0077 |         106.7996 |        -160.6312 |
[32m[20221214 00:35:22 @agent_ppo2.py:185][0m |          -0.0136 |         105.3409 |        -161.1806 |
[32m[20221214 00:35:22 @agent_ppo2.py:185][0m |          -0.0147 |         107.3543 |        -160.5818 |
[32m[20221214 00:35:22 @agent_ppo2.py:185][0m |          -0.0132 |         103.5995 |        -160.2653 |
[32m[20221214 00:35:23 @agent_ppo2.py:185][0m |          -0.0102 |         106.2404 |        -161.2524 |
[32m[20221214 00:35:23 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:35:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 592.09
[32m[20221214 00:35:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 721.82
[32m[20221214 00:35:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 807.67
[32m[20221214 00:35:23 @agent_ppo2.py:143][0m Total time:      37.87 min
[32m[20221214 00:35:23 @agent_ppo2.py:145][0m 3411968 total steps have happened
[32m[20221214 00:35:23 @agent_ppo2.py:121][0m #------------------------ Iteration 5666 --------------------------#
[32m[20221214 00:35:23 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:35:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:35:23 @agent_ppo2.py:185][0m |           0.0011 |         141.2782 |        -165.2433 |
[32m[20221214 00:35:23 @agent_ppo2.py:185][0m |          -0.0039 |         130.5072 |        -164.6458 |
[32m[20221214 00:35:23 @agent_ppo2.py:185][0m |           0.0025 |         130.9064 |        -163.7418 |
[32m[20221214 00:35:23 @agent_ppo2.py:185][0m |          -0.0080 |         128.9561 |        -164.3636 |
[32m[20221214 00:35:23 @agent_ppo2.py:185][0m |           0.0119 |         138.3083 |        -163.9741 |
[32m[20221214 00:35:23 @agent_ppo2.py:185][0m |           0.0062 |         134.3524 |        -163.7475 |
[32m[20221214 00:35:24 @agent_ppo2.py:185][0m |          -0.0034 |         126.1995 |        -164.1211 |
[32m[20221214 00:35:24 @agent_ppo2.py:185][0m |          -0.0050 |         125.7412 |        -164.1854 |
[32m[20221214 00:35:24 @agent_ppo2.py:185][0m |          -0.0055 |         125.1667 |        -163.7821 |
[32m[20221214 00:35:24 @agent_ppo2.py:185][0m |           0.0000 |         126.5944 |        -164.2241 |
[32m[20221214 00:35:24 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:35:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 789.34
[32m[20221214 00:35:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.21
[32m[20221214 00:35:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 786.34
[32m[20221214 00:35:24 @agent_ppo2.py:143][0m Total time:      37.89 min
[32m[20221214 00:35:24 @agent_ppo2.py:145][0m 3414016 total steps have happened
[32m[20221214 00:35:24 @agent_ppo2.py:121][0m #------------------------ Iteration 5667 --------------------------#
[32m[20221214 00:35:24 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:35:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:35:24 @agent_ppo2.py:185][0m |           0.0038 |         121.3001 |        -166.3835 |
[32m[20221214 00:35:25 @agent_ppo2.py:185][0m |          -0.0057 |         106.5929 |        -166.6187 |
[32m[20221214 00:35:25 @agent_ppo2.py:185][0m |           0.0021 |         104.1634 |        -165.8610 |
[32m[20221214 00:35:25 @agent_ppo2.py:185][0m |          -0.0022 |         111.0807 |        -166.5121 |
[32m[20221214 00:35:25 @agent_ppo2.py:185][0m |          -0.0129 |         102.7527 |        -167.1591 |
[32m[20221214 00:35:25 @agent_ppo2.py:185][0m |          -0.0026 |         111.5295 |        -167.0939 |
[32m[20221214 00:35:25 @agent_ppo2.py:185][0m |          -0.0107 |         100.4524 |        -166.8009 |
[32m[20221214 00:35:25 @agent_ppo2.py:185][0m |          -0.0152 |         100.2178 |        -167.1433 |
[32m[20221214 00:35:25 @agent_ppo2.py:185][0m |          -0.0158 |          99.6965 |        -167.4518 |
[32m[20221214 00:35:25 @agent_ppo2.py:185][0m |          -0.0130 |          99.1701 |        -166.7894 |
[32m[20221214 00:35:25 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:35:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 635.20
[32m[20221214 00:35:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 716.73
[32m[20221214 00:35:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 843.11
[32m[20221214 00:35:25 @agent_ppo2.py:143][0m Total time:      37.92 min
[32m[20221214 00:35:25 @agent_ppo2.py:145][0m 3416064 total steps have happened
[32m[20221214 00:35:25 @agent_ppo2.py:121][0m #------------------------ Iteration 5668 --------------------------#
[32m[20221214 00:35:26 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:35:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:35:26 @agent_ppo2.py:185][0m |           0.0033 |          88.1286 |        -163.2734 |
[32m[20221214 00:35:26 @agent_ppo2.py:185][0m |          -0.0005 |          74.4343 |        -162.9401 |
[32m[20221214 00:35:26 @agent_ppo2.py:185][0m |          -0.0095 |          72.0950 |        -162.3294 |
[32m[20221214 00:35:26 @agent_ppo2.py:185][0m |          -0.0036 |          70.8816 |        -162.0918 |
[32m[20221214 00:35:26 @agent_ppo2.py:185][0m |          -0.0034 |          70.2379 |        -161.9439 |
[32m[20221214 00:35:26 @agent_ppo2.py:185][0m |           0.0040 |          73.4386 |        -160.5532 |
[32m[20221214 00:35:26 @agent_ppo2.py:185][0m |          -0.0084 |          68.9683 |        -161.2623 |
[32m[20221214 00:35:27 @agent_ppo2.py:185][0m |          -0.0067 |          67.4971 |        -160.7614 |
[32m[20221214 00:35:27 @agent_ppo2.py:185][0m |          -0.0140 |          67.2219 |        -160.1897 |
[32m[20221214 00:35:27 @agent_ppo2.py:185][0m |          -0.0014 |          76.2626 |        -160.2662 |
[32m[20221214 00:35:27 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221214 00:35:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 666.09
[32m[20221214 00:35:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 805.28
[32m[20221214 00:35:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 843.57
[32m[20221214 00:35:27 @agent_ppo2.py:143][0m Total time:      37.94 min
[32m[20221214 00:35:27 @agent_ppo2.py:145][0m 3418112 total steps have happened
[32m[20221214 00:35:27 @agent_ppo2.py:121][0m #------------------------ Iteration 5669 --------------------------#
[32m[20221214 00:35:27 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:35:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:35:27 @agent_ppo2.py:185][0m |          -0.0019 |         116.9085 |        -164.1653 |
[32m[20221214 00:35:27 @agent_ppo2.py:185][0m |          -0.0060 |         109.1812 |        -163.9823 |
[32m[20221214 00:35:28 @agent_ppo2.py:185][0m |          -0.0055 |         105.9099 |        -163.7818 |
[32m[20221214 00:35:28 @agent_ppo2.py:185][0m |          -0.0150 |         104.0126 |        -164.2332 |
[32m[20221214 00:35:28 @agent_ppo2.py:185][0m |          -0.0093 |         103.1396 |        -163.5047 |
[32m[20221214 00:35:28 @agent_ppo2.py:185][0m |          -0.0086 |         104.2400 |        -163.4901 |
[32m[20221214 00:35:28 @agent_ppo2.py:185][0m |          -0.0128 |         101.5150 |        -163.0404 |
[32m[20221214 00:35:28 @agent_ppo2.py:185][0m |          -0.0122 |         100.9868 |        -164.0570 |
[32m[20221214 00:35:28 @agent_ppo2.py:185][0m |          -0.0199 |         100.5038 |        -163.4125 |
[32m[20221214 00:35:28 @agent_ppo2.py:185][0m |          -0.0165 |         100.1225 |        -163.1269 |
[32m[20221214 00:35:28 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:35:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 564.70
[32m[20221214 00:35:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 614.36
[32m[20221214 00:35:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 660.03
[32m[20221214 00:35:28 @agent_ppo2.py:143][0m Total time:      37.97 min
[32m[20221214 00:35:28 @agent_ppo2.py:145][0m 3420160 total steps have happened
[32m[20221214 00:35:28 @agent_ppo2.py:121][0m #------------------------ Iteration 5670 --------------------------#
[32m[20221214 00:35:29 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:35:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:35:29 @agent_ppo2.py:185][0m |           0.0001 |         144.8703 |        -166.7036 |
[32m[20221214 00:35:29 @agent_ppo2.py:185][0m |          -0.0016 |         138.6119 |        -166.0126 |
[32m[20221214 00:35:29 @agent_ppo2.py:185][0m |          -0.0058 |         140.6526 |        -166.3520 |
[32m[20221214 00:35:29 @agent_ppo2.py:185][0m |          -0.0033 |         135.3576 |        -164.9979 |
[32m[20221214 00:35:29 @agent_ppo2.py:185][0m |          -0.0077 |         134.4144 |        -165.1678 |
[32m[20221214 00:35:29 @agent_ppo2.py:185][0m |          -0.0091 |         133.3904 |        -164.9594 |
[32m[20221214 00:35:29 @agent_ppo2.py:185][0m |          -0.0097 |         133.8699 |        -164.6940 |
[32m[20221214 00:35:29 @agent_ppo2.py:185][0m |           0.0028 |         135.3487 |        -164.3092 |
[32m[20221214 00:35:30 @agent_ppo2.py:185][0m |          -0.0070 |         131.5382 |        -164.9546 |
[32m[20221214 00:35:30 @agent_ppo2.py:185][0m |          -0.0031 |         133.5530 |        -165.3336 |
[32m[20221214 00:35:30 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:35:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 703.49
[32m[20221214 00:35:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 787.88
[32m[20221214 00:35:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 624.15
[32m[20221214 00:35:30 @agent_ppo2.py:143][0m Total time:      37.99 min
[32m[20221214 00:35:30 @agent_ppo2.py:145][0m 3422208 total steps have happened
[32m[20221214 00:35:30 @agent_ppo2.py:121][0m #------------------------ Iteration 5671 --------------------------#
[32m[20221214 00:35:30 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:35:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:35:30 @agent_ppo2.py:185][0m |          -0.0029 |         128.2262 |        -158.8024 |
[32m[20221214 00:35:30 @agent_ppo2.py:185][0m |          -0.0019 |         123.3257 |        -158.8868 |
[32m[20221214 00:35:30 @agent_ppo2.py:185][0m |          -0.0012 |         120.7235 |        -158.7455 |
[32m[20221214 00:35:31 @agent_ppo2.py:185][0m |          -0.0055 |         122.1607 |        -158.7089 |
[32m[20221214 00:35:31 @agent_ppo2.py:185][0m |          -0.0039 |         119.0781 |        -158.2390 |
[32m[20221214 00:35:31 @agent_ppo2.py:185][0m |          -0.0078 |         118.4874 |        -158.3950 |
[32m[20221214 00:35:31 @agent_ppo2.py:185][0m |          -0.0030 |         118.2186 |        -159.0978 |
[32m[20221214 00:35:31 @agent_ppo2.py:185][0m |          -0.0115 |         117.0326 |        -157.8318 |
[32m[20221214 00:35:31 @agent_ppo2.py:185][0m |          -0.0043 |         116.4012 |        -157.5299 |
[32m[20221214 00:35:31 @agent_ppo2.py:185][0m |          -0.0077 |         114.7830 |        -157.7918 |
[32m[20221214 00:35:31 @agent_ppo2.py:130][0m Policy update time: 1.19 s
[32m[20221214 00:35:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 763.50
[32m[20221214 00:35:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 832.71
[32m[20221214 00:35:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 693.01
[32m[20221214 00:35:31 @agent_ppo2.py:143][0m Total time:      38.02 min
[32m[20221214 00:35:31 @agent_ppo2.py:145][0m 3424256 total steps have happened
[32m[20221214 00:35:31 @agent_ppo2.py:121][0m #------------------------ Iteration 5672 --------------------------#
[32m[20221214 00:35:32 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:35:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:35:32 @agent_ppo2.py:185][0m |           0.0039 |         139.0396 |        -161.6181 |
[32m[20221214 00:35:32 @agent_ppo2.py:185][0m |          -0.0050 |         129.2599 |        -162.2692 |
[32m[20221214 00:35:32 @agent_ppo2.py:185][0m |          -0.0049 |         126.3441 |        -161.5993 |
[32m[20221214 00:35:32 @agent_ppo2.py:185][0m |          -0.0136 |         125.0786 |        -161.7072 |
[32m[20221214 00:35:32 @agent_ppo2.py:185][0m |          -0.0113 |         123.2189 |        -161.7466 |
[32m[20221214 00:35:32 @agent_ppo2.py:185][0m |          -0.0096 |         122.3687 |        -161.4564 |
[32m[20221214 00:35:32 @agent_ppo2.py:185][0m |          -0.0124 |         121.9844 |        -161.8014 |
[32m[20221214 00:35:32 @agent_ppo2.py:185][0m |          -0.0075 |         121.2683 |        -160.5545 |
[32m[20221214 00:35:33 @agent_ppo2.py:185][0m |          -0.0116 |         121.1821 |        -161.2255 |
[32m[20221214 00:35:33 @agent_ppo2.py:185][0m |          -0.0094 |         120.5864 |        -160.6139 |
[32m[20221214 00:35:33 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:35:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 583.38
[32m[20221214 00:35:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 706.93
[32m[20221214 00:35:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 834.24
[32m[20221214 00:35:33 @agent_ppo2.py:143][0m Total time:      38.04 min
[32m[20221214 00:35:33 @agent_ppo2.py:145][0m 3426304 total steps have happened
[32m[20221214 00:35:33 @agent_ppo2.py:121][0m #------------------------ Iteration 5673 --------------------------#
[32m[20221214 00:35:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:35:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:35:33 @agent_ppo2.py:185][0m |           0.0063 |         104.7579 |        -160.9149 |
[32m[20221214 00:35:33 @agent_ppo2.py:185][0m |          -0.0028 |          94.9399 |        -159.1503 |
[32m[20221214 00:35:33 @agent_ppo2.py:185][0m |           0.0007 |          93.4058 |        -159.9838 |
[32m[20221214 00:35:33 @agent_ppo2.py:185][0m |          -0.0037 |          91.4545 |        -159.9911 |
[32m[20221214 00:35:34 @agent_ppo2.py:185][0m |           0.0093 |          99.9738 |        -160.0717 |
[32m[20221214 00:35:34 @agent_ppo2.py:185][0m |          -0.0032 |          91.0357 |        -160.2090 |
[32m[20221214 00:35:34 @agent_ppo2.py:185][0m |          -0.0052 |          89.9974 |        -160.4836 |
[32m[20221214 00:35:34 @agent_ppo2.py:185][0m |          -0.0115 |          90.1577 |        -161.0849 |
[32m[20221214 00:35:34 @agent_ppo2.py:185][0m |          -0.0144 |          89.7518 |        -159.7971 |
[32m[20221214 00:35:34 @agent_ppo2.py:185][0m |          -0.0133 |          88.7731 |        -160.5657 |
[32m[20221214 00:35:34 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:35:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 561.24
[32m[20221214 00:35:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 758.52
[32m[20221214 00:35:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 396.96
[32m[20221214 00:35:34 @agent_ppo2.py:143][0m Total time:      38.07 min
[32m[20221214 00:35:34 @agent_ppo2.py:145][0m 3428352 total steps have happened
[32m[20221214 00:35:34 @agent_ppo2.py:121][0m #------------------------ Iteration 5674 --------------------------#
[32m[20221214 00:35:34 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:35:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:35:35 @agent_ppo2.py:185][0m |           0.0041 |         125.4930 |        -161.6050 |
[32m[20221214 00:35:35 @agent_ppo2.py:185][0m |          -0.0045 |         118.0476 |        -161.3596 |
[32m[20221214 00:35:35 @agent_ppo2.py:185][0m |          -0.0031 |         115.7053 |        -161.9608 |
[32m[20221214 00:35:35 @agent_ppo2.py:185][0m |          -0.0022 |         114.9107 |        -161.8642 |
[32m[20221214 00:35:35 @agent_ppo2.py:185][0m |          -0.0063 |         114.3997 |        -161.8007 |
[32m[20221214 00:35:35 @agent_ppo2.py:185][0m |          -0.0120 |         113.7114 |        -162.5467 |
[32m[20221214 00:35:35 @agent_ppo2.py:185][0m |          -0.0053 |         113.2524 |        -162.3573 |
[32m[20221214 00:35:35 @agent_ppo2.py:185][0m |          -0.0064 |         112.7108 |        -161.8527 |
[32m[20221214 00:35:35 @agent_ppo2.py:185][0m |          -0.0085 |         112.1761 |        -162.0989 |
[32m[20221214 00:35:36 @agent_ppo2.py:185][0m |          -0.0047 |         111.7899 |        -161.4050 |
[32m[20221214 00:35:36 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221214 00:35:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 673.11
[32m[20221214 00:35:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 749.88
[32m[20221214 00:35:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 798.11
[32m[20221214 00:35:36 @agent_ppo2.py:143][0m Total time:      38.09 min
[32m[20221214 00:35:36 @agent_ppo2.py:145][0m 3430400 total steps have happened
[32m[20221214 00:35:36 @agent_ppo2.py:121][0m #------------------------ Iteration 5675 --------------------------#
[32m[20221214 00:35:36 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:35:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:35:36 @agent_ppo2.py:185][0m |           0.0095 |         129.2545 |        -162.3094 |
[32m[20221214 00:35:36 @agent_ppo2.py:185][0m |          -0.0018 |         123.2205 |        -162.1269 |
[32m[20221214 00:35:36 @agent_ppo2.py:185][0m |          -0.0041 |         120.0771 |        -162.2402 |
[32m[20221214 00:35:36 @agent_ppo2.py:185][0m |          -0.0088 |         119.0251 |        -162.5587 |
[32m[20221214 00:35:37 @agent_ppo2.py:185][0m |          -0.0079 |         117.4312 |        -163.6010 |
[32m[20221214 00:35:37 @agent_ppo2.py:185][0m |          -0.0087 |         117.7582 |        -163.7758 |
[32m[20221214 00:35:37 @agent_ppo2.py:185][0m |          -0.0067 |         116.2161 |        -164.4437 |
[32m[20221214 00:35:37 @agent_ppo2.py:185][0m |          -0.0072 |         116.6554 |        -164.3264 |
[32m[20221214 00:35:37 @agent_ppo2.py:185][0m |          -0.0086 |         115.2496 |        -163.7092 |
[32m[20221214 00:35:37 @agent_ppo2.py:185][0m |          -0.0103 |         116.2541 |        -165.1696 |
[32m[20221214 00:35:37 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 00:35:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 738.22
[32m[20221214 00:35:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 794.56
[32m[20221214 00:35:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 662.93
[32m[20221214 00:35:37 @agent_ppo2.py:143][0m Total time:      38.11 min
[32m[20221214 00:35:37 @agent_ppo2.py:145][0m 3432448 total steps have happened
[32m[20221214 00:35:37 @agent_ppo2.py:121][0m #------------------------ Iteration 5676 --------------------------#
[32m[20221214 00:35:37 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:35:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:35:38 @agent_ppo2.py:185][0m |           0.0081 |         156.2937 |        -167.7623 |
[32m[20221214 00:35:38 @agent_ppo2.py:185][0m |          -0.0012 |         139.5118 |        -166.5658 |
[32m[20221214 00:35:38 @agent_ppo2.py:185][0m |          -0.0020 |         136.5927 |        -166.2727 |
[32m[20221214 00:35:38 @agent_ppo2.py:185][0m |          -0.0048 |         134.1781 |        -166.1585 |
[32m[20221214 00:35:38 @agent_ppo2.py:185][0m |          -0.0080 |         133.3909 |        -167.4054 |
[32m[20221214 00:35:38 @agent_ppo2.py:185][0m |          -0.0077 |         133.4067 |        -166.4609 |
[32m[20221214 00:35:38 @agent_ppo2.py:185][0m |          -0.0035 |         132.4133 |        -166.4974 |
[32m[20221214 00:35:38 @agent_ppo2.py:185][0m |          -0.0084 |         130.7399 |        -166.2571 |
[32m[20221214 00:35:38 @agent_ppo2.py:185][0m |          -0.0083 |         130.4035 |        -166.1074 |
[32m[20221214 00:35:38 @agent_ppo2.py:185][0m |          -0.0089 |         130.4488 |        -166.2885 |
[32m[20221214 00:35:38 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:35:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 812.12
[32m[20221214 00:35:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 878.10
[32m[20221214 00:35:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 757.09
[32m[20221214 00:35:39 @agent_ppo2.py:143][0m Total time:      38.14 min
[32m[20221214 00:35:39 @agent_ppo2.py:145][0m 3434496 total steps have happened
[32m[20221214 00:35:39 @agent_ppo2.py:121][0m #------------------------ Iteration 5677 --------------------------#
[32m[20221214 00:35:39 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:35:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:35:39 @agent_ppo2.py:185][0m |          -0.0014 |         158.3661 |        -169.3585 |
[32m[20221214 00:35:39 @agent_ppo2.py:185][0m |          -0.0037 |         147.2761 |        -169.6615 |
[32m[20221214 00:35:39 @agent_ppo2.py:185][0m |          -0.0055 |         145.1394 |        -170.4985 |
[32m[20221214 00:35:39 @agent_ppo2.py:185][0m |          -0.0068 |         142.9244 |        -170.3235 |
[32m[20221214 00:35:39 @agent_ppo2.py:185][0m |          -0.0065 |         141.2382 |        -170.0799 |
[32m[20221214 00:35:40 @agent_ppo2.py:185][0m |          -0.0078 |         140.4661 |        -172.0844 |
[32m[20221214 00:35:40 @agent_ppo2.py:185][0m |          -0.0083 |         139.5656 |        -171.3717 |
[32m[20221214 00:35:40 @agent_ppo2.py:185][0m |          -0.0121 |         139.6633 |        -171.3059 |
[32m[20221214 00:35:40 @agent_ppo2.py:185][0m |          -0.0121 |         138.2926 |        -172.2000 |
[32m[20221214 00:35:40 @agent_ppo2.py:185][0m |          -0.0021 |         153.0654 |        -172.2920 |
[32m[20221214 00:35:40 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:35:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 637.02
[32m[20221214 00:35:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 785.48
[32m[20221214 00:35:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 343.56
[32m[20221214 00:35:40 @agent_ppo2.py:143][0m Total time:      38.16 min
[32m[20221214 00:35:40 @agent_ppo2.py:145][0m 3436544 total steps have happened
[32m[20221214 00:35:40 @agent_ppo2.py:121][0m #------------------------ Iteration 5678 --------------------------#
[32m[20221214 00:35:40 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:35:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:35:40 @agent_ppo2.py:185][0m |           0.0031 |         178.2462 |        -174.6944 |
[32m[20221214 00:35:41 @agent_ppo2.py:185][0m |           0.0001 |         173.5388 |        -173.6920 |
[32m[20221214 00:35:41 @agent_ppo2.py:185][0m |           0.0077 |         193.0856 |        -174.6297 |
[32m[20221214 00:35:41 @agent_ppo2.py:185][0m |          -0.0062 |         171.7944 |        -174.8757 |
[32m[20221214 00:35:41 @agent_ppo2.py:185][0m |           0.0083 |         183.9684 |        -174.9645 |
[32m[20221214 00:35:41 @agent_ppo2.py:185][0m |          -0.0091 |         169.3548 |        -174.5771 |
[32m[20221214 00:35:41 @agent_ppo2.py:185][0m |          -0.0097 |         169.0865 |        -174.6500 |
[32m[20221214 00:35:41 @agent_ppo2.py:185][0m |          -0.0124 |         169.2100 |        -174.7596 |
[32m[20221214 00:35:41 @agent_ppo2.py:185][0m |          -0.0054 |         171.8098 |        -173.7785 |
[32m[20221214 00:35:42 @agent_ppo2.py:185][0m |          -0.0121 |         169.1875 |        -174.4902 |
[32m[20221214 00:35:42 @agent_ppo2.py:130][0m Policy update time: 1.27 s
[32m[20221214 00:35:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 705.04
[32m[20221214 00:35:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 788.48
[32m[20221214 00:35:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 798.09
[32m[20221214 00:35:42 @agent_ppo2.py:143][0m Total time:      38.19 min
[32m[20221214 00:35:42 @agent_ppo2.py:145][0m 3438592 total steps have happened
[32m[20221214 00:35:42 @agent_ppo2.py:121][0m #------------------------ Iteration 5679 --------------------------#
[32m[20221214 00:35:42 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:35:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:35:42 @agent_ppo2.py:185][0m |           0.0002 |          91.6226 |        -168.8872 |
[32m[20221214 00:35:42 @agent_ppo2.py:185][0m |           0.0003 |          84.1618 |        -168.1764 |
[32m[20221214 00:35:42 @agent_ppo2.py:185][0m |          -0.0099 |          81.0680 |        -168.9862 |
[32m[20221214 00:35:42 @agent_ppo2.py:185][0m |          -0.0088 |          80.1061 |        -168.1279 |
[32m[20221214 00:35:42 @agent_ppo2.py:185][0m |          -0.0034 |          80.3650 |        -169.3223 |
[32m[20221214 00:35:43 @agent_ppo2.py:185][0m |          -0.0114 |          77.8883 |        -168.8759 |
[32m[20221214 00:35:43 @agent_ppo2.py:185][0m |          -0.0181 |          77.6362 |        -168.8430 |
[32m[20221214 00:35:43 @agent_ppo2.py:185][0m |          -0.0025 |          83.6553 |        -169.2150 |
[32m[20221214 00:35:43 @agent_ppo2.py:185][0m |          -0.0100 |          82.2919 |        -168.3336 |
[32m[20221214 00:35:43 @agent_ppo2.py:185][0m |          -0.0217 |          75.3156 |        -168.6157 |
[32m[20221214 00:35:43 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:35:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 486.44
[32m[20221214 00:35:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 655.53
[32m[20221214 00:35:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 745.36
[32m[20221214 00:35:43 @agent_ppo2.py:143][0m Total time:      38.21 min
[32m[20221214 00:35:43 @agent_ppo2.py:145][0m 3440640 total steps have happened
[32m[20221214 00:35:43 @agent_ppo2.py:121][0m #------------------------ Iteration 5680 --------------------------#
[32m[20221214 00:35:43 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:35:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:35:43 @agent_ppo2.py:185][0m |           0.0013 |         113.9352 |        -176.2077 |
[32m[20221214 00:35:44 @agent_ppo2.py:185][0m |           0.0005 |         105.9825 |        -176.3709 |
[32m[20221214 00:35:44 @agent_ppo2.py:185][0m |          -0.0085 |         102.3165 |        -176.2203 |
[32m[20221214 00:35:44 @agent_ppo2.py:185][0m |          -0.0035 |         100.3217 |        -175.0976 |
[32m[20221214 00:35:44 @agent_ppo2.py:185][0m |          -0.0024 |          99.8241 |        -176.0671 |
[32m[20221214 00:35:44 @agent_ppo2.py:185][0m |          -0.0046 |         100.3045 |        -176.1487 |
[32m[20221214 00:35:44 @agent_ppo2.py:185][0m |          -0.0095 |          97.1994 |        -175.6850 |
[32m[20221214 00:35:44 @agent_ppo2.py:185][0m |          -0.0108 |          96.7140 |        -175.9462 |
[32m[20221214 00:35:44 @agent_ppo2.py:185][0m |          -0.0120 |          95.4793 |        -176.3942 |
[32m[20221214 00:35:44 @agent_ppo2.py:185][0m |          -0.0125 |          94.6452 |        -176.5564 |
[32m[20221214 00:35:44 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:35:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 742.35
[32m[20221214 00:35:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 783.06
[32m[20221214 00:35:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 756.84
[32m[20221214 00:35:45 @agent_ppo2.py:143][0m Total time:      38.24 min
[32m[20221214 00:35:45 @agent_ppo2.py:145][0m 3442688 total steps have happened
[32m[20221214 00:35:45 @agent_ppo2.py:121][0m #------------------------ Iteration 5681 --------------------------#
[32m[20221214 00:35:45 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:35:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:35:45 @agent_ppo2.py:185][0m |           0.0051 |          78.9698 |        -178.9369 |
[32m[20221214 00:35:45 @agent_ppo2.py:185][0m |          -0.0042 |          69.7136 |        -178.9178 |
[32m[20221214 00:35:45 @agent_ppo2.py:185][0m |           0.0050 |          71.0812 |        -177.8701 |
[32m[20221214 00:35:45 @agent_ppo2.py:185][0m |          -0.0022 |          64.2709 |        -178.5689 |
[32m[20221214 00:35:45 @agent_ppo2.py:185][0m |          -0.0078 |          62.5732 |        -178.7405 |
[32m[20221214 00:35:45 @agent_ppo2.py:185][0m |          -0.0023 |          64.8563 |        -178.6113 |
[32m[20221214 00:35:46 @agent_ppo2.py:185][0m |          -0.0107 |          60.0852 |        -178.7565 |
[32m[20221214 00:35:46 @agent_ppo2.py:185][0m |          -0.0032 |          66.0413 |        -179.4642 |
[32m[20221214 00:35:46 @agent_ppo2.py:185][0m |          -0.0003 |          67.2452 |        -178.0452 |
[32m[20221214 00:35:46 @agent_ppo2.py:185][0m |          -0.0154 |          59.0355 |        -178.1216 |
[32m[20221214 00:35:46 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:35:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 541.03
[32m[20221214 00:35:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 713.86
[32m[20221214 00:35:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 797.96
[32m[20221214 00:35:46 @agent_ppo2.py:143][0m Total time:      38.26 min
[32m[20221214 00:35:46 @agent_ppo2.py:145][0m 3444736 total steps have happened
[32m[20221214 00:35:46 @agent_ppo2.py:121][0m #------------------------ Iteration 5682 --------------------------#
[32m[20221214 00:35:46 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:35:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:35:46 @agent_ppo2.py:185][0m |           0.0024 |          79.7964 |        -181.0514 |
[32m[20221214 00:35:46 @agent_ppo2.py:185][0m |          -0.0028 |          71.8089 |        -180.7278 |
[32m[20221214 00:35:47 @agent_ppo2.py:185][0m |          -0.0038 |          68.9926 |        -180.5703 |
[32m[20221214 00:35:47 @agent_ppo2.py:185][0m |          -0.0059 |          68.0648 |        -179.6865 |
[32m[20221214 00:35:47 @agent_ppo2.py:185][0m |          -0.0062 |          66.7667 |        -179.4360 |
[32m[20221214 00:35:47 @agent_ppo2.py:185][0m |          -0.0068 |          66.0701 |        -179.6592 |
[32m[20221214 00:35:47 @agent_ppo2.py:185][0m |          -0.0098 |          64.5607 |        -178.9659 |
[32m[20221214 00:35:47 @agent_ppo2.py:185][0m |          -0.0150 |          63.9283 |        -179.5595 |
[32m[20221214 00:35:47 @agent_ppo2.py:185][0m |          -0.0077 |          63.2975 |        -179.4542 |
[32m[20221214 00:35:47 @agent_ppo2.py:185][0m |          -0.0129 |          62.2287 |        -178.3654 |
[32m[20221214 00:35:47 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:35:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 527.54
[32m[20221214 00:35:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 719.14
[32m[20221214 00:35:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 704.99
[32m[20221214 00:35:47 @agent_ppo2.py:143][0m Total time:      38.28 min
[32m[20221214 00:35:47 @agent_ppo2.py:145][0m 3446784 total steps have happened
[32m[20221214 00:35:47 @agent_ppo2.py:121][0m #------------------------ Iteration 5683 --------------------------#
[32m[20221214 00:35:48 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:35:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:35:48 @agent_ppo2.py:185][0m |           0.0033 |         138.2480 |        -174.0457 |
[32m[20221214 00:35:48 @agent_ppo2.py:185][0m |          -0.0038 |         127.0579 |        -174.6268 |
[32m[20221214 00:35:48 @agent_ppo2.py:185][0m |           0.0028 |         131.7968 |        -174.1985 |
[32m[20221214 00:35:48 @agent_ppo2.py:185][0m |          -0.0067 |         123.5919 |        -174.7491 |
[32m[20221214 00:35:48 @agent_ppo2.py:185][0m |          -0.0098 |         122.1930 |        -174.7400 |
[32m[20221214 00:35:48 @agent_ppo2.py:185][0m |          -0.0070 |         121.4134 |        -174.9945 |
[32m[20221214 00:35:48 @agent_ppo2.py:185][0m |          -0.0050 |         124.9094 |        -175.5780 |
[32m[20221214 00:35:48 @agent_ppo2.py:185][0m |          -0.0096 |         120.1432 |        -174.2417 |
[32m[20221214 00:35:49 @agent_ppo2.py:185][0m |          -0.0102 |         119.3475 |        -175.2427 |
[32m[20221214 00:35:49 @agent_ppo2.py:185][0m |          -0.0091 |         118.1944 |        -174.2369 |
[32m[20221214 00:35:49 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:35:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 646.63
[32m[20221214 00:35:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 725.75
[32m[20221214 00:35:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 774.56
[32m[20221214 00:35:49 @agent_ppo2.py:143][0m Total time:      38.31 min
[32m[20221214 00:35:49 @agent_ppo2.py:145][0m 3448832 total steps have happened
[32m[20221214 00:35:49 @agent_ppo2.py:121][0m #------------------------ Iteration 5684 --------------------------#
[32m[20221214 00:35:49 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:35:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:35:49 @agent_ppo2.py:185][0m |           0.0067 |          72.3074 |        -169.8167 |
[32m[20221214 00:35:49 @agent_ppo2.py:185][0m |          -0.0056 |          61.7880 |        -169.0759 |
[32m[20221214 00:35:49 @agent_ppo2.py:185][0m |          -0.0070 |          58.6115 |        -168.7997 |
[32m[20221214 00:35:50 @agent_ppo2.py:185][0m |          -0.0074 |          56.8528 |        -168.8883 |
[32m[20221214 00:35:50 @agent_ppo2.py:185][0m |          -0.0102 |          54.6468 |        -168.7180 |
[32m[20221214 00:35:50 @agent_ppo2.py:185][0m |          -0.0171 |          53.6128 |        -168.7553 |
[32m[20221214 00:35:50 @agent_ppo2.py:185][0m |          -0.0120 |          52.3706 |        -168.7193 |
[32m[20221214 00:35:50 @agent_ppo2.py:185][0m |          -0.0165 |          51.3532 |        -168.5870 |
[32m[20221214 00:35:50 @agent_ppo2.py:185][0m |          -0.0183 |          50.7539 |        -168.8446 |
[32m[20221214 00:35:50 @agent_ppo2.py:185][0m |          -0.0160 |          49.9170 |        -169.0644 |
[32m[20221214 00:35:50 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221214 00:35:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 475.31
[32m[20221214 00:35:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 583.70
[32m[20221214 00:35:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 647.09
[32m[20221214 00:35:50 @agent_ppo2.py:143][0m Total time:      38.33 min
[32m[20221214 00:35:50 @agent_ppo2.py:145][0m 3450880 total steps have happened
[32m[20221214 00:35:50 @agent_ppo2.py:121][0m #------------------------ Iteration 5685 --------------------------#
[32m[20221214 00:35:50 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:35:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:35:51 @agent_ppo2.py:185][0m |           0.0085 |          88.2645 |        -172.1748 |
[32m[20221214 00:35:51 @agent_ppo2.py:185][0m |          -0.0113 |          76.4831 |        -172.1817 |
[32m[20221214 00:35:51 @agent_ppo2.py:185][0m |          -0.0131 |          73.3490 |        -172.5715 |
[32m[20221214 00:35:51 @agent_ppo2.py:185][0m |          -0.0083 |          71.7620 |        -171.7844 |
[32m[20221214 00:35:51 @agent_ppo2.py:185][0m |          -0.0071 |          69.8643 |        -172.3545 |
[32m[20221214 00:35:51 @agent_ppo2.py:185][0m |          -0.0111 |          69.3939 |        -172.7377 |
[32m[20221214 00:35:51 @agent_ppo2.py:185][0m |          -0.0129 |          67.8287 |        -172.0506 |
[32m[20221214 00:35:51 @agent_ppo2.py:185][0m |          -0.0158 |          66.8096 |        -172.3296 |
[32m[20221214 00:35:51 @agent_ppo2.py:185][0m |          -0.0125 |          66.0892 |        -171.9891 |
[32m[20221214 00:35:52 @agent_ppo2.py:185][0m |          -0.0171 |          65.3447 |        -172.5244 |
[32m[20221214 00:35:52 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:35:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 462.29
[32m[20221214 00:35:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 679.29
[32m[20221214 00:35:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 679.39
[32m[20221214 00:35:52 @agent_ppo2.py:143][0m Total time:      38.36 min
[32m[20221214 00:35:52 @agent_ppo2.py:145][0m 3452928 total steps have happened
[32m[20221214 00:35:52 @agent_ppo2.py:121][0m #------------------------ Iteration 5686 --------------------------#
[32m[20221214 00:35:52 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:35:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:35:52 @agent_ppo2.py:185][0m |          -0.0008 |         101.1738 |        -180.4058 |
[32m[20221214 00:35:52 @agent_ppo2.py:185][0m |          -0.0025 |          94.5974 |        -179.8487 |
[32m[20221214 00:35:52 @agent_ppo2.py:185][0m |          -0.0063 |          91.6873 |        -179.3235 |
[32m[20221214 00:35:52 @agent_ppo2.py:185][0m |          -0.0091 |          92.7142 |        -179.9107 |
[32m[20221214 00:35:53 @agent_ppo2.py:185][0m |          -0.0062 |          89.3035 |        -178.8878 |
[32m[20221214 00:35:53 @agent_ppo2.py:185][0m |          -0.0098 |          89.9901 |        -178.8674 |
[32m[20221214 00:35:53 @agent_ppo2.py:185][0m |          -0.0171 |          88.0055 |        -178.7688 |
[32m[20221214 00:35:53 @agent_ppo2.py:185][0m |          -0.0077 |          88.9908 |        -178.3691 |
[32m[20221214 00:35:53 @agent_ppo2.py:185][0m |          -0.0135 |          86.8523 |        -178.5683 |
[32m[20221214 00:35:53 @agent_ppo2.py:185][0m |          -0.0104 |          86.3961 |        -177.8818 |
[32m[20221214 00:35:53 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221214 00:35:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 487.35
[32m[20221214 00:35:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 623.24
[32m[20221214 00:35:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 760.89
[32m[20221214 00:35:53 @agent_ppo2.py:143][0m Total time:      38.38 min
[32m[20221214 00:35:53 @agent_ppo2.py:145][0m 3454976 total steps have happened
[32m[20221214 00:35:53 @agent_ppo2.py:121][0m #------------------------ Iteration 5687 --------------------------#
[32m[20221214 00:35:53 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:35:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:35:54 @agent_ppo2.py:185][0m |           0.0063 |         142.3367 |        -171.8370 |
[32m[20221214 00:35:54 @agent_ppo2.py:185][0m |          -0.0018 |         130.8337 |        -172.4475 |
[32m[20221214 00:35:54 @agent_ppo2.py:185][0m |          -0.0043 |         122.9514 |        -171.6279 |
[32m[20221214 00:35:54 @agent_ppo2.py:185][0m |          -0.0103 |         118.1438 |        -170.8503 |
[32m[20221214 00:35:54 @agent_ppo2.py:185][0m |          -0.0104 |         114.0047 |        -171.6453 |
[32m[20221214 00:35:54 @agent_ppo2.py:185][0m |          -0.0135 |         112.9790 |        -170.9050 |
[32m[20221214 00:35:54 @agent_ppo2.py:185][0m |          -0.0030 |         115.3270 |        -171.8269 |
[32m[20221214 00:35:54 @agent_ppo2.py:185][0m |          -0.0077 |         111.8815 |        -171.4421 |
[32m[20221214 00:35:54 @agent_ppo2.py:185][0m |          -0.0159 |         110.5115 |        -171.0773 |
[32m[20221214 00:35:55 @agent_ppo2.py:185][0m |          -0.0198 |         110.3489 |        -171.7681 |
[32m[20221214 00:35:55 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:35:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 599.45
[32m[20221214 00:35:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 738.31
[32m[20221214 00:35:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 325.87
[32m[20221214 00:35:55 @agent_ppo2.py:143][0m Total time:      38.40 min
[32m[20221214 00:35:55 @agent_ppo2.py:145][0m 3457024 total steps have happened
[32m[20221214 00:35:55 @agent_ppo2.py:121][0m #------------------------ Iteration 5688 --------------------------#
[32m[20221214 00:35:55 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:35:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:35:55 @agent_ppo2.py:185][0m |           0.0080 |         147.0083 |        -170.5290 |
[32m[20221214 00:35:55 @agent_ppo2.py:185][0m |           0.0028 |         125.3425 |        -169.4301 |
[32m[20221214 00:35:55 @agent_ppo2.py:185][0m |          -0.0043 |         120.2354 |        -170.3751 |
[32m[20221214 00:35:55 @agent_ppo2.py:185][0m |          -0.0066 |         116.2424 |        -170.2101 |
[32m[20221214 00:35:55 @agent_ppo2.py:185][0m |          -0.0063 |         114.6209 |        -171.2742 |
[32m[20221214 00:35:56 @agent_ppo2.py:185][0m |          -0.0071 |         113.3496 |        -170.4631 |
[32m[20221214 00:35:56 @agent_ppo2.py:185][0m |          -0.0086 |         112.1858 |        -170.8378 |
[32m[20221214 00:35:56 @agent_ppo2.py:185][0m |          -0.0098 |         112.0538 |        -170.4335 |
[32m[20221214 00:35:56 @agent_ppo2.py:185][0m |          -0.0112 |         110.7541 |        -170.6978 |
[32m[20221214 00:35:56 @agent_ppo2.py:185][0m |          -0.0111 |         110.0410 |        -170.9977 |
[32m[20221214 00:35:56 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:35:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 703.82
[32m[20221214 00:35:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 836.74
[32m[20221214 00:35:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 698.99
[32m[20221214 00:35:56 @agent_ppo2.py:143][0m Total time:      38.43 min
[32m[20221214 00:35:56 @agent_ppo2.py:145][0m 3459072 total steps have happened
[32m[20221214 00:35:56 @agent_ppo2.py:121][0m #------------------------ Iteration 5689 --------------------------#
[32m[20221214 00:35:56 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:35:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:35:56 @agent_ppo2.py:185][0m |           0.0038 |         113.8731 |        -180.4790 |
[32m[20221214 00:35:57 @agent_ppo2.py:185][0m |          -0.0001 |          86.4217 |        -179.2848 |
[32m[20221214 00:35:57 @agent_ppo2.py:185][0m |          -0.0096 |          82.8571 |        -179.2480 |
[32m[20221214 00:35:57 @agent_ppo2.py:185][0m |          -0.0033 |          89.7261 |        -178.8309 |
[32m[20221214 00:35:57 @agent_ppo2.py:185][0m |          -0.0193 |          79.6886 |        -178.7391 |
[32m[20221214 00:35:57 @agent_ppo2.py:185][0m |          -0.0166 |          77.4814 |        -179.1294 |
[32m[20221214 00:35:57 @agent_ppo2.py:185][0m |          -0.0202 |          76.4149 |        -178.6313 |
[32m[20221214 00:35:57 @agent_ppo2.py:185][0m |          -0.0110 |          75.9435 |        -178.4209 |
[32m[20221214 00:35:57 @agent_ppo2.py:185][0m |          -0.0067 |          78.9287 |        -177.9982 |
[32m[20221214 00:35:57 @agent_ppo2.py:185][0m |          -0.0167 |          74.5173 |        -178.1714 |
[32m[20221214 00:35:57 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:35:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 485.49
[32m[20221214 00:35:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 664.11
[32m[20221214 00:35:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 200.76
[32m[20221214 00:35:57 @agent_ppo2.py:143][0m Total time:      38.45 min
[32m[20221214 00:35:57 @agent_ppo2.py:145][0m 3461120 total steps have happened
[32m[20221214 00:35:57 @agent_ppo2.py:121][0m #------------------------ Iteration 5690 --------------------------#
[32m[20221214 00:35:58 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:35:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:35:58 @agent_ppo2.py:185][0m |           0.0007 |         127.6498 |        -172.6333 |
[32m[20221214 00:35:58 @agent_ppo2.py:185][0m |          -0.0091 |         122.6796 |        -173.2085 |
[32m[20221214 00:35:58 @agent_ppo2.py:185][0m |          -0.0070 |         121.2916 |        -172.3540 |
[32m[20221214 00:35:58 @agent_ppo2.py:185][0m |           0.0010 |         127.1719 |        -171.9090 |
[32m[20221214 00:35:58 @agent_ppo2.py:185][0m |          -0.0115 |         118.8530 |        -172.7440 |
[32m[20221214 00:35:58 @agent_ppo2.py:185][0m |          -0.0114 |         118.5415 |        -172.8652 |
[32m[20221214 00:35:58 @agent_ppo2.py:185][0m |          -0.0128 |         117.7767 |        -173.0428 |
[32m[20221214 00:35:59 @agent_ppo2.py:185][0m |          -0.0163 |         117.4660 |        -173.3379 |
[32m[20221214 00:35:59 @agent_ppo2.py:185][0m |          -0.0128 |         116.5475 |        -173.4672 |
[32m[20221214 00:35:59 @agent_ppo2.py:185][0m |          -0.0139 |         116.4650 |        -173.3216 |
[32m[20221214 00:35:59 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:35:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 741.05
[32m[20221214 00:35:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 828.15
[32m[20221214 00:35:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 756.82
[32m[20221214 00:35:59 @agent_ppo2.py:143][0m Total time:      38.48 min
[32m[20221214 00:35:59 @agent_ppo2.py:145][0m 3463168 total steps have happened
[32m[20221214 00:35:59 @agent_ppo2.py:121][0m #------------------------ Iteration 5691 --------------------------#
[32m[20221214 00:35:59 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:35:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:35:59 @agent_ppo2.py:185][0m |          -0.0016 |         166.3657 |        -177.0980 |
[32m[20221214 00:35:59 @agent_ppo2.py:185][0m |          -0.0030 |         159.0462 |        -176.7443 |
[32m[20221214 00:35:59 @agent_ppo2.py:185][0m |           0.0004 |         157.1039 |        -175.9104 |
[32m[20221214 00:36:00 @agent_ppo2.py:185][0m |           0.0011 |         156.7766 |        -176.2809 |
[32m[20221214 00:36:00 @agent_ppo2.py:185][0m |           0.0007 |         156.0962 |        -175.9127 |
[32m[20221214 00:36:00 @agent_ppo2.py:185][0m |          -0.0044 |         155.8261 |        -175.5817 |
[32m[20221214 00:36:00 @agent_ppo2.py:185][0m |          -0.0037 |         155.4639 |        -176.3922 |
[32m[20221214 00:36:00 @agent_ppo2.py:185][0m |          -0.0071 |         155.0491 |        -177.1057 |
[32m[20221214 00:36:00 @agent_ppo2.py:185][0m |          -0.0028 |         155.3820 |        -176.0692 |
[32m[20221214 00:36:00 @agent_ppo2.py:185][0m |          -0.0054 |         155.2274 |        -174.2378 |
[32m[20221214 00:36:00 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:36:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 650.54
[32m[20221214 00:36:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 827.32
[32m[20221214 00:36:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 793.59
[32m[20221214 00:36:00 @agent_ppo2.py:143][0m Total time:      38.50 min
[32m[20221214 00:36:00 @agent_ppo2.py:145][0m 3465216 total steps have happened
[32m[20221214 00:36:00 @agent_ppo2.py:121][0m #------------------------ Iteration 5692 --------------------------#
[32m[20221214 00:36:00 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:36:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:36:01 @agent_ppo2.py:185][0m |           0.0052 |          67.9511 |        -181.5252 |
[32m[20221214 00:36:01 @agent_ppo2.py:185][0m |           0.0052 |          60.2757 |        -181.8175 |
[32m[20221214 00:36:01 @agent_ppo2.py:185][0m |          -0.0034 |          56.3678 |        -180.6324 |
[32m[20221214 00:36:01 @agent_ppo2.py:185][0m |          -0.0104 |          55.2856 |        -180.1131 |
[32m[20221214 00:36:01 @agent_ppo2.py:185][0m |          -0.0111 |          53.7218 |        -180.1950 |
[32m[20221214 00:36:01 @agent_ppo2.py:185][0m |          -0.0151 |          53.3799 |        -180.2918 |
[32m[20221214 00:36:01 @agent_ppo2.py:185][0m |          -0.0210 |          52.2771 |        -179.3252 |
[32m[20221214 00:36:01 @agent_ppo2.py:185][0m |          -0.0073 |          51.7872 |        -178.0527 |
[32m[20221214 00:36:01 @agent_ppo2.py:185][0m |          -0.0124 |          51.8489 |        -177.8148 |
[32m[20221214 00:36:02 @agent_ppo2.py:185][0m |          -0.0143 |          51.0389 |        -178.3899 |
[32m[20221214 00:36:02 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:36:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 478.63
[32m[20221214 00:36:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 706.94
[32m[20221214 00:36:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 737.41
[32m[20221214 00:36:02 @agent_ppo2.py:143][0m Total time:      38.52 min
[32m[20221214 00:36:02 @agent_ppo2.py:145][0m 3467264 total steps have happened
[32m[20221214 00:36:02 @agent_ppo2.py:121][0m #------------------------ Iteration 5693 --------------------------#
[32m[20221214 00:36:02 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:36:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:36:02 @agent_ppo2.py:185][0m |           0.0106 |          77.5913 |        -179.1829 |
[32m[20221214 00:36:02 @agent_ppo2.py:185][0m |          -0.0073 |          62.2626 |        -178.0952 |
[32m[20221214 00:36:02 @agent_ppo2.py:185][0m |          -0.0057 |          59.6200 |        -178.3515 |
[32m[20221214 00:36:02 @agent_ppo2.py:185][0m |          -0.0105 |          57.7257 |        -177.8257 |
[32m[20221214 00:36:03 @agent_ppo2.py:185][0m |          -0.0117 |          56.2152 |        -178.1864 |
[32m[20221214 00:36:03 @agent_ppo2.py:185][0m |          -0.0094 |          55.3752 |        -178.3127 |
[32m[20221214 00:36:03 @agent_ppo2.py:185][0m |          -0.0111 |          54.3972 |        -178.3653 |
[32m[20221214 00:36:03 @agent_ppo2.py:185][0m |          -0.0092 |          57.3353 |        -179.0782 |
[32m[20221214 00:36:03 @agent_ppo2.py:185][0m |          -0.0092 |          53.0953 |        -179.2081 |
[32m[20221214 00:36:03 @agent_ppo2.py:185][0m |          -0.0135 |          52.9701 |        -179.1916 |
[32m[20221214 00:36:03 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:36:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 495.39
[32m[20221214 00:36:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 618.29
[32m[20221214 00:36:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 842.16
[32m[20221214 00:36:03 @agent_ppo2.py:143][0m Total time:      38.55 min
[32m[20221214 00:36:03 @agent_ppo2.py:145][0m 3469312 total steps have happened
[32m[20221214 00:36:03 @agent_ppo2.py:121][0m #------------------------ Iteration 5694 --------------------------#
[32m[20221214 00:36:03 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:36:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:36:04 @agent_ppo2.py:185][0m |           0.0119 |         194.9966 |        -177.9454 |
[32m[20221214 00:36:04 @agent_ppo2.py:185][0m |          -0.0040 |         165.1805 |        -178.0501 |
[32m[20221214 00:36:04 @agent_ppo2.py:185][0m |          -0.0065 |         160.6291 |        -178.1489 |
[32m[20221214 00:36:04 @agent_ppo2.py:185][0m |          -0.0075 |         158.0550 |        -177.0617 |
[32m[20221214 00:36:04 @agent_ppo2.py:185][0m |          -0.0105 |         156.2219 |        -177.4405 |
[32m[20221214 00:36:04 @agent_ppo2.py:185][0m |          -0.0055 |         166.7802 |        -177.6429 |
[32m[20221214 00:36:04 @agent_ppo2.py:185][0m |          -0.0109 |         153.9486 |        -177.7551 |
[32m[20221214 00:36:04 @agent_ppo2.py:185][0m |          -0.0128 |         153.1641 |        -177.9136 |
[32m[20221214 00:36:04 @agent_ppo2.py:185][0m |          -0.0060 |         155.8038 |        -177.3808 |
[32m[20221214 00:36:04 @agent_ppo2.py:185][0m |          -0.0127 |         151.7076 |        -175.4813 |
[32m[20221214 00:36:04 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:36:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 620.57
[32m[20221214 00:36:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 734.71
[32m[20221214 00:36:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 799.21
[32m[20221214 00:36:05 @agent_ppo2.py:143][0m Total time:      38.57 min
[32m[20221214 00:36:05 @agent_ppo2.py:145][0m 3471360 total steps have happened
[32m[20221214 00:36:05 @agent_ppo2.py:121][0m #------------------------ Iteration 5695 --------------------------#
[32m[20221214 00:36:05 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:36:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:36:05 @agent_ppo2.py:185][0m |           0.0066 |         174.3942 |        -177.1137 |
[32m[20221214 00:36:05 @agent_ppo2.py:185][0m |          -0.0004 |         170.8521 |        -177.6270 |
[32m[20221214 00:36:05 @agent_ppo2.py:185][0m |           0.0007 |         169.8608 |        -178.0557 |
[32m[20221214 00:36:05 @agent_ppo2.py:185][0m |          -0.0011 |         169.8968 |        -177.6335 |
[32m[20221214 00:36:05 @agent_ppo2.py:185][0m |          -0.0024 |         170.1627 |        -177.0358 |
[32m[20221214 00:36:05 @agent_ppo2.py:185][0m |          -0.0040 |         169.3912 |        -176.2423 |
[32m[20221214 00:36:06 @agent_ppo2.py:185][0m |          -0.0035 |         169.2925 |        -177.9184 |
[32m[20221214 00:36:06 @agent_ppo2.py:185][0m |           0.0003 |         169.0459 |        -177.8251 |
[32m[20221214 00:36:06 @agent_ppo2.py:185][0m |           0.0053 |         183.5979 |        -177.7110 |
[32m[20221214 00:36:06 @agent_ppo2.py:185][0m |          -0.0014 |         169.3246 |        -176.5281 |
[32m[20221214 00:36:06 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:36:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 773.80
[32m[20221214 00:36:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 827.56
[32m[20221214 00:36:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.63
[32m[20221214 00:36:06 @agent_ppo2.py:143][0m Total time:      38.59 min
[32m[20221214 00:36:06 @agent_ppo2.py:145][0m 3473408 total steps have happened
[32m[20221214 00:36:06 @agent_ppo2.py:121][0m #------------------------ Iteration 5696 --------------------------#
[32m[20221214 00:36:06 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:36:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:36:06 @agent_ppo2.py:185][0m |           0.0032 |         166.6405 |        -184.1702 |
[32m[20221214 00:36:06 @agent_ppo2.py:185][0m |           0.0003 |         164.7267 |        -185.2754 |
[32m[20221214 00:36:07 @agent_ppo2.py:185][0m |           0.0192 |         181.7819 |        -186.5441 |
[32m[20221214 00:36:07 @agent_ppo2.py:185][0m |           0.0018 |         163.5106 |        -186.1537 |
[32m[20221214 00:36:07 @agent_ppo2.py:185][0m |           0.0008 |         162.1679 |        -188.1122 |
[32m[20221214 00:36:07 @agent_ppo2.py:185][0m |           0.0002 |         162.2234 |        -186.4696 |
[32m[20221214 00:36:07 @agent_ppo2.py:185][0m |           0.0050 |         166.0431 |        -186.9954 |
[32m[20221214 00:36:07 @agent_ppo2.py:185][0m |          -0.0042 |         161.2253 |        -187.4427 |
[32m[20221214 00:36:07 @agent_ppo2.py:185][0m |          -0.0028 |         162.1831 |        -187.3772 |
[32m[20221214 00:36:07 @agent_ppo2.py:185][0m |          -0.0046 |         161.0960 |        -188.8654 |
[32m[20221214 00:36:07 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:36:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 811.59
[32m[20221214 00:36:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.50
[32m[20221214 00:36:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 817.57
[32m[20221214 00:36:07 @agent_ppo2.py:143][0m Total time:      38.62 min
[32m[20221214 00:36:07 @agent_ppo2.py:145][0m 3475456 total steps have happened
[32m[20221214 00:36:07 @agent_ppo2.py:121][0m #------------------------ Iteration 5697 --------------------------#
[32m[20221214 00:36:08 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:36:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:36:08 @agent_ppo2.py:185][0m |           0.0043 |         109.5733 |        -180.7188 |
[32m[20221214 00:36:08 @agent_ppo2.py:185][0m |          -0.0014 |          98.6536 |        -181.1429 |
[32m[20221214 00:36:08 @agent_ppo2.py:185][0m |          -0.0051 |          98.2409 |        -179.9074 |
[32m[20221214 00:36:08 @agent_ppo2.py:185][0m |          -0.0081 |          94.2643 |        -179.8542 |
[32m[20221214 00:36:08 @agent_ppo2.py:185][0m |          -0.0089 |          92.8827 |        -180.8814 |
[32m[20221214 00:36:08 @agent_ppo2.py:185][0m |          -0.0120 |          92.4336 |        -180.9269 |
[32m[20221214 00:36:08 @agent_ppo2.py:185][0m |          -0.0128 |          91.1644 |        -180.7695 |
[32m[20221214 00:36:08 @agent_ppo2.py:185][0m |          -0.0163 |          90.1932 |        -180.6018 |
[32m[20221214 00:36:09 @agent_ppo2.py:185][0m |          -0.0168 |          89.5524 |        -180.8212 |
[32m[20221214 00:36:09 @agent_ppo2.py:185][0m |          -0.0172 |          89.5632 |        -180.3964 |
[32m[20221214 00:36:09 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:36:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 591.98
[32m[20221214 00:36:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 619.41
[32m[20221214 00:36:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 308.44
[32m[20221214 00:36:09 @agent_ppo2.py:143][0m Total time:      38.64 min
[32m[20221214 00:36:09 @agent_ppo2.py:145][0m 3477504 total steps have happened
[32m[20221214 00:36:09 @agent_ppo2.py:121][0m #------------------------ Iteration 5698 --------------------------#
[32m[20221214 00:36:09 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:36:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:36:09 @agent_ppo2.py:185][0m |          -0.0018 |         177.4044 |        -187.7177 |
[32m[20221214 00:36:09 @agent_ppo2.py:185][0m |          -0.0016 |         175.0470 |        -187.0367 |
[32m[20221214 00:36:09 @agent_ppo2.py:185][0m |           0.0114 |         198.9974 |        -187.5888 |
[32m[20221214 00:36:10 @agent_ppo2.py:185][0m |          -0.0038 |         174.3959 |        -187.9282 |
[32m[20221214 00:36:10 @agent_ppo2.py:185][0m |          -0.0016 |         173.9564 |        -186.1842 |
[32m[20221214 00:36:10 @agent_ppo2.py:185][0m |          -0.0037 |         174.9833 |        -187.7714 |
[32m[20221214 00:36:10 @agent_ppo2.py:185][0m |          -0.0044 |         173.3606 |        -186.8462 |
[32m[20221214 00:36:10 @agent_ppo2.py:185][0m |          -0.0026 |         173.4441 |        -186.4913 |
[32m[20221214 00:36:10 @agent_ppo2.py:185][0m |          -0.0033 |         173.1759 |        -186.5547 |
[32m[20221214 00:36:10 @agent_ppo2.py:185][0m |           0.0018 |         174.1526 |        -186.8266 |
[32m[20221214 00:36:10 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221214 00:36:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 760.46
[32m[20221214 00:36:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.34
[32m[20221214 00:36:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 804.47
[32m[20221214 00:36:10 @agent_ppo2.py:143][0m Total time:      38.67 min
[32m[20221214 00:36:10 @agent_ppo2.py:145][0m 3479552 total steps have happened
[32m[20221214 00:36:10 @agent_ppo2.py:121][0m #------------------------ Iteration 5699 --------------------------#
[32m[20221214 00:36:11 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:36:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:36:11 @agent_ppo2.py:185][0m |          -0.0031 |          87.1648 |        -179.4586 |
[32m[20221214 00:36:11 @agent_ppo2.py:185][0m |          -0.0092 |          77.7192 |        -179.4625 |
[32m[20221214 00:36:11 @agent_ppo2.py:185][0m |          -0.0100 |          74.4834 |        -179.6278 |
[32m[20221214 00:36:11 @agent_ppo2.py:185][0m |          -0.0040 |          75.2744 |        -179.9857 |
[32m[20221214 00:36:11 @agent_ppo2.py:185][0m |          -0.0137 |          71.1313 |        -179.8655 |
[32m[20221214 00:36:11 @agent_ppo2.py:185][0m |          -0.0089 |          70.0867 |        -178.9227 |
[32m[20221214 00:36:11 @agent_ppo2.py:185][0m |          -0.0144 |          70.3787 |        -179.7848 |
[32m[20221214 00:36:11 @agent_ppo2.py:185][0m |          -0.0141 |          68.5712 |        -177.6026 |
[32m[20221214 00:36:12 @agent_ppo2.py:185][0m |          -0.0086 |          68.2163 |        -178.7107 |
[32m[20221214 00:36:12 @agent_ppo2.py:185][0m |          -0.0168 |          68.0419 |        -178.5277 |
[32m[20221214 00:36:12 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:36:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 660.92
[32m[20221214 00:36:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 721.48
[32m[20221214 00:36:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 718.47
[32m[20221214 00:36:12 @agent_ppo2.py:143][0m Total time:      38.69 min
[32m[20221214 00:36:12 @agent_ppo2.py:145][0m 3481600 total steps have happened
[32m[20221214 00:36:12 @agent_ppo2.py:121][0m #------------------------ Iteration 5700 --------------------------#
[32m[20221214 00:36:12 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:36:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:36:12 @agent_ppo2.py:185][0m |           0.0005 |         141.6852 |        -185.6848 |
[32m[20221214 00:36:12 @agent_ppo2.py:185][0m |          -0.0054 |         129.4444 |        -185.4103 |
[32m[20221214 00:36:12 @agent_ppo2.py:185][0m |           0.0082 |         135.8632 |        -184.3329 |
[32m[20221214 00:36:13 @agent_ppo2.py:185][0m |          -0.0047 |         122.6202 |        -183.7283 |
[32m[20221214 00:36:13 @agent_ppo2.py:185][0m |          -0.0044 |         122.7726 |        -183.6271 |
[32m[20221214 00:36:13 @agent_ppo2.py:185][0m |          -0.0054 |         120.9247 |        -183.3650 |
[32m[20221214 00:36:13 @agent_ppo2.py:185][0m |          -0.0089 |         120.5968 |        -183.1803 |
[32m[20221214 00:36:13 @agent_ppo2.py:185][0m |          -0.0087 |         119.4957 |        -184.7065 |
[32m[20221214 00:36:13 @agent_ppo2.py:185][0m |          -0.0140 |         117.7836 |        -184.2570 |
[32m[20221214 00:36:13 @agent_ppo2.py:185][0m |          -0.0084 |         119.1484 |        -183.0829 |
[32m[20221214 00:36:13 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:36:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 684.65
[32m[20221214 00:36:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 798.58
[32m[20221214 00:36:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 637.30
[32m[20221214 00:36:13 @agent_ppo2.py:143][0m Total time:      38.72 min
[32m[20221214 00:36:13 @agent_ppo2.py:145][0m 3483648 total steps have happened
[32m[20221214 00:36:13 @agent_ppo2.py:121][0m #------------------------ Iteration 5701 --------------------------#
[32m[20221214 00:36:13 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:36:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:36:14 @agent_ppo2.py:185][0m |           0.0005 |          78.3181 |        -188.3429 |
[32m[20221214 00:36:14 @agent_ppo2.py:185][0m |          -0.0066 |          66.0554 |        -188.2707 |
[32m[20221214 00:36:14 @agent_ppo2.py:185][0m |           0.0010 |          66.0815 |        -189.4157 |
[32m[20221214 00:36:14 @agent_ppo2.py:185][0m |          -0.0011 |          61.8081 |        -188.6044 |
[32m[20221214 00:36:14 @agent_ppo2.py:185][0m |          -0.0145 |          56.1422 |        -189.1546 |
[32m[20221214 00:36:14 @agent_ppo2.py:185][0m |          -0.0238 |          55.9294 |        -189.2675 |
[32m[20221214 00:36:14 @agent_ppo2.py:185][0m |          -0.0172 |          54.3356 |        -188.6912 |
[32m[20221214 00:36:14 @agent_ppo2.py:185][0m |          -0.0206 |          52.6152 |        -189.0882 |
[32m[20221214 00:36:14 @agent_ppo2.py:185][0m |          -0.0183 |          51.7634 |        -189.0438 |
[32m[20221214 00:36:15 @agent_ppo2.py:185][0m |          -0.0174 |          50.8941 |        -188.5482 |
[32m[20221214 00:36:15 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:36:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 529.33
[32m[20221214 00:36:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 708.67
[32m[20221214 00:36:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 670.61
[32m[20221214 00:36:15 @agent_ppo2.py:143][0m Total time:      38.74 min
[32m[20221214 00:36:15 @agent_ppo2.py:145][0m 3485696 total steps have happened
[32m[20221214 00:36:15 @agent_ppo2.py:121][0m #------------------------ Iteration 5702 --------------------------#
[32m[20221214 00:36:15 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:36:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:36:15 @agent_ppo2.py:185][0m |           0.0044 |          81.1642 |        -189.1631 |
[32m[20221214 00:36:15 @agent_ppo2.py:185][0m |          -0.0046 |          62.7124 |        -189.0118 |
[32m[20221214 00:36:15 @agent_ppo2.py:185][0m |          -0.0142 |          58.5032 |        -188.2666 |
[32m[20221214 00:36:15 @agent_ppo2.py:185][0m |          -0.0129 |          56.2235 |        -188.3993 |
[32m[20221214 00:36:15 @agent_ppo2.py:185][0m |          -0.0084 |          55.3305 |        -187.9675 |
[32m[20221214 00:36:16 @agent_ppo2.py:185][0m |          -0.0042 |          57.2735 |        -187.6581 |
[32m[20221214 00:36:16 @agent_ppo2.py:185][0m |          -0.0121 |          53.5424 |        -188.0854 |
[32m[20221214 00:36:16 @agent_ppo2.py:185][0m |          -0.0159 |          52.6789 |        -187.7525 |
[32m[20221214 00:36:16 @agent_ppo2.py:185][0m |          -0.0207 |          51.7320 |        -188.0434 |
[32m[20221214 00:36:16 @agent_ppo2.py:185][0m |          -0.0173 |          51.2492 |        -188.0388 |
[32m[20221214 00:36:16 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:36:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 467.36
[32m[20221214 00:36:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 619.07
[32m[20221214 00:36:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 762.14
[32m[20221214 00:36:16 @agent_ppo2.py:143][0m Total time:      38.76 min
[32m[20221214 00:36:16 @agent_ppo2.py:145][0m 3487744 total steps have happened
[32m[20221214 00:36:16 @agent_ppo2.py:121][0m #------------------------ Iteration 5703 --------------------------#
[32m[20221214 00:36:16 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:36:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:36:16 @agent_ppo2.py:185][0m |           0.0042 |          88.3613 |        -191.9256 |
[32m[20221214 00:36:17 @agent_ppo2.py:185][0m |          -0.0028 |          79.0830 |        -192.4892 |
[32m[20221214 00:36:17 @agent_ppo2.py:185][0m |          -0.0079 |          76.5943 |        -191.3808 |
[32m[20221214 00:36:17 @agent_ppo2.py:185][0m |          -0.0050 |          76.8633 |        -192.0323 |
[32m[20221214 00:36:17 @agent_ppo2.py:185][0m |          -0.0093 |          74.9640 |        -191.5374 |
[32m[20221214 00:36:17 @agent_ppo2.py:185][0m |          -0.0054 |          74.4365 |        -191.9973 |
[32m[20221214 00:36:17 @agent_ppo2.py:185][0m |          -0.0134 |          74.7185 |        -192.3565 |
[32m[20221214 00:36:17 @agent_ppo2.py:185][0m |          -0.0094 |          73.3447 |        -191.8441 |
[32m[20221214 00:36:17 @agent_ppo2.py:185][0m |          -0.0071 |          73.7492 |        -192.2609 |
[32m[20221214 00:36:17 @agent_ppo2.py:185][0m |          -0.0063 |          74.3345 |        -190.9466 |
[32m[20221214 00:36:17 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:36:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 678.27
[32m[20221214 00:36:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 740.29
[32m[20221214 00:36:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 571.57
[32m[20221214 00:36:18 @agent_ppo2.py:143][0m Total time:      38.79 min
[32m[20221214 00:36:18 @agent_ppo2.py:145][0m 3489792 total steps have happened
[32m[20221214 00:36:18 @agent_ppo2.py:121][0m #------------------------ Iteration 5704 --------------------------#
[32m[20221214 00:36:18 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:36:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:36:18 @agent_ppo2.py:185][0m |           0.0010 |         101.2750 |        -186.5971 |
[32m[20221214 00:36:18 @agent_ppo2.py:185][0m |           0.0024 |          97.9898 |        -186.5009 |
[32m[20221214 00:36:18 @agent_ppo2.py:185][0m |          -0.0010 |          96.1235 |        -185.5878 |
[32m[20221214 00:36:18 @agent_ppo2.py:185][0m |          -0.0055 |          95.8232 |        -185.3400 |
[32m[20221214 00:36:18 @agent_ppo2.py:185][0m |          -0.0070 |          95.8539 |        -184.9277 |
[32m[20221214 00:36:18 @agent_ppo2.py:185][0m |          -0.0042 |          95.4134 |        -184.5242 |
[32m[20221214 00:36:19 @agent_ppo2.py:185][0m |          -0.0071 |          93.9673 |        -185.8830 |
[32m[20221214 00:36:19 @agent_ppo2.py:185][0m |          -0.0035 |          94.1754 |        -185.3787 |
[32m[20221214 00:36:19 @agent_ppo2.py:185][0m |          -0.0064 |          94.4415 |        -185.9253 |
[32m[20221214 00:36:19 @agent_ppo2.py:185][0m |          -0.0107 |          93.2258 |        -185.3210 |
[32m[20221214 00:36:19 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:36:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 735.32
[32m[20221214 00:36:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 790.39
[32m[20221214 00:36:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 766.10
[32m[20221214 00:36:19 @agent_ppo2.py:143][0m Total time:      38.81 min
[32m[20221214 00:36:19 @agent_ppo2.py:145][0m 3491840 total steps have happened
[32m[20221214 00:36:19 @agent_ppo2.py:121][0m #------------------------ Iteration 5705 --------------------------#
[32m[20221214 00:36:19 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:36:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:36:19 @agent_ppo2.py:185][0m |           0.0054 |          98.9829 |        -188.0958 |
[32m[20221214 00:36:19 @agent_ppo2.py:185][0m |           0.0008 |          94.8681 |        -186.0917 |
[32m[20221214 00:36:20 @agent_ppo2.py:185][0m |          -0.0033 |          93.8623 |        -185.9097 |
[32m[20221214 00:36:20 @agent_ppo2.py:185][0m |          -0.0028 |          95.7996 |        -187.6793 |
[32m[20221214 00:36:20 @agent_ppo2.py:185][0m |          -0.0037 |          92.5457 |        -187.1690 |
[32m[20221214 00:36:20 @agent_ppo2.py:185][0m |          -0.0101 |          93.6007 |        -186.6369 |
[32m[20221214 00:36:20 @agent_ppo2.py:185][0m |          -0.0011 |          93.3056 |        -185.7244 |
[32m[20221214 00:36:20 @agent_ppo2.py:185][0m |          -0.0068 |          92.7659 |        -186.5040 |
[32m[20221214 00:36:20 @agent_ppo2.py:185][0m |          -0.0075 |          92.4005 |        -186.0439 |
[32m[20221214 00:36:20 @agent_ppo2.py:185][0m |          -0.0059 |          92.0192 |        -186.5065 |
[32m[20221214 00:36:20 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:36:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 687.32
[32m[20221214 00:36:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 835.31
[32m[20221214 00:36:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 377.60
[32m[20221214 00:36:20 @agent_ppo2.py:143][0m Total time:      38.83 min
[32m[20221214 00:36:20 @agent_ppo2.py:145][0m 3493888 total steps have happened
[32m[20221214 00:36:20 @agent_ppo2.py:121][0m #------------------------ Iteration 5706 --------------------------#
[32m[20221214 00:36:21 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:36:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:36:21 @agent_ppo2.py:185][0m |           0.0003 |         142.7685 |        -192.1609 |
[32m[20221214 00:36:21 @agent_ppo2.py:185][0m |          -0.0038 |         138.4195 |        -191.3457 |
[32m[20221214 00:36:21 @agent_ppo2.py:185][0m |           0.0111 |         155.4864 |        -191.6565 |
[32m[20221214 00:36:21 @agent_ppo2.py:185][0m |           0.0056 |         139.1231 |        -190.2257 |
[32m[20221214 00:36:21 @agent_ppo2.py:185][0m |          -0.0072 |         134.8213 |        -190.0587 |
[32m[20221214 00:36:21 @agent_ppo2.py:185][0m |          -0.0055 |         133.3695 |        -190.8024 |
[32m[20221214 00:36:21 @agent_ppo2.py:185][0m |          -0.0051 |         132.9629 |        -189.4541 |
[32m[20221214 00:36:22 @agent_ppo2.py:185][0m |           0.0038 |         137.9595 |        -190.8829 |
[32m[20221214 00:36:22 @agent_ppo2.py:185][0m |          -0.0087 |         134.0490 |        -189.6176 |
[32m[20221214 00:36:22 @agent_ppo2.py:185][0m |          -0.0084 |         132.8595 |        -189.3715 |
[32m[20221214 00:36:22 @agent_ppo2.py:130][0m Policy update time: 1.19 s
[32m[20221214 00:36:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 675.21
[32m[20221214 00:36:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 784.17
[32m[20221214 00:36:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 652.58
[32m[20221214 00:36:22 @agent_ppo2.py:143][0m Total time:      38.86 min
[32m[20221214 00:36:22 @agent_ppo2.py:145][0m 3495936 total steps have happened
[32m[20221214 00:36:22 @agent_ppo2.py:121][0m #------------------------ Iteration 5707 --------------------------#
[32m[20221214 00:36:22 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:36:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:36:22 @agent_ppo2.py:185][0m |           0.0105 |         125.7699 |        -189.3264 |
[32m[20221214 00:36:22 @agent_ppo2.py:185][0m |          -0.0024 |         115.2809 |        -190.3072 |
[32m[20221214 00:36:23 @agent_ppo2.py:185][0m |          -0.0075 |         110.7308 |        -189.8537 |
[32m[20221214 00:36:23 @agent_ppo2.py:185][0m |          -0.0093 |         109.2934 |        -190.8259 |
[32m[20221214 00:36:23 @agent_ppo2.py:185][0m |          -0.0029 |         108.8450 |        -190.6419 |
[32m[20221214 00:36:23 @agent_ppo2.py:185][0m |          -0.0092 |         106.4890 |        -190.7451 |
[32m[20221214 00:36:23 @agent_ppo2.py:185][0m |          -0.0061 |         106.3278 |        -191.0102 |
[32m[20221214 00:36:23 @agent_ppo2.py:185][0m |          -0.0125 |         104.5823 |        -191.6223 |
[32m[20221214 00:36:23 @agent_ppo2.py:185][0m |           0.0020 |         107.6728 |        -191.3074 |
[32m[20221214 00:36:23 @agent_ppo2.py:185][0m |          -0.0016 |         109.6338 |        -191.7993 |
[32m[20221214 00:36:23 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221214 00:36:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 607.18
[32m[20221214 00:36:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 748.69
[32m[20221214 00:36:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 805.63
[32m[20221214 00:36:23 @agent_ppo2.py:143][0m Total time:      38.89 min
[32m[20221214 00:36:23 @agent_ppo2.py:145][0m 3497984 total steps have happened
[32m[20221214 00:36:23 @agent_ppo2.py:121][0m #------------------------ Iteration 5708 --------------------------#
[32m[20221214 00:36:24 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:36:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:36:24 @agent_ppo2.py:185][0m |           0.0105 |         126.2357 |        -180.2976 |
[32m[20221214 00:36:24 @agent_ppo2.py:185][0m |           0.0039 |         118.6445 |        -179.1970 |
[32m[20221214 00:36:24 @agent_ppo2.py:185][0m |          -0.0036 |         111.6501 |        -179.2691 |
[32m[20221214 00:36:24 @agent_ppo2.py:185][0m |          -0.0069 |         109.7884 |        -179.4055 |
[32m[20221214 00:36:24 @agent_ppo2.py:185][0m |          -0.0046 |         108.4733 |        -178.9185 |
[32m[20221214 00:36:24 @agent_ppo2.py:185][0m |          -0.0121 |         107.1968 |        -179.4130 |
[32m[20221214 00:36:24 @agent_ppo2.py:185][0m |          -0.0121 |         106.1307 |        -177.8739 |
[32m[20221214 00:36:25 @agent_ppo2.py:185][0m |          -0.0092 |         105.7984 |        -178.7187 |
[32m[20221214 00:36:25 @agent_ppo2.py:185][0m |          -0.0100 |         105.3112 |        -178.6877 |
[32m[20221214 00:36:25 @agent_ppo2.py:185][0m |          -0.0141 |         104.0988 |        -179.4898 |
[32m[20221214 00:36:25 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:36:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 556.94
[32m[20221214 00:36:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 633.74
[32m[20221214 00:36:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 613.51
[32m[20221214 00:36:25 @agent_ppo2.py:143][0m Total time:      38.91 min
[32m[20221214 00:36:25 @agent_ppo2.py:145][0m 3500032 total steps have happened
[32m[20221214 00:36:25 @agent_ppo2.py:121][0m #------------------------ Iteration 5709 --------------------------#
[32m[20221214 00:36:25 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:36:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:36:25 @agent_ppo2.py:185][0m |           0.0038 |         130.9269 |        -180.4488 |
[32m[20221214 00:36:25 @agent_ppo2.py:185][0m |           0.0003 |         128.4411 |        -181.1734 |
[32m[20221214 00:36:25 @agent_ppo2.py:185][0m |           0.0016 |         128.3710 |        -180.2476 |
[32m[20221214 00:36:26 @agent_ppo2.py:185][0m |           0.0066 |         131.4733 |        -181.0070 |
[32m[20221214 00:36:26 @agent_ppo2.py:185][0m |          -0.0014 |         124.7949 |        -180.4843 |
[32m[20221214 00:36:26 @agent_ppo2.py:185][0m |          -0.0078 |         124.4186 |        -181.9034 |
[32m[20221214 00:36:26 @agent_ppo2.py:185][0m |          -0.0018 |         123.8329 |        -180.0895 |
[32m[20221214 00:36:26 @agent_ppo2.py:185][0m |          -0.0068 |         123.6684 |        -179.9406 |
[32m[20221214 00:36:26 @agent_ppo2.py:185][0m |          -0.0059 |         123.0324 |        -180.1509 |
[32m[20221214 00:36:26 @agent_ppo2.py:185][0m |          -0.0092 |         123.0924 |        -180.4645 |
[32m[20221214 00:36:26 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 00:36:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 682.94
[32m[20221214 00:36:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 750.57
[32m[20221214 00:36:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 338.91
[32m[20221214 00:36:26 @agent_ppo2.py:143][0m Total time:      38.93 min
[32m[20221214 00:36:26 @agent_ppo2.py:145][0m 3502080 total steps have happened
[32m[20221214 00:36:26 @agent_ppo2.py:121][0m #------------------------ Iteration 5710 --------------------------#
[32m[20221214 00:36:27 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:36:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:36:27 @agent_ppo2.py:185][0m |          -0.0005 |         147.2059 |        -183.7270 |
[32m[20221214 00:36:27 @agent_ppo2.py:185][0m |           0.0001 |         139.9873 |        -183.6193 |
[32m[20221214 00:36:27 @agent_ppo2.py:185][0m |          -0.0020 |         137.3373 |        -182.7653 |
[32m[20221214 00:36:27 @agent_ppo2.py:185][0m |          -0.0089 |         136.3391 |        -182.8192 |
[32m[20221214 00:36:27 @agent_ppo2.py:185][0m |          -0.0086 |         135.1747 |        -183.6519 |
[32m[20221214 00:36:27 @agent_ppo2.py:185][0m |          -0.0088 |         134.1408 |        -182.3450 |
[32m[20221214 00:36:27 @agent_ppo2.py:185][0m |           0.0033 |         141.5384 |        -182.6121 |
[32m[20221214 00:36:27 @agent_ppo2.py:185][0m |          -0.0070 |         133.7049 |        -182.4211 |
[32m[20221214 00:36:28 @agent_ppo2.py:185][0m |          -0.0104 |         132.5141 |        -183.1338 |
[32m[20221214 00:36:28 @agent_ppo2.py:185][0m |          -0.0102 |         132.3184 |        -183.4892 |
[32m[20221214 00:36:28 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:36:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 562.10
[32m[20221214 00:36:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 680.79
[32m[20221214 00:36:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 362.41
[32m[20221214 00:36:28 @agent_ppo2.py:143][0m Total time:      38.96 min
[32m[20221214 00:36:28 @agent_ppo2.py:145][0m 3504128 total steps have happened
[32m[20221214 00:36:28 @agent_ppo2.py:121][0m #------------------------ Iteration 5711 --------------------------#
[32m[20221214 00:36:28 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:36:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:36:28 @agent_ppo2.py:185][0m |           0.0013 |          99.3965 |        -192.4960 |
[32m[20221214 00:36:28 @agent_ppo2.py:185][0m |          -0.0040 |          87.1777 |        -192.4842 |
[32m[20221214 00:36:28 @agent_ppo2.py:185][0m |          -0.0064 |          83.7193 |        -191.9336 |
[32m[20221214 00:36:28 @agent_ppo2.py:185][0m |          -0.0042 |          82.2874 |        -191.9998 |
[32m[20221214 00:36:29 @agent_ppo2.py:185][0m |          -0.0114 |          79.8803 |        -192.4406 |
[32m[20221214 00:36:29 @agent_ppo2.py:185][0m |          -0.0151 |          78.6112 |        -192.3602 |
[32m[20221214 00:36:29 @agent_ppo2.py:185][0m |          -0.0121 |          77.4324 |        -192.9642 |
[32m[20221214 00:36:29 @agent_ppo2.py:185][0m |          -0.0183 |          76.4831 |        -192.6525 |
[32m[20221214 00:36:29 @agent_ppo2.py:185][0m |          -0.0142 |          75.8021 |        -192.4378 |
[32m[20221214 00:36:29 @agent_ppo2.py:185][0m |          -0.0127 |          75.4071 |        -192.2205 |
[32m[20221214 00:36:29 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 00:36:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 549.10
[32m[20221214 00:36:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 671.72
[32m[20221214 00:36:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 750.77
[32m[20221214 00:36:29 @agent_ppo2.py:143][0m Total time:      38.98 min
[32m[20221214 00:36:29 @agent_ppo2.py:145][0m 3506176 total steps have happened
[32m[20221214 00:36:29 @agent_ppo2.py:121][0m #------------------------ Iteration 5712 --------------------------#
[32m[20221214 00:36:29 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:36:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:36:30 @agent_ppo2.py:185][0m |           0.0091 |         100.3726 |        -187.7862 |
[32m[20221214 00:36:30 @agent_ppo2.py:185][0m |          -0.0028 |          89.7924 |        -187.4628 |
[32m[20221214 00:36:30 @agent_ppo2.py:185][0m |          -0.0025 |          87.2638 |        -187.2301 |
[32m[20221214 00:36:30 @agent_ppo2.py:185][0m |          -0.0047 |          84.9593 |        -187.2804 |
[32m[20221214 00:36:30 @agent_ppo2.py:185][0m |          -0.0085 |          83.8929 |        -188.6827 |
[32m[20221214 00:36:30 @agent_ppo2.py:185][0m |          -0.0085 |          83.0023 |        -187.7409 |
[32m[20221214 00:36:30 @agent_ppo2.py:185][0m |          -0.0072 |          82.3172 |        -188.3477 |
[32m[20221214 00:36:30 @agent_ppo2.py:185][0m |          -0.0113 |          81.1536 |        -188.4871 |
[32m[20221214 00:36:30 @agent_ppo2.py:185][0m |          -0.0130 |          80.0658 |        -188.7177 |
[32m[20221214 00:36:31 @agent_ppo2.py:185][0m |          -0.0125 |          79.7072 |        -189.3135 |
[32m[20221214 00:36:31 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:36:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 545.53
[32m[20221214 00:36:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 699.23
[32m[20221214 00:36:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 812.04
[32m[20221214 00:36:31 @agent_ppo2.py:143][0m Total time:      39.01 min
[32m[20221214 00:36:31 @agent_ppo2.py:145][0m 3508224 total steps have happened
[32m[20221214 00:36:31 @agent_ppo2.py:121][0m #------------------------ Iteration 5713 --------------------------#
[32m[20221214 00:36:31 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:36:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:36:31 @agent_ppo2.py:185][0m |           0.0005 |         112.9452 |        -193.4174 |
[32m[20221214 00:36:31 @agent_ppo2.py:185][0m |           0.0002 |         107.0460 |        -191.7033 |
[32m[20221214 00:36:31 @agent_ppo2.py:185][0m |           0.0020 |         106.4201 |        -192.0113 |
[32m[20221214 00:36:31 @agent_ppo2.py:185][0m |           0.0118 |         116.4294 |        -191.2540 |
[32m[20221214 00:36:31 @agent_ppo2.py:185][0m |          -0.0060 |         103.9229 |        -191.3670 |
[32m[20221214 00:36:32 @agent_ppo2.py:185][0m |          -0.0051 |         102.7395 |        -191.8992 |
[32m[20221214 00:36:32 @agent_ppo2.py:185][0m |          -0.0055 |         102.8260 |        -191.4196 |
[32m[20221214 00:36:32 @agent_ppo2.py:185][0m |          -0.0068 |         102.0931 |        -192.0308 |
[32m[20221214 00:36:32 @agent_ppo2.py:185][0m |          -0.0072 |         101.5955 |        -191.0387 |
[32m[20221214 00:36:32 @agent_ppo2.py:185][0m |           0.0154 |         117.8386 |        -190.7883 |
[32m[20221214 00:36:32 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:36:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 532.49
[32m[20221214 00:36:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 571.15
[32m[20221214 00:36:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 785.12
[32m[20221214 00:36:32 @agent_ppo2.py:143][0m Total time:      39.03 min
[32m[20221214 00:36:32 @agent_ppo2.py:145][0m 3510272 total steps have happened
[32m[20221214 00:36:32 @agent_ppo2.py:121][0m #------------------------ Iteration 5714 --------------------------#
[32m[20221214 00:36:32 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:36:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:36:33 @agent_ppo2.py:185][0m |          -0.0047 |         129.9012 |        -187.5499 |
[32m[20221214 00:36:33 @agent_ppo2.py:185][0m |          -0.0051 |         112.0661 |        -187.7517 |
[32m[20221214 00:36:33 @agent_ppo2.py:185][0m |          -0.0131 |         106.2734 |        -187.1868 |
[32m[20221214 00:36:33 @agent_ppo2.py:185][0m |          -0.0166 |         100.8308 |        -186.6944 |
[32m[20221214 00:36:33 @agent_ppo2.py:185][0m |          -0.0160 |          98.2413 |        -186.8458 |
[32m[20221214 00:36:33 @agent_ppo2.py:185][0m |          -0.0134 |          96.7686 |        -187.1294 |
[32m[20221214 00:36:33 @agent_ppo2.py:185][0m |          -0.0199 |          95.4209 |        -186.0691 |
[32m[20221214 00:36:33 @agent_ppo2.py:185][0m |          -0.0199 |          93.7463 |        -185.9380 |
[32m[20221214 00:36:33 @agent_ppo2.py:185][0m |          -0.0173 |          92.8915 |        -184.8650 |
[32m[20221214 00:36:33 @agent_ppo2.py:185][0m |          -0.0163 |          92.7096 |        -184.6410 |
[32m[20221214 00:36:33 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:36:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 299.17
[32m[20221214 00:36:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 497.28
[32m[20221214 00:36:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 755.81
[32m[20221214 00:36:34 @agent_ppo2.py:143][0m Total time:      39.05 min
[32m[20221214 00:36:34 @agent_ppo2.py:145][0m 3512320 total steps have happened
[32m[20221214 00:36:34 @agent_ppo2.py:121][0m #------------------------ Iteration 5715 --------------------------#
[32m[20221214 00:36:34 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:36:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:36:34 @agent_ppo2.py:185][0m |           0.0088 |         176.8401 |        -183.3040 |
[32m[20221214 00:36:34 @agent_ppo2.py:185][0m |          -0.0015 |         153.8480 |        -183.1149 |
[32m[20221214 00:36:34 @agent_ppo2.py:185][0m |          -0.0052 |         149.0907 |        -182.2999 |
[32m[20221214 00:36:34 @agent_ppo2.py:185][0m |           0.0062 |         155.8355 |        -182.8770 |
[32m[20221214 00:36:34 @agent_ppo2.py:185][0m |          -0.0082 |         143.8527 |        -182.7757 |
[32m[20221214 00:36:34 @agent_ppo2.py:185][0m |           0.0042 |         160.3776 |        -182.2312 |
[32m[20221214 00:36:35 @agent_ppo2.py:185][0m |          -0.0093 |         141.8200 |        -182.3605 |
[32m[20221214 00:36:35 @agent_ppo2.py:185][0m |          -0.0124 |         139.2913 |        -182.3216 |
[32m[20221214 00:36:35 @agent_ppo2.py:185][0m |          -0.0124 |         138.3491 |        -182.0259 |
[32m[20221214 00:36:35 @agent_ppo2.py:185][0m |          -0.0188 |         137.5060 |        -182.2521 |
[32m[20221214 00:36:35 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:36:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 592.79
[32m[20221214 00:36:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 827.26
[32m[20221214 00:36:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 743.24
[32m[20221214 00:36:35 @agent_ppo2.py:143][0m Total time:      39.08 min
[32m[20221214 00:36:35 @agent_ppo2.py:145][0m 3514368 total steps have happened
[32m[20221214 00:36:35 @agent_ppo2.py:121][0m #------------------------ Iteration 5716 --------------------------#
[32m[20221214 00:36:35 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:36:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:36:35 @agent_ppo2.py:185][0m |           0.0090 |         115.7416 |        -194.8438 |
[32m[20221214 00:36:35 @agent_ppo2.py:185][0m |          -0.0003 |         100.0597 |        -195.0596 |
[32m[20221214 00:36:36 @agent_ppo2.py:185][0m |           0.0002 |          98.8868 |        -193.6204 |
[32m[20221214 00:36:36 @agent_ppo2.py:185][0m |          -0.0050 |          97.3971 |        -194.2070 |
[32m[20221214 00:36:36 @agent_ppo2.py:185][0m |          -0.0105 |          95.6744 |        -194.0935 |
[32m[20221214 00:36:36 @agent_ppo2.py:185][0m |          -0.0016 |          99.5806 |        -194.6614 |
[32m[20221214 00:36:36 @agent_ppo2.py:185][0m |          -0.0103 |          94.8338 |        -194.6876 |
[32m[20221214 00:36:36 @agent_ppo2.py:185][0m |          -0.0132 |          93.5614 |        -194.3332 |
[32m[20221214 00:36:36 @agent_ppo2.py:185][0m |          -0.0093 |          92.9231 |        -195.1035 |
[32m[20221214 00:36:36 @agent_ppo2.py:185][0m |          -0.0075 |          94.2365 |        -194.8498 |
[32m[20221214 00:36:36 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:36:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 662.17
[32m[20221214 00:36:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 737.64
[32m[20221214 00:36:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 793.40
[32m[20221214 00:36:36 @agent_ppo2.py:143][0m Total time:      39.10 min
[32m[20221214 00:36:36 @agent_ppo2.py:145][0m 3516416 total steps have happened
[32m[20221214 00:36:36 @agent_ppo2.py:121][0m #------------------------ Iteration 5717 --------------------------#
[32m[20221214 00:36:37 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:36:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:36:37 @agent_ppo2.py:185][0m |           0.0060 |         116.8869 |        -188.7016 |
[32m[20221214 00:36:37 @agent_ppo2.py:185][0m |          -0.0037 |         109.3833 |        -188.1875 |
[32m[20221214 00:36:37 @agent_ppo2.py:185][0m |           0.0004 |         107.1764 |        -187.0342 |
[32m[20221214 00:36:37 @agent_ppo2.py:185][0m |          -0.0058 |         105.1734 |        -189.0946 |
[32m[20221214 00:36:37 @agent_ppo2.py:185][0m |          -0.0094 |         104.2084 |        -188.5521 |
[32m[20221214 00:36:37 @agent_ppo2.py:185][0m |          -0.0050 |         104.0760 |        -189.0415 |
[32m[20221214 00:36:37 @agent_ppo2.py:185][0m |          -0.0069 |         102.9969 |        -188.6539 |
[32m[20221214 00:36:38 @agent_ppo2.py:185][0m |          -0.0019 |         107.1420 |        -189.0432 |
[32m[20221214 00:36:38 @agent_ppo2.py:185][0m |          -0.0107 |         102.2965 |        -188.4400 |
[32m[20221214 00:36:38 @agent_ppo2.py:185][0m |          -0.0070 |         101.6065 |        -189.5713 |
[32m[20221214 00:36:38 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:36:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 612.31
[32m[20221214 00:36:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 723.85
[32m[20221214 00:36:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 764.19
[32m[20221214 00:36:38 @agent_ppo2.py:143][0m Total time:      39.13 min
[32m[20221214 00:36:38 @agent_ppo2.py:145][0m 3518464 total steps have happened
[32m[20221214 00:36:38 @agent_ppo2.py:121][0m #------------------------ Iteration 5718 --------------------------#
[32m[20221214 00:36:38 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:36:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:36:38 @agent_ppo2.py:185][0m |           0.0071 |         112.0033 |        -185.0640 |
[32m[20221214 00:36:38 @agent_ppo2.py:185][0m |          -0.0015 |         101.8040 |        -184.8383 |
[32m[20221214 00:36:38 @agent_ppo2.py:185][0m |           0.0049 |         108.5860 |        -184.7875 |
[32m[20221214 00:36:39 @agent_ppo2.py:185][0m |          -0.0070 |          98.5417 |        -184.3902 |
[32m[20221214 00:36:39 @agent_ppo2.py:185][0m |          -0.0117 |          97.3350 |        -183.6696 |
[32m[20221214 00:36:39 @agent_ppo2.py:185][0m |          -0.0047 |          96.1274 |        -184.4356 |
[32m[20221214 00:36:39 @agent_ppo2.py:185][0m |          -0.0103 |          95.6187 |        -183.5683 |
[32m[20221214 00:36:39 @agent_ppo2.py:185][0m |          -0.0113 |          96.3181 |        -184.0051 |
[32m[20221214 00:36:39 @agent_ppo2.py:185][0m |          -0.0084 |          94.8161 |        -184.3288 |
[32m[20221214 00:36:39 @agent_ppo2.py:185][0m |          -0.0116 |          95.1033 |        -184.0448 |
[32m[20221214 00:36:39 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:36:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 582.58
[32m[20221214 00:36:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 691.39
[32m[20221214 00:36:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 794.22
[32m[20221214 00:36:39 @agent_ppo2.py:143][0m Total time:      39.15 min
[32m[20221214 00:36:39 @agent_ppo2.py:145][0m 3520512 total steps have happened
[32m[20221214 00:36:39 @agent_ppo2.py:121][0m #------------------------ Iteration 5719 --------------------------#
[32m[20221214 00:36:39 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:36:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:36:40 @agent_ppo2.py:185][0m |          -0.0009 |         122.3745 |        -198.6073 |
[32m[20221214 00:36:40 @agent_ppo2.py:185][0m |           0.0005 |         113.5367 |        -199.0752 |
[32m[20221214 00:36:40 @agent_ppo2.py:185][0m |          -0.0069 |         109.5378 |        -198.4982 |
[32m[20221214 00:36:40 @agent_ppo2.py:185][0m |          -0.0088 |         107.5589 |        -197.9217 |
[32m[20221214 00:36:40 @agent_ppo2.py:185][0m |          -0.0082 |         106.1809 |        -197.6203 |
[32m[20221214 00:36:40 @agent_ppo2.py:185][0m |          -0.0109 |         105.1450 |        -196.5701 |
[32m[20221214 00:36:40 @agent_ppo2.py:185][0m |          -0.0080 |         104.2933 |        -197.7542 |
[32m[20221214 00:36:40 @agent_ppo2.py:185][0m |          -0.0097 |         103.2050 |        -196.6776 |
[32m[20221214 00:36:40 @agent_ppo2.py:185][0m |          -0.0083 |         103.4670 |        -196.5763 |
[32m[20221214 00:36:41 @agent_ppo2.py:185][0m |          -0.0095 |         101.9357 |        -197.4177 |
[32m[20221214 00:36:41 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:36:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 722.28
[32m[20221214 00:36:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 744.56
[32m[20221214 00:36:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 739.47
[32m[20221214 00:36:41 @agent_ppo2.py:143][0m Total time:      39.17 min
[32m[20221214 00:36:41 @agent_ppo2.py:145][0m 3522560 total steps have happened
[32m[20221214 00:36:41 @agent_ppo2.py:121][0m #------------------------ Iteration 5720 --------------------------#
[32m[20221214 00:36:41 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:36:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:36:41 @agent_ppo2.py:185][0m |           0.0011 |         118.6014 |        -190.1854 |
[32m[20221214 00:36:41 @agent_ppo2.py:185][0m |          -0.0041 |         113.0510 |        -189.0738 |
[32m[20221214 00:36:41 @agent_ppo2.py:185][0m |          -0.0076 |         110.2041 |        -189.3883 |
[32m[20221214 00:36:41 @agent_ppo2.py:185][0m |          -0.0110 |         108.7194 |        -189.9715 |
[32m[20221214 00:36:42 @agent_ppo2.py:185][0m |          -0.0075 |         107.9710 |        -188.5743 |
[32m[20221214 00:36:42 @agent_ppo2.py:185][0m |          -0.0113 |         106.8501 |        -188.4911 |
[32m[20221214 00:36:42 @agent_ppo2.py:185][0m |          -0.0039 |         105.9885 |        -188.5384 |
[32m[20221214 00:36:42 @agent_ppo2.py:185][0m |          -0.0143 |         104.1717 |        -188.0324 |
[32m[20221214 00:36:42 @agent_ppo2.py:185][0m |          -0.0119 |         104.2195 |        -187.9006 |
[32m[20221214 00:36:42 @agent_ppo2.py:185][0m |          -0.0136 |         104.0027 |        -188.2213 |
[32m[20221214 00:36:42 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:36:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 594.98
[32m[20221214 00:36:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 681.55
[32m[20221214 00:36:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 637.61
[32m[20221214 00:36:42 @agent_ppo2.py:143][0m Total time:      39.20 min
[32m[20221214 00:36:42 @agent_ppo2.py:145][0m 3524608 total steps have happened
[32m[20221214 00:36:42 @agent_ppo2.py:121][0m #------------------------ Iteration 5721 --------------------------#
[32m[20221214 00:36:42 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:36:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:36:43 @agent_ppo2.py:185][0m |           0.0014 |         103.9582 |        -190.3406 |
[32m[20221214 00:36:43 @agent_ppo2.py:185][0m |          -0.0056 |          95.0791 |        -188.7052 |
[32m[20221214 00:36:43 @agent_ppo2.py:185][0m |          -0.0089 |          92.1456 |        -188.8627 |
[32m[20221214 00:36:43 @agent_ppo2.py:185][0m |          -0.0086 |          90.7225 |        -187.9164 |
[32m[20221214 00:36:43 @agent_ppo2.py:185][0m |          -0.0129 |          89.4279 |        -188.4950 |
[32m[20221214 00:36:43 @agent_ppo2.py:185][0m |          -0.0082 |          89.1928 |        -187.0941 |
[32m[20221214 00:36:43 @agent_ppo2.py:185][0m |          -0.0154 |          88.0622 |        -186.1653 |
[32m[20221214 00:36:43 @agent_ppo2.py:185][0m |          -0.0075 |          87.5489 |        -186.7064 |
[32m[20221214 00:36:43 @agent_ppo2.py:185][0m |          -0.0128 |          87.1682 |        -185.8856 |
[32m[20221214 00:36:43 @agent_ppo2.py:185][0m |          -0.0157 |          86.6620 |        -185.3375 |
[32m[20221214 00:36:43 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:36:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 493.94
[32m[20221214 00:36:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 714.74
[32m[20221214 00:36:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 656.26
[32m[20221214 00:36:44 @agent_ppo2.py:143][0m Total time:      39.22 min
[32m[20221214 00:36:44 @agent_ppo2.py:145][0m 3526656 total steps have happened
[32m[20221214 00:36:44 @agent_ppo2.py:121][0m #------------------------ Iteration 5722 --------------------------#
[32m[20221214 00:36:44 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:36:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:36:44 @agent_ppo2.py:185][0m |           0.0126 |         128.3155 |        -180.3361 |
[32m[20221214 00:36:44 @agent_ppo2.py:185][0m |          -0.0039 |         118.9757 |        -179.9613 |
[32m[20221214 00:36:44 @agent_ppo2.py:185][0m |          -0.0101 |         116.6419 |        -179.4245 |
[32m[20221214 00:36:44 @agent_ppo2.py:185][0m |          -0.0115 |         115.9063 |        -179.5602 |
[32m[20221214 00:36:44 @agent_ppo2.py:185][0m |          -0.0116 |         115.1881 |        -178.4171 |
[32m[20221214 00:36:44 @agent_ppo2.py:185][0m |          -0.0036 |         117.1387 |        -179.1130 |
[32m[20221214 00:36:45 @agent_ppo2.py:185][0m |          -0.0099 |         114.5083 |        -178.7255 |
[32m[20221214 00:36:45 @agent_ppo2.py:185][0m |          -0.0143 |         113.7402 |        -178.7112 |
[32m[20221214 00:36:45 @agent_ppo2.py:185][0m |          -0.0130 |         113.3982 |        -178.6987 |
[32m[20221214 00:36:45 @agent_ppo2.py:185][0m |          -0.0105 |         113.7525 |        -177.8322 |
[32m[20221214 00:36:45 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:36:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 580.60
[32m[20221214 00:36:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 697.89
[32m[20221214 00:36:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 775.92
[32m[20221214 00:36:45 @agent_ppo2.py:143][0m Total time:      39.24 min
[32m[20221214 00:36:45 @agent_ppo2.py:145][0m 3528704 total steps have happened
[32m[20221214 00:36:45 @agent_ppo2.py:121][0m #------------------------ Iteration 5723 --------------------------#
[32m[20221214 00:36:45 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:36:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:36:45 @agent_ppo2.py:185][0m |           0.0132 |         122.0544 |        -179.0879 |
[32m[20221214 00:36:45 @agent_ppo2.py:185][0m |          -0.0065 |         113.1245 |        -178.4684 |
[32m[20221214 00:36:46 @agent_ppo2.py:185][0m |          -0.0076 |         110.1196 |        -179.0229 |
[32m[20221214 00:36:46 @agent_ppo2.py:185][0m |          -0.0131 |         108.4908 |        -178.9024 |
[32m[20221214 00:36:46 @agent_ppo2.py:185][0m |          -0.0024 |         107.6723 |        -178.8752 |
[32m[20221214 00:36:46 @agent_ppo2.py:185][0m |          -0.0137 |         105.6664 |        -179.5758 |
[32m[20221214 00:36:46 @agent_ppo2.py:185][0m |          -0.0136 |         104.8747 |        -179.6173 |
[32m[20221214 00:36:46 @agent_ppo2.py:185][0m |          -0.0179 |         103.6291 |        -180.7634 |
[32m[20221214 00:36:46 @agent_ppo2.py:185][0m |          -0.0141 |         103.2998 |        -180.9605 |
[32m[20221214 00:36:46 @agent_ppo2.py:185][0m |          -0.0202 |         102.0158 |        -180.6306 |
[32m[20221214 00:36:46 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 00:36:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 595.05
[32m[20221214 00:36:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 719.81
[32m[20221214 00:36:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 803.87
[32m[20221214 00:36:47 @agent_ppo2.py:143][0m Total time:      39.27 min
[32m[20221214 00:36:47 @agent_ppo2.py:145][0m 3530752 total steps have happened
[32m[20221214 00:36:47 @agent_ppo2.py:121][0m #------------------------ Iteration 5724 --------------------------#
[32m[20221214 00:36:47 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:36:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:36:47 @agent_ppo2.py:185][0m |           0.0003 |         144.6981 |        -180.8913 |
[32m[20221214 00:36:47 @agent_ppo2.py:185][0m |          -0.0017 |         139.4403 |        -181.6984 |
[32m[20221214 00:36:47 @agent_ppo2.py:185][0m |          -0.0041 |         136.5460 |        -181.4067 |
[32m[20221214 00:36:47 @agent_ppo2.py:185][0m |          -0.0059 |         135.3688 |        -181.9339 |
[32m[20221214 00:36:47 @agent_ppo2.py:185][0m |          -0.0024 |         134.3700 |        -181.1029 |
[32m[20221214 00:36:47 @agent_ppo2.py:185][0m |          -0.0064 |         134.9282 |        -181.7239 |
[32m[20221214 00:36:47 @agent_ppo2.py:185][0m |          -0.0108 |         132.6929 |        -182.2404 |
[32m[20221214 00:36:48 @agent_ppo2.py:185][0m |           0.0084 |         141.7153 |        -182.6128 |
[32m[20221214 00:36:48 @agent_ppo2.py:185][0m |          -0.0121 |         133.1493 |        -181.5555 |
[32m[20221214 00:36:48 @agent_ppo2.py:185][0m |          -0.0092 |         130.7220 |        -182.1070 |
[32m[20221214 00:36:48 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:36:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 614.48
[32m[20221214 00:36:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 728.62
[32m[20221214 00:36:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 756.30
[32m[20221214 00:36:48 @agent_ppo2.py:143][0m Total time:      39.29 min
[32m[20221214 00:36:48 @agent_ppo2.py:145][0m 3532800 total steps have happened
[32m[20221214 00:36:48 @agent_ppo2.py:121][0m #------------------------ Iteration 5725 --------------------------#
[32m[20221214 00:36:48 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:36:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:36:48 @agent_ppo2.py:185][0m |           0.0011 |         123.4741 |        -201.6456 |
[32m[20221214 00:36:48 @agent_ppo2.py:185][0m |          -0.0007 |         116.0131 |        -200.9272 |
[32m[20221214 00:36:48 @agent_ppo2.py:185][0m |          -0.0047 |         113.7926 |        -199.5915 |
[32m[20221214 00:36:49 @agent_ppo2.py:185][0m |          -0.0073 |         112.4399 |        -200.2482 |
[32m[20221214 00:36:49 @agent_ppo2.py:185][0m |          -0.0074 |         111.9515 |        -199.2523 |
[32m[20221214 00:36:49 @agent_ppo2.py:185][0m |          -0.0095 |         111.2909 |        -199.3952 |
[32m[20221214 00:36:49 @agent_ppo2.py:185][0m |          -0.0091 |         111.3559 |        -198.7065 |
[32m[20221214 00:36:49 @agent_ppo2.py:185][0m |          -0.0086 |         110.0675 |        -198.0179 |
[32m[20221214 00:36:49 @agent_ppo2.py:185][0m |          -0.0128 |         109.8322 |        -198.6864 |
[32m[20221214 00:36:49 @agent_ppo2.py:185][0m |          -0.0103 |         109.0803 |        -198.9438 |
[32m[20221214 00:36:49 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:36:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 563.48
[32m[20221214 00:36:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 773.00
[32m[20221214 00:36:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 751.79
[32m[20221214 00:36:49 @agent_ppo2.py:143][0m Total time:      39.32 min
[32m[20221214 00:36:49 @agent_ppo2.py:145][0m 3534848 total steps have happened
[32m[20221214 00:36:49 @agent_ppo2.py:121][0m #------------------------ Iteration 5726 --------------------------#
[32m[20221214 00:36:50 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:36:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:36:50 @agent_ppo2.py:185][0m |           0.0013 |         157.0718 |        -197.6817 |
[32m[20221214 00:36:50 @agent_ppo2.py:185][0m |           0.0018 |         149.2484 |        -196.9452 |
[32m[20221214 00:36:50 @agent_ppo2.py:185][0m |          -0.0051 |         145.3610 |        -197.2776 |
[32m[20221214 00:36:50 @agent_ppo2.py:185][0m |          -0.0052 |         142.7406 |        -196.9749 |
[32m[20221214 00:36:50 @agent_ppo2.py:185][0m |          -0.0048 |         140.5445 |        -196.6394 |
[32m[20221214 00:36:50 @agent_ppo2.py:185][0m |          -0.0075 |         138.1001 |        -196.2514 |
[32m[20221214 00:36:50 @agent_ppo2.py:185][0m |          -0.0013 |         138.5788 |        -196.3828 |
[32m[20221214 00:36:50 @agent_ppo2.py:185][0m |          -0.0075 |         136.7284 |        -197.0866 |
[32m[20221214 00:36:51 @agent_ppo2.py:185][0m |          -0.0082 |         136.3280 |        -196.2467 |
[32m[20221214 00:36:51 @agent_ppo2.py:185][0m |          -0.0097 |         135.4768 |        -196.1525 |
[32m[20221214 00:36:51 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221214 00:36:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 618.22
[32m[20221214 00:36:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 757.57
[32m[20221214 00:36:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 802.09
[32m[20221214 00:36:51 @agent_ppo2.py:143][0m Total time:      39.34 min
[32m[20221214 00:36:51 @agent_ppo2.py:145][0m 3536896 total steps have happened
[32m[20221214 00:36:51 @agent_ppo2.py:121][0m #------------------------ Iteration 5727 --------------------------#
[32m[20221214 00:36:51 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:36:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:36:51 @agent_ppo2.py:185][0m |           0.0023 |         134.6415 |        -188.9440 |
[32m[20221214 00:36:51 @agent_ppo2.py:185][0m |          -0.0013 |         129.8503 |        -189.0515 |
[32m[20221214 00:36:51 @agent_ppo2.py:185][0m |          -0.0025 |         128.4176 |        -187.5315 |
[32m[20221214 00:36:52 @agent_ppo2.py:185][0m |          -0.0059 |         126.9740 |        -187.9691 |
[32m[20221214 00:36:52 @agent_ppo2.py:185][0m |          -0.0089 |         126.2194 |        -187.4216 |
[32m[20221214 00:36:52 @agent_ppo2.py:185][0m |          -0.0091 |         125.7196 |        -187.6777 |
[32m[20221214 00:36:52 @agent_ppo2.py:185][0m |          -0.0045 |         125.8655 |        -186.8659 |
[32m[20221214 00:36:52 @agent_ppo2.py:185][0m |          -0.0053 |         124.3652 |        -187.2952 |
[32m[20221214 00:36:52 @agent_ppo2.py:185][0m |          -0.0110 |         122.9609 |        -187.0636 |
[32m[20221214 00:36:52 @agent_ppo2.py:185][0m |          -0.0080 |         123.4285 |        -186.9104 |
[32m[20221214 00:36:52 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:36:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 516.24
[32m[20221214 00:36:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 686.60
[32m[20221214 00:36:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 597.04
[32m[20221214 00:36:52 @agent_ppo2.py:143][0m Total time:      39.37 min
[32m[20221214 00:36:52 @agent_ppo2.py:145][0m 3538944 total steps have happened
[32m[20221214 00:36:52 @agent_ppo2.py:121][0m #------------------------ Iteration 5728 --------------------------#
[32m[20221214 00:36:52 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:36:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:36:53 @agent_ppo2.py:185][0m |           0.0047 |         103.3305 |        -184.9659 |
[32m[20221214 00:36:53 @agent_ppo2.py:185][0m |          -0.0075 |          92.0597 |        -184.6712 |
[32m[20221214 00:36:53 @agent_ppo2.py:185][0m |          -0.0047 |          87.7817 |        -184.4003 |
[32m[20221214 00:36:53 @agent_ppo2.py:185][0m |          -0.0120 |          85.2529 |        -183.2338 |
[32m[20221214 00:36:53 @agent_ppo2.py:185][0m |          -0.0119 |          83.3252 |        -183.6252 |
[32m[20221214 00:36:53 @agent_ppo2.py:185][0m |          -0.0165 |          81.5090 |        -183.2669 |
[32m[20221214 00:36:53 @agent_ppo2.py:185][0m |          -0.0192 |          80.1912 |        -183.2372 |
[32m[20221214 00:36:53 @agent_ppo2.py:185][0m |          -0.0205 |          79.3974 |        -183.0972 |
[32m[20221214 00:36:53 @agent_ppo2.py:185][0m |          -0.0214 |          78.4303 |        -183.0861 |
[32m[20221214 00:36:54 @agent_ppo2.py:185][0m |          -0.0192 |          77.4174 |        -182.5425 |
[32m[20221214 00:36:54 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:36:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 584.55
[32m[20221214 00:36:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 716.02
[32m[20221214 00:36:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 805.88
[32m[20221214 00:36:54 @agent_ppo2.py:143][0m Total time:      39.39 min
[32m[20221214 00:36:54 @agent_ppo2.py:145][0m 3540992 total steps have happened
[32m[20221214 00:36:54 @agent_ppo2.py:121][0m #------------------------ Iteration 5729 --------------------------#
[32m[20221214 00:36:54 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:36:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:36:54 @agent_ppo2.py:185][0m |          -0.0038 |         115.3574 |        -183.3298 |
[32m[20221214 00:36:54 @agent_ppo2.py:185][0m |          -0.0083 |         109.0412 |        -182.8427 |
[32m[20221214 00:36:54 @agent_ppo2.py:185][0m |          -0.0066 |         106.2639 |        -182.5236 |
[32m[20221214 00:36:54 @agent_ppo2.py:185][0m |          -0.0107 |         105.8640 |        -182.5642 |
[32m[20221214 00:36:54 @agent_ppo2.py:185][0m |          -0.0095 |         104.7541 |        -181.9502 |
[32m[20221214 00:36:55 @agent_ppo2.py:185][0m |          -0.0102 |         103.6369 |        -182.5631 |
[32m[20221214 00:36:55 @agent_ppo2.py:185][0m |          -0.0087 |         103.6576 |        -181.8428 |
[32m[20221214 00:36:55 @agent_ppo2.py:185][0m |          -0.0158 |         102.6758 |        -182.5861 |
[32m[20221214 00:36:55 @agent_ppo2.py:185][0m |          -0.0156 |         102.7292 |        -182.8277 |
[32m[20221214 00:36:55 @agent_ppo2.py:185][0m |          -0.0065 |         106.8536 |        -182.2057 |
[32m[20221214 00:36:55 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:36:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 731.11
[32m[20221214 00:36:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.19
[32m[20221214 00:36:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 741.49
[32m[20221214 00:36:55 @agent_ppo2.py:143][0m Total time:      39.41 min
[32m[20221214 00:36:55 @agent_ppo2.py:145][0m 3543040 total steps have happened
[32m[20221214 00:36:55 @agent_ppo2.py:121][0m #------------------------ Iteration 5730 --------------------------#
[32m[20221214 00:36:55 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:36:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:36:56 @agent_ppo2.py:185][0m |           0.0012 |         172.8325 |        -185.5875 |
[32m[20221214 00:36:56 @agent_ppo2.py:185][0m |           0.0134 |         167.2595 |        -185.0241 |
[32m[20221214 00:36:56 @agent_ppo2.py:185][0m |          -0.0032 |         152.2571 |        -185.1524 |
[32m[20221214 00:36:56 @agent_ppo2.py:185][0m |          -0.0042 |         143.1456 |        -184.7035 |
[32m[20221214 00:36:56 @agent_ppo2.py:185][0m |          -0.0080 |         140.3537 |        -185.7083 |
[32m[20221214 00:36:56 @agent_ppo2.py:185][0m |          -0.0066 |         138.6327 |        -185.0458 |
[32m[20221214 00:36:56 @agent_ppo2.py:185][0m |          -0.0071 |         137.5152 |        -184.6022 |
[32m[20221214 00:36:56 @agent_ppo2.py:185][0m |          -0.0054 |         135.8071 |        -184.2533 |
[32m[20221214 00:36:56 @agent_ppo2.py:185][0m |          -0.0047 |         133.9140 |        -184.6033 |
[32m[20221214 00:36:56 @agent_ppo2.py:185][0m |          -0.0064 |         133.2064 |        -184.6761 |
[32m[20221214 00:36:56 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:36:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 681.50
[32m[20221214 00:36:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 817.32
[32m[20221214 00:36:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 844.42
[32m[20221214 00:36:57 @agent_ppo2.py:143][0m Total time:      39.44 min
[32m[20221214 00:36:57 @agent_ppo2.py:145][0m 3545088 total steps have happened
[32m[20221214 00:36:57 @agent_ppo2.py:121][0m #------------------------ Iteration 5731 --------------------------#
[32m[20221214 00:36:57 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:36:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:36:57 @agent_ppo2.py:185][0m |           0.0023 |         147.6111 |        -183.1941 |
[32m[20221214 00:36:57 @agent_ppo2.py:185][0m |           0.0003 |         130.2197 |        -182.4921 |
[32m[20221214 00:36:57 @agent_ppo2.py:185][0m |           0.0026 |         137.2680 |        -182.3523 |
[32m[20221214 00:36:57 @agent_ppo2.py:185][0m |          -0.0052 |         126.4836 |        -181.4396 |
[32m[20221214 00:36:57 @agent_ppo2.py:185][0m |          -0.0027 |         125.2431 |        -181.0358 |
[32m[20221214 00:36:57 @agent_ppo2.py:185][0m |          -0.0081 |         124.6225 |        -180.7601 |
[32m[20221214 00:36:58 @agent_ppo2.py:185][0m |          -0.0094 |         124.0740 |        -180.1190 |
[32m[20221214 00:36:58 @agent_ppo2.py:185][0m |          -0.0072 |         124.1180 |        -180.5891 |
[32m[20221214 00:36:58 @agent_ppo2.py:185][0m |          -0.0031 |         124.9481 |        -180.1996 |
[32m[20221214 00:36:58 @agent_ppo2.py:185][0m |          -0.0103 |         123.6965 |        -179.3676 |
[32m[20221214 00:36:58 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:36:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 626.81
[32m[20221214 00:36:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 680.77
[32m[20221214 00:36:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 756.56
[32m[20221214 00:36:58 @agent_ppo2.py:143][0m Total time:      39.46 min
[32m[20221214 00:36:58 @agent_ppo2.py:145][0m 3547136 total steps have happened
[32m[20221214 00:36:58 @agent_ppo2.py:121][0m #------------------------ Iteration 5732 --------------------------#
[32m[20221214 00:36:58 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:36:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:36:58 @agent_ppo2.py:185][0m |           0.0050 |         113.6857 |        -180.6268 |
[32m[20221214 00:36:58 @agent_ppo2.py:185][0m |           0.0017 |         104.8036 |        -179.7499 |
[32m[20221214 00:36:59 @agent_ppo2.py:185][0m |          -0.0071 |         103.4004 |        -179.3306 |
[32m[20221214 00:36:59 @agent_ppo2.py:185][0m |          -0.0050 |         102.4486 |        -178.9443 |
[32m[20221214 00:36:59 @agent_ppo2.py:185][0m |          -0.0084 |         101.6313 |        -179.4810 |
[32m[20221214 00:36:59 @agent_ppo2.py:185][0m |          -0.0124 |         101.3841 |        -178.1307 |
[32m[20221214 00:36:59 @agent_ppo2.py:185][0m |          -0.0020 |         101.2276 |        -178.0949 |
[32m[20221214 00:36:59 @agent_ppo2.py:185][0m |          -0.0067 |         100.7713 |        -178.1376 |
[32m[20221214 00:36:59 @agent_ppo2.py:185][0m |          -0.0092 |          99.9077 |        -178.7412 |
[32m[20221214 00:36:59 @agent_ppo2.py:185][0m |          -0.0101 |          99.4590 |        -177.3936 |
[32m[20221214 00:36:59 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:36:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 585.08
[32m[20221214 00:36:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 649.68
[32m[20221214 00:36:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 721.83
[32m[20221214 00:36:59 @agent_ppo2.py:143][0m Total time:      39.48 min
[32m[20221214 00:36:59 @agent_ppo2.py:145][0m 3549184 total steps have happened
[32m[20221214 00:36:59 @agent_ppo2.py:121][0m #------------------------ Iteration 5733 --------------------------#
[32m[20221214 00:37:00 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:37:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:37:00 @agent_ppo2.py:185][0m |           0.0129 |         193.5771 |        -182.9687 |
[32m[20221214 00:37:00 @agent_ppo2.py:185][0m |          -0.0046 |         174.1304 |        -183.0156 |
[32m[20221214 00:37:00 @agent_ppo2.py:185][0m |          -0.0035 |         171.0270 |        -181.1611 |
[32m[20221214 00:37:00 @agent_ppo2.py:185][0m |          -0.0083 |         168.9724 |        -181.1837 |
[32m[20221214 00:37:00 @agent_ppo2.py:185][0m |          -0.0078 |         168.5651 |        -181.8509 |
[32m[20221214 00:37:00 @agent_ppo2.py:185][0m |          -0.0105 |         167.3270 |        -181.7511 |
[32m[20221214 00:37:00 @agent_ppo2.py:185][0m |          -0.0114 |         168.8170 |        -181.6329 |
[32m[20221214 00:37:00 @agent_ppo2.py:185][0m |          -0.0071 |         166.5473 |        -181.8579 |
[32m[20221214 00:37:01 @agent_ppo2.py:185][0m |          -0.0115 |         167.5332 |        -181.5555 |
[32m[20221214 00:37:01 @agent_ppo2.py:185][0m |          -0.0099 |         168.5349 |        -181.4770 |
[32m[20221214 00:37:01 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:37:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 502.85
[32m[20221214 00:37:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 683.04
[32m[20221214 00:37:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 820.77
[32m[20221214 00:37:01 @agent_ppo2.py:143][0m Total time:      39.51 min
[32m[20221214 00:37:01 @agent_ppo2.py:145][0m 3551232 total steps have happened
[32m[20221214 00:37:01 @agent_ppo2.py:121][0m #------------------------ Iteration 5734 --------------------------#
[32m[20221214 00:37:01 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:37:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:37:01 @agent_ppo2.py:185][0m |           0.0012 |         135.8453 |        -171.8022 |
[32m[20221214 00:37:01 @agent_ppo2.py:185][0m |          -0.0035 |         127.0612 |        -171.1172 |
[32m[20221214 00:37:01 @agent_ppo2.py:185][0m |          -0.0026 |         124.8387 |        -171.5370 |
[32m[20221214 00:37:01 @agent_ppo2.py:185][0m |          -0.0020 |         124.9442 |        -171.3483 |
[32m[20221214 00:37:02 @agent_ppo2.py:185][0m |          -0.0059 |         123.8652 |        -171.5729 |
[32m[20221214 00:37:02 @agent_ppo2.py:185][0m |          -0.0027 |         122.6865 |        -171.2449 |
[32m[20221214 00:37:02 @agent_ppo2.py:185][0m |          -0.0055 |         122.0743 |        -170.4769 |
[32m[20221214 00:37:02 @agent_ppo2.py:185][0m |          -0.0064 |         121.7437 |        -171.0775 |
[32m[20221214 00:37:02 @agent_ppo2.py:185][0m |          -0.0083 |         121.5806 |        -169.7713 |
[32m[20221214 00:37:02 @agent_ppo2.py:185][0m |          -0.0122 |         121.2077 |        -170.4147 |
[32m[20221214 00:37:02 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:37:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 578.28
[32m[20221214 00:37:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 703.17
[32m[20221214 00:37:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 743.50
[32m[20221214 00:37:02 @agent_ppo2.py:143][0m Total time:      39.53 min
[32m[20221214 00:37:02 @agent_ppo2.py:145][0m 3553280 total steps have happened
[32m[20221214 00:37:02 @agent_ppo2.py:121][0m #------------------------ Iteration 5735 --------------------------#
[32m[20221214 00:37:02 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:37:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:37:03 @agent_ppo2.py:185][0m |           0.0017 |          91.1781 |        -173.3128 |
[32m[20221214 00:37:03 @agent_ppo2.py:185][0m |          -0.0011 |          85.5557 |        -173.2457 |
[32m[20221214 00:37:03 @agent_ppo2.py:185][0m |          -0.0017 |          80.8202 |        -174.3126 |
[32m[20221214 00:37:03 @agent_ppo2.py:185][0m |           0.0050 |          79.0102 |        -174.1564 |
[32m[20221214 00:37:03 @agent_ppo2.py:185][0m |          -0.0087 |          78.8533 |        -173.5388 |
[32m[20221214 00:37:03 @agent_ppo2.py:185][0m |          -0.0073 |          77.9270 |        -174.4481 |
[32m[20221214 00:37:03 @agent_ppo2.py:185][0m |          -0.0083 |          77.0777 |        -174.4705 |
[32m[20221214 00:37:03 @agent_ppo2.py:185][0m |          -0.0072 |          77.2380 |        -174.7581 |
[32m[20221214 00:37:03 @agent_ppo2.py:185][0m |          -0.0123 |          76.4116 |        -175.7330 |
[32m[20221214 00:37:04 @agent_ppo2.py:185][0m |          -0.0093 |          76.7727 |        -175.9041 |
[32m[20221214 00:37:04 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 00:37:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 722.69
[32m[20221214 00:37:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 824.32
[32m[20221214 00:37:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 843.44
[32m[20221214 00:37:04 @agent_ppo2.py:143][0m Total time:      39.56 min
[32m[20221214 00:37:04 @agent_ppo2.py:145][0m 3555328 total steps have happened
[32m[20221214 00:37:04 @agent_ppo2.py:121][0m #------------------------ Iteration 5736 --------------------------#
[32m[20221214 00:37:04 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:37:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:37:04 @agent_ppo2.py:185][0m |           0.0009 |         117.8825 |        -174.0881 |
[32m[20221214 00:37:04 @agent_ppo2.py:185][0m |           0.0031 |         103.3983 |        -173.9529 |
[32m[20221214 00:37:04 @agent_ppo2.py:185][0m |          -0.0053 |          99.7902 |        -174.2133 |
[32m[20221214 00:37:04 @agent_ppo2.py:185][0m |          -0.0033 |          99.0298 |        -173.5458 |
[32m[20221214 00:37:04 @agent_ppo2.py:185][0m |          -0.0182 |          96.2075 |        -173.3918 |
[32m[20221214 00:37:05 @agent_ppo2.py:185][0m |          -0.0129 |          95.1744 |        -173.3810 |
[32m[20221214 00:37:05 @agent_ppo2.py:185][0m |          -0.0054 |          94.4233 |        -172.9917 |
[32m[20221214 00:37:05 @agent_ppo2.py:185][0m |          -0.0084 |          94.6335 |        -173.1461 |
[32m[20221214 00:37:05 @agent_ppo2.py:185][0m |          -0.0167 |          93.9428 |        -173.2386 |
[32m[20221214 00:37:05 @agent_ppo2.py:185][0m |          -0.0205 |          93.1581 |        -172.6982 |
[32m[20221214 00:37:05 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:37:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 510.61
[32m[20221214 00:37:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 715.32
[32m[20221214 00:37:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 703.87
[32m[20221214 00:37:05 @agent_ppo2.py:143][0m Total time:      39.58 min
[32m[20221214 00:37:05 @agent_ppo2.py:145][0m 3557376 total steps have happened
[32m[20221214 00:37:05 @agent_ppo2.py:121][0m #------------------------ Iteration 5737 --------------------------#
[32m[20221214 00:37:05 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:37:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:37:05 @agent_ppo2.py:185][0m |           0.0028 |         124.9361 |        -183.1139 |
[32m[20221214 00:37:06 @agent_ppo2.py:185][0m |          -0.0073 |         116.9945 |        -182.7787 |
[32m[20221214 00:37:06 @agent_ppo2.py:185][0m |           0.0007 |         116.2409 |        -181.8279 |
[32m[20221214 00:37:06 @agent_ppo2.py:185][0m |          -0.0119 |         114.4705 |        -181.3655 |
[32m[20221214 00:37:06 @agent_ppo2.py:185][0m |          -0.0063 |         113.9867 |        -180.7093 |
[32m[20221214 00:37:06 @agent_ppo2.py:185][0m |          -0.0099 |         112.7392 |        -180.9344 |
[32m[20221214 00:37:06 @agent_ppo2.py:185][0m |          -0.0068 |         112.2077 |        -179.7871 |
[32m[20221214 00:37:06 @agent_ppo2.py:185][0m |          -0.0111 |         112.2927 |        -179.2221 |
[32m[20221214 00:37:06 @agent_ppo2.py:185][0m |          -0.0075 |         111.4967 |        -179.0821 |
[32m[20221214 00:37:06 @agent_ppo2.py:185][0m |          -0.0087 |         111.3479 |        -178.4854 |
[32m[20221214 00:37:06 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221214 00:37:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 548.52
[32m[20221214 00:37:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 645.10
[32m[20221214 00:37:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 583.42
[32m[20221214 00:37:07 @agent_ppo2.py:143][0m Total time:      39.60 min
[32m[20221214 00:37:07 @agent_ppo2.py:145][0m 3559424 total steps have happened
[32m[20221214 00:37:07 @agent_ppo2.py:121][0m #------------------------ Iteration 5738 --------------------------#
[32m[20221214 00:37:07 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:37:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:37:07 @agent_ppo2.py:185][0m |           0.0041 |         143.7843 |        -175.0369 |
[32m[20221214 00:37:07 @agent_ppo2.py:185][0m |           0.0043 |         133.9594 |        -175.0149 |
[32m[20221214 00:37:07 @agent_ppo2.py:185][0m |          -0.0073 |         129.7510 |        -174.6293 |
[32m[20221214 00:37:07 @agent_ppo2.py:185][0m |          -0.0040 |         128.2081 |        -174.7734 |
[32m[20221214 00:37:07 @agent_ppo2.py:185][0m |          -0.0086 |         127.2286 |        -174.1022 |
[32m[20221214 00:37:08 @agent_ppo2.py:185][0m |          -0.0108 |         126.8889 |        -173.7455 |
[32m[20221214 00:37:08 @agent_ppo2.py:185][0m |          -0.0149 |         125.7368 |        -173.8750 |
[32m[20221214 00:37:08 @agent_ppo2.py:185][0m |          -0.0129 |         125.3336 |        -173.1351 |
[32m[20221214 00:37:08 @agent_ppo2.py:185][0m |          -0.0137 |         124.7164 |        -172.8609 |
[32m[20221214 00:37:08 @agent_ppo2.py:185][0m |          -0.0120 |         124.7341 |        -173.2880 |
[32m[20221214 00:37:08 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:37:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 518.62
[32m[20221214 00:37:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 656.99
[32m[20221214 00:37:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 834.71
[32m[20221214 00:37:08 @agent_ppo2.py:143][0m Total time:      39.63 min
[32m[20221214 00:37:08 @agent_ppo2.py:145][0m 3561472 total steps have happened
[32m[20221214 00:37:08 @agent_ppo2.py:121][0m #------------------------ Iteration 5739 --------------------------#
[32m[20221214 00:37:08 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:37:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:37:08 @agent_ppo2.py:185][0m |           0.0042 |         143.1319 |        -168.5343 |
[32m[20221214 00:37:09 @agent_ppo2.py:185][0m |          -0.0026 |         134.8836 |        -167.6526 |
[32m[20221214 00:37:09 @agent_ppo2.py:185][0m |          -0.0028 |         133.8876 |        -168.9315 |
[32m[20221214 00:37:09 @agent_ppo2.py:185][0m |          -0.0048 |         132.1476 |        -167.9319 |
[32m[20221214 00:37:09 @agent_ppo2.py:185][0m |          -0.0036 |         131.5941 |        -167.6522 |
[32m[20221214 00:37:09 @agent_ppo2.py:185][0m |          -0.0110 |         131.0350 |        -168.4328 |
[32m[20221214 00:37:09 @agent_ppo2.py:185][0m |          -0.0123 |         130.3384 |        -167.1459 |
[32m[20221214 00:37:09 @agent_ppo2.py:185][0m |          -0.0113 |         130.1935 |        -167.7566 |
[32m[20221214 00:37:09 @agent_ppo2.py:185][0m |          -0.0122 |         130.1335 |        -166.7631 |
[32m[20221214 00:37:09 @agent_ppo2.py:185][0m |          -0.0076 |         129.5466 |        -167.2062 |
[32m[20221214 00:37:09 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:37:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 630.07
[32m[20221214 00:37:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 689.20
[32m[20221214 00:37:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 389.40
[32m[20221214 00:37:10 @agent_ppo2.py:143][0m Total time:      39.65 min
[32m[20221214 00:37:10 @agent_ppo2.py:145][0m 3563520 total steps have happened
[32m[20221214 00:37:10 @agent_ppo2.py:121][0m #------------------------ Iteration 5740 --------------------------#
[32m[20221214 00:37:10 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:37:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:37:10 @agent_ppo2.py:185][0m |           0.0117 |         182.3843 |        -165.4887 |
[32m[20221214 00:37:10 @agent_ppo2.py:185][0m |           0.0007 |         167.1839 |        -165.0579 |
[32m[20221214 00:37:10 @agent_ppo2.py:185][0m |           0.0171 |         182.9297 |        -165.6870 |
[32m[20221214 00:37:10 @agent_ppo2.py:185][0m |          -0.0006 |         165.1469 |        -165.6783 |
[32m[20221214 00:37:10 @agent_ppo2.py:185][0m |           0.0014 |         164.3729 |        -165.9225 |
[32m[20221214 00:37:10 @agent_ppo2.py:185][0m |           0.0025 |         167.9984 |        -165.8696 |
[32m[20221214 00:37:11 @agent_ppo2.py:185][0m |          -0.0026 |         162.7701 |        -165.8165 |
[32m[20221214 00:37:11 @agent_ppo2.py:185][0m |           0.0052 |         171.6637 |        -166.1434 |
[32m[20221214 00:37:11 @agent_ppo2.py:185][0m |          -0.0071 |         161.7672 |        -164.5592 |
[32m[20221214 00:37:11 @agent_ppo2.py:185][0m |          -0.0062 |         161.7321 |        -166.0786 |
[32m[20221214 00:37:11 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:37:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 630.74
[32m[20221214 00:37:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 718.46
[32m[20221214 00:37:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 567.36
[32m[20221214 00:37:11 @agent_ppo2.py:143][0m Total time:      39.68 min
[32m[20221214 00:37:11 @agent_ppo2.py:145][0m 3565568 total steps have happened
[32m[20221214 00:37:11 @agent_ppo2.py:121][0m #------------------------ Iteration 5741 --------------------------#
[32m[20221214 00:37:11 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:37:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:37:11 @agent_ppo2.py:185][0m |           0.0022 |         138.2326 |        -170.2932 |
[32m[20221214 00:37:11 @agent_ppo2.py:185][0m |          -0.0064 |         132.8555 |        -169.9989 |
[32m[20221214 00:37:12 @agent_ppo2.py:185][0m |           0.0029 |         132.5749 |        -169.5989 |
[32m[20221214 00:37:12 @agent_ppo2.py:185][0m |          -0.0059 |         128.8613 |        -168.8308 |
[32m[20221214 00:37:12 @agent_ppo2.py:185][0m |           0.0004 |         131.5916 |        -169.1413 |
[32m[20221214 00:37:12 @agent_ppo2.py:185][0m |          -0.0031 |         127.9011 |        -169.8779 |
[32m[20221214 00:37:12 @agent_ppo2.py:185][0m |           0.0018 |         133.1578 |        -168.7062 |
[32m[20221214 00:37:12 @agent_ppo2.py:185][0m |          -0.0037 |         128.9112 |        -169.3385 |
[32m[20221214 00:37:12 @agent_ppo2.py:185][0m |          -0.0111 |         125.0981 |        -169.0973 |
[32m[20221214 00:37:12 @agent_ppo2.py:185][0m |          -0.0088 |         124.9592 |        -168.7699 |
[32m[20221214 00:37:12 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:37:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 573.41
[32m[20221214 00:37:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 690.62
[32m[20221214 00:37:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 826.83
[32m[20221214 00:37:12 @agent_ppo2.py:143][0m Total time:      39.70 min
[32m[20221214 00:37:12 @agent_ppo2.py:145][0m 3567616 total steps have happened
[32m[20221214 00:37:12 @agent_ppo2.py:121][0m #------------------------ Iteration 5742 --------------------------#
[32m[20221214 00:37:13 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:37:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:37:13 @agent_ppo2.py:185][0m |           0.0050 |         111.3440 |        -168.9353 |
[32m[20221214 00:37:13 @agent_ppo2.py:185][0m |          -0.0044 |          90.2378 |        -168.2332 |
[32m[20221214 00:37:13 @agent_ppo2.py:185][0m |          -0.0088 |          82.9931 |        -167.9727 |
[32m[20221214 00:37:13 @agent_ppo2.py:185][0m |          -0.0086 |          78.6034 |        -169.0300 |
[32m[20221214 00:37:13 @agent_ppo2.py:185][0m |          -0.0122 |          77.1240 |        -167.8576 |
[32m[20221214 00:37:13 @agent_ppo2.py:185][0m |          -0.0103 |          75.0752 |        -168.6966 |
[32m[20221214 00:37:13 @agent_ppo2.py:185][0m |          -0.0095 |          72.4273 |        -168.2886 |
[32m[20221214 00:37:13 @agent_ppo2.py:185][0m |          -0.0114 |          69.9527 |        -168.2859 |
[32m[20221214 00:37:14 @agent_ppo2.py:185][0m |          -0.0048 |          69.1719 |        -168.3229 |
[32m[20221214 00:37:14 @agent_ppo2.py:185][0m |          -0.0137 |          68.1793 |        -168.0206 |
[32m[20221214 00:37:14 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:37:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 620.49
[32m[20221214 00:37:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 708.23
[32m[20221214 00:37:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 695.53
[32m[20221214 00:37:14 @agent_ppo2.py:143][0m Total time:      39.72 min
[32m[20221214 00:37:14 @agent_ppo2.py:145][0m 3569664 total steps have happened
[32m[20221214 00:37:14 @agent_ppo2.py:121][0m #------------------------ Iteration 5743 --------------------------#
[32m[20221214 00:37:14 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:37:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:37:14 @agent_ppo2.py:185][0m |           0.0020 |          89.4526 |        -154.5928 |
[32m[20221214 00:37:14 @agent_ppo2.py:185][0m |          -0.0059 |          82.4185 |        -153.9031 |
[32m[20221214 00:37:14 @agent_ppo2.py:185][0m |          -0.0020 |          78.0972 |        -153.9249 |
[32m[20221214 00:37:14 @agent_ppo2.py:185][0m |          -0.0097 |          77.1681 |        -155.4361 |
[32m[20221214 00:37:15 @agent_ppo2.py:185][0m |          -0.0101 |          75.4068 |        -155.0201 |
[32m[20221214 00:37:15 @agent_ppo2.py:185][0m |          -0.0132 |          75.2498 |        -155.1140 |
[32m[20221214 00:37:15 @agent_ppo2.py:185][0m |          -0.0104 |          73.1330 |        -154.6458 |
[32m[20221214 00:37:15 @agent_ppo2.py:185][0m |          -0.0103 |          74.5188 |        -154.4964 |
[32m[20221214 00:37:15 @agent_ppo2.py:185][0m |          -0.0166 |          74.1922 |        -155.2094 |
[32m[20221214 00:37:15 @agent_ppo2.py:185][0m |           0.0001 |          81.4867 |        -154.8827 |
[32m[20221214 00:37:15 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:37:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 715.46
[32m[20221214 00:37:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 786.72
[32m[20221214 00:37:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 680.28
[32m[20221214 00:37:15 @agent_ppo2.py:143][0m Total time:      39.75 min
[32m[20221214 00:37:15 @agent_ppo2.py:145][0m 3571712 total steps have happened
[32m[20221214 00:37:15 @agent_ppo2.py:121][0m #------------------------ Iteration 5744 --------------------------#
[32m[20221214 00:37:15 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:37:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:37:16 @agent_ppo2.py:185][0m |           0.0002 |         122.2586 |        -168.7516 |
[32m[20221214 00:37:16 @agent_ppo2.py:185][0m |          -0.0071 |         109.8035 |        -167.9385 |
[32m[20221214 00:37:16 @agent_ppo2.py:185][0m |          -0.0067 |         106.4411 |        -167.7980 |
[32m[20221214 00:37:16 @agent_ppo2.py:185][0m |          -0.0096 |         103.8254 |        -167.0186 |
[32m[20221214 00:37:16 @agent_ppo2.py:185][0m |          -0.0095 |         102.4269 |        -167.1285 |
[32m[20221214 00:37:16 @agent_ppo2.py:185][0m |          -0.0180 |         101.4571 |        -166.5193 |
[32m[20221214 00:37:16 @agent_ppo2.py:185][0m |          -0.0118 |         101.2297 |        -166.2894 |
[32m[20221214 00:37:16 @agent_ppo2.py:185][0m |          -0.0144 |          99.3925 |        -166.0532 |
[32m[20221214 00:37:16 @agent_ppo2.py:185][0m |          -0.0158 |          99.2408 |        -166.0618 |
[32m[20221214 00:37:17 @agent_ppo2.py:185][0m |          -0.0229 |         101.0131 |        -165.6577 |
[32m[20221214 00:37:17 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:37:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 471.16
[32m[20221214 00:37:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 505.05
[32m[20221214 00:37:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 660.02
[32m[20221214 00:37:17 @agent_ppo2.py:143][0m Total time:      39.77 min
[32m[20221214 00:37:17 @agent_ppo2.py:145][0m 3573760 total steps have happened
[32m[20221214 00:37:17 @agent_ppo2.py:121][0m #------------------------ Iteration 5745 --------------------------#
[32m[20221214 00:37:17 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:37:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:37:17 @agent_ppo2.py:185][0m |           0.0024 |         149.9188 |        -168.4123 |
[32m[20221214 00:37:17 @agent_ppo2.py:185][0m |           0.0013 |         138.9303 |        -167.8114 |
[32m[20221214 00:37:17 @agent_ppo2.py:185][0m |          -0.0025 |         132.9100 |        -168.4083 |
[32m[20221214 00:37:17 @agent_ppo2.py:185][0m |          -0.0030 |         130.7110 |        -167.5818 |
[32m[20221214 00:37:17 @agent_ppo2.py:185][0m |          -0.0042 |         128.0984 |        -167.5570 |
[32m[20221214 00:37:18 @agent_ppo2.py:185][0m |          -0.0044 |         127.7076 |        -167.8181 |
[32m[20221214 00:37:18 @agent_ppo2.py:185][0m |          -0.0108 |         129.0300 |        -167.8086 |
[32m[20221214 00:37:18 @agent_ppo2.py:185][0m |          -0.0089 |         125.4135 |        -167.3904 |
[32m[20221214 00:37:18 @agent_ppo2.py:185][0m |          -0.0081 |         126.8230 |        -167.1766 |
[32m[20221214 00:37:18 @agent_ppo2.py:185][0m |          -0.0019 |         130.6328 |        -167.0762 |
[32m[20221214 00:37:18 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221214 00:37:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 634.03
[32m[20221214 00:37:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 769.94
[32m[20221214 00:37:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 684.23
[32m[20221214 00:37:18 @agent_ppo2.py:143][0m Total time:      39.80 min
[32m[20221214 00:37:18 @agent_ppo2.py:145][0m 3575808 total steps have happened
[32m[20221214 00:37:18 @agent_ppo2.py:121][0m #------------------------ Iteration 5746 --------------------------#
[32m[20221214 00:37:18 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:37:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:37:19 @agent_ppo2.py:185][0m |           0.0046 |         135.4677 |        -161.6888 |
[32m[20221214 00:37:19 @agent_ppo2.py:185][0m |           0.0046 |         128.2577 |        -161.4871 |
[32m[20221214 00:37:19 @agent_ppo2.py:185][0m |          -0.0105 |         117.3160 |        -161.9484 |
[32m[20221214 00:37:19 @agent_ppo2.py:185][0m |          -0.0149 |         115.6906 |        -162.1352 |
[32m[20221214 00:37:19 @agent_ppo2.py:185][0m |          -0.0119 |         114.4098 |        -162.3648 |
[32m[20221214 00:37:19 @agent_ppo2.py:185][0m |          -0.0137 |         114.0958 |        -161.9032 |
[32m[20221214 00:37:19 @agent_ppo2.py:185][0m |          -0.0102 |         113.6880 |        -161.3439 |
[32m[20221214 00:37:19 @agent_ppo2.py:185][0m |           0.0044 |         122.3650 |        -162.1344 |
[32m[20221214 00:37:19 @agent_ppo2.py:185][0m |          -0.0152 |         116.0623 |        -161.9777 |
[32m[20221214 00:37:20 @agent_ppo2.py:185][0m |          -0.0129 |         112.6164 |        -162.0019 |
[32m[20221214 00:37:20 @agent_ppo2.py:130][0m Policy update time: 1.17 s
[32m[20221214 00:37:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 673.85
[32m[20221214 00:37:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 795.88
[32m[20221214 00:37:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 569.10
[32m[20221214 00:37:20 @agent_ppo2.py:143][0m Total time:      39.82 min
[32m[20221214 00:37:20 @agent_ppo2.py:145][0m 3577856 total steps have happened
[32m[20221214 00:37:20 @agent_ppo2.py:121][0m #------------------------ Iteration 5747 --------------------------#
[32m[20221214 00:37:20 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:37:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:37:20 @agent_ppo2.py:185][0m |           0.0009 |         111.9645 |        -169.0668 |
[32m[20221214 00:37:20 @agent_ppo2.py:185][0m |           0.0011 |         106.9448 |        -168.4573 |
[32m[20221214 00:37:20 @agent_ppo2.py:185][0m |          -0.0029 |         105.3780 |        -168.1130 |
[32m[20221214 00:37:20 @agent_ppo2.py:185][0m |          -0.0067 |         104.9327 |        -167.5655 |
[32m[20221214 00:37:21 @agent_ppo2.py:185][0m |          -0.0068 |         103.4511 |        -167.4144 |
[32m[20221214 00:37:21 @agent_ppo2.py:185][0m |          -0.0052 |         106.2589 |        -167.8195 |
[32m[20221214 00:37:21 @agent_ppo2.py:185][0m |          -0.0124 |         103.9930 |        -167.7874 |
[32m[20221214 00:37:21 @agent_ppo2.py:185][0m |          -0.0060 |         102.0115 |        -167.2891 |
[32m[20221214 00:37:21 @agent_ppo2.py:185][0m |          -0.0124 |         101.8799 |        -167.5429 |
[32m[20221214 00:37:21 @agent_ppo2.py:185][0m |          -0.0143 |         100.5485 |        -167.9015 |
[32m[20221214 00:37:21 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221214 00:37:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 509.14
[32m[20221214 00:37:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 729.48
[32m[20221214 00:37:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 849.83
[32m[20221214 00:37:21 @agent_ppo2.py:143][0m Total time:      39.85 min
[32m[20221214 00:37:21 @agent_ppo2.py:145][0m 3579904 total steps have happened
[32m[20221214 00:37:21 @agent_ppo2.py:121][0m #------------------------ Iteration 5748 --------------------------#
[32m[20221214 00:37:21 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:37:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:37:22 @agent_ppo2.py:185][0m |           0.0004 |         195.3713 |        -162.6770 |
[32m[20221214 00:37:22 @agent_ppo2.py:185][0m |           0.0006 |         189.3416 |        -162.2915 |
[32m[20221214 00:37:22 @agent_ppo2.py:185][0m |          -0.0037 |         189.7811 |        -162.5567 |
[32m[20221214 00:37:22 @agent_ppo2.py:185][0m |          -0.0015 |         188.3533 |        -161.2332 |
[32m[20221214 00:37:22 @agent_ppo2.py:185][0m |          -0.0007 |         195.8243 |        -163.2369 |
[32m[20221214 00:37:22 @agent_ppo2.py:185][0m |          -0.0016 |         188.8656 |        -161.4929 |
[32m[20221214 00:37:22 @agent_ppo2.py:185][0m |          -0.0059 |         186.0321 |        -161.7985 |
[32m[20221214 00:37:22 @agent_ppo2.py:185][0m |           0.0017 |         188.9673 |        -161.0954 |
[32m[20221214 00:37:22 @agent_ppo2.py:185][0m |          -0.0025 |         185.9858 |        -161.4186 |
[32m[20221214 00:37:22 @agent_ppo2.py:185][0m |          -0.0077 |         184.9593 |        -161.2336 |
[32m[20221214 00:37:22 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221214 00:37:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 695.23
[32m[20221214 00:37:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 779.19
[32m[20221214 00:37:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 827.45
[32m[20221214 00:37:23 @agent_ppo2.py:143][0m Total time:      39.87 min
[32m[20221214 00:37:23 @agent_ppo2.py:145][0m 3581952 total steps have happened
[32m[20221214 00:37:23 @agent_ppo2.py:121][0m #------------------------ Iteration 5749 --------------------------#
[32m[20221214 00:37:23 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:37:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:37:23 @agent_ppo2.py:185][0m |           0.0062 |         157.8772 |        -166.7111 |
[32m[20221214 00:37:23 @agent_ppo2.py:185][0m |          -0.0018 |         153.9724 |        -166.2510 |
[32m[20221214 00:37:23 @agent_ppo2.py:185][0m |          -0.0066 |         151.9182 |        -166.1210 |
[32m[20221214 00:37:23 @agent_ppo2.py:185][0m |          -0.0079 |         150.5526 |        -167.1938 |
[32m[20221214 00:37:23 @agent_ppo2.py:185][0m |           0.0030 |         169.9077 |        -166.8683 |
[32m[20221214 00:37:24 @agent_ppo2.py:185][0m |          -0.0078 |         150.2685 |        -167.0051 |
[32m[20221214 00:37:24 @agent_ppo2.py:185][0m |          -0.0131 |         150.6961 |        -167.6221 |
[32m[20221214 00:37:24 @agent_ppo2.py:185][0m |          -0.0075 |         148.2738 |        -166.8635 |
[32m[20221214 00:37:24 @agent_ppo2.py:185][0m |          -0.0056 |         151.6183 |        -168.0248 |
[32m[20221214 00:37:24 @agent_ppo2.py:185][0m |          -0.0114 |         147.3144 |        -167.8307 |
[32m[20221214 00:37:24 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221214 00:37:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 518.91
[32m[20221214 00:37:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 619.76
[32m[20221214 00:37:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 531.08
[32m[20221214 00:37:24 @agent_ppo2.py:143][0m Total time:      39.90 min
[32m[20221214 00:37:24 @agent_ppo2.py:145][0m 3584000 total steps have happened
[32m[20221214 00:37:24 @agent_ppo2.py:121][0m #------------------------ Iteration 5750 --------------------------#
[32m[20221214 00:37:24 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:37:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:37:24 @agent_ppo2.py:185][0m |          -0.0005 |         156.9131 |        -161.4620 |
[32m[20221214 00:37:25 @agent_ppo2.py:185][0m |           0.0047 |         161.5281 |        -161.3821 |
[32m[20221214 00:37:25 @agent_ppo2.py:185][0m |          -0.0018 |         151.7827 |        -160.8396 |
[32m[20221214 00:37:25 @agent_ppo2.py:185][0m |           0.0003 |         164.1378 |        -161.4689 |
[32m[20221214 00:37:25 @agent_ppo2.py:185][0m |          -0.0113 |         149.2227 |        -161.6214 |
[32m[20221214 00:37:25 @agent_ppo2.py:185][0m |          -0.0052 |         148.6837 |        -161.1008 |
[32m[20221214 00:37:25 @agent_ppo2.py:185][0m |           0.0037 |         172.0978 |        -161.2392 |
[32m[20221214 00:37:25 @agent_ppo2.py:185][0m |          -0.0104 |         148.2644 |        -161.2448 |
[32m[20221214 00:37:25 @agent_ppo2.py:185][0m |          -0.0078 |         148.5116 |        -161.2815 |
[32m[20221214 00:37:25 @agent_ppo2.py:185][0m |          -0.0089 |         149.3574 |        -160.9924 |
[32m[20221214 00:37:25 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:37:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 624.86
[32m[20221214 00:37:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 657.84
[32m[20221214 00:37:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 846.55
[32m[20221214 00:37:26 @agent_ppo2.py:143][0m Total time:      39.92 min
[32m[20221214 00:37:26 @agent_ppo2.py:145][0m 3586048 total steps have happened
[32m[20221214 00:37:26 @agent_ppo2.py:121][0m #------------------------ Iteration 5751 --------------------------#
[32m[20221214 00:37:26 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:37:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:37:26 @agent_ppo2.py:185][0m |           0.0053 |         137.7194 |        -165.3309 |
[32m[20221214 00:37:26 @agent_ppo2.py:185][0m |          -0.0037 |         129.0645 |        -165.7256 |
[32m[20221214 00:37:26 @agent_ppo2.py:185][0m |          -0.0064 |         127.0633 |        -164.6388 |
[32m[20221214 00:37:26 @agent_ppo2.py:185][0m |          -0.0050 |         127.4540 |        -164.5844 |
[32m[20221214 00:37:26 @agent_ppo2.py:185][0m |          -0.0102 |         126.1840 |        -164.4957 |
[32m[20221214 00:37:26 @agent_ppo2.py:185][0m |          -0.0088 |         126.2039 |        -165.1288 |
[32m[20221214 00:37:27 @agent_ppo2.py:185][0m |          -0.0120 |         125.9661 |        -163.6865 |
[32m[20221214 00:37:27 @agent_ppo2.py:185][0m |          -0.0109 |         124.2407 |        -164.0817 |
[32m[20221214 00:37:27 @agent_ppo2.py:185][0m |          -0.0091 |         124.1673 |        -164.4678 |
[32m[20221214 00:37:27 @agent_ppo2.py:185][0m |          -0.0124 |         125.0329 |        -163.4706 |
[32m[20221214 00:37:27 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:37:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 619.50
[32m[20221214 00:37:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 795.99
[32m[20221214 00:37:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 613.91
[32m[20221214 00:37:27 @agent_ppo2.py:143][0m Total time:      39.94 min
[32m[20221214 00:37:27 @agent_ppo2.py:145][0m 3588096 total steps have happened
[32m[20221214 00:37:27 @agent_ppo2.py:121][0m #------------------------ Iteration 5752 --------------------------#
[32m[20221214 00:37:27 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:37:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:37:27 @agent_ppo2.py:185][0m |           0.0028 |         116.3777 |        -159.1532 |
[32m[20221214 00:37:27 @agent_ppo2.py:185][0m |          -0.0033 |         108.3137 |        -158.2019 |
[32m[20221214 00:37:28 @agent_ppo2.py:185][0m |          -0.0101 |         104.6554 |        -158.5302 |
[32m[20221214 00:37:28 @agent_ppo2.py:185][0m |          -0.0106 |         102.9181 |        -159.2128 |
[32m[20221214 00:37:28 @agent_ppo2.py:185][0m |          -0.0076 |         101.2941 |        -159.1339 |
[32m[20221214 00:37:28 @agent_ppo2.py:185][0m |          -0.0105 |          99.9751 |        -157.6911 |
[32m[20221214 00:37:28 @agent_ppo2.py:185][0m |          -0.0125 |          98.6745 |        -158.5815 |
[32m[20221214 00:37:28 @agent_ppo2.py:185][0m |          -0.0017 |         102.5615 |        -159.1630 |
[32m[20221214 00:37:28 @agent_ppo2.py:185][0m |          -0.0108 |          97.5838 |        -158.2979 |
[32m[20221214 00:37:28 @agent_ppo2.py:185][0m |          -0.0166 |          96.3313 |        -158.9014 |
[32m[20221214 00:37:28 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:37:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 505.66
[32m[20221214 00:37:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 659.83
[32m[20221214 00:37:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 497.92
[32m[20221214 00:37:28 @agent_ppo2.py:143][0m Total time:      39.97 min
[32m[20221214 00:37:28 @agent_ppo2.py:145][0m 3590144 total steps have happened
[32m[20221214 00:37:28 @agent_ppo2.py:121][0m #------------------------ Iteration 5753 --------------------------#
[32m[20221214 00:37:29 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:37:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:37:29 @agent_ppo2.py:185][0m |           0.0186 |         198.9994 |        -160.7142 |
[32m[20221214 00:37:29 @agent_ppo2.py:185][0m |          -0.0015 |         181.5073 |        -160.6713 |
[32m[20221214 00:37:29 @agent_ppo2.py:185][0m |           0.0013 |         179.0698 |        -160.0113 |
[32m[20221214 00:37:29 @agent_ppo2.py:185][0m |           0.0036 |         178.5513 |        -159.3610 |
[32m[20221214 00:37:29 @agent_ppo2.py:185][0m |          -0.0061 |         178.4593 |        -159.6339 |
[32m[20221214 00:37:29 @agent_ppo2.py:185][0m |          -0.0046 |         178.5341 |        -159.1267 |
[32m[20221214 00:37:29 @agent_ppo2.py:185][0m |          -0.0041 |         178.7148 |        -159.8755 |
[32m[20221214 00:37:29 @agent_ppo2.py:185][0m |          -0.0025 |         177.5013 |        -160.5904 |
[32m[20221214 00:37:30 @agent_ppo2.py:185][0m |          -0.0092 |         177.4950 |        -159.8533 |
[32m[20221214 00:37:30 @agent_ppo2.py:185][0m |          -0.0029 |         176.7532 |        -159.3344 |
[32m[20221214 00:37:30 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:37:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 674.08
[32m[20221214 00:37:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 768.49
[32m[20221214 00:37:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 420.12
[32m[20221214 00:37:30 @agent_ppo2.py:143][0m Total time:      39.99 min
[32m[20221214 00:37:30 @agent_ppo2.py:145][0m 3592192 total steps have happened
[32m[20221214 00:37:30 @agent_ppo2.py:121][0m #------------------------ Iteration 5754 --------------------------#
[32m[20221214 00:37:30 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:37:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:37:30 @agent_ppo2.py:185][0m |           0.0000 |         139.8278 |        -162.8914 |
[32m[20221214 00:37:30 @agent_ppo2.py:185][0m |          -0.0044 |         132.1862 |        -161.4876 |
[32m[20221214 00:37:30 @agent_ppo2.py:185][0m |          -0.0095 |         128.8333 |        -163.0982 |
[32m[20221214 00:37:31 @agent_ppo2.py:185][0m |          -0.0135 |         127.9275 |        -162.9755 |
[32m[20221214 00:37:31 @agent_ppo2.py:185][0m |          -0.0143 |         126.0261 |        -163.4915 |
[32m[20221214 00:37:31 @agent_ppo2.py:185][0m |          -0.0150 |         124.3412 |        -163.9357 |
[32m[20221214 00:37:31 @agent_ppo2.py:185][0m |          -0.0172 |         123.2804 |        -163.7243 |
[32m[20221214 00:37:31 @agent_ppo2.py:185][0m |          -0.0183 |         122.8012 |        -163.2516 |
[32m[20221214 00:37:31 @agent_ppo2.py:185][0m |          -0.0185 |         121.0401 |        -164.2778 |
[32m[20221214 00:37:31 @agent_ppo2.py:185][0m |          -0.0163 |         121.0451 |        -164.1143 |
[32m[20221214 00:37:31 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:37:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 510.16
[32m[20221214 00:37:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 643.46
[32m[20221214 00:37:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 840.28
[32m[20221214 00:37:31 @agent_ppo2.py:143][0m Total time:      40.02 min
[32m[20221214 00:37:31 @agent_ppo2.py:145][0m 3594240 total steps have happened
[32m[20221214 00:37:31 @agent_ppo2.py:121][0m #------------------------ Iteration 5755 --------------------------#
[32m[20221214 00:37:31 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:37:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:37:32 @agent_ppo2.py:185][0m |           0.0022 |         154.1347 |        -160.1036 |
[32m[20221214 00:37:32 @agent_ppo2.py:185][0m |          -0.0011 |         150.7712 |        -160.5286 |
[32m[20221214 00:37:32 @agent_ppo2.py:185][0m |          -0.0057 |         149.6769 |        -160.6246 |
[32m[20221214 00:37:32 @agent_ppo2.py:185][0m |          -0.0096 |         148.4583 |        -160.8208 |
[32m[20221214 00:37:32 @agent_ppo2.py:185][0m |          -0.0108 |         148.3465 |        -160.5495 |
[32m[20221214 00:37:32 @agent_ppo2.py:185][0m |          -0.0084 |         147.8820 |        -160.4847 |
[32m[20221214 00:37:32 @agent_ppo2.py:185][0m |           0.0005 |         152.2920 |        -160.1163 |
[32m[20221214 00:37:32 @agent_ppo2.py:185][0m |          -0.0107 |         147.6624 |        -160.8436 |
[32m[20221214 00:37:32 @agent_ppo2.py:185][0m |          -0.0029 |         150.3666 |        -160.1879 |
[32m[20221214 00:37:33 @agent_ppo2.py:185][0m |          -0.0076 |         148.5990 |        -160.2725 |
[32m[20221214 00:37:33 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 00:37:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 662.90
[32m[20221214 00:37:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 833.67
[32m[20221214 00:37:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 687.60
[32m[20221214 00:37:33 @agent_ppo2.py:143][0m Total time:      40.04 min
[32m[20221214 00:37:33 @agent_ppo2.py:145][0m 3596288 total steps have happened
[32m[20221214 00:37:33 @agent_ppo2.py:121][0m #------------------------ Iteration 5756 --------------------------#
[32m[20221214 00:37:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:37:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:37:33 @agent_ppo2.py:185][0m |           0.0050 |         160.4615 |        -168.5981 |
[32m[20221214 00:37:33 @agent_ppo2.py:185][0m |           0.0128 |         171.3416 |        -168.7860 |
[32m[20221214 00:37:33 @agent_ppo2.py:185][0m |          -0.0007 |         157.6178 |        -168.1316 |
[32m[20221214 00:37:33 @agent_ppo2.py:185][0m |          -0.0018 |         155.5405 |        -168.7467 |
[32m[20221214 00:37:34 @agent_ppo2.py:185][0m |          -0.0020 |         154.9803 |        -169.1718 |
[32m[20221214 00:37:34 @agent_ppo2.py:185][0m |          -0.0014 |         153.8268 |        -168.2237 |
[32m[20221214 00:37:34 @agent_ppo2.py:185][0m |          -0.0056 |         153.4645 |        -169.8765 |
[32m[20221214 00:37:34 @agent_ppo2.py:185][0m |          -0.0060 |         152.3208 |        -169.8440 |
[32m[20221214 00:37:34 @agent_ppo2.py:185][0m |          -0.0047 |         152.0280 |        -168.9597 |
[32m[20221214 00:37:34 @agent_ppo2.py:185][0m |          -0.0012 |         152.0637 |        -169.2459 |
[32m[20221214 00:37:34 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 00:37:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 688.87
[32m[20221214 00:37:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 828.32
[32m[20221214 00:37:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 784.89
[32m[20221214 00:37:34 @agent_ppo2.py:143][0m Total time:      40.06 min
[32m[20221214 00:37:34 @agent_ppo2.py:145][0m 3598336 total steps have happened
[32m[20221214 00:37:34 @agent_ppo2.py:121][0m #------------------------ Iteration 5757 --------------------------#
[32m[20221214 00:37:34 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:37:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:37:35 @agent_ppo2.py:185][0m |           0.0017 |          37.3161 |        -166.5631 |
[32m[20221214 00:37:35 @agent_ppo2.py:185][0m |          -0.0023 |          31.2757 |        -166.5807 |
[32m[20221214 00:37:35 @agent_ppo2.py:185][0m |           0.0042 |          33.2127 |        -166.8895 |
[32m[20221214 00:37:35 @agent_ppo2.py:185][0m |          -0.0024 |          30.9779 |        -166.9087 |
[32m[20221214 00:37:35 @agent_ppo2.py:185][0m |           0.0026 |          30.6237 |        -166.6960 |
[32m[20221214 00:37:35 @agent_ppo2.py:185][0m |          -0.0044 |          29.9978 |        -165.4068 |
[32m[20221214 00:37:35 @agent_ppo2.py:185][0m |          -0.0057 |          29.9629 |        -165.6467 |
[32m[20221214 00:37:35 @agent_ppo2.py:185][0m |           0.0035 |          31.0494 |        -166.0522 |
[32m[20221214 00:37:35 @agent_ppo2.py:185][0m |          -0.0058 |          29.8028 |        -166.2191 |
[32m[20221214 00:37:35 @agent_ppo2.py:185][0m |          -0.0085 |          29.5901 |        -166.4438 |
[32m[20221214 00:37:35 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:37:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221214 00:37:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221214 00:37:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 456.47
[32m[20221214 00:37:36 @agent_ppo2.py:143][0m Total time:      40.09 min
[32m[20221214 00:37:36 @agent_ppo2.py:145][0m 3600384 total steps have happened
[32m[20221214 00:37:36 @agent_ppo2.py:121][0m #------------------------ Iteration 5758 --------------------------#
[32m[20221214 00:37:36 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:37:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:37:36 @agent_ppo2.py:185][0m |           0.0034 |         141.1285 |        -170.1806 |
[32m[20221214 00:37:36 @agent_ppo2.py:185][0m |           0.0114 |         142.6125 |        -170.5512 |
[32m[20221214 00:37:36 @agent_ppo2.py:185][0m |          -0.0031 |         134.1687 |        -169.9893 |
[32m[20221214 00:37:36 @agent_ppo2.py:185][0m |          -0.0044 |         131.5854 |        -170.8920 |
[32m[20221214 00:37:36 @agent_ppo2.py:185][0m |          -0.0032 |         129.4024 |        -170.0614 |
[32m[20221214 00:37:36 @agent_ppo2.py:185][0m |           0.0085 |         140.0992 |        -171.2664 |
[32m[20221214 00:37:37 @agent_ppo2.py:185][0m |          -0.0093 |         132.1307 |        -170.3828 |
[32m[20221214 00:37:37 @agent_ppo2.py:185][0m |          -0.0072 |         129.3840 |        -171.1433 |
[32m[20221214 00:37:37 @agent_ppo2.py:185][0m |          -0.0072 |         128.2612 |        -171.3968 |
[32m[20221214 00:37:37 @agent_ppo2.py:185][0m |           0.0008 |         133.7263 |        -171.3793 |
[32m[20221214 00:37:37 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:37:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 688.37
[32m[20221214 00:37:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 804.90
[32m[20221214 00:37:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 581.79
[32m[20221214 00:37:37 @agent_ppo2.py:143][0m Total time:      40.11 min
[32m[20221214 00:37:37 @agent_ppo2.py:145][0m 3602432 total steps have happened
[32m[20221214 00:37:37 @agent_ppo2.py:121][0m #------------------------ Iteration 5759 --------------------------#
[32m[20221214 00:37:37 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:37:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:37:37 @agent_ppo2.py:185][0m |           0.0104 |         151.7126 |        -166.4103 |
[32m[20221214 00:37:37 @agent_ppo2.py:185][0m |           0.0013 |         134.1696 |        -166.6782 |
[32m[20221214 00:37:38 @agent_ppo2.py:185][0m |          -0.0067 |         130.0395 |        -165.7521 |
[32m[20221214 00:37:38 @agent_ppo2.py:185][0m |          -0.0082 |         128.6453 |        -165.1162 |
[32m[20221214 00:37:38 @agent_ppo2.py:185][0m |          -0.0085 |         127.2271 |        -164.9017 |
[32m[20221214 00:37:38 @agent_ppo2.py:185][0m |          -0.0084 |         126.6269 |        -164.9834 |
[32m[20221214 00:37:38 @agent_ppo2.py:185][0m |          -0.0144 |         125.5944 |        -164.0294 |
[32m[20221214 00:37:38 @agent_ppo2.py:185][0m |          -0.0076 |         124.8325 |        -164.1365 |
[32m[20221214 00:37:38 @agent_ppo2.py:185][0m |          -0.0076 |         124.5221 |        -162.9911 |
[32m[20221214 00:37:38 @agent_ppo2.py:185][0m |          -0.0110 |         123.7303 |        -162.9647 |
[32m[20221214 00:37:38 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:37:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 673.60
[32m[20221214 00:37:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 823.81
[32m[20221214 00:37:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 845.25
[32m[20221214 00:37:38 @agent_ppo2.py:143][0m Total time:      40.13 min
[32m[20221214 00:37:38 @agent_ppo2.py:145][0m 3604480 total steps have happened
[32m[20221214 00:37:38 @agent_ppo2.py:121][0m #------------------------ Iteration 5760 --------------------------#
[32m[20221214 00:37:39 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:37:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:37:39 @agent_ppo2.py:185][0m |           0.0031 |         140.9970 |        -164.5357 |
[32m[20221214 00:37:39 @agent_ppo2.py:185][0m |          -0.0070 |         131.9603 |        -164.2169 |
[32m[20221214 00:37:39 @agent_ppo2.py:185][0m |          -0.0025 |         127.6749 |        -164.1414 |
[32m[20221214 00:37:39 @agent_ppo2.py:185][0m |          -0.0072 |         125.5734 |        -163.9885 |
[32m[20221214 00:37:39 @agent_ppo2.py:185][0m |           0.0025 |         144.7517 |        -164.2623 |
[32m[20221214 00:37:39 @agent_ppo2.py:185][0m |          -0.0064 |         124.3594 |        -163.8918 |
[32m[20221214 00:37:39 @agent_ppo2.py:185][0m |          -0.0110 |         122.0269 |        -163.9944 |
[32m[20221214 00:37:39 @agent_ppo2.py:185][0m |          -0.0086 |         121.6954 |        -164.1092 |
[32m[20221214 00:37:40 @agent_ppo2.py:185][0m |          -0.0092 |         121.1147 |        -164.1293 |
[32m[20221214 00:37:40 @agent_ppo2.py:185][0m |          -0.0082 |         121.0197 |        -163.7308 |
[32m[20221214 00:37:40 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:37:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 678.38
[32m[20221214 00:37:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.07
[32m[20221214 00:37:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 680.04
[32m[20221214 00:37:40 @agent_ppo2.py:143][0m Total time:      40.16 min
[32m[20221214 00:37:40 @agent_ppo2.py:145][0m 3606528 total steps have happened
[32m[20221214 00:37:40 @agent_ppo2.py:121][0m #------------------------ Iteration 5761 --------------------------#
[32m[20221214 00:37:40 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:37:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:37:40 @agent_ppo2.py:185][0m |           0.0028 |         120.3661 |        -161.7277 |
[32m[20221214 00:37:40 @agent_ppo2.py:185][0m |           0.0026 |         103.1529 |        -162.2323 |
[32m[20221214 00:37:40 @agent_ppo2.py:185][0m |           0.0001 |         101.6105 |        -162.0607 |
[32m[20221214 00:37:40 @agent_ppo2.py:185][0m |           0.0024 |         105.9115 |        -161.7336 |
[32m[20221214 00:37:41 @agent_ppo2.py:185][0m |          -0.0023 |          97.6945 |        -161.7900 |
[32m[20221214 00:37:41 @agent_ppo2.py:185][0m |          -0.0055 |          96.4373 |        -161.9871 |
[32m[20221214 00:37:41 @agent_ppo2.py:185][0m |          -0.0027 |         107.0782 |        -162.1211 |
[32m[20221214 00:37:41 @agent_ppo2.py:185][0m |          -0.0033 |          98.4054 |        -161.1254 |
[32m[20221214 00:37:41 @agent_ppo2.py:185][0m |           0.0058 |         111.9073 |        -160.8632 |
[32m[20221214 00:37:41 @agent_ppo2.py:185][0m |          -0.0040 |          97.6810 |        -160.9016 |
[32m[20221214 00:37:41 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:37:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 685.31
[32m[20221214 00:37:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 781.49
[32m[20221214 00:37:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 844.65
[32m[20221214 00:37:41 @agent_ppo2.py:143][0m Total time:      40.18 min
[32m[20221214 00:37:41 @agent_ppo2.py:145][0m 3608576 total steps have happened
[32m[20221214 00:37:41 @agent_ppo2.py:121][0m #------------------------ Iteration 5762 --------------------------#
[32m[20221214 00:37:41 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:37:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:37:42 @agent_ppo2.py:185][0m |           0.0007 |         142.8148 |        -165.1633 |
[32m[20221214 00:37:42 @agent_ppo2.py:185][0m |           0.0006 |         140.4184 |        -166.1195 |
[32m[20221214 00:37:42 @agent_ppo2.py:185][0m |          -0.0013 |         138.4050 |        -164.4256 |
[32m[20221214 00:37:42 @agent_ppo2.py:185][0m |           0.0034 |         145.3450 |        -165.3585 |
[32m[20221214 00:37:42 @agent_ppo2.py:185][0m |          -0.0061 |         138.1413 |        -163.6835 |
[32m[20221214 00:37:42 @agent_ppo2.py:185][0m |          -0.0016 |         139.2435 |        -164.4853 |
[32m[20221214 00:37:42 @agent_ppo2.py:185][0m |           0.0021 |         138.2739 |        -164.4012 |
[32m[20221214 00:37:42 @agent_ppo2.py:185][0m |          -0.0057 |         136.2457 |        -163.3054 |
[32m[20221214 00:37:43 @agent_ppo2.py:185][0m |          -0.0077 |         135.7428 |        -164.0686 |
[32m[20221214 00:37:43 @agent_ppo2.py:185][0m |           0.0019 |         136.5606 |        -163.9133 |
[32m[20221214 00:37:43 @agent_ppo2.py:130][0m Policy update time: 1.37 s
[32m[20221214 00:37:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 740.73
[32m[20221214 00:37:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 832.50
[32m[20221214 00:37:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 850.95
[32m[20221214 00:37:43 @agent_ppo2.py:143][0m Total time:      40.21 min
[32m[20221214 00:37:43 @agent_ppo2.py:145][0m 3610624 total steps have happened
[32m[20221214 00:37:43 @agent_ppo2.py:121][0m #------------------------ Iteration 5763 --------------------------#
[32m[20221214 00:37:43 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:37:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:37:43 @agent_ppo2.py:185][0m |           0.0067 |         112.6604 |        -161.6855 |
[32m[20221214 00:37:44 @agent_ppo2.py:185][0m |          -0.0006 |         102.6161 |        -161.5859 |
[32m[20221214 00:37:44 @agent_ppo2.py:185][0m |           0.0005 |         102.7171 |        -160.9808 |
[32m[20221214 00:37:44 @agent_ppo2.py:185][0m |          -0.0075 |          98.1730 |        -161.7339 |
[32m[20221214 00:37:44 @agent_ppo2.py:185][0m |          -0.0126 |          97.7670 |        -161.1898 |
[32m[20221214 00:37:44 @agent_ppo2.py:185][0m |          -0.0118 |          96.6359 |        -161.2669 |
[32m[20221214 00:37:44 @agent_ppo2.py:185][0m |          -0.0219 |          96.0525 |        -160.5739 |
[32m[20221214 00:37:44 @agent_ppo2.py:185][0m |          -0.0157 |          94.8323 |        -161.4098 |
[32m[20221214 00:37:44 @agent_ppo2.py:185][0m |          -0.0108 |          96.4171 |        -161.7022 |
[32m[20221214 00:37:44 @agent_ppo2.py:185][0m |          -0.0147 |          94.0414 |        -160.9199 |
[32m[20221214 00:37:44 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:37:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 427.24
[32m[20221214 00:37:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 512.06
[32m[20221214 00:37:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 836.81
[32m[20221214 00:37:45 @agent_ppo2.py:143][0m Total time:      40.24 min
[32m[20221214 00:37:45 @agent_ppo2.py:145][0m 3612672 total steps have happened
[32m[20221214 00:37:45 @agent_ppo2.py:121][0m #------------------------ Iteration 5764 --------------------------#
[32m[20221214 00:37:45 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:37:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:37:45 @agent_ppo2.py:185][0m |          -0.0002 |          61.6434 |        -157.1532 |
[32m[20221214 00:37:45 @agent_ppo2.py:185][0m |          -0.0023 |          52.4209 |        -157.8435 |
[32m[20221214 00:37:45 @agent_ppo2.py:185][0m |          -0.0021 |          47.6705 |        -157.7525 |
[32m[20221214 00:37:45 @agent_ppo2.py:185][0m |          -0.0007 |          48.8714 |        -157.7434 |
[32m[20221214 00:37:45 @agent_ppo2.py:185][0m |          -0.0091 |          43.3716 |        -157.7435 |
[32m[20221214 00:37:45 @agent_ppo2.py:185][0m |          -0.0145 |          40.0594 |        -157.9482 |
[32m[20221214 00:37:45 @agent_ppo2.py:185][0m |          -0.0014 |          40.7227 |        -158.5784 |
[32m[20221214 00:37:46 @agent_ppo2.py:185][0m |           0.0009 |          42.9080 |        -158.6450 |
[32m[20221214 00:37:46 @agent_ppo2.py:185][0m |          -0.0123 |          38.4436 |        -158.7638 |
[32m[20221214 00:37:46 @agent_ppo2.py:185][0m |          -0.0100 |          37.2018 |        -158.3769 |
[32m[20221214 00:37:46 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:37:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 699.28
[32m[20221214 00:37:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 746.98
[32m[20221214 00:37:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 623.17
[32m[20221214 00:37:46 @agent_ppo2.py:143][0m Total time:      40.26 min
[32m[20221214 00:37:46 @agent_ppo2.py:145][0m 3614720 total steps have happened
[32m[20221214 00:37:46 @agent_ppo2.py:121][0m #------------------------ Iteration 5765 --------------------------#
[32m[20221214 00:37:46 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:37:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:37:46 @agent_ppo2.py:185][0m |           0.0000 |         126.2155 |        -171.5748 |
[32m[20221214 00:37:46 @agent_ppo2.py:185][0m |           0.0020 |         122.6008 |        -170.6994 |
[32m[20221214 00:37:46 @agent_ppo2.py:185][0m |          -0.0069 |         120.0292 |        -171.3807 |
[32m[20221214 00:37:47 @agent_ppo2.py:185][0m |           0.0037 |         126.3428 |        -169.6904 |
[32m[20221214 00:37:47 @agent_ppo2.py:185][0m |          -0.0033 |         118.8085 |        -170.4262 |
[32m[20221214 00:37:47 @agent_ppo2.py:185][0m |          -0.0076 |         117.3200 |        -170.2691 |
[32m[20221214 00:37:47 @agent_ppo2.py:185][0m |          -0.0122 |         117.2416 |        -169.9754 |
[32m[20221214 00:37:47 @agent_ppo2.py:185][0m |          -0.0123 |         116.0046 |        -170.1564 |
[32m[20221214 00:37:47 @agent_ppo2.py:185][0m |          -0.0030 |         120.7611 |        -169.3049 |
[32m[20221214 00:37:47 @agent_ppo2.py:185][0m |          -0.0110 |         116.0141 |        -169.2293 |
[32m[20221214 00:37:47 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:37:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 680.60
[32m[20221214 00:37:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.62
[32m[20221214 00:37:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 593.59
[32m[20221214 00:37:47 @agent_ppo2.py:143][0m Total time:      40.28 min
[32m[20221214 00:37:47 @agent_ppo2.py:145][0m 3616768 total steps have happened
[32m[20221214 00:37:47 @agent_ppo2.py:121][0m #------------------------ Iteration 5766 --------------------------#
[32m[20221214 00:37:47 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:37:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:37:48 @agent_ppo2.py:185][0m |           0.0027 |          95.4112 |        -162.8135 |
[32m[20221214 00:37:48 @agent_ppo2.py:185][0m |          -0.0031 |          86.6944 |        -162.4059 |
[32m[20221214 00:37:48 @agent_ppo2.py:185][0m |          -0.0034 |          84.6883 |        -161.9164 |
[32m[20221214 00:37:48 @agent_ppo2.py:185][0m |          -0.0054 |          83.7471 |        -162.1519 |
[32m[20221214 00:37:48 @agent_ppo2.py:185][0m |          -0.0058 |          83.7120 |        -161.5369 |
[32m[20221214 00:37:48 @agent_ppo2.py:185][0m |          -0.0052 |          81.8749 |        -161.9051 |
[32m[20221214 00:37:48 @agent_ppo2.py:185][0m |          -0.0047 |          80.8522 |        -161.1212 |
[32m[20221214 00:37:48 @agent_ppo2.py:185][0m |           0.0075 |          89.2067 |        -160.7756 |
[32m[20221214 00:37:49 @agent_ppo2.py:185][0m |          -0.0126 |          80.5180 |        -160.6834 |
[32m[20221214 00:37:49 @agent_ppo2.py:185][0m |          -0.0112 |          80.2186 |        -161.1216 |
[32m[20221214 00:37:49 @agent_ppo2.py:130][0m Policy update time: 1.19 s
[32m[20221214 00:37:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 690.71
[32m[20221214 00:37:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 801.13
[32m[20221214 00:37:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 747.61
[32m[20221214 00:37:49 @agent_ppo2.py:143][0m Total time:      40.31 min
[32m[20221214 00:37:49 @agent_ppo2.py:145][0m 3618816 total steps have happened
[32m[20221214 00:37:49 @agent_ppo2.py:121][0m #------------------------ Iteration 5767 --------------------------#
[32m[20221214 00:37:49 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:37:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:37:49 @agent_ppo2.py:185][0m |           0.0034 |         122.1392 |        -156.3925 |
[32m[20221214 00:37:49 @agent_ppo2.py:185][0m |          -0.0067 |         111.0512 |        -155.2626 |
[32m[20221214 00:37:49 @agent_ppo2.py:185][0m |          -0.0007 |         110.5341 |        -156.0807 |
[32m[20221214 00:37:49 @agent_ppo2.py:185][0m |          -0.0075 |         105.9698 |        -155.6776 |
[32m[20221214 00:37:50 @agent_ppo2.py:185][0m |          -0.0101 |         105.1594 |        -155.5157 |
[32m[20221214 00:37:50 @agent_ppo2.py:185][0m |          -0.0126 |         103.6854 |        -155.3714 |
[32m[20221214 00:37:50 @agent_ppo2.py:185][0m |           0.0065 |         123.3525 |        -156.0275 |
[32m[20221214 00:37:50 @agent_ppo2.py:185][0m |          -0.0114 |         103.6078 |        -156.3211 |
[32m[20221214 00:37:50 @agent_ppo2.py:185][0m |          -0.0121 |         101.6119 |        -157.1174 |
[32m[20221214 00:37:50 @agent_ppo2.py:185][0m |          -0.0094 |         101.0579 |        -156.0117 |
[32m[20221214 00:37:50 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:37:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 691.30
[32m[20221214 00:37:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.02
[32m[20221214 00:37:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 335.59
[32m[20221214 00:37:50 @agent_ppo2.py:143][0m Total time:      40.33 min
[32m[20221214 00:37:50 @agent_ppo2.py:145][0m 3620864 total steps have happened
[32m[20221214 00:37:50 @agent_ppo2.py:121][0m #------------------------ Iteration 5768 --------------------------#
[32m[20221214 00:37:50 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:37:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:37:51 @agent_ppo2.py:185][0m |           0.0046 |         104.4053 |        -169.2331 |
[32m[20221214 00:37:51 @agent_ppo2.py:185][0m |          -0.0038 |          93.0524 |        -167.7704 |
[32m[20221214 00:37:51 @agent_ppo2.py:185][0m |          -0.0045 |          91.7506 |        -167.6759 |
[32m[20221214 00:37:51 @agent_ppo2.py:185][0m |          -0.0030 |          90.2628 |        -167.5231 |
[32m[20221214 00:37:51 @agent_ppo2.py:185][0m |          -0.0097 |          89.1532 |        -167.5099 |
[32m[20221214 00:37:51 @agent_ppo2.py:185][0m |          -0.0081 |          86.7892 |        -166.1762 |
[32m[20221214 00:37:51 @agent_ppo2.py:185][0m |          -0.0147 |          85.4500 |        -167.7859 |
[32m[20221214 00:37:51 @agent_ppo2.py:185][0m |          -0.0093 |          87.0135 |        -166.5826 |
[32m[20221214 00:37:51 @agent_ppo2.py:185][0m |          -0.0141 |          85.4398 |        -167.4468 |
[32m[20221214 00:37:51 @agent_ppo2.py:185][0m |          -0.0125 |          86.6939 |        -166.4844 |
[32m[20221214 00:37:51 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:37:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 477.87
[32m[20221214 00:37:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 600.07
[32m[20221214 00:37:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 436.48
[32m[20221214 00:37:52 @agent_ppo2.py:143][0m Total time:      40.35 min
[32m[20221214 00:37:52 @agent_ppo2.py:145][0m 3622912 total steps have happened
[32m[20221214 00:37:52 @agent_ppo2.py:121][0m #------------------------ Iteration 5769 --------------------------#
[32m[20221214 00:37:52 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:37:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:37:52 @agent_ppo2.py:185][0m |           0.0037 |         137.5955 |        -170.4918 |
[32m[20221214 00:37:52 @agent_ppo2.py:185][0m |           0.0017 |         131.7422 |        -169.9015 |
[32m[20221214 00:37:52 @agent_ppo2.py:185][0m |           0.0014 |         129.1485 |        -170.0486 |
[32m[20221214 00:37:52 @agent_ppo2.py:185][0m |           0.0011 |         129.3256 |        -170.5531 |
[32m[20221214 00:37:53 @agent_ppo2.py:185][0m |          -0.0011 |         127.9795 |        -170.8185 |
[32m[20221214 00:37:53 @agent_ppo2.py:185][0m |          -0.0066 |         125.4308 |        -171.0345 |
[32m[20221214 00:37:53 @agent_ppo2.py:185][0m |          -0.0052 |         126.1462 |        -171.3021 |
[32m[20221214 00:37:53 @agent_ppo2.py:185][0m |           0.0078 |         133.7432 |        -171.3678 |
[32m[20221214 00:37:53 @agent_ppo2.py:185][0m |          -0.0058 |         126.3824 |        -170.9632 |
[32m[20221214 00:37:53 @agent_ppo2.py:185][0m |          -0.0099 |         124.7186 |        -172.6236 |
[32m[20221214 00:37:53 @agent_ppo2.py:130][0m Policy update time: 1.27 s
[32m[20221214 00:37:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 722.86
[32m[20221214 00:37:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 813.53
[32m[20221214 00:37:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 364.51
[32m[20221214 00:37:53 @agent_ppo2.py:143][0m Total time:      40.38 min
[32m[20221214 00:37:53 @agent_ppo2.py:145][0m 3624960 total steps have happened
[32m[20221214 00:37:53 @agent_ppo2.py:121][0m #------------------------ Iteration 5770 --------------------------#
[32m[20221214 00:37:53 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:37:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:37:54 @agent_ppo2.py:185][0m |          -0.0006 |          97.5503 |        -167.0244 |
[32m[20221214 00:37:54 @agent_ppo2.py:185][0m |          -0.0122 |          88.8601 |        -167.5599 |
[32m[20221214 00:37:54 @agent_ppo2.py:185][0m |          -0.0047 |          86.1566 |        -167.3174 |
[32m[20221214 00:37:54 @agent_ppo2.py:185][0m |          -0.0042 |          86.9943 |        -166.4127 |
[32m[20221214 00:37:54 @agent_ppo2.py:185][0m |          -0.0074 |          84.2899 |        -166.7715 |
[32m[20221214 00:37:54 @agent_ppo2.py:185][0m |          -0.0101 |          83.6271 |        -166.9825 |
[32m[20221214 00:37:54 @agent_ppo2.py:185][0m |          -0.0159 |          83.4353 |        -167.5074 |
[32m[20221214 00:37:54 @agent_ppo2.py:185][0m |          -0.0165 |          82.5016 |        -166.7418 |
[32m[20221214 00:37:54 @agent_ppo2.py:185][0m |          -0.0141 |          82.0653 |        -166.8395 |
[32m[20221214 00:37:55 @agent_ppo2.py:185][0m |           0.0015 |          94.1284 |        -166.7841 |
[32m[20221214 00:37:55 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 00:37:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 516.67
[32m[20221214 00:37:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 583.59
[32m[20221214 00:37:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 765.64
[32m[20221214 00:37:55 @agent_ppo2.py:143][0m Total time:      40.41 min
[32m[20221214 00:37:55 @agent_ppo2.py:145][0m 3627008 total steps have happened
[32m[20221214 00:37:55 @agent_ppo2.py:121][0m #------------------------ Iteration 5771 --------------------------#
[32m[20221214 00:37:55 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:37:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:37:55 @agent_ppo2.py:185][0m |           0.0015 |         182.8054 |        -166.3341 |
[32m[20221214 00:37:55 @agent_ppo2.py:185][0m |          -0.0019 |         180.9613 |        -166.6733 |
[32m[20221214 00:37:55 @agent_ppo2.py:185][0m |           0.0038 |         182.9163 |        -165.0539 |
[32m[20221214 00:37:55 @agent_ppo2.py:185][0m |           0.0057 |         188.4824 |        -165.4694 |
[32m[20221214 00:37:56 @agent_ppo2.py:185][0m |          -0.0014 |         179.7799 |        -164.9885 |
[32m[20221214 00:37:56 @agent_ppo2.py:185][0m |          -0.0009 |         179.3217 |        -165.7505 |
[32m[20221214 00:37:56 @agent_ppo2.py:185][0m |          -0.0040 |         177.7542 |        -164.5348 |
[32m[20221214 00:37:56 @agent_ppo2.py:185][0m |          -0.0024 |         179.1254 |        -166.1372 |
[32m[20221214 00:37:56 @agent_ppo2.py:185][0m |          -0.0049 |         178.2952 |        -164.3466 |
[32m[20221214 00:37:56 @agent_ppo2.py:185][0m |          -0.0036 |         178.0941 |        -164.2467 |
[32m[20221214 00:37:56 @agent_ppo2.py:130][0m Policy update time: 1.24 s
[32m[20221214 00:37:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 725.71
[32m[20221214 00:37:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 833.58
[32m[20221214 00:37:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 680.68
[32m[20221214 00:37:56 @agent_ppo2.py:143][0m Total time:      40.43 min
[32m[20221214 00:37:56 @agent_ppo2.py:145][0m 3629056 total steps have happened
[32m[20221214 00:37:56 @agent_ppo2.py:121][0m #------------------------ Iteration 5772 --------------------------#
[32m[20221214 00:37:56 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 00:37:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:37:57 @agent_ppo2.py:185][0m |           0.0051 |          84.0497 |        -158.5079 |
[32m[20221214 00:37:57 @agent_ppo2.py:185][0m |          -0.0060 |          75.2948 |        -158.1295 |
[32m[20221214 00:37:57 @agent_ppo2.py:185][0m |          -0.0060 |          73.4845 |        -158.0207 |
[32m[20221214 00:37:57 @agent_ppo2.py:185][0m |          -0.0141 |          72.1803 |        -158.4946 |
[32m[20221214 00:37:57 @agent_ppo2.py:185][0m |          -0.0133 |          71.3058 |        -157.7630 |
[32m[20221214 00:37:57 @agent_ppo2.py:185][0m |          -0.0142 |          70.7479 |        -158.5713 |
[32m[20221214 00:37:57 @agent_ppo2.py:185][0m |          -0.0094 |          70.3944 |        -158.4519 |
[32m[20221214 00:37:57 @agent_ppo2.py:185][0m |          -0.0147 |          69.7933 |        -158.9086 |
[32m[20221214 00:37:57 @agent_ppo2.py:185][0m |          -0.0153 |          69.1926 |        -158.5625 |
[32m[20221214 00:37:58 @agent_ppo2.py:185][0m |          -0.0153 |          69.0302 |        -159.1248 |
[32m[20221214 00:37:58 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:37:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 454.23
[32m[20221214 00:37:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 557.59
[32m[20221214 00:37:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 712.09
[32m[20221214 00:37:58 @agent_ppo2.py:143][0m Total time:      40.46 min
[32m[20221214 00:37:58 @agent_ppo2.py:145][0m 3631104 total steps have happened
[32m[20221214 00:37:58 @agent_ppo2.py:121][0m #------------------------ Iteration 5773 --------------------------#
[32m[20221214 00:37:58 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:37:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:37:58 @agent_ppo2.py:185][0m |           0.0051 |         146.9102 |        -168.0215 |
[32m[20221214 00:37:58 @agent_ppo2.py:185][0m |          -0.0009 |         141.1570 |        -166.9515 |
[32m[20221214 00:37:58 @agent_ppo2.py:185][0m |          -0.0010 |         138.0293 |        -166.9203 |
[32m[20221214 00:37:58 @agent_ppo2.py:185][0m |          -0.0015 |         135.8989 |        -167.0432 |
[32m[20221214 00:37:58 @agent_ppo2.py:185][0m |           0.0025 |         136.4879 |        -166.3129 |
[32m[20221214 00:37:59 @agent_ppo2.py:185][0m |          -0.0062 |         134.0240 |        -166.3601 |
[32m[20221214 00:37:59 @agent_ppo2.py:185][0m |          -0.0032 |         132.8979 |        -166.8465 |
[32m[20221214 00:37:59 @agent_ppo2.py:185][0m |          -0.0073 |         132.9736 |        -165.8612 |
[32m[20221214 00:37:59 @agent_ppo2.py:185][0m |          -0.0107 |         132.2835 |        -166.7508 |
[32m[20221214 00:37:59 @agent_ppo2.py:185][0m |          -0.0072 |         132.1950 |        -166.9129 |
[32m[20221214 00:37:59 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:37:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 663.14
[32m[20221214 00:37:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 772.29
[32m[20221214 00:37:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 805.22
[32m[20221214 00:37:59 @agent_ppo2.py:143][0m Total time:      40.48 min
[32m[20221214 00:37:59 @agent_ppo2.py:145][0m 3633152 total steps have happened
[32m[20221214 00:37:59 @agent_ppo2.py:121][0m #------------------------ Iteration 5774 --------------------------#
[32m[20221214 00:37:59 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:37:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:37:59 @agent_ppo2.py:185][0m |           0.0030 |          89.5407 |        -166.8243 |
[32m[20221214 00:38:00 @agent_ppo2.py:185][0m |          -0.0041 |          80.5933 |        -165.3244 |
[32m[20221214 00:38:00 @agent_ppo2.py:185][0m |          -0.0063 |          78.9080 |        -165.1570 |
[32m[20221214 00:38:00 @agent_ppo2.py:185][0m |          -0.0090 |          75.5844 |        -165.6931 |
[32m[20221214 00:38:00 @agent_ppo2.py:185][0m |          -0.0079 |          73.4446 |        -164.5089 |
[32m[20221214 00:38:00 @agent_ppo2.py:185][0m |          -0.0094 |          72.9564 |        -164.8835 |
[32m[20221214 00:38:00 @agent_ppo2.py:185][0m |          -0.0075 |          72.5997 |        -165.3467 |
[32m[20221214 00:38:00 @agent_ppo2.py:185][0m |          -0.0095 |          72.1926 |        -165.2263 |
[32m[20221214 00:38:00 @agent_ppo2.py:185][0m |          -0.0106 |          71.0246 |        -164.3999 |
[32m[20221214 00:38:00 @agent_ppo2.py:185][0m |          -0.0106 |          70.7420 |        -165.1409 |
[32m[20221214 00:38:00 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:38:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 713.01
[32m[20221214 00:38:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 793.31
[32m[20221214 00:38:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 805.42
[32m[20221214 00:38:01 @agent_ppo2.py:143][0m Total time:      40.50 min
[32m[20221214 00:38:01 @agent_ppo2.py:145][0m 3635200 total steps have happened
[32m[20221214 00:38:01 @agent_ppo2.py:121][0m #------------------------ Iteration 5775 --------------------------#
[32m[20221214 00:38:01 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:38:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:38:01 @agent_ppo2.py:185][0m |           0.0005 |          79.5087 |        -163.0531 |
[32m[20221214 00:38:01 @agent_ppo2.py:185][0m |          -0.0047 |          60.7153 |        -163.0298 |
[32m[20221214 00:38:01 @agent_ppo2.py:185][0m |          -0.0118 |          56.4563 |        -163.0177 |
[32m[20221214 00:38:01 @agent_ppo2.py:185][0m |          -0.0126 |          54.0971 |        -162.2756 |
[32m[20221214 00:38:01 @agent_ppo2.py:185][0m |          -0.0219 |          52.2044 |        -162.6122 |
[32m[20221214 00:38:01 @agent_ppo2.py:185][0m |          -0.0148 |          51.4003 |        -163.2316 |
[32m[20221214 00:38:01 @agent_ppo2.py:185][0m |          -0.0243 |          50.6134 |        -162.2846 |
[32m[20221214 00:38:02 @agent_ppo2.py:185][0m |          -0.0126 |          52.6827 |        -162.6155 |
[32m[20221214 00:38:02 @agent_ppo2.py:185][0m |          -0.0186 |          49.5135 |        -162.5047 |
[32m[20221214 00:38:02 @agent_ppo2.py:185][0m |          -0.0213 |          48.8580 |        -162.2515 |
[32m[20221214 00:38:02 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:38:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 401.66
[32m[20221214 00:38:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 607.04
[32m[20221214 00:38:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 803.81
[32m[20221214 00:38:02 @agent_ppo2.py:143][0m Total time:      40.53 min
[32m[20221214 00:38:02 @agent_ppo2.py:145][0m 3637248 total steps have happened
[32m[20221214 00:38:02 @agent_ppo2.py:121][0m #------------------------ Iteration 5776 --------------------------#
[32m[20221214 00:38:02 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:38:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:38:02 @agent_ppo2.py:185][0m |           0.0183 |         114.8109 |        -158.9800 |
[32m[20221214 00:38:02 @agent_ppo2.py:185][0m |          -0.0001 |          94.7303 |        -158.2541 |
[32m[20221214 00:38:02 @agent_ppo2.py:185][0m |          -0.0042 |          89.2580 |        -158.8900 |
[32m[20221214 00:38:03 @agent_ppo2.py:185][0m |          -0.0116 |          85.7441 |        -158.3383 |
[32m[20221214 00:38:03 @agent_ppo2.py:185][0m |          -0.0152 |          82.8719 |        -157.9098 |
[32m[20221214 00:38:03 @agent_ppo2.py:185][0m |          -0.0154 |          81.0629 |        -158.0308 |
[32m[20221214 00:38:03 @agent_ppo2.py:185][0m |          -0.0182 |          80.4007 |        -157.8339 |
[32m[20221214 00:38:03 @agent_ppo2.py:185][0m |          -0.0089 |          80.5443 |        -157.9824 |
[32m[20221214 00:38:03 @agent_ppo2.py:185][0m |          -0.0185 |          78.2576 |        -157.7723 |
[32m[20221214 00:38:03 @agent_ppo2.py:185][0m |          -0.0148 |          78.0928 |        -157.5739 |
[32m[20221214 00:38:03 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:38:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 467.38
[32m[20221214 00:38:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 603.43
[32m[20221214 00:38:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 754.29
[32m[20221214 00:38:03 @agent_ppo2.py:143][0m Total time:      40.55 min
[32m[20221214 00:38:03 @agent_ppo2.py:145][0m 3639296 total steps have happened
[32m[20221214 00:38:03 @agent_ppo2.py:121][0m #------------------------ Iteration 5777 --------------------------#
[32m[20221214 00:38:04 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:38:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:38:04 @agent_ppo2.py:185][0m |           0.0066 |          81.8907 |        -158.1745 |
[32m[20221214 00:38:04 @agent_ppo2.py:185][0m |           0.0024 |          71.7516 |        -158.2353 |
[32m[20221214 00:38:04 @agent_ppo2.py:185][0m |          -0.0098 |          68.9569 |        -158.5617 |
[32m[20221214 00:38:04 @agent_ppo2.py:185][0m |           0.0020 |          70.9287 |        -158.5753 |
[32m[20221214 00:38:04 @agent_ppo2.py:185][0m |          -0.0101 |          65.6544 |        -158.8219 |
[32m[20221214 00:38:04 @agent_ppo2.py:185][0m |          -0.0117 |          64.3289 |        -159.0665 |
[32m[20221214 00:38:04 @agent_ppo2.py:185][0m |          -0.0173 |          63.8419 |        -159.4670 |
[32m[20221214 00:38:04 @agent_ppo2.py:185][0m |          -0.0167 |          63.3431 |        -159.4454 |
[32m[20221214 00:38:05 @agent_ppo2.py:185][0m |          -0.0083 |          67.3342 |        -159.4172 |
[32m[20221214 00:38:05 @agent_ppo2.py:185][0m |          -0.0142 |          62.6270 |        -159.7714 |
[32m[20221214 00:38:05 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221214 00:38:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 603.60
[32m[20221214 00:38:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 674.21
[32m[20221214 00:38:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 776.51
[32m[20221214 00:38:05 @agent_ppo2.py:143][0m Total time:      40.58 min
[32m[20221214 00:38:05 @agent_ppo2.py:145][0m 3641344 total steps have happened
[32m[20221214 00:38:05 @agent_ppo2.py:121][0m #------------------------ Iteration 5778 --------------------------#
[32m[20221214 00:38:05 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:38:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:38:05 @agent_ppo2.py:185][0m |           0.0157 |         157.3701 |        -156.0008 |
[32m[20221214 00:38:05 @agent_ppo2.py:185][0m |           0.0020 |         125.2977 |        -158.3222 |
[32m[20221214 00:38:05 @agent_ppo2.py:185][0m |           0.0006 |         120.6849 |        -157.5103 |
[32m[20221214 00:38:06 @agent_ppo2.py:185][0m |           0.0022 |         120.2659 |        -157.0350 |
[32m[20221214 00:38:06 @agent_ppo2.py:185][0m |          -0.0020 |         118.5446 |        -158.4577 |
[32m[20221214 00:38:06 @agent_ppo2.py:185][0m |          -0.0001 |         117.5717 |        -156.9092 |
[32m[20221214 00:38:06 @agent_ppo2.py:185][0m |          -0.0008 |         118.0479 |        -158.1361 |
[32m[20221214 00:38:06 @agent_ppo2.py:185][0m |          -0.0046 |         117.1679 |        -158.0120 |
[32m[20221214 00:38:06 @agent_ppo2.py:185][0m |          -0.0065 |         117.0116 |        -158.7475 |
[32m[20221214 00:38:06 @agent_ppo2.py:185][0m |          -0.0060 |         116.5456 |        -157.7263 |
[32m[20221214 00:38:06 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:38:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 774.42
[32m[20221214 00:38:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 833.61
[32m[20221214 00:38:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 348.69
[32m[20221214 00:38:06 @agent_ppo2.py:143][0m Total time:      40.60 min
[32m[20221214 00:38:06 @agent_ppo2.py:145][0m 3643392 total steps have happened
[32m[20221214 00:38:06 @agent_ppo2.py:121][0m #------------------------ Iteration 5779 --------------------------#
[32m[20221214 00:38:06 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:38:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:38:07 @agent_ppo2.py:185][0m |          -0.0005 |          90.6779 |        -160.2713 |
[32m[20221214 00:38:07 @agent_ppo2.py:185][0m |          -0.0120 |          82.8329 |        -160.0209 |
[32m[20221214 00:38:07 @agent_ppo2.py:185][0m |          -0.0102 |          80.1009 |        -159.9706 |
[32m[20221214 00:38:07 @agent_ppo2.py:185][0m |          -0.0116 |          77.9743 |        -159.6239 |
[32m[20221214 00:38:07 @agent_ppo2.py:185][0m |          -0.0174 |          77.2338 |        -160.2643 |
[32m[20221214 00:38:07 @agent_ppo2.py:185][0m |          -0.0047 |          80.4277 |        -159.7525 |
[32m[20221214 00:38:07 @agent_ppo2.py:185][0m |          -0.0170 |          75.8567 |        -160.4513 |
[32m[20221214 00:38:07 @agent_ppo2.py:185][0m |          -0.0178 |          74.9085 |        -160.5237 |
[32m[20221214 00:38:07 @agent_ppo2.py:185][0m |          -0.0230 |          74.2187 |        -160.5761 |
[32m[20221214 00:38:07 @agent_ppo2.py:185][0m |          -0.0187 |          74.1183 |        -161.0731 |
[32m[20221214 00:38:07 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:38:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 572.34
[32m[20221214 00:38:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 625.18
[32m[20221214 00:38:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 740.50
[32m[20221214 00:38:08 @agent_ppo2.py:143][0m Total time:      40.62 min
[32m[20221214 00:38:08 @agent_ppo2.py:145][0m 3645440 total steps have happened
[32m[20221214 00:38:08 @agent_ppo2.py:121][0m #------------------------ Iteration 5780 --------------------------#
[32m[20221214 00:38:08 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:38:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:38:08 @agent_ppo2.py:185][0m |          -0.0027 |         101.0257 |        -169.1080 |
[32m[20221214 00:38:08 @agent_ppo2.py:185][0m |          -0.0107 |          91.4613 |        -168.8284 |
[32m[20221214 00:38:08 @agent_ppo2.py:185][0m |          -0.0066 |          89.2288 |        -168.5561 |
[32m[20221214 00:38:08 @agent_ppo2.py:185][0m |          -0.0095 |          86.9723 |        -167.6115 |
[32m[20221214 00:38:08 @agent_ppo2.py:185][0m |          -0.0148 |          85.8172 |        -168.4800 |
[32m[20221214 00:38:08 @agent_ppo2.py:185][0m |          -0.0151 |          84.5679 |        -169.0975 |
[32m[20221214 00:38:09 @agent_ppo2.py:185][0m |          -0.0144 |          83.6154 |        -168.8622 |
[32m[20221214 00:38:09 @agent_ppo2.py:185][0m |          -0.0161 |          84.8848 |        -168.1522 |
[32m[20221214 00:38:09 @agent_ppo2.py:185][0m |          -0.0132 |          82.3540 |        -168.9534 |
[32m[20221214 00:38:09 @agent_ppo2.py:185][0m |          -0.0157 |          82.1643 |        -168.9547 |
[32m[20221214 00:38:09 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:38:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 710.47
[32m[20221214 00:38:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 837.30
[32m[20221214 00:38:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 725.94
[32m[20221214 00:38:09 @agent_ppo2.py:143][0m Total time:      40.64 min
[32m[20221214 00:38:09 @agent_ppo2.py:145][0m 3647488 total steps have happened
[32m[20221214 00:38:09 @agent_ppo2.py:121][0m #------------------------ Iteration 5781 --------------------------#
[32m[20221214 00:38:09 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:38:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:38:09 @agent_ppo2.py:185][0m |          -0.0019 |         109.1550 |        -161.2812 |
[32m[20221214 00:38:10 @agent_ppo2.py:185][0m |          -0.0044 |         100.2264 |        -161.3561 |
[32m[20221214 00:38:10 @agent_ppo2.py:185][0m |          -0.0031 |          98.7227 |        -159.9078 |
[32m[20221214 00:38:10 @agent_ppo2.py:185][0m |          -0.0062 |          96.2860 |        -159.7461 |
[32m[20221214 00:38:10 @agent_ppo2.py:185][0m |          -0.0076 |          95.1347 |        -159.2319 |
[32m[20221214 00:38:10 @agent_ppo2.py:185][0m |          -0.0100 |          94.2710 |        -160.2503 |
[32m[20221214 00:38:10 @agent_ppo2.py:185][0m |          -0.0119 |          93.1669 |        -158.7027 |
[32m[20221214 00:38:10 @agent_ppo2.py:185][0m |          -0.0114 |          93.1241 |        -159.1289 |
[32m[20221214 00:38:10 @agent_ppo2.py:185][0m |          -0.0143 |          92.2178 |        -158.9207 |
[32m[20221214 00:38:10 @agent_ppo2.py:185][0m |          -0.0149 |          92.7383 |        -158.4050 |
[32m[20221214 00:38:10 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 00:38:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 719.73
[32m[20221214 00:38:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 797.41
[32m[20221214 00:38:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 425.45
[32m[20221214 00:38:11 @agent_ppo2.py:143][0m Total time:      40.67 min
[32m[20221214 00:38:11 @agent_ppo2.py:145][0m 3649536 total steps have happened
[32m[20221214 00:38:11 @agent_ppo2.py:121][0m #------------------------ Iteration 5782 --------------------------#
[32m[20221214 00:38:11 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:38:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:38:11 @agent_ppo2.py:185][0m |           0.0020 |         141.2830 |        -165.3490 |
[32m[20221214 00:38:11 @agent_ppo2.py:185][0m |          -0.0034 |         131.3351 |        -165.8589 |
[32m[20221214 00:38:11 @agent_ppo2.py:185][0m |          -0.0066 |         126.6574 |        -165.9388 |
[32m[20221214 00:38:11 @agent_ppo2.py:185][0m |          -0.0110 |         123.8949 |        -166.7339 |
[32m[20221214 00:38:11 @agent_ppo2.py:185][0m |          -0.0105 |         125.2055 |        -166.6776 |
[32m[20221214 00:38:11 @agent_ppo2.py:185][0m |          -0.0087 |         124.4327 |        -167.2210 |
[32m[20221214 00:38:11 @agent_ppo2.py:185][0m |          -0.0009 |         139.4514 |        -166.9812 |
[32m[20221214 00:38:12 @agent_ppo2.py:185][0m |          -0.0045 |         135.5755 |        -166.5658 |
[32m[20221214 00:38:12 @agent_ppo2.py:185][0m |          -0.0121 |         128.1237 |        -167.8116 |
[32m[20221214 00:38:12 @agent_ppo2.py:185][0m |          -0.0143 |         119.5106 |        -167.7313 |
[32m[20221214 00:38:12 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221214 00:38:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 613.92
[32m[20221214 00:38:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 710.46
[32m[20221214 00:38:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 684.54
[32m[20221214 00:38:12 @agent_ppo2.py:143][0m Total time:      40.69 min
[32m[20221214 00:38:12 @agent_ppo2.py:145][0m 3651584 total steps have happened
[32m[20221214 00:38:12 @agent_ppo2.py:121][0m #------------------------ Iteration 5783 --------------------------#
[32m[20221214 00:38:12 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:38:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:38:12 @agent_ppo2.py:185][0m |           0.0040 |         110.8810 |        -166.8289 |
[32m[20221214 00:38:12 @agent_ppo2.py:185][0m |          -0.0092 |         100.6003 |        -166.4833 |
[32m[20221214 00:38:13 @agent_ppo2.py:185][0m |          -0.0031 |          96.6218 |        -165.9051 |
[32m[20221214 00:38:13 @agent_ppo2.py:185][0m |          -0.0117 |          94.1124 |        -165.7816 |
[32m[20221214 00:38:13 @agent_ppo2.py:185][0m |          -0.0086 |          93.5732 |        -165.9488 |
[32m[20221214 00:38:13 @agent_ppo2.py:185][0m |          -0.0079 |          91.7305 |        -166.3617 |
[32m[20221214 00:38:13 @agent_ppo2.py:185][0m |          -0.0126 |          91.6209 |        -166.0705 |
[32m[20221214 00:38:13 @agent_ppo2.py:185][0m |          -0.0157 |          91.3886 |        -165.7293 |
[32m[20221214 00:38:13 @agent_ppo2.py:185][0m |          -0.0159 |          90.2927 |        -165.8821 |
[32m[20221214 00:38:13 @agent_ppo2.py:185][0m |          -0.0144 |          89.8030 |        -165.8090 |
[32m[20221214 00:38:13 @agent_ppo2.py:130][0m Policy update time: 1.28 s
[32m[20221214 00:38:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 540.30
[32m[20221214 00:38:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 586.00
[32m[20221214 00:38:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 687.15
[32m[20221214 00:38:14 @agent_ppo2.py:143][0m Total time:      40.72 min
[32m[20221214 00:38:14 @agent_ppo2.py:145][0m 3653632 total steps have happened
[32m[20221214 00:38:14 @agent_ppo2.py:121][0m #------------------------ Iteration 5784 --------------------------#
[32m[20221214 00:38:14 @agent_ppo2.py:127][0m Sampling time: 0.24 s by 5 slaves
[32m[20221214 00:38:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:38:14 @agent_ppo2.py:185][0m |           0.0016 |          91.0149 |        -167.6264 |
[32m[20221214 00:38:14 @agent_ppo2.py:185][0m |          -0.0051 |          82.1072 |        -167.4303 |
[32m[20221214 00:38:14 @agent_ppo2.py:185][0m |          -0.0084 |          79.7367 |        -167.0555 |
[32m[20221214 00:38:15 @agent_ppo2.py:185][0m |          -0.0111 |          77.4310 |        -167.6288 |
[32m[20221214 00:38:15 @agent_ppo2.py:185][0m |          -0.0140 |          77.4293 |        -167.0542 |
[32m[20221214 00:38:15 @agent_ppo2.py:185][0m |          -0.0110 |          76.2599 |        -167.4245 |
[32m[20221214 00:38:15 @agent_ppo2.py:185][0m |          -0.0115 |          75.3292 |        -166.8096 |
[32m[20221214 00:38:15 @agent_ppo2.py:185][0m |          -0.0152 |          74.5282 |        -166.9342 |
[32m[20221214 00:38:15 @agent_ppo2.py:185][0m |          -0.0158 |          73.6556 |        -167.0054 |
[32m[20221214 00:38:15 @agent_ppo2.py:185][0m |          -0.0199 |          74.4865 |        -166.5358 |
[32m[20221214 00:38:15 @agent_ppo2.py:130][0m Policy update time: 1.28 s
[32m[20221214 00:38:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 564.58
[32m[20221214 00:38:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 741.47
[32m[20221214 00:38:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 802.51
[32m[20221214 00:38:15 @agent_ppo2.py:143][0m Total time:      40.75 min
[32m[20221214 00:38:15 @agent_ppo2.py:145][0m 3655680 total steps have happened
[32m[20221214 00:38:15 @agent_ppo2.py:121][0m #------------------------ Iteration 5785 --------------------------#
[32m[20221214 00:38:16 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:38:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:38:16 @agent_ppo2.py:185][0m |           0.0023 |          91.8046 |        -176.0187 |
[32m[20221214 00:38:16 @agent_ppo2.py:185][0m |          -0.0025 |          83.4897 |        -176.1519 |
[32m[20221214 00:38:16 @agent_ppo2.py:185][0m |          -0.0054 |          82.3114 |        -175.9158 |
[32m[20221214 00:38:16 @agent_ppo2.py:185][0m |          -0.0087 |          81.1726 |        -175.6267 |
[32m[20221214 00:38:16 @agent_ppo2.py:185][0m |          -0.0075 |          79.8008 |        -176.7211 |
[32m[20221214 00:38:16 @agent_ppo2.py:185][0m |          -0.0133 |          79.6464 |        -176.2352 |
[32m[20221214 00:38:16 @agent_ppo2.py:185][0m |          -0.0091 |          78.7256 |        -176.8718 |
[32m[20221214 00:38:17 @agent_ppo2.py:185][0m |          -0.0102 |          78.1324 |        -176.7207 |
[32m[20221214 00:38:17 @agent_ppo2.py:185][0m |          -0.0135 |          77.6174 |        -176.6699 |
[32m[20221214 00:38:17 @agent_ppo2.py:185][0m |          -0.0106 |          77.6266 |        -176.5887 |
[32m[20221214 00:38:17 @agent_ppo2.py:130][0m Policy update time: 1.28 s
[32m[20221214 00:38:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 611.28
[32m[20221214 00:38:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 774.53
[32m[20221214 00:38:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 683.41
[32m[20221214 00:38:17 @agent_ppo2.py:143][0m Total time:      40.78 min
[32m[20221214 00:38:17 @agent_ppo2.py:145][0m 3657728 total steps have happened
[32m[20221214 00:38:17 @agent_ppo2.py:121][0m #------------------------ Iteration 5786 --------------------------#
[32m[20221214 00:38:17 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:38:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:38:17 @agent_ppo2.py:185][0m |           0.0013 |         111.9189 |        -178.3232 |
[32m[20221214 00:38:18 @agent_ppo2.py:185][0m |           0.0053 |         113.2322 |        -178.0248 |
[32m[20221214 00:38:18 @agent_ppo2.py:185][0m |          -0.0060 |         106.2965 |        -176.4638 |
[32m[20221214 00:38:18 @agent_ppo2.py:185][0m |          -0.0038 |         106.0369 |        -177.3408 |
[32m[20221214 00:38:18 @agent_ppo2.py:185][0m |          -0.0027 |         103.9272 |        -178.4781 |
[32m[20221214 00:38:18 @agent_ppo2.py:185][0m |          -0.0010 |         102.9525 |        -177.3037 |
[32m[20221214 00:38:18 @agent_ppo2.py:185][0m |          -0.0077 |         103.0433 |        -178.4136 |
[32m[20221214 00:38:18 @agent_ppo2.py:185][0m |          -0.0069 |         103.0756 |        -178.4590 |
[32m[20221214 00:38:18 @agent_ppo2.py:185][0m |          -0.0088 |         102.0960 |        -178.1814 |
[32m[20221214 00:38:18 @agent_ppo2.py:185][0m |          -0.0040 |         103.7419 |        -177.5428 |
[32m[20221214 00:38:18 @agent_ppo2.py:130][0m Policy update time: 1.26 s
[32m[20221214 00:38:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 600.29
[32m[20221214 00:38:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 698.28
[32m[20221214 00:38:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 468.72
[32m[20221214 00:38:19 @agent_ppo2.py:143][0m Total time:      40.80 min
[32m[20221214 00:38:19 @agent_ppo2.py:145][0m 3659776 total steps have happened
[32m[20221214 00:38:19 @agent_ppo2.py:121][0m #------------------------ Iteration 5787 --------------------------#
[32m[20221214 00:38:19 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:38:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:38:19 @agent_ppo2.py:185][0m |          -0.0007 |         109.9223 |        -168.6787 |
[32m[20221214 00:38:19 @agent_ppo2.py:185][0m |          -0.0061 |          98.7075 |        -168.3567 |
[32m[20221214 00:38:19 @agent_ppo2.py:185][0m |          -0.0084 |          95.7527 |        -167.8661 |
[32m[20221214 00:38:19 @agent_ppo2.py:185][0m |          -0.0119 |          93.2178 |        -167.7387 |
[32m[20221214 00:38:20 @agent_ppo2.py:185][0m |          -0.0178 |          91.8105 |        -167.6236 |
[32m[20221214 00:38:20 @agent_ppo2.py:185][0m |          -0.0039 |          90.0220 |        -167.1574 |
[32m[20221214 00:38:20 @agent_ppo2.py:185][0m |          -0.0143 |          88.4968 |        -167.0162 |
[32m[20221214 00:38:20 @agent_ppo2.py:185][0m |          -0.0148 |          88.3393 |        -167.2341 |
[32m[20221214 00:38:20 @agent_ppo2.py:185][0m |          -0.0130 |          87.5490 |        -166.7012 |
[32m[20221214 00:38:20 @agent_ppo2.py:185][0m |          -0.0161 |          85.3471 |        -167.0174 |
[32m[20221214 00:38:20 @agent_ppo2.py:130][0m Policy update time: 1.26 s
[32m[20221214 00:38:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 568.60
[32m[20221214 00:38:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 642.68
[32m[20221214 00:38:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 720.48
[32m[20221214 00:38:20 @agent_ppo2.py:143][0m Total time:      40.83 min
[32m[20221214 00:38:20 @agent_ppo2.py:145][0m 3661824 total steps have happened
[32m[20221214 00:38:20 @agent_ppo2.py:121][0m #------------------------ Iteration 5788 --------------------------#
[32m[20221214 00:38:20 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:38:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:38:21 @agent_ppo2.py:185][0m |          -0.0027 |          73.1792 |        -163.7321 |
[32m[20221214 00:38:21 @agent_ppo2.py:185][0m |          -0.0037 |          61.3026 |        -164.0667 |
[32m[20221214 00:38:21 @agent_ppo2.py:185][0m |          -0.0052 |          58.2003 |        -163.4659 |
[32m[20221214 00:38:21 @agent_ppo2.py:185][0m |          -0.0103 |          56.0494 |        -163.7943 |
[32m[20221214 00:38:21 @agent_ppo2.py:185][0m |          -0.0147 |          55.1567 |        -163.4635 |
[32m[20221214 00:38:21 @agent_ppo2.py:185][0m |          -0.0066 |          55.3719 |        -163.7781 |
[32m[20221214 00:38:21 @agent_ppo2.py:185][0m |          -0.0080 |          54.2847 |        -163.2505 |
[32m[20221214 00:38:21 @agent_ppo2.py:185][0m |          -0.0092 |          52.7569 |        -163.7614 |
[32m[20221214 00:38:22 @agent_ppo2.py:185][0m |          -0.0123 |          53.6784 |        -163.9462 |
[32m[20221214 00:38:22 @agent_ppo2.py:185][0m |          -0.0164 |          52.2655 |        -164.7596 |
[32m[20221214 00:38:22 @agent_ppo2.py:130][0m Policy update time: 1.25 s
[32m[20221214 00:38:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 668.04
[32m[20221214 00:38:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 807.12
[32m[20221214 00:38:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 718.11
[32m[20221214 00:38:22 @agent_ppo2.py:143][0m Total time:      40.86 min
[32m[20221214 00:38:22 @agent_ppo2.py:145][0m 3663872 total steps have happened
[32m[20221214 00:38:22 @agent_ppo2.py:121][0m #------------------------ Iteration 5789 --------------------------#
[32m[20221214 00:38:22 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:38:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:38:22 @agent_ppo2.py:185][0m |           0.0026 |          89.7868 |        -167.1569 |
[32m[20221214 00:38:22 @agent_ppo2.py:185][0m |          -0.0054 |          85.0515 |        -165.7850 |
[32m[20221214 00:38:23 @agent_ppo2.py:185][0m |          -0.0060 |          83.8201 |        -165.9282 |
[32m[20221214 00:38:23 @agent_ppo2.py:185][0m |          -0.0015 |          84.8703 |        -165.6706 |
[32m[20221214 00:38:23 @agent_ppo2.py:185][0m |          -0.0109 |          83.0523 |        -164.6295 |
[32m[20221214 00:38:23 @agent_ppo2.py:185][0m |          -0.0091 |          83.1346 |        -164.2854 |
[32m[20221214 00:38:23 @agent_ppo2.py:185][0m |          -0.0104 |          81.8107 |        -165.5825 |
[32m[20221214 00:38:23 @agent_ppo2.py:185][0m |          -0.0100 |          83.7276 |        -164.5796 |
[32m[20221214 00:38:23 @agent_ppo2.py:185][0m |           0.0006 |          87.1185 |        -164.5768 |
[32m[20221214 00:38:23 @agent_ppo2.py:185][0m |          -0.0138 |          81.4960 |        -165.3373 |
[32m[20221214 00:38:23 @agent_ppo2.py:130][0m Policy update time: 1.26 s
[32m[20221214 00:38:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 605.84
[32m[20221214 00:38:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 746.45
[32m[20221214 00:38:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 755.59
[32m[20221214 00:38:23 @agent_ppo2.py:143][0m Total time:      40.89 min
[32m[20221214 00:38:23 @agent_ppo2.py:145][0m 3665920 total steps have happened
[32m[20221214 00:38:23 @agent_ppo2.py:121][0m #------------------------ Iteration 5790 --------------------------#
[32m[20221214 00:38:24 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221214 00:38:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:38:24 @agent_ppo2.py:185][0m |          -0.0007 |          93.5251 |        -175.0803 |
[32m[20221214 00:38:24 @agent_ppo2.py:185][0m |          -0.0076 |          88.6168 |        -174.5909 |
[32m[20221214 00:38:24 @agent_ppo2.py:185][0m |          -0.0038 |          88.0581 |        -173.6213 |
[32m[20221214 00:38:24 @agent_ppo2.py:185][0m |          -0.0078 |          86.1799 |        -173.0101 |
[32m[20221214 00:38:24 @agent_ppo2.py:185][0m |           0.0000 |          90.2900 |        -173.5070 |
[32m[20221214 00:38:25 @agent_ppo2.py:185][0m |          -0.0081 |          87.3862 |        -173.1803 |
[32m[20221214 00:38:25 @agent_ppo2.py:185][0m |          -0.0150 |          84.4299 |        -173.8372 |
[32m[20221214 00:38:25 @agent_ppo2.py:185][0m |          -0.0154 |          83.4710 |        -173.9260 |
[32m[20221214 00:38:25 @agent_ppo2.py:185][0m |          -0.0147 |          84.1177 |        -173.1468 |
[32m[20221214 00:38:25 @agent_ppo2.py:185][0m |          -0.0086 |          85.6621 |        -172.1096 |
[32m[20221214 00:38:25 @agent_ppo2.py:130][0m Policy update time: 1.30 s
[32m[20221214 00:38:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 576.74
[32m[20221214 00:38:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 650.88
[32m[20221214 00:38:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 643.75
[32m[20221214 00:38:25 @agent_ppo2.py:143][0m Total time:      40.91 min
[32m[20221214 00:38:25 @agent_ppo2.py:145][0m 3667968 total steps have happened
[32m[20221214 00:38:25 @agent_ppo2.py:121][0m #------------------------ Iteration 5791 --------------------------#
[32m[20221214 00:38:25 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:38:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:38:26 @agent_ppo2.py:185][0m |          -0.0002 |         105.8219 |        -165.6322 |
[32m[20221214 00:38:26 @agent_ppo2.py:185][0m |          -0.0081 |          91.4427 |        -164.5921 |
[32m[20221214 00:38:26 @agent_ppo2.py:185][0m |          -0.0092 |          89.3638 |        -164.7004 |
[32m[20221214 00:38:26 @agent_ppo2.py:185][0m |          -0.0163 |          87.6309 |        -165.1942 |
[32m[20221214 00:38:26 @agent_ppo2.py:185][0m |          -0.0142 |          86.6946 |        -165.1722 |
[32m[20221214 00:38:26 @agent_ppo2.py:185][0m |          -0.0102 |          86.5507 |        -166.1584 |
[32m[20221214 00:38:26 @agent_ppo2.py:185][0m |          -0.0117 |          87.7653 |        -166.4722 |
[32m[20221214 00:38:26 @agent_ppo2.py:185][0m |          -0.0166 |          85.9117 |        -165.8995 |
[32m[20221214 00:38:26 @agent_ppo2.py:185][0m |          -0.0115 |          84.3293 |        -166.1334 |
[32m[20221214 00:38:27 @agent_ppo2.py:185][0m |          -0.0180 |          84.6428 |        -164.4089 |
[32m[20221214 00:38:27 @agent_ppo2.py:130][0m Policy update time: 1.27 s
[32m[20221214 00:38:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 658.21
[32m[20221214 00:38:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 755.37
[32m[20221214 00:38:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 733.65
[32m[20221214 00:38:27 @agent_ppo2.py:143][0m Total time:      40.94 min
[32m[20221214 00:38:27 @agent_ppo2.py:145][0m 3670016 total steps have happened
[32m[20221214 00:38:27 @agent_ppo2.py:121][0m #------------------------ Iteration 5792 --------------------------#
[32m[20221214 00:38:27 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:38:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:38:27 @agent_ppo2.py:185][0m |           0.0044 |          81.3326 |        -164.5238 |
[32m[20221214 00:38:27 @agent_ppo2.py:185][0m |           0.0001 |          75.3198 |        -164.0629 |
[32m[20221214 00:38:27 @agent_ppo2.py:185][0m |          -0.0037 |          74.6748 |        -163.8197 |
[32m[20221214 00:38:28 @agent_ppo2.py:185][0m |          -0.0022 |          72.9832 |        -163.5723 |
[32m[20221214 00:38:28 @agent_ppo2.py:185][0m |          -0.0015 |          72.9842 |        -162.6514 |
[32m[20221214 00:38:28 @agent_ppo2.py:185][0m |           0.0048 |          76.6655 |        -163.0537 |
[32m[20221214 00:38:28 @agent_ppo2.py:185][0m |           0.0016 |          74.4964 |        -162.4661 |
[32m[20221214 00:38:28 @agent_ppo2.py:185][0m |           0.0003 |          73.9059 |        -163.7106 |
[32m[20221214 00:38:28 @agent_ppo2.py:185][0m |          -0.0066 |          75.0202 |        -163.3495 |
[32m[20221214 00:38:28 @agent_ppo2.py:185][0m |          -0.0053 |          71.9037 |        -162.5513 |
[32m[20221214 00:38:28 @agent_ppo2.py:130][0m Policy update time: 1.26 s
[32m[20221214 00:38:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 737.77
[32m[20221214 00:38:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 825.59
[32m[20221214 00:38:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 757.24
[32m[20221214 00:38:28 @agent_ppo2.py:143][0m Total time:      40.97 min
[32m[20221214 00:38:28 @agent_ppo2.py:145][0m 3672064 total steps have happened
[32m[20221214 00:38:28 @agent_ppo2.py:121][0m #------------------------ Iteration 5793 --------------------------#
[32m[20221214 00:38:29 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:38:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:38:29 @agent_ppo2.py:185][0m |          -0.0007 |         102.6538 |        -164.0723 |
[32m[20221214 00:38:29 @agent_ppo2.py:185][0m |          -0.0048 |          96.8388 |        -163.4435 |
[32m[20221214 00:38:29 @agent_ppo2.py:185][0m |          -0.0062 |          95.2667 |        -163.8185 |
[32m[20221214 00:38:29 @agent_ppo2.py:185][0m |          -0.0114 |          93.8000 |        -163.4086 |
[32m[20221214 00:38:29 @agent_ppo2.py:185][0m |          -0.0152 |          92.3248 |        -163.6671 |
[32m[20221214 00:38:29 @agent_ppo2.py:185][0m |          -0.0131 |          91.8180 |        -163.9452 |
[32m[20221214 00:38:29 @agent_ppo2.py:185][0m |          -0.0158 |          90.9821 |        -163.1355 |
[32m[20221214 00:38:30 @agent_ppo2.py:185][0m |          -0.0117 |          92.0287 |        -163.6604 |
[32m[20221214 00:38:30 @agent_ppo2.py:185][0m |          -0.0161 |          89.7443 |        -163.7055 |
[32m[20221214 00:38:30 @agent_ppo2.py:185][0m |          -0.0201 |          89.6349 |        -163.5977 |
[32m[20221214 00:38:30 @agent_ppo2.py:130][0m Policy update time: 1.26 s
[32m[20221214 00:38:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 533.29
[32m[20221214 00:38:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 606.47
[32m[20221214 00:38:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 795.63
[32m[20221214 00:38:30 @agent_ppo2.py:143][0m Total time:      40.99 min
[32m[20221214 00:38:30 @agent_ppo2.py:145][0m 3674112 total steps have happened
[32m[20221214 00:38:30 @agent_ppo2.py:121][0m #------------------------ Iteration 5794 --------------------------#
[32m[20221214 00:38:30 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:38:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:38:30 @agent_ppo2.py:185][0m |          -0.0002 |         156.7966 |        -169.3420 |
[32m[20221214 00:38:31 @agent_ppo2.py:185][0m |          -0.0002 |         146.6475 |        -169.4927 |
[32m[20221214 00:38:31 @agent_ppo2.py:185][0m |          -0.0069 |         142.5904 |        -168.9091 |
[32m[20221214 00:38:31 @agent_ppo2.py:185][0m |          -0.0072 |         140.4668 |        -168.8113 |
[32m[20221214 00:38:31 @agent_ppo2.py:185][0m |          -0.0093 |         137.3975 |        -167.9575 |
[32m[20221214 00:38:31 @agent_ppo2.py:185][0m |          -0.0059 |         137.3168 |        -167.8470 |
[32m[20221214 00:38:31 @agent_ppo2.py:185][0m |          -0.0089 |         134.7806 |        -168.4521 |
[32m[20221214 00:38:31 @agent_ppo2.py:185][0m |          -0.0095 |         134.4804 |        -168.7742 |
[32m[20221214 00:38:31 @agent_ppo2.py:185][0m |          -0.0129 |         133.8293 |        -167.6095 |
[32m[20221214 00:38:31 @agent_ppo2.py:185][0m |          -0.0160 |         133.2733 |        -167.8793 |
[32m[20221214 00:38:31 @agent_ppo2.py:130][0m Policy update time: 1.27 s
[32m[20221214 00:38:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 655.00
[32m[20221214 00:38:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 777.62
[32m[20221214 00:38:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 676.77
[32m[20221214 00:38:32 @agent_ppo2.py:143][0m Total time:      41.02 min
[32m[20221214 00:38:32 @agent_ppo2.py:145][0m 3676160 total steps have happened
[32m[20221214 00:38:32 @agent_ppo2.py:121][0m #------------------------ Iteration 5795 --------------------------#
[32m[20221214 00:38:32 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:38:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:38:32 @agent_ppo2.py:185][0m |           0.0001 |          82.5751 |        -165.8708 |
[32m[20221214 00:38:32 @agent_ppo2.py:185][0m |          -0.0058 |          70.6984 |        -166.4748 |
[32m[20221214 00:38:32 @agent_ppo2.py:185][0m |          -0.0074 |          67.0772 |        -166.9744 |
[32m[20221214 00:38:32 @agent_ppo2.py:185][0m |          -0.0094 |          64.5587 |        -166.8037 |
[32m[20221214 00:38:33 @agent_ppo2.py:185][0m |          -0.0169 |          63.5352 |        -166.9309 |
[32m[20221214 00:38:33 @agent_ppo2.py:185][0m |          -0.0193 |          62.1224 |        -167.7385 |
[32m[20221214 00:38:33 @agent_ppo2.py:185][0m |          -0.0138 |          62.4163 |        -167.3501 |
[32m[20221214 00:38:33 @agent_ppo2.py:185][0m |          -0.0202 |          60.6720 |        -167.8101 |
[32m[20221214 00:38:33 @agent_ppo2.py:185][0m |          -0.0169 |          59.7329 |        -166.9755 |
[32m[20221214 00:38:33 @agent_ppo2.py:185][0m |          -0.0227 |          59.3630 |        -167.8546 |
[32m[20221214 00:38:33 @agent_ppo2.py:130][0m Policy update time: 1.26 s
[32m[20221214 00:38:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 437.68
[32m[20221214 00:38:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 605.64
[32m[20221214 00:38:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 771.90
[32m[20221214 00:38:33 @agent_ppo2.py:143][0m Total time:      41.05 min
[32m[20221214 00:38:33 @agent_ppo2.py:145][0m 3678208 total steps have happened
[32m[20221214 00:38:33 @agent_ppo2.py:121][0m #------------------------ Iteration 5796 --------------------------#
[32m[20221214 00:38:33 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:38:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:38:34 @agent_ppo2.py:185][0m |          -0.0011 |         103.2946 |        -166.6633 |
[32m[20221214 00:38:34 @agent_ppo2.py:185][0m |          -0.0008 |          94.7430 |        -165.0612 |
[32m[20221214 00:38:34 @agent_ppo2.py:185][0m |          -0.0064 |          88.8230 |        -165.8154 |
[32m[20221214 00:38:34 @agent_ppo2.py:185][0m |          -0.0079 |          86.6983 |        -166.0156 |
[32m[20221214 00:38:34 @agent_ppo2.py:185][0m |          -0.0091 |          86.6683 |        -166.2880 |
[32m[20221214 00:38:34 @agent_ppo2.py:185][0m |          -0.0104 |          85.2833 |        -165.7309 |
[32m[20221214 00:38:34 @agent_ppo2.py:185][0m |          -0.0112 |          84.1894 |        -165.9492 |
[32m[20221214 00:38:34 @agent_ppo2.py:185][0m |           0.0039 |          92.3287 |        -166.2632 |
[32m[20221214 00:38:35 @agent_ppo2.py:185][0m |          -0.0086 |          84.0262 |        -166.2824 |
[32m[20221214 00:38:35 @agent_ppo2.py:185][0m |          -0.0126 |          82.3554 |        -166.6155 |
[32m[20221214 00:38:35 @agent_ppo2.py:130][0m Policy update time: 1.26 s
[32m[20221214 00:38:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 574.43
[32m[20221214 00:38:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 612.97
[32m[20221214 00:38:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221214 00:38:35 @agent_ppo2.py:143][0m Total time:      41.08 min
[32m[20221214 00:38:35 @agent_ppo2.py:145][0m 3680256 total steps have happened
[32m[20221214 00:38:35 @agent_ppo2.py:121][0m #------------------------ Iteration 5797 --------------------------#
[32m[20221214 00:38:35 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:38:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:38:35 @agent_ppo2.py:185][0m |           0.0046 |         208.6165 |        -172.6324 |
[32m[20221214 00:38:35 @agent_ppo2.py:185][0m |           0.0093 |         218.5920 |        -172.8380 |
[32m[20221214 00:38:36 @agent_ppo2.py:185][0m |          -0.0026 |         204.2774 |        -172.5330 |
[32m[20221214 00:38:36 @agent_ppo2.py:185][0m |          -0.0006 |         201.8622 |        -170.4766 |
[32m[20221214 00:38:36 @agent_ppo2.py:185][0m |           0.0132 |         222.6227 |        -170.7933 |
[32m[20221214 00:38:36 @agent_ppo2.py:185][0m |           0.0067 |         208.8179 |        -170.2020 |
[32m[20221214 00:38:36 @agent_ppo2.py:185][0m |           0.0023 |         204.8934 |        -170.2833 |
[32m[20221214 00:38:36 @agent_ppo2.py:185][0m |          -0.0022 |         202.3409 |        -171.5867 |
[32m[20221214 00:38:36 @agent_ppo2.py:185][0m |          -0.0032 |         200.9210 |        -170.7284 |
[32m[20221214 00:38:36 @agent_ppo2.py:185][0m |          -0.0063 |         201.8270 |        -170.1952 |
[32m[20221214 00:38:36 @agent_ppo2.py:130][0m Policy update time: 1.26 s
[32m[20221214 00:38:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 807.45
[32m[20221214 00:38:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 835.29
[32m[20221214 00:38:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 676.91
[32m[20221214 00:38:37 @agent_ppo2.py:143][0m Total time:      41.10 min
[32m[20221214 00:38:37 @agent_ppo2.py:145][0m 3682304 total steps have happened
[32m[20221214 00:38:37 @agent_ppo2.py:121][0m #------------------------ Iteration 5798 --------------------------#
[32m[20221214 00:38:37 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:38:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:38:37 @agent_ppo2.py:185][0m |          -0.0020 |         211.6872 |        -164.1477 |
[32m[20221214 00:38:37 @agent_ppo2.py:185][0m |          -0.0009 |         205.3097 |        -164.4063 |
[32m[20221214 00:38:37 @agent_ppo2.py:185][0m |           0.0109 |         231.2670 |        -163.8235 |
[32m[20221214 00:38:37 @agent_ppo2.py:185][0m |          -0.0040 |         202.9261 |        -163.3990 |
[32m[20221214 00:38:37 @agent_ppo2.py:185][0m |          -0.0041 |         201.2578 |        -162.9061 |
[32m[20221214 00:38:38 @agent_ppo2.py:185][0m |          -0.0040 |         201.3430 |        -162.8270 |
[32m[20221214 00:38:38 @agent_ppo2.py:185][0m |          -0.0046 |         200.2891 |        -164.1449 |
[32m[20221214 00:38:38 @agent_ppo2.py:185][0m |          -0.0067 |         199.9684 |        -163.8853 |
[32m[20221214 00:38:38 @agent_ppo2.py:185][0m |          -0.0071 |         199.9448 |        -164.1533 |
[32m[20221214 00:38:38 @agent_ppo2.py:185][0m |          -0.0029 |         198.7311 |        -164.8001 |
[32m[20221214 00:38:38 @agent_ppo2.py:130][0m Policy update time: 1.27 s
[32m[20221214 00:38:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 715.87
[32m[20221214 00:38:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 832.21
[32m[20221214 00:38:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 519.97
[32m[20221214 00:38:38 @agent_ppo2.py:143][0m Total time:      41.13 min
[32m[20221214 00:38:38 @agent_ppo2.py:145][0m 3684352 total steps have happened
[32m[20221214 00:38:38 @agent_ppo2.py:121][0m #------------------------ Iteration 5799 --------------------------#
[32m[20221214 00:38:38 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:38:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:38:39 @agent_ppo2.py:185][0m |           0.0097 |         106.5260 |        -161.1491 |
[32m[20221214 00:38:39 @agent_ppo2.py:185][0m |          -0.0100 |          94.1761 |        -160.8996 |
[32m[20221214 00:38:39 @agent_ppo2.py:185][0m |          -0.0040 |          99.6187 |        -160.7758 |
[32m[20221214 00:38:39 @agent_ppo2.py:185][0m |          -0.0169 |          91.2828 |        -160.2794 |
[32m[20221214 00:38:39 @agent_ppo2.py:185][0m |          -0.0175 |          90.2948 |        -160.5323 |
[32m[20221214 00:38:39 @agent_ppo2.py:185][0m |          -0.0176 |          89.6646 |        -159.8749 |
[32m[20221214 00:38:39 @agent_ppo2.py:185][0m |          -0.0208 |          89.1422 |        -160.7426 |
[32m[20221214 00:38:39 @agent_ppo2.py:185][0m |          -0.0204 |          88.8952 |        -160.6575 |
[32m[20221214 00:38:39 @agent_ppo2.py:185][0m |          -0.0203 |          88.2389 |        -160.8883 |
[32m[20221214 00:38:40 @agent_ppo2.py:185][0m |          -0.0238 |          88.0404 |        -161.0464 |
[32m[20221214 00:38:40 @agent_ppo2.py:130][0m Policy update time: 1.26 s
[32m[20221214 00:38:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 586.78
[32m[20221214 00:38:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 717.83
[32m[20221214 00:38:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 718.27
[32m[20221214 00:38:40 @agent_ppo2.py:143][0m Total time:      41.16 min
[32m[20221214 00:38:40 @agent_ppo2.py:145][0m 3686400 total steps have happened
[32m[20221214 00:38:40 @agent_ppo2.py:121][0m #------------------------ Iteration 5800 --------------------------#
[32m[20221214 00:38:40 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221214 00:38:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:38:40 @agent_ppo2.py:185][0m |           0.0041 |         132.3704 |        -172.0325 |
[32m[20221214 00:38:40 @agent_ppo2.py:185][0m |           0.0035 |         124.0345 |        -173.2736 |
[32m[20221214 00:38:40 @agent_ppo2.py:185][0m |          -0.0034 |         117.4841 |        -173.9183 |
[32m[20221214 00:38:41 @agent_ppo2.py:185][0m |          -0.0087 |         113.7426 |        -173.6925 |
[32m[20221214 00:38:41 @agent_ppo2.py:185][0m |          -0.0020 |         114.0444 |        -173.9676 |
[32m[20221214 00:38:41 @agent_ppo2.py:185][0m |          -0.0095 |         110.2070 |        -175.6990 |
[32m[20221214 00:38:41 @agent_ppo2.py:185][0m |          -0.0140 |         108.8695 |        -175.9106 |
[32m[20221214 00:38:41 @agent_ppo2.py:185][0m |          -0.0187 |         107.9660 |        -176.1699 |
[32m[20221214 00:38:41 @agent_ppo2.py:185][0m |          -0.0149 |         107.5271 |        -176.2888 |
[32m[20221214 00:38:41 @agent_ppo2.py:185][0m |          -0.0184 |         105.8481 |        -177.4102 |
[32m[20221214 00:38:41 @agent_ppo2.py:130][0m Policy update time: 1.26 s
[32m[20221214 00:38:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 538.20
[32m[20221214 00:38:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 745.18
[32m[20221214 00:38:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 799.50
[32m[20221214 00:38:41 @agent_ppo2.py:143][0m Total time:      41.18 min
[32m[20221214 00:38:41 @agent_ppo2.py:145][0m 3688448 total steps have happened
[32m[20221214 00:38:41 @agent_ppo2.py:121][0m #------------------------ Iteration 5801 --------------------------#
[32m[20221214 00:38:42 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:38:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:38:42 @agent_ppo2.py:185][0m |           0.0017 |         108.5319 |        -173.6953 |
[32m[20221214 00:38:42 @agent_ppo2.py:185][0m |          -0.0085 |          99.1336 |        -172.5404 |
[32m[20221214 00:38:42 @agent_ppo2.py:185][0m |          -0.0040 |          95.4521 |        -172.5277 |
[32m[20221214 00:38:42 @agent_ppo2.py:185][0m |          -0.0095 |          92.6016 |        -172.1362 |
[32m[20221214 00:38:42 @agent_ppo2.py:185][0m |          -0.0126 |          90.6672 |        -171.3384 |
[32m[20221214 00:38:42 @agent_ppo2.py:185][0m |          -0.0148 |          89.8055 |        -171.2560 |
[32m[20221214 00:38:43 @agent_ppo2.py:185][0m |          -0.0130 |          89.0367 |        -171.2307 |
[32m[20221214 00:38:43 @agent_ppo2.py:185][0m |          -0.0139 |          87.5900 |        -171.2384 |
[32m[20221214 00:38:43 @agent_ppo2.py:185][0m |          -0.0139 |          89.5245 |        -170.8488 |
[32m[20221214 00:38:43 @agent_ppo2.py:185][0m |          -0.0139 |          86.7203 |        -169.1033 |
[32m[20221214 00:38:43 @agent_ppo2.py:130][0m Policy update time: 1.27 s
[32m[20221214 00:38:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 694.17
[32m[20221214 00:38:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 828.24
[32m[20221214 00:38:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 580.28
[32m[20221214 00:38:43 @agent_ppo2.py:143][0m Total time:      41.21 min
[32m[20221214 00:38:43 @agent_ppo2.py:145][0m 3690496 total steps have happened
[32m[20221214 00:38:43 @agent_ppo2.py:121][0m #------------------------ Iteration 5802 --------------------------#
[32m[20221214 00:38:43 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:38:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:38:43 @agent_ppo2.py:185][0m |           0.0004 |         187.8042 |        -173.5008 |
[32m[20221214 00:38:44 @agent_ppo2.py:185][0m |          -0.0023 |         178.8471 |        -173.4160 |
[32m[20221214 00:38:44 @agent_ppo2.py:185][0m |          -0.0006 |         176.3848 |        -173.9260 |
[32m[20221214 00:38:44 @agent_ppo2.py:185][0m |          -0.0021 |         172.9906 |        -174.0544 |
[32m[20221214 00:38:44 @agent_ppo2.py:185][0m |           0.0082 |         184.5188 |        -174.1796 |
[32m[20221214 00:38:44 @agent_ppo2.py:185][0m |          -0.0030 |         171.4952 |        -173.2940 |
[32m[20221214 00:38:44 @agent_ppo2.py:185][0m |          -0.0046 |         167.6790 |        -173.0801 |
[32m[20221214 00:38:44 @agent_ppo2.py:185][0m |          -0.0059 |         166.9043 |        -174.1835 |
[32m[20221214 00:38:44 @agent_ppo2.py:185][0m |          -0.0059 |         166.5518 |        -173.3298 |
[32m[20221214 00:38:44 @agent_ppo2.py:185][0m |          -0.0076 |         165.1345 |        -173.6574 |
[32m[20221214 00:38:44 @agent_ppo2.py:130][0m Policy update time: 1.27 s
[32m[20221214 00:38:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 715.02
[32m[20221214 00:38:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.95
[32m[20221214 00:38:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 871.53
[32m[20221214 00:38:45 @agent_ppo2.py:143][0m Total time:      41.24 min
[32m[20221214 00:38:45 @agent_ppo2.py:145][0m 3692544 total steps have happened
[32m[20221214 00:38:45 @agent_ppo2.py:121][0m #------------------------ Iteration 5803 --------------------------#
[32m[20221214 00:38:45 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:38:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:38:45 @agent_ppo2.py:185][0m |           0.0004 |         144.5685 |        -167.6549 |
[32m[20221214 00:38:45 @agent_ppo2.py:185][0m |           0.0073 |         142.7891 |        -166.8702 |
[32m[20221214 00:38:45 @agent_ppo2.py:185][0m |          -0.0062 |         128.4114 |        -167.0986 |
[32m[20221214 00:38:45 @agent_ppo2.py:185][0m |          -0.0093 |         125.4531 |        -166.8604 |
[32m[20221214 00:38:45 @agent_ppo2.py:185][0m |          -0.0121 |         124.2060 |        -166.6755 |
[32m[20221214 00:38:46 @agent_ppo2.py:185][0m |          -0.0060 |         129.4750 |        -167.3176 |
[32m[20221214 00:38:46 @agent_ppo2.py:185][0m |          -0.0094 |         122.5495 |        -165.0167 |
[32m[20221214 00:38:46 @agent_ppo2.py:185][0m |          -0.0138 |         121.1648 |        -167.4465 |
[32m[20221214 00:38:46 @agent_ppo2.py:185][0m |          -0.0180 |         120.3421 |        -167.1296 |
[32m[20221214 00:38:46 @agent_ppo2.py:185][0m |          -0.0160 |         119.4034 |        -166.4306 |
[32m[20221214 00:38:46 @agent_ppo2.py:130][0m Policy update time: 1.26 s
[32m[20221214 00:38:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 708.15
[32m[20221214 00:38:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 843.71
[32m[20221214 00:38:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 664.61
[32m[20221214 00:38:46 @agent_ppo2.py:143][0m Total time:      41.27 min
[32m[20221214 00:38:46 @agent_ppo2.py:145][0m 3694592 total steps have happened
[32m[20221214 00:38:46 @agent_ppo2.py:121][0m #------------------------ Iteration 5804 --------------------------#
[32m[20221214 00:38:46 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:38:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:38:47 @agent_ppo2.py:185][0m |           0.0003 |          79.6621 |        -169.9148 |
[32m[20221214 00:38:47 @agent_ppo2.py:185][0m |          -0.0060 |          70.5989 |        -169.3946 |
[32m[20221214 00:38:47 @agent_ppo2.py:185][0m |          -0.0025 |          67.6642 |        -169.5591 |
[32m[20221214 00:38:47 @agent_ppo2.py:185][0m |          -0.0077 |          66.1664 |        -169.3900 |
[32m[20221214 00:38:47 @agent_ppo2.py:185][0m |          -0.0040 |          66.5185 |        -168.9502 |
[32m[20221214 00:38:47 @agent_ppo2.py:185][0m |          -0.0037 |          73.6354 |        -169.9463 |
[32m[20221214 00:38:47 @agent_ppo2.py:185][0m |          -0.0118 |          63.6390 |        -169.9063 |
[32m[20221214 00:38:47 @agent_ppo2.py:185][0m |          -0.0211 |          61.7516 |        -169.5881 |
[32m[20221214 00:38:48 @agent_ppo2.py:185][0m |          -0.0168 |          60.7060 |        -170.1077 |
[32m[20221214 00:38:48 @agent_ppo2.py:185][0m |          -0.0109 |          60.1360 |        -169.9088 |
[32m[20221214 00:38:48 @agent_ppo2.py:130][0m Policy update time: 1.28 s
[32m[20221214 00:38:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 408.81
[32m[20221214 00:38:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 569.43
[32m[20221214 00:38:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 650.14
[32m[20221214 00:38:48 @agent_ppo2.py:143][0m Total time:      41.29 min
[32m[20221214 00:38:48 @agent_ppo2.py:145][0m 3696640 total steps have happened
[32m[20221214 00:38:48 @agent_ppo2.py:121][0m #------------------------ Iteration 5805 --------------------------#
[32m[20221214 00:38:48 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:38:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:38:48 @agent_ppo2.py:185][0m |           0.0029 |         154.2001 |        -174.9192 |
[32m[20221214 00:38:48 @agent_ppo2.py:185][0m |          -0.0080 |         144.9342 |        -174.0929 |
[32m[20221214 00:38:49 @agent_ppo2.py:185][0m |          -0.0066 |         141.6388 |        -174.1569 |
[32m[20221214 00:38:49 @agent_ppo2.py:185][0m |           0.0061 |         153.5529 |        -174.2972 |
[32m[20221214 00:38:49 @agent_ppo2.py:185][0m |          -0.0115 |         138.0072 |        -173.7972 |
[32m[20221214 00:38:49 @agent_ppo2.py:185][0m |          -0.0067 |         136.6486 |        -174.4284 |
[32m[20221214 00:38:49 @agent_ppo2.py:185][0m |          -0.0120 |         135.4494 |        -174.4429 |
[32m[20221214 00:38:49 @agent_ppo2.py:185][0m |          -0.0101 |         136.3029 |        -174.0227 |
[32m[20221214 00:38:49 @agent_ppo2.py:185][0m |          -0.0030 |         146.3566 |        -174.2798 |
[32m[20221214 00:38:49 @agent_ppo2.py:185][0m |          -0.0144 |         133.5454 |        -171.9847 |
[32m[20221214 00:38:49 @agent_ppo2.py:130][0m Policy update time: 1.25 s
[32m[20221214 00:38:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 490.85
[32m[20221214 00:38:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 690.04
[32m[20221214 00:38:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 748.46
[32m[20221214 00:38:50 @agent_ppo2.py:143][0m Total time:      41.32 min
[32m[20221214 00:38:50 @agent_ppo2.py:145][0m 3698688 total steps have happened
[32m[20221214 00:38:50 @agent_ppo2.py:121][0m #------------------------ Iteration 5806 --------------------------#
[32m[20221214 00:38:50 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:38:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:38:50 @agent_ppo2.py:185][0m |           0.0033 |         162.0289 |        -170.6941 |
[32m[20221214 00:38:50 @agent_ppo2.py:185][0m |           0.0008 |         147.4278 |        -169.5882 |
[32m[20221214 00:38:50 @agent_ppo2.py:185][0m |          -0.0032 |         144.7941 |        -170.1437 |
[32m[20221214 00:38:50 @agent_ppo2.py:185][0m |          -0.0060 |         141.6875 |        -168.5516 |
[32m[20221214 00:38:50 @agent_ppo2.py:185][0m |          -0.0028 |         140.5379 |        -168.3296 |
[32m[20221214 00:38:50 @agent_ppo2.py:185][0m |           0.0008 |         138.3074 |        -167.8072 |
[32m[20221214 00:38:51 @agent_ppo2.py:185][0m |          -0.0080 |         137.1387 |        -170.1068 |
[32m[20221214 00:38:51 @agent_ppo2.py:185][0m |          -0.0108 |         137.0467 |        -170.2144 |
[32m[20221214 00:38:51 @agent_ppo2.py:185][0m |          -0.0080 |         135.2591 |        -169.3861 |
[32m[20221214 00:38:51 @agent_ppo2.py:185][0m |          -0.0095 |         135.3825 |        -169.7544 |
[32m[20221214 00:38:51 @agent_ppo2.py:130][0m Policy update time: 1.25 s
[32m[20221214 00:38:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 649.52
[32m[20221214 00:38:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 841.75
[32m[20221214 00:38:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 781.97
[32m[20221214 00:38:51 @agent_ppo2.py:143][0m Total time:      41.35 min
[32m[20221214 00:38:51 @agent_ppo2.py:145][0m 3700736 total steps have happened
[32m[20221214 00:38:51 @agent_ppo2.py:121][0m #------------------------ Iteration 5807 --------------------------#
[32m[20221214 00:38:51 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:38:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:38:52 @agent_ppo2.py:185][0m |           0.0001 |         102.2043 |        -171.8730 |
[32m[20221214 00:38:52 @agent_ppo2.py:185][0m |          -0.0010 |          94.2269 |        -172.5185 |
[32m[20221214 00:38:52 @agent_ppo2.py:185][0m |           0.0029 |          92.1123 |        -172.4477 |
[32m[20221214 00:38:52 @agent_ppo2.py:185][0m |           0.0078 |          98.8157 |        -172.0846 |
[32m[20221214 00:38:52 @agent_ppo2.py:185][0m |           0.0049 |          91.2217 |        -171.5751 |
[32m[20221214 00:38:52 @agent_ppo2.py:185][0m |          -0.0044 |          89.1571 |        -171.8598 |
[32m[20221214 00:38:52 @agent_ppo2.py:185][0m |          -0.0047 |          88.1878 |        -172.5642 |
[32m[20221214 00:38:52 @agent_ppo2.py:185][0m |          -0.0079 |          87.4377 |        -171.7928 |
[32m[20221214 00:38:52 @agent_ppo2.py:185][0m |          -0.0051 |          87.9136 |        -172.4308 |
[32m[20221214 00:38:53 @agent_ppo2.py:185][0m |          -0.0046 |          87.4668 |        -172.0573 |
[32m[20221214 00:38:53 @agent_ppo2.py:130][0m Policy update time: 1.27 s
[32m[20221214 00:38:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 701.47
[32m[20221214 00:38:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 717.64
[32m[20221214 00:38:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 629.34
[32m[20221214 00:38:53 @agent_ppo2.py:143][0m Total time:      41.37 min
[32m[20221214 00:38:53 @agent_ppo2.py:145][0m 3702784 total steps have happened
[32m[20221214 00:38:53 @agent_ppo2.py:121][0m #------------------------ Iteration 5808 --------------------------#
[32m[20221214 00:38:53 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:38:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:38:53 @agent_ppo2.py:185][0m |           0.0092 |         118.3340 |        -175.7218 |
[32m[20221214 00:38:53 @agent_ppo2.py:185][0m |          -0.0002 |         111.3685 |        -176.2736 |
[32m[20221214 00:38:53 @agent_ppo2.py:185][0m |          -0.0058 |         108.0657 |        -175.7169 |
[32m[20221214 00:38:53 @agent_ppo2.py:185][0m |          -0.0075 |         105.9760 |        -175.2716 |
[32m[20221214 00:38:54 @agent_ppo2.py:185][0m |          -0.0125 |         104.4525 |        -174.8397 |
[32m[20221214 00:38:54 @agent_ppo2.py:185][0m |          -0.0062 |         105.7353 |        -175.3292 |
[32m[20221214 00:38:54 @agent_ppo2.py:185][0m |          -0.0102 |         102.7257 |        -175.8136 |
[32m[20221214 00:38:54 @agent_ppo2.py:185][0m |          -0.0066 |         105.0563 |        -175.5038 |
[32m[20221214 00:38:54 @agent_ppo2.py:185][0m |          -0.0129 |         102.1624 |        -175.9413 |
[32m[20221214 00:38:54 @agent_ppo2.py:185][0m |          -0.0127 |         101.3356 |        -175.5203 |
[32m[20221214 00:38:54 @agent_ppo2.py:130][0m Policy update time: 1.29 s
[32m[20221214 00:38:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 684.29
[32m[20221214 00:38:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 776.70
[32m[20221214 00:38:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 624.69
[32m[20221214 00:38:54 @agent_ppo2.py:143][0m Total time:      41.40 min
[32m[20221214 00:38:54 @agent_ppo2.py:145][0m 3704832 total steps have happened
[32m[20221214 00:38:54 @agent_ppo2.py:121][0m #------------------------ Iteration 5809 --------------------------#
[32m[20221214 00:38:55 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:38:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:38:55 @agent_ppo2.py:185][0m |           0.0118 |          60.0676 |        -178.3362 |
[32m[20221214 00:38:55 @agent_ppo2.py:185][0m |           0.0067 |          58.3795 |        -178.4520 |
[32m[20221214 00:38:55 @agent_ppo2.py:185][0m |          -0.0098 |          44.6972 |        -178.6171 |
[32m[20221214 00:38:55 @agent_ppo2.py:185][0m |          -0.0157 |          42.1814 |        -178.5862 |
[32m[20221214 00:38:55 @agent_ppo2.py:185][0m |          -0.0180 |          40.4570 |        -178.8070 |
[32m[20221214 00:38:55 @agent_ppo2.py:185][0m |          -0.0112 |          39.4484 |        -178.8035 |
[32m[20221214 00:38:55 @agent_ppo2.py:185][0m |          -0.0196 |          38.7032 |        -178.9345 |
[32m[20221214 00:38:56 @agent_ppo2.py:185][0m |          -0.0141 |          39.3244 |        -179.1065 |
[32m[20221214 00:38:56 @agent_ppo2.py:185][0m |          -0.0207 |          37.2606 |        -178.2349 |
[32m[20221214 00:38:56 @agent_ppo2.py:185][0m |          -0.0210 |          36.2638 |        -178.1948 |
[32m[20221214 00:38:56 @agent_ppo2.py:130][0m Policy update time: 1.25 s
[32m[20221214 00:38:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 681.31
[32m[20221214 00:38:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.73
[32m[20221214 00:38:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 800.48
[32m[20221214 00:38:56 @agent_ppo2.py:143][0m Total time:      41.43 min
[32m[20221214 00:38:56 @agent_ppo2.py:145][0m 3706880 total steps have happened
[32m[20221214 00:38:56 @agent_ppo2.py:121][0m #------------------------ Iteration 5810 --------------------------#
[32m[20221214 00:38:56 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221214 00:38:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:38:56 @agent_ppo2.py:185][0m |           0.0028 |         123.5884 |        -171.9887 |
[32m[20221214 00:38:57 @agent_ppo2.py:185][0m |          -0.0038 |         110.0195 |        -171.9624 |
[32m[20221214 00:38:57 @agent_ppo2.py:185][0m |           0.0004 |         104.8701 |        -171.9494 |
[32m[20221214 00:38:57 @agent_ppo2.py:185][0m |          -0.0016 |         104.6699 |        -172.5353 |
[32m[20221214 00:38:57 @agent_ppo2.py:185][0m |          -0.0098 |         102.2982 |        -170.9671 |
[32m[20221214 00:38:57 @agent_ppo2.py:185][0m |          -0.0113 |         101.9073 |        -171.8427 |
[32m[20221214 00:38:57 @agent_ppo2.py:185][0m |          -0.0028 |         103.4258 |        -171.4036 |
[32m[20221214 00:38:57 @agent_ppo2.py:185][0m |          -0.0085 |         102.0262 |        -171.2072 |
[32m[20221214 00:38:57 @agent_ppo2.py:185][0m |          -0.0098 |         100.2678 |        -170.3659 |
[32m[20221214 00:38:57 @agent_ppo2.py:185][0m |          -0.0085 |         100.3957 |        -170.8214 |
[32m[20221214 00:38:57 @agent_ppo2.py:130][0m Policy update time: 1.26 s
[32m[20221214 00:38:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 616.79
[32m[20221214 00:38:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 768.35
[32m[20221214 00:38:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 723.00
[32m[20221214 00:38:58 @agent_ppo2.py:143][0m Total time:      41.45 min
[32m[20221214 00:38:58 @agent_ppo2.py:145][0m 3708928 total steps have happened
[32m[20221214 00:38:58 @agent_ppo2.py:121][0m #------------------------ Iteration 5811 --------------------------#
[32m[20221214 00:38:58 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:38:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:38:58 @agent_ppo2.py:185][0m |           0.0028 |          75.1117 |        -177.5602 |
[32m[20221214 00:38:58 @agent_ppo2.py:185][0m |          -0.0012 |          68.7756 |        -177.7450 |
[32m[20221214 00:38:58 @agent_ppo2.py:185][0m |          -0.0065 |          67.1595 |        -177.6777 |
[32m[20221214 00:38:58 @agent_ppo2.py:185][0m |           0.0030 |          67.6887 |        -177.5543 |
[32m[20221214 00:38:58 @agent_ppo2.py:185][0m |          -0.0073 |          64.5478 |        -176.8002 |
[32m[20221214 00:38:59 @agent_ppo2.py:185][0m |          -0.0072 |          63.6411 |        -176.9700 |
[32m[20221214 00:38:59 @agent_ppo2.py:185][0m |          -0.0145 |          62.7315 |        -176.3993 |
[32m[20221214 00:38:59 @agent_ppo2.py:185][0m |          -0.0100 |          62.2224 |        -177.6503 |
[32m[20221214 00:38:59 @agent_ppo2.py:185][0m |          -0.0120 |          62.5766 |        -177.8117 |
[32m[20221214 00:38:59 @agent_ppo2.py:185][0m |          -0.0112 |          61.9395 |        -177.2336 |
[32m[20221214 00:38:59 @agent_ppo2.py:130][0m Policy update time: 1.28 s
[32m[20221214 00:38:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 540.79
[32m[20221214 00:38:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 720.55
[32m[20221214 00:38:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 568.35
[32m[20221214 00:38:59 @agent_ppo2.py:143][0m Total time:      41.48 min
[32m[20221214 00:38:59 @agent_ppo2.py:145][0m 3710976 total steps have happened
[32m[20221214 00:38:59 @agent_ppo2.py:121][0m #------------------------ Iteration 5812 --------------------------#
[32m[20221214 00:38:59 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:39:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:39:00 @agent_ppo2.py:185][0m |           0.0077 |          97.2118 |        -179.4188 |
[32m[20221214 00:39:00 @agent_ppo2.py:185][0m |          -0.0056 |          89.7672 |        -178.9271 |
[32m[20221214 00:39:00 @agent_ppo2.py:185][0m |           0.0073 |          92.0786 |        -178.4527 |
[32m[20221214 00:39:00 @agent_ppo2.py:185][0m |          -0.0047 |          86.9582 |        -178.6182 |
[32m[20221214 00:39:00 @agent_ppo2.py:185][0m |          -0.0098 |          85.6377 |        -178.6073 |
[32m[20221214 00:39:00 @agent_ppo2.py:185][0m |          -0.0067 |          86.4112 |        -178.7971 |
[32m[20221214 00:39:00 @agent_ppo2.py:185][0m |          -0.0103 |          85.2975 |        -179.0890 |
[32m[20221214 00:39:00 @agent_ppo2.py:185][0m |          -0.0053 |          84.6737 |        -178.9203 |
[32m[20221214 00:39:01 @agent_ppo2.py:185][0m |          -0.0108 |          84.4021 |        -178.5996 |
[32m[20221214 00:39:01 @agent_ppo2.py:185][0m |          -0.0145 |          83.8548 |        -178.7097 |
[32m[20221214 00:39:01 @agent_ppo2.py:130][0m Policy update time: 1.25 s
[32m[20221214 00:39:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 627.17
[32m[20221214 00:39:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 746.59
[32m[20221214 00:39:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 662.12
[32m[20221214 00:39:01 @agent_ppo2.py:143][0m Total time:      41.51 min
[32m[20221214 00:39:01 @agent_ppo2.py:145][0m 3713024 total steps have happened
[32m[20221214 00:39:01 @agent_ppo2.py:121][0m #------------------------ Iteration 5813 --------------------------#
[32m[20221214 00:39:01 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:39:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:39:01 @agent_ppo2.py:185][0m |           0.0083 |          54.2989 |        -187.3847 |
[32m[20221214 00:39:01 @agent_ppo2.py:185][0m |          -0.0032 |          44.6899 |        -186.1291 |
[32m[20221214 00:39:01 @agent_ppo2.py:185][0m |          -0.0075 |          42.4725 |        -186.1860 |
[32m[20221214 00:39:02 @agent_ppo2.py:185][0m |          -0.0098 |          41.3672 |        -185.5598 |
[32m[20221214 00:39:02 @agent_ppo2.py:185][0m |          -0.0101 |          40.5325 |        -186.6787 |
[32m[20221214 00:39:02 @agent_ppo2.py:185][0m |          -0.0134 |          39.8551 |        -187.1051 |
[32m[20221214 00:39:02 @agent_ppo2.py:185][0m |          -0.0106 |          39.1671 |        -186.3801 |
[32m[20221214 00:39:02 @agent_ppo2.py:185][0m |          -0.0146 |          38.8466 |        -185.6141 |
[32m[20221214 00:39:02 @agent_ppo2.py:185][0m |          -0.0107 |          38.7175 |        -186.0428 |
[32m[20221214 00:39:02 @agent_ppo2.py:185][0m |          -0.0156 |          38.0798 |        -186.0825 |
[32m[20221214 00:39:02 @agent_ppo2.py:130][0m Policy update time: 1.27 s
[32m[20221214 00:39:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 663.94
[32m[20221214 00:39:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 774.14
[32m[20221214 00:39:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 579.09
[32m[20221214 00:39:02 @agent_ppo2.py:143][0m Total time:      41.54 min
[32m[20221214 00:39:02 @agent_ppo2.py:145][0m 3715072 total steps have happened
[32m[20221214 00:39:02 @agent_ppo2.py:121][0m #------------------------ Iteration 5814 --------------------------#
[32m[20221214 00:39:03 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:39:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:39:03 @agent_ppo2.py:185][0m |          -0.0028 |          85.2131 |        -176.7415 |
[32m[20221214 00:39:03 @agent_ppo2.py:185][0m |          -0.0078 |          72.6492 |        -176.5438 |
[32m[20221214 00:39:03 @agent_ppo2.py:185][0m |          -0.0133 |          68.2084 |        -175.9161 |
[32m[20221214 00:39:03 @agent_ppo2.py:185][0m |          -0.0102 |          64.8316 |        -174.7799 |
[32m[20221214 00:39:03 @agent_ppo2.py:185][0m |          -0.0150 |          62.6866 |        -174.4739 |
[32m[20221214 00:39:03 @agent_ppo2.py:185][0m |          -0.0147 |          60.8749 |        -175.3750 |
[32m[20221214 00:39:04 @agent_ppo2.py:185][0m |          -0.0175 |          60.4708 |        -174.9337 |
[32m[20221214 00:39:04 @agent_ppo2.py:185][0m |          -0.0145 |          59.2333 |        -174.3728 |
[32m[20221214 00:39:04 @agent_ppo2.py:185][0m |          -0.0177 |          58.0851 |        -173.8592 |
[32m[20221214 00:39:04 @agent_ppo2.py:185][0m |          -0.0217 |          57.2414 |        -174.5209 |
[32m[20221214 00:39:04 @agent_ppo2.py:130][0m Policy update time: 1.25 s
[32m[20221214 00:39:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 602.49
[32m[20221214 00:39:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 675.95
[32m[20221214 00:39:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 696.75
[32m[20221214 00:39:04 @agent_ppo2.py:143][0m Total time:      41.56 min
[32m[20221214 00:39:04 @agent_ppo2.py:145][0m 3717120 total steps have happened
[32m[20221214 00:39:04 @agent_ppo2.py:121][0m #------------------------ Iteration 5815 --------------------------#
[32m[20221214 00:39:04 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:39:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:39:04 @agent_ppo2.py:185][0m |           0.0024 |          51.5485 |        -181.5479 |
[32m[20221214 00:39:05 @agent_ppo2.py:185][0m |           0.0050 |          37.8774 |        -180.5326 |
[32m[20221214 00:39:05 @agent_ppo2.py:185][0m |          -0.0077 |          33.0573 |        -180.2060 |
[32m[20221214 00:39:05 @agent_ppo2.py:185][0m |          -0.0019 |          30.9047 |        -179.5583 |
[32m[20221214 00:39:05 @agent_ppo2.py:185][0m |          -0.0118 |          29.8267 |        -180.2025 |
[32m[20221214 00:39:05 @agent_ppo2.py:185][0m |          -0.0128 |          29.0263 |        -179.9296 |
[32m[20221214 00:39:05 @agent_ppo2.py:185][0m |          -0.0080 |          28.7466 |        -179.6908 |
[32m[20221214 00:39:05 @agent_ppo2.py:185][0m |          -0.0129 |          28.2114 |        -180.3223 |
[32m[20221214 00:39:05 @agent_ppo2.py:185][0m |          -0.0142 |          27.1812 |        -180.3105 |
[32m[20221214 00:39:06 @agent_ppo2.py:185][0m |          -0.0208 |          26.8092 |        -179.9068 |
[32m[20221214 00:39:06 @agent_ppo2.py:130][0m Policy update time: 1.28 s
[32m[20221214 00:39:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 552.50
[32m[20221214 00:39:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 636.08
[32m[20221214 00:39:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 852.81
[32m[20221214 00:39:06 @agent_ppo2.py:143][0m Total time:      41.59 min
[32m[20221214 00:39:06 @agent_ppo2.py:145][0m 3719168 total steps have happened
[32m[20221214 00:39:06 @agent_ppo2.py:121][0m #------------------------ Iteration 5816 --------------------------#
[32m[20221214 00:39:06 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:39:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:39:06 @agent_ppo2.py:185][0m |           0.0050 |          54.9199 |        -180.1024 |
[32m[20221214 00:39:06 @agent_ppo2.py:185][0m |          -0.0064 |          45.1955 |        -178.5683 |
[32m[20221214 00:39:06 @agent_ppo2.py:185][0m |          -0.0074 |          42.9089 |        -178.8693 |
[32m[20221214 00:39:06 @agent_ppo2.py:185][0m |          -0.0095 |          41.4032 |        -178.7715 |
[32m[20221214 00:39:07 @agent_ppo2.py:185][0m |          -0.0167 |          40.6813 |        -178.4808 |
[32m[20221214 00:39:07 @agent_ppo2.py:185][0m |          -0.0132 |          39.7677 |        -177.6878 |
[32m[20221214 00:39:07 @agent_ppo2.py:185][0m |          -0.0139 |          39.1239 |        -178.2864 |
[32m[20221214 00:39:07 @agent_ppo2.py:185][0m |          -0.0039 |          43.9215 |        -178.4193 |
[32m[20221214 00:39:07 @agent_ppo2.py:185][0m |          -0.0136 |          39.8849 |        -177.2456 |
[32m[20221214 00:39:07 @agent_ppo2.py:185][0m |          -0.0196 |          38.0610 |        -177.2255 |
[32m[20221214 00:39:07 @agent_ppo2.py:130][0m Policy update time: 1.26 s
[32m[20221214 00:39:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 550.23
[32m[20221214 00:39:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 681.92
[32m[20221214 00:39:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 671.21
[32m[20221214 00:39:07 @agent_ppo2.py:143][0m Total time:      41.62 min
[32m[20221214 00:39:07 @agent_ppo2.py:145][0m 3721216 total steps have happened
[32m[20221214 00:39:07 @agent_ppo2.py:121][0m #------------------------ Iteration 5817 --------------------------#
[32m[20221214 00:39:08 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:39:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:39:08 @agent_ppo2.py:185][0m |           0.0080 |          54.4180 |        -173.6544 |
[32m[20221214 00:39:08 @agent_ppo2.py:185][0m |          -0.0051 |          46.9791 |        -173.0541 |
[32m[20221214 00:39:08 @agent_ppo2.py:185][0m |          -0.0056 |          45.7613 |        -172.9226 |
[32m[20221214 00:39:08 @agent_ppo2.py:185][0m |          -0.0088 |          44.9188 |        -173.5075 |
[32m[20221214 00:39:08 @agent_ppo2.py:185][0m |          -0.0107 |          44.2447 |        -173.2555 |
[32m[20221214 00:39:08 @agent_ppo2.py:185][0m |          -0.0045 |          44.8748 |        -172.5460 |
[32m[20221214 00:39:08 @agent_ppo2.py:185][0m |          -0.0164 |          42.9457 |        -172.2958 |
[32m[20221214 00:39:09 @agent_ppo2.py:185][0m |          -0.0151 |          42.3637 |        -173.3405 |
[32m[20221214 00:39:09 @agent_ppo2.py:185][0m |          -0.0172 |          42.0316 |        -172.6103 |
[32m[20221214 00:39:09 @agent_ppo2.py:185][0m |          -0.0140 |          41.9951 |        -172.2349 |
[32m[20221214 00:39:09 @agent_ppo2.py:130][0m Policy update time: 1.26 s
[32m[20221214 00:39:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 578.13
[32m[20221214 00:39:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 673.78
[32m[20221214 00:39:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 781.23
[32m[20221214 00:39:09 @agent_ppo2.py:143][0m Total time:      41.64 min
[32m[20221214 00:39:09 @agent_ppo2.py:145][0m 3723264 total steps have happened
[32m[20221214 00:39:09 @agent_ppo2.py:121][0m #------------------------ Iteration 5818 --------------------------#
[32m[20221214 00:39:09 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:39:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:39:09 @agent_ppo2.py:185][0m |           0.0057 |          44.7640 |        -178.7171 |
[32m[20221214 00:39:09 @agent_ppo2.py:185][0m |          -0.0061 |          39.0445 |        -178.0317 |
[32m[20221214 00:39:10 @agent_ppo2.py:185][0m |          -0.0095 |          36.8256 |        -178.6704 |
[32m[20221214 00:39:10 @agent_ppo2.py:185][0m |          -0.0152 |          35.7220 |        -177.9689 |
[32m[20221214 00:39:10 @agent_ppo2.py:185][0m |          -0.0145 |          35.3267 |        -177.9962 |
[32m[20221214 00:39:10 @agent_ppo2.py:185][0m |          -0.0155 |          34.3321 |        -177.0958 |
[32m[20221214 00:39:10 @agent_ppo2.py:185][0m |          -0.0125 |          34.1676 |        -178.0846 |
[32m[20221214 00:39:10 @agent_ppo2.py:185][0m |          -0.0211 |          33.5407 |        -177.1302 |
[32m[20221214 00:39:10 @agent_ppo2.py:185][0m |          -0.0232 |          33.0149 |        -177.8467 |
[32m[20221214 00:39:10 @agent_ppo2.py:185][0m |          -0.0221 |          32.6539 |        -178.4248 |
[32m[20221214 00:39:10 @agent_ppo2.py:130][0m Policy update time: 1.25 s
[32m[20221214 00:39:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 536.33
[32m[20221214 00:39:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 564.18
[32m[20221214 00:39:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 776.63
[32m[20221214 00:39:11 @agent_ppo2.py:143][0m Total time:      41.67 min
[32m[20221214 00:39:11 @agent_ppo2.py:145][0m 3725312 total steps have happened
[32m[20221214 00:39:11 @agent_ppo2.py:121][0m #------------------------ Iteration 5819 --------------------------#
[32m[20221214 00:39:11 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:39:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:39:11 @agent_ppo2.py:185][0m |           0.0057 |          62.3884 |        -181.3632 |
[32m[20221214 00:39:11 @agent_ppo2.py:185][0m |          -0.0049 |          57.8951 |        -181.5679 |
[32m[20221214 00:39:11 @agent_ppo2.py:185][0m |          -0.0021 |          57.0726 |        -180.9134 |
[32m[20221214 00:39:11 @agent_ppo2.py:185][0m |          -0.0103 |          55.4560 |        -180.4702 |
[32m[20221214 00:39:11 @agent_ppo2.py:185][0m |          -0.0095 |          54.9650 |        -180.8611 |
[32m[20221214 00:39:12 @agent_ppo2.py:185][0m |          -0.0103 |          54.0795 |        -180.4164 |
[32m[20221214 00:39:12 @agent_ppo2.py:185][0m |          -0.0109 |          53.4152 |        -180.6288 |
[32m[20221214 00:39:12 @agent_ppo2.py:185][0m |          -0.0113 |          53.9378 |        -180.1823 |
[32m[20221214 00:39:12 @agent_ppo2.py:185][0m |          -0.0118 |          52.9623 |        -180.4374 |
[32m[20221214 00:39:12 @agent_ppo2.py:185][0m |          -0.0132 |          52.2841 |        -179.9745 |
[32m[20221214 00:39:12 @agent_ppo2.py:130][0m Policy update time: 1.27 s
[32m[20221214 00:39:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 607.82
[32m[20221214 00:39:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 682.48
[32m[20221214 00:39:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 831.72
[32m[20221214 00:39:12 @agent_ppo2.py:143][0m Total time:      41.70 min
[32m[20221214 00:39:12 @agent_ppo2.py:145][0m 3727360 total steps have happened
[32m[20221214 00:39:12 @agent_ppo2.py:121][0m #------------------------ Iteration 5820 --------------------------#
[32m[20221214 00:39:12 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221214 00:39:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:39:13 @agent_ppo2.py:185][0m |           0.0039 |         201.0303 |        -176.1209 |
[32m[20221214 00:39:13 @agent_ppo2.py:185][0m |           0.0278 |         233.2733 |        -175.6079 |
[32m[20221214 00:39:13 @agent_ppo2.py:185][0m |          -0.0054 |         188.5608 |        -176.0574 |
[32m[20221214 00:39:13 @agent_ppo2.py:185][0m |          -0.0037 |         184.2146 |        -175.6598 |
[32m[20221214 00:39:13 @agent_ppo2.py:185][0m |          -0.0056 |         182.7436 |        -174.8881 |
[32m[20221214 00:39:13 @agent_ppo2.py:185][0m |           0.0007 |         182.8783 |        -173.7265 |
[32m[20221214 00:39:13 @agent_ppo2.py:185][0m |          -0.0058 |         181.2272 |        -175.1344 |
[32m[20221214 00:39:13 @agent_ppo2.py:185][0m |          -0.0042 |         180.6837 |        -174.3034 |
[32m[20221214 00:39:14 @agent_ppo2.py:185][0m |          -0.0078 |         179.7985 |        -173.8641 |
[32m[20221214 00:39:14 @agent_ppo2.py:185][0m |          -0.0012 |         182.5186 |        -174.3489 |
[32m[20221214 00:39:14 @agent_ppo2.py:130][0m Policy update time: 1.25 s
[32m[20221214 00:39:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 681.15
[32m[20221214 00:39:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 752.69
[32m[20221214 00:39:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 800.08
[32m[20221214 00:39:14 @agent_ppo2.py:143][0m Total time:      41.72 min
[32m[20221214 00:39:14 @agent_ppo2.py:145][0m 3729408 total steps have happened
[32m[20221214 00:39:14 @agent_ppo2.py:121][0m #------------------------ Iteration 5821 --------------------------#
[32m[20221214 00:39:14 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:39:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:39:14 @agent_ppo2.py:185][0m |           0.0083 |          69.2462 |        -173.1436 |
[32m[20221214 00:39:14 @agent_ppo2.py:185][0m |           0.0050 |          63.6891 |        -172.6118 |
[32m[20221214 00:39:14 @agent_ppo2.py:185][0m |           0.0086 |          65.5637 |        -173.0438 |
[32m[20221214 00:39:15 @agent_ppo2.py:185][0m |          -0.0051 |          60.4715 |        -172.1004 |
[32m[20221214 00:39:15 @agent_ppo2.py:185][0m |          -0.0007 |          60.4736 |        -172.0803 |
[32m[20221214 00:39:15 @agent_ppo2.py:185][0m |          -0.0053 |          60.0371 |        -171.2372 |
[32m[20221214 00:39:15 @agent_ppo2.py:185][0m |          -0.0004 |          60.4385 |        -169.2768 |
[32m[20221214 00:39:15 @agent_ppo2.py:185][0m |          -0.0021 |          58.8936 |        -170.4942 |
[32m[20221214 00:39:15 @agent_ppo2.py:185][0m |          -0.0039 |          58.5126 |        -170.3654 |
[32m[20221214 00:39:15 @agent_ppo2.py:185][0m |          -0.0064 |          59.7328 |        -170.8056 |
[32m[20221214 00:39:15 @agent_ppo2.py:130][0m Policy update time: 1.26 s
[32m[20221214 00:39:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 728.69
[32m[20221214 00:39:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 780.73
[32m[20221214 00:39:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 763.83
[32m[20221214 00:39:15 @agent_ppo2.py:143][0m Total time:      41.75 min
[32m[20221214 00:39:15 @agent_ppo2.py:145][0m 3731456 total steps have happened
[32m[20221214 00:39:15 @agent_ppo2.py:121][0m #------------------------ Iteration 5822 --------------------------#
[32m[20221214 00:39:16 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:39:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:39:16 @agent_ppo2.py:185][0m |           0.0076 |          72.2073 |        -168.9643 |
[32m[20221214 00:39:16 @agent_ppo2.py:185][0m |           0.0007 |          66.3108 |        -168.9925 |
[32m[20221214 00:39:16 @agent_ppo2.py:185][0m |           0.0070 |          68.7874 |        -170.3802 |
[32m[20221214 00:39:16 @agent_ppo2.py:185][0m |          -0.0057 |          64.6791 |        -169.0182 |
[32m[20221214 00:39:16 @agent_ppo2.py:185][0m |          -0.0059 |          66.1319 |        -168.5860 |
[32m[20221214 00:39:16 @agent_ppo2.py:185][0m |          -0.0042 |          63.3286 |        -167.3805 |
[32m[20221214 00:39:17 @agent_ppo2.py:185][0m |          -0.0057 |          63.1387 |        -168.7194 |
[32m[20221214 00:39:17 @agent_ppo2.py:185][0m |          -0.0069 |          61.9143 |        -168.3530 |
[32m[20221214 00:39:17 @agent_ppo2.py:185][0m |          -0.0078 |          63.2330 |        -167.8067 |
[32m[20221214 00:39:17 @agent_ppo2.py:185][0m |          -0.0079 |          63.1963 |        -167.7580 |
[32m[20221214 00:39:17 @agent_ppo2.py:130][0m Policy update time: 1.27 s
[32m[20221214 00:39:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 740.25
[32m[20221214 00:39:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 762.56
[32m[20221214 00:39:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 447.85
[32m[20221214 00:39:17 @agent_ppo2.py:143][0m Total time:      41.78 min
[32m[20221214 00:39:17 @agent_ppo2.py:145][0m 3733504 total steps have happened
[32m[20221214 00:39:17 @agent_ppo2.py:121][0m #------------------------ Iteration 5823 --------------------------#
[32m[20221214 00:39:17 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:39:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:39:17 @agent_ppo2.py:185][0m |           0.0025 |         184.0143 |        -176.6544 |
[32m[20221214 00:39:18 @agent_ppo2.py:185][0m |           0.0002 |         178.7810 |        -175.9607 |
[32m[20221214 00:39:18 @agent_ppo2.py:185][0m |           0.0073 |         184.6178 |        -175.8646 |
[32m[20221214 00:39:18 @agent_ppo2.py:185][0m |          -0.0068 |         169.9025 |        -175.6485 |
[32m[20221214 00:39:18 @agent_ppo2.py:185][0m |          -0.0051 |         169.0859 |        -175.6409 |
[32m[20221214 00:39:18 @agent_ppo2.py:185][0m |          -0.0074 |         168.4560 |        -175.4375 |
[32m[20221214 00:39:18 @agent_ppo2.py:185][0m |          -0.0081 |         167.5679 |        -173.7532 |
[32m[20221214 00:39:18 @agent_ppo2.py:185][0m |           0.0146 |         196.2027 |        -174.5673 |
[32m[20221214 00:39:18 @agent_ppo2.py:185][0m |          -0.0076 |         172.8907 |        -174.6767 |
[32m[20221214 00:39:18 @agent_ppo2.py:185][0m |          -0.0088 |         166.7211 |        -175.1008 |
[32m[20221214 00:39:18 @agent_ppo2.py:130][0m Policy update time: 1.26 s
[32m[20221214 00:39:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 661.47
[32m[20221214 00:39:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 818.24
[32m[20221214 00:39:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 250.44
[32m[20221214 00:39:19 @agent_ppo2.py:143][0m Total time:      41.81 min
[32m[20221214 00:39:19 @agent_ppo2.py:145][0m 3735552 total steps have happened
[32m[20221214 00:39:19 @agent_ppo2.py:121][0m #------------------------ Iteration 5824 --------------------------#
[32m[20221214 00:39:19 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:39:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:39:19 @agent_ppo2.py:185][0m |           0.0010 |         146.8072 |        -167.6825 |
[32m[20221214 00:39:19 @agent_ppo2.py:185][0m |          -0.0018 |         141.0331 |        -167.3845 |
[32m[20221214 00:39:19 @agent_ppo2.py:185][0m |          -0.0063 |         138.9691 |        -166.7162 |
[32m[20221214 00:39:19 @agent_ppo2.py:185][0m |          -0.0083 |         137.6610 |        -166.1767 |
[32m[20221214 00:39:20 @agent_ppo2.py:185][0m |          -0.0106 |         137.0049 |        -166.8936 |
[32m[20221214 00:39:20 @agent_ppo2.py:185][0m |          -0.0066 |         137.2314 |        -166.4697 |
[32m[20221214 00:39:20 @agent_ppo2.py:185][0m |          -0.0083 |         135.8692 |        -165.9159 |
[32m[20221214 00:39:20 @agent_ppo2.py:185][0m |          -0.0095 |         135.6957 |        -165.8040 |
[32m[20221214 00:39:20 @agent_ppo2.py:185][0m |          -0.0090 |         135.5266 |        -165.9781 |
[32m[20221214 00:39:20 @agent_ppo2.py:185][0m |          -0.0144 |         134.8510 |        -165.7719 |
[32m[20221214 00:39:20 @agent_ppo2.py:130][0m Policy update time: 1.31 s
[32m[20221214 00:39:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 650.95
[32m[20221214 00:39:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 793.30
[32m[20221214 00:39:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 784.34
[32m[20221214 00:39:20 @agent_ppo2.py:143][0m Total time:      41.83 min
[32m[20221214 00:39:20 @agent_ppo2.py:145][0m 3737600 total steps have happened
[32m[20221214 00:39:20 @agent_ppo2.py:121][0m #------------------------ Iteration 5825 --------------------------#
[32m[20221214 00:39:21 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:39:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:39:21 @agent_ppo2.py:185][0m |           0.0122 |         146.9686 |        -167.5948 |
[32m[20221214 00:39:21 @agent_ppo2.py:185][0m |          -0.0058 |         127.8852 |        -167.4682 |
[32m[20221214 00:39:21 @agent_ppo2.py:185][0m |          -0.0113 |         124.8523 |        -167.6940 |
[32m[20221214 00:39:21 @agent_ppo2.py:185][0m |          -0.0097 |         124.0394 |        -167.8518 |
[32m[20221214 00:39:21 @agent_ppo2.py:185][0m |          -0.0129 |         123.4190 |        -168.5394 |
[32m[20221214 00:39:21 @agent_ppo2.py:185][0m |          -0.0156 |         122.0361 |        -168.0727 |
[32m[20221214 00:39:21 @agent_ppo2.py:185][0m |          -0.0115 |         121.9343 |        -168.2476 |
[32m[20221214 00:39:22 @agent_ppo2.py:185][0m |          -0.0149 |         122.0720 |        -168.1125 |
[32m[20221214 00:39:22 @agent_ppo2.py:185][0m |           0.0014 |         139.0179 |        -167.8469 |
[32m[20221214 00:39:22 @agent_ppo2.py:185][0m |          -0.0161 |         123.4932 |        -168.8993 |
[32m[20221214 00:39:22 @agent_ppo2.py:130][0m Policy update time: 1.32 s
[32m[20221214 00:39:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 660.85
[32m[20221214 00:39:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 703.73
[32m[20221214 00:39:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 794.92
[32m[20221214 00:39:22 @agent_ppo2.py:143][0m Total time:      41.86 min
[32m[20221214 00:39:22 @agent_ppo2.py:145][0m 3739648 total steps have happened
[32m[20221214 00:39:22 @agent_ppo2.py:121][0m #------------------------ Iteration 5826 --------------------------#
[32m[20221214 00:39:22 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:39:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:39:22 @agent_ppo2.py:185][0m |           0.0037 |         177.5048 |        -176.2640 |
[32m[20221214 00:39:23 @agent_ppo2.py:185][0m |           0.0011 |         174.4050 |        -176.7151 |
[32m[20221214 00:39:23 @agent_ppo2.py:185][0m |          -0.0021 |         174.1576 |        -176.6226 |
[32m[20221214 00:39:23 @agent_ppo2.py:185][0m |          -0.0025 |         173.1430 |        -176.9829 |
[32m[20221214 00:39:23 @agent_ppo2.py:185][0m |          -0.0027 |         172.4092 |        -176.1425 |
[32m[20221214 00:39:23 @agent_ppo2.py:185][0m |           0.0021 |         171.9476 |        -176.6528 |
[32m[20221214 00:39:23 @agent_ppo2.py:185][0m |          -0.0012 |         171.7943 |        -177.7020 |
[32m[20221214 00:39:23 @agent_ppo2.py:185][0m |          -0.0015 |         171.4828 |        -175.9600 |
[32m[20221214 00:39:23 @agent_ppo2.py:185][0m |           0.0110 |         189.5636 |        -176.5036 |
[32m[20221214 00:39:24 @agent_ppo2.py:185][0m |          -0.0045 |         171.1909 |        -176.7017 |
[32m[20221214 00:39:24 @agent_ppo2.py:130][0m Policy update time: 1.39 s
[32m[20221214 00:39:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 751.49
[32m[20221214 00:39:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 789.34
[32m[20221214 00:39:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 647.14
[32m[20221214 00:39:24 @agent_ppo2.py:143][0m Total time:      41.89 min
[32m[20221214 00:39:24 @agent_ppo2.py:145][0m 3741696 total steps have happened
[32m[20221214 00:39:24 @agent_ppo2.py:121][0m #------------------------ Iteration 5827 --------------------------#
[32m[20221214 00:39:24 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:39:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:39:24 @agent_ppo2.py:185][0m |           0.0002 |         148.9937 |        -175.7191 |
[32m[20221214 00:39:24 @agent_ppo2.py:185][0m |           0.0012 |         147.4925 |        -174.8193 |
[32m[20221214 00:39:24 @agent_ppo2.py:185][0m |          -0.0060 |         136.6666 |        -174.9309 |
[32m[20221214 00:39:25 @agent_ppo2.py:185][0m |          -0.0101 |         132.5676 |        -175.0437 |
[32m[20221214 00:39:25 @agent_ppo2.py:185][0m |          -0.0089 |         130.9621 |        -174.5534 |
[32m[20221214 00:39:25 @agent_ppo2.py:185][0m |          -0.0024 |         134.1797 |        -174.5352 |
[32m[20221214 00:39:25 @agent_ppo2.py:185][0m |          -0.0145 |         129.0067 |        -174.4708 |
[32m[20221214 00:39:25 @agent_ppo2.py:185][0m |          -0.0136 |         128.8985 |        -174.5997 |
[32m[20221214 00:39:25 @agent_ppo2.py:185][0m |          -0.0147 |         127.6822 |        -174.2795 |
[32m[20221214 00:39:25 @agent_ppo2.py:185][0m |          -0.0155 |         126.9891 |        -174.9368 |
[32m[20221214 00:39:25 @agent_ppo2.py:130][0m Policy update time: 1.30 s
[32m[20221214 00:39:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 541.68
[32m[20221214 00:39:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 660.67
[32m[20221214 00:39:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 718.32
[32m[20221214 00:39:25 @agent_ppo2.py:143][0m Total time:      41.92 min
[32m[20221214 00:39:25 @agent_ppo2.py:145][0m 3743744 total steps have happened
[32m[20221214 00:39:25 @agent_ppo2.py:121][0m #------------------------ Iteration 5828 --------------------------#
[32m[20221214 00:39:26 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:39:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:39:26 @agent_ppo2.py:185][0m |           0.0054 |         140.0056 |        -170.4326 |
[32m[20221214 00:39:26 @agent_ppo2.py:185][0m |          -0.0098 |         132.8994 |        -171.8838 |
[32m[20221214 00:39:26 @agent_ppo2.py:185][0m |          -0.0104 |         130.5245 |        -171.8833 |
[32m[20221214 00:39:26 @agent_ppo2.py:185][0m |          -0.0145 |         128.6829 |        -172.2714 |
[32m[20221214 00:39:26 @agent_ppo2.py:185][0m |          -0.0160 |         127.3515 |        -171.4604 |
[32m[20221214 00:39:26 @agent_ppo2.py:185][0m |          -0.0171 |         126.1759 |        -171.0826 |
[32m[20221214 00:39:27 @agent_ppo2.py:185][0m |          -0.0192 |         124.8102 |        -171.8272 |
[32m[20221214 00:39:27 @agent_ppo2.py:185][0m |          -0.0123 |         128.3601 |        -171.1031 |
[32m[20221214 00:39:27 @agent_ppo2.py:185][0m |          -0.0184 |         123.8371 |        -171.5423 |
[32m[20221214 00:39:27 @agent_ppo2.py:185][0m |          -0.0194 |         122.8940 |        -171.7679 |
[32m[20221214 00:39:27 @agent_ppo2.py:130][0m Policy update time: 1.30 s
[32m[20221214 00:39:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 628.37
[32m[20221214 00:39:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 701.23
[32m[20221214 00:39:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 251.17
[32m[20221214 00:39:27 @agent_ppo2.py:143][0m Total time:      41.95 min
[32m[20221214 00:39:27 @agent_ppo2.py:145][0m 3745792 total steps have happened
[32m[20221214 00:39:27 @agent_ppo2.py:121][0m #------------------------ Iteration 5829 --------------------------#
[32m[20221214 00:39:27 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:39:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:39:27 @agent_ppo2.py:185][0m |           0.0020 |         146.6685 |        -169.3037 |
[32m[20221214 00:39:28 @agent_ppo2.py:185][0m |          -0.0052 |         134.6619 |        -168.7654 |
[32m[20221214 00:39:28 @agent_ppo2.py:185][0m |          -0.0089 |         129.9760 |        -169.2000 |
[32m[20221214 00:39:28 @agent_ppo2.py:185][0m |          -0.0068 |         127.5705 |        -168.9678 |
[32m[20221214 00:39:28 @agent_ppo2.py:185][0m |          -0.0024 |         125.4141 |        -169.5962 |
[32m[20221214 00:39:28 @agent_ppo2.py:185][0m |           0.0102 |         150.2646 |        -169.4231 |
[32m[20221214 00:39:28 @agent_ppo2.py:185][0m |          -0.0040 |         128.4127 |        -168.6901 |
[32m[20221214 00:39:28 @agent_ppo2.py:185][0m |          -0.0121 |         117.6140 |        -169.9220 |
[32m[20221214 00:39:28 @agent_ppo2.py:185][0m |          -0.0135 |         116.2842 |        -169.8951 |
[32m[20221214 00:39:29 @agent_ppo2.py:185][0m |          -0.0146 |         115.0142 |        -169.8809 |
[32m[20221214 00:39:29 @agent_ppo2.py:130][0m Policy update time: 1.27 s
[32m[20221214 00:39:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 617.64
[32m[20221214 00:39:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 657.94
[32m[20221214 00:39:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 726.11
[32m[20221214 00:39:29 @agent_ppo2.py:143][0m Total time:      41.97 min
[32m[20221214 00:39:29 @agent_ppo2.py:145][0m 3747840 total steps have happened
[32m[20221214 00:39:29 @agent_ppo2.py:121][0m #------------------------ Iteration 5830 --------------------------#
[32m[20221214 00:39:29 @agent_ppo2.py:127][0m Sampling time: 0.21 s by 5 slaves
[32m[20221214 00:39:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:39:29 @agent_ppo2.py:185][0m |           0.0019 |         127.0981 |        -166.6670 |
[32m[20221214 00:39:29 @agent_ppo2.py:185][0m |          -0.0075 |         112.8375 |        -166.1681 |
[32m[20221214 00:39:29 @agent_ppo2.py:185][0m |          -0.0044 |         106.8045 |        -166.5095 |
[32m[20221214 00:39:29 @agent_ppo2.py:185][0m |          -0.0128 |         104.1803 |        -165.5230 |
[32m[20221214 00:39:30 @agent_ppo2.py:185][0m |          -0.0108 |         101.6835 |        -164.9880 |
[32m[20221214 00:39:30 @agent_ppo2.py:185][0m |          -0.0160 |         100.1697 |        -165.3455 |
[32m[20221214 00:39:30 @agent_ppo2.py:185][0m |          -0.0178 |          98.2814 |        -164.9374 |
[32m[20221214 00:39:30 @agent_ppo2.py:185][0m |          -0.0210 |          97.0201 |        -165.1964 |
[32m[20221214 00:39:30 @agent_ppo2.py:185][0m |          -0.0198 |          96.3845 |        -164.9411 |
[32m[20221214 00:39:30 @agent_ppo2.py:185][0m |          -0.0179 |          95.9668 |        -164.6564 |
[32m[20221214 00:39:30 @agent_ppo2.py:130][0m Policy update time: 1.26 s
[32m[20221214 00:39:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 560.22
[32m[20221214 00:39:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 747.70
[32m[20221214 00:39:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 530.29
[32m[20221214 00:39:30 @agent_ppo2.py:143][0m Total time:      42.00 min
[32m[20221214 00:39:30 @agent_ppo2.py:145][0m 3749888 total steps have happened
[32m[20221214 00:39:30 @agent_ppo2.py:121][0m #------------------------ Iteration 5831 --------------------------#
[32m[20221214 00:39:31 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:39:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:39:31 @agent_ppo2.py:185][0m |           0.0022 |         132.3252 |        -174.7186 |
[32m[20221214 00:39:31 @agent_ppo2.py:185][0m |           0.0074 |         139.7253 |        -175.8478 |
[32m[20221214 00:39:31 @agent_ppo2.py:185][0m |          -0.0064 |         123.7528 |        -173.8267 |
[32m[20221214 00:39:31 @agent_ppo2.py:185][0m |          -0.0062 |         125.6208 |        -175.0509 |
[32m[20221214 00:39:31 @agent_ppo2.py:185][0m |          -0.0063 |         121.3235 |        -174.5202 |
[32m[20221214 00:39:31 @agent_ppo2.py:185][0m |          -0.0105 |         119.9068 |        -175.1125 |
[32m[20221214 00:39:31 @agent_ppo2.py:185][0m |          -0.0107 |         119.8048 |        -173.7994 |
[32m[20221214 00:39:32 @agent_ppo2.py:185][0m |           0.0011 |         128.5444 |        -173.8485 |
[32m[20221214 00:39:32 @agent_ppo2.py:185][0m |          -0.0075 |         123.2973 |        -174.6120 |
[32m[20221214 00:39:32 @agent_ppo2.py:185][0m |          -0.0062 |         119.6863 |        -173.2141 |
[32m[20221214 00:39:32 @agent_ppo2.py:130][0m Policy update time: 1.26 s
[32m[20221214 00:39:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 569.11
[32m[20221214 00:39:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 630.69
[32m[20221214 00:39:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 755.96
[32m[20221214 00:39:32 @agent_ppo2.py:143][0m Total time:      42.03 min
[32m[20221214 00:39:32 @agent_ppo2.py:145][0m 3751936 total steps have happened
[32m[20221214 00:39:32 @agent_ppo2.py:121][0m #------------------------ Iteration 5832 --------------------------#
[32m[20221214 00:39:32 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:39:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:39:32 @agent_ppo2.py:185][0m |           0.0264 |         156.0488 |        -171.0903 |
[32m[20221214 00:39:32 @agent_ppo2.py:185][0m |          -0.0087 |         123.2901 |        -170.4474 |
[32m[20221214 00:39:33 @agent_ppo2.py:185][0m |          -0.0035 |         116.9459 |        -169.6151 |
[32m[20221214 00:39:33 @agent_ppo2.py:185][0m |          -0.0088 |         114.5708 |        -168.7047 |
[32m[20221214 00:39:33 @agent_ppo2.py:185][0m |           0.0048 |         126.0666 |        -169.2665 |
[32m[20221214 00:39:33 @agent_ppo2.py:185][0m |          -0.0097 |         110.8787 |        -169.2109 |
[32m[20221214 00:39:33 @agent_ppo2.py:185][0m |          -0.0040 |         112.2880 |        -169.5359 |
[32m[20221214 00:39:33 @agent_ppo2.py:185][0m |          -0.0160 |         107.6022 |        -168.7335 |
[32m[20221214 00:39:33 @agent_ppo2.py:185][0m |          -0.0138 |         105.9362 |        -168.7296 |
[32m[20221214 00:39:33 @agent_ppo2.py:185][0m |          -0.0151 |         105.5474 |        -168.7090 |
[32m[20221214 00:39:33 @agent_ppo2.py:130][0m Policy update time: 1.24 s
[32m[20221214 00:39:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 545.64
[32m[20221214 00:39:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 619.38
[32m[20221214 00:39:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 732.60
[32m[20221214 00:39:34 @agent_ppo2.py:143][0m Total time:      42.05 min
[32m[20221214 00:39:34 @agent_ppo2.py:145][0m 3753984 total steps have happened
[32m[20221214 00:39:34 @agent_ppo2.py:121][0m #------------------------ Iteration 5833 --------------------------#
[32m[20221214 00:39:34 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221214 00:39:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:39:34 @agent_ppo2.py:185][0m |          -0.0032 |         120.8040 |        -168.8127 |
[32m[20221214 00:39:34 @agent_ppo2.py:185][0m |          -0.0023 |         109.7996 |        -168.5329 |
[32m[20221214 00:39:34 @agent_ppo2.py:185][0m |           0.0008 |         106.0211 |        -168.3479 |
[32m[20221214 00:39:34 @agent_ppo2.py:185][0m |          -0.0088 |         100.8933 |        -167.9936 |
[32m[20221214 00:39:34 @agent_ppo2.py:185][0m |           0.0091 |         116.5176 |        -167.5314 |
[32m[20221214 00:39:34 @agent_ppo2.py:185][0m |          -0.0114 |          98.1599 |        -166.7078 |
[32m[20221214 00:39:35 @agent_ppo2.py:185][0m |          -0.0135 |          95.2786 |        -167.3209 |
[32m[20221214 00:39:35 @agent_ppo2.py:185][0m |          -0.0156 |          94.8008 |        -166.8607 |
[32m[20221214 00:39:35 @agent_ppo2.py:185][0m |          -0.0160 |          93.8276 |        -166.9868 |
[32m[20221214 00:39:35 @agent_ppo2.py:185][0m |          -0.0110 |          96.6961 |        -166.4400 |
[32m[20221214 00:39:35 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:39:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 639.35
[32m[20221214 00:39:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 724.14
[32m[20221214 00:39:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 734.29
[32m[20221214 00:39:35 @agent_ppo2.py:143][0m Total time:      42.08 min
[32m[20221214 00:39:35 @agent_ppo2.py:145][0m 3756032 total steps have happened
[32m[20221214 00:39:35 @agent_ppo2.py:121][0m #------------------------ Iteration 5834 --------------------------#
[32m[20221214 00:39:35 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:39:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:39:35 @agent_ppo2.py:185][0m |          -0.0004 |         106.7574 |        -165.0571 |
[32m[20221214 00:39:36 @agent_ppo2.py:185][0m |          -0.0050 |          96.9710 |        -164.7024 |
[32m[20221214 00:39:36 @agent_ppo2.py:185][0m |          -0.0049 |          96.6442 |        -165.3563 |
[32m[20221214 00:39:36 @agent_ppo2.py:185][0m |          -0.0070 |          93.3969 |        -163.6502 |
[32m[20221214 00:39:36 @agent_ppo2.py:185][0m |          -0.0107 |          90.2735 |        -165.0516 |
[32m[20221214 00:39:36 @agent_ppo2.py:185][0m |          -0.0131 |          89.7204 |        -165.4289 |
[32m[20221214 00:39:36 @agent_ppo2.py:185][0m |          -0.0032 |          96.3765 |        -164.8091 |
[32m[20221214 00:39:36 @agent_ppo2.py:185][0m |          -0.0143 |          88.0435 |        -165.6406 |
[32m[20221214 00:39:36 @agent_ppo2.py:185][0m |          -0.0167 |          86.8963 |        -165.6454 |
[32m[20221214 00:39:36 @agent_ppo2.py:185][0m |          -0.0169 |          87.2145 |        -165.4938 |
[32m[20221214 00:39:36 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 00:39:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 502.95
[32m[20221214 00:39:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 725.89
[32m[20221214 00:39:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 708.91
[32m[20221214 00:39:37 @agent_ppo2.py:143][0m Total time:      42.10 min
[32m[20221214 00:39:37 @agent_ppo2.py:145][0m 3758080 total steps have happened
[32m[20221214 00:39:37 @agent_ppo2.py:121][0m #------------------------ Iteration 5835 --------------------------#
[32m[20221214 00:39:37 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:39:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:39:37 @agent_ppo2.py:185][0m |           0.0012 |         149.6738 |        -175.9694 |
[32m[20221214 00:39:37 @agent_ppo2.py:185][0m |          -0.0067 |         139.5503 |        -176.4041 |
[32m[20221214 00:39:37 @agent_ppo2.py:185][0m |          -0.0043 |         137.2956 |        -175.8545 |
[32m[20221214 00:39:37 @agent_ppo2.py:185][0m |          -0.0034 |         135.6179 |        -174.8638 |
[32m[20221214 00:39:37 @agent_ppo2.py:185][0m |          -0.0103 |         132.4565 |        -175.2047 |
[32m[20221214 00:39:37 @agent_ppo2.py:185][0m |          -0.0142 |         131.8485 |        -174.9520 |
[32m[20221214 00:39:37 @agent_ppo2.py:185][0m |          -0.0078 |         129.8953 |        -174.5857 |
[32m[20221214 00:39:38 @agent_ppo2.py:185][0m |          -0.0100 |         128.9132 |        -174.6542 |
[32m[20221214 00:39:38 @agent_ppo2.py:185][0m |          -0.0019 |         135.9994 |        -174.5738 |
[32m[20221214 00:39:38 @agent_ppo2.py:185][0m |          -0.0125 |         128.8103 |        -174.7273 |
[32m[20221214 00:39:38 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:39:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 604.22
[32m[20221214 00:39:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 686.71
[32m[20221214 00:39:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 643.68
[32m[20221214 00:39:38 @agent_ppo2.py:143][0m Total time:      42.13 min
[32m[20221214 00:39:38 @agent_ppo2.py:145][0m 3760128 total steps have happened
[32m[20221214 00:39:38 @agent_ppo2.py:121][0m #------------------------ Iteration 5836 --------------------------#
[32m[20221214 00:39:38 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:39:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:39:38 @agent_ppo2.py:185][0m |           0.0041 |         141.3500 |        -171.2967 |
[32m[20221214 00:39:38 @agent_ppo2.py:185][0m |          -0.0037 |         135.9726 |        -171.0111 |
[32m[20221214 00:39:38 @agent_ppo2.py:185][0m |          -0.0057 |         135.7838 |        -171.0844 |
[32m[20221214 00:39:39 @agent_ppo2.py:185][0m |          -0.0025 |         133.5805 |        -170.8345 |
[32m[20221214 00:39:39 @agent_ppo2.py:185][0m |          -0.0008 |         133.7618 |        -171.0170 |
[32m[20221214 00:39:39 @agent_ppo2.py:185][0m |          -0.0050 |         132.6656 |        -171.5302 |
[32m[20221214 00:39:39 @agent_ppo2.py:185][0m |          -0.0079 |         131.1755 |        -171.9864 |
[32m[20221214 00:39:39 @agent_ppo2.py:185][0m |          -0.0060 |         130.0052 |        -171.2824 |
[32m[20221214 00:39:39 @agent_ppo2.py:185][0m |          -0.0070 |         130.8902 |        -171.9043 |
[32m[20221214 00:39:39 @agent_ppo2.py:185][0m |          -0.0061 |         129.3082 |        -171.3692 |
[32m[20221214 00:39:39 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:39:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 623.91
[32m[20221214 00:39:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 724.51
[32m[20221214 00:39:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 832.52
[32m[20221214 00:39:39 @agent_ppo2.py:143][0m Total time:      42.15 min
[32m[20221214 00:39:39 @agent_ppo2.py:145][0m 3762176 total steps have happened
[32m[20221214 00:39:39 @agent_ppo2.py:121][0m #------------------------ Iteration 5837 --------------------------#
[32m[20221214 00:39:40 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:39:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:39:40 @agent_ppo2.py:185][0m |          -0.0008 |         135.2201 |        -171.0265 |
[32m[20221214 00:39:40 @agent_ppo2.py:185][0m |          -0.0055 |         125.9388 |        -170.5301 |
[32m[20221214 00:39:40 @agent_ppo2.py:185][0m |          -0.0045 |         122.7235 |        -170.8847 |
[32m[20221214 00:39:40 @agent_ppo2.py:185][0m |          -0.0053 |         120.7517 |        -170.7946 |
[32m[20221214 00:39:40 @agent_ppo2.py:185][0m |          -0.0095 |         119.2115 |        -170.2810 |
[32m[20221214 00:39:40 @agent_ppo2.py:185][0m |          -0.0123 |         117.6854 |        -170.7317 |
[32m[20221214 00:39:40 @agent_ppo2.py:185][0m |          -0.0103 |         118.3831 |        -169.5755 |
[32m[20221214 00:39:40 @agent_ppo2.py:185][0m |          -0.0124 |         115.9534 |        -168.9337 |
[32m[20221214 00:39:40 @agent_ppo2.py:185][0m |          -0.0031 |         122.8686 |        -169.4561 |
[32m[20221214 00:39:41 @agent_ppo2.py:185][0m |          -0.0044 |         124.8161 |        -169.8165 |
[32m[20221214 00:39:41 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:39:41 @agent_ppo2.py:138][0m Average TRAINING episode reward: 682.49
[32m[20221214 00:39:41 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 779.88
[32m[20221214 00:39:41 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 770.33
[32m[20221214 00:39:41 @agent_ppo2.py:143][0m Total time:      42.17 min
[32m[20221214 00:39:41 @agent_ppo2.py:145][0m 3764224 total steps have happened
[32m[20221214 00:39:41 @agent_ppo2.py:121][0m #------------------------ Iteration 5838 --------------------------#
[32m[20221214 00:39:41 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:39:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:39:41 @agent_ppo2.py:185][0m |           0.0074 |         117.9738 |        -164.4954 |
[32m[20221214 00:39:41 @agent_ppo2.py:185][0m |          -0.0005 |         102.5253 |        -163.6232 |
[32m[20221214 00:39:41 @agent_ppo2.py:185][0m |           0.0006 |         103.4441 |        -163.5374 |
[32m[20221214 00:39:41 @agent_ppo2.py:185][0m |          -0.0127 |          97.0682 |        -163.1624 |
[32m[20221214 00:39:41 @agent_ppo2.py:185][0m |          -0.0145 |          95.6056 |        -162.7921 |
[32m[20221214 00:39:42 @agent_ppo2.py:185][0m |          -0.0173 |          94.4355 |        -163.1442 |
[32m[20221214 00:39:42 @agent_ppo2.py:185][0m |          -0.0164 |          94.0439 |        -163.3691 |
[32m[20221214 00:39:42 @agent_ppo2.py:185][0m |          -0.0143 |          93.7063 |        -162.9445 |
[32m[20221214 00:39:42 @agent_ppo2.py:185][0m |          -0.0185 |          92.4741 |        -162.9491 |
[32m[20221214 00:39:42 @agent_ppo2.py:185][0m |          -0.0187 |          92.1787 |        -163.0194 |
[32m[20221214 00:39:42 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:39:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 581.30
[32m[20221214 00:39:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 767.02
[32m[20221214 00:39:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 703.44
[32m[20221214 00:39:42 @agent_ppo2.py:143][0m Total time:      42.20 min
[32m[20221214 00:39:42 @agent_ppo2.py:145][0m 3766272 total steps have happened
[32m[20221214 00:39:42 @agent_ppo2.py:121][0m #------------------------ Iteration 5839 --------------------------#
[32m[20221214 00:39:42 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:39:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:39:42 @agent_ppo2.py:185][0m |           0.0038 |         118.3656 |        -168.7159 |
[32m[20221214 00:39:43 @agent_ppo2.py:185][0m |          -0.0003 |         112.7000 |        -167.6009 |
[32m[20221214 00:39:43 @agent_ppo2.py:185][0m |          -0.0030 |         109.5080 |        -167.7224 |
[32m[20221214 00:39:43 @agent_ppo2.py:185][0m |          -0.0010 |         108.3538 |        -169.1752 |
[32m[20221214 00:39:43 @agent_ppo2.py:185][0m |          -0.0044 |         106.6136 |        -167.7952 |
[32m[20221214 00:39:43 @agent_ppo2.py:185][0m |          -0.0089 |         106.3224 |        -167.4746 |
[32m[20221214 00:39:43 @agent_ppo2.py:185][0m |          -0.0086 |         105.7657 |        -168.3136 |
[32m[20221214 00:39:43 @agent_ppo2.py:185][0m |          -0.0007 |         113.5039 |        -168.2545 |
[32m[20221214 00:39:43 @agent_ppo2.py:185][0m |          -0.0151 |         105.2931 |        -168.0861 |
[32m[20221214 00:39:43 @agent_ppo2.py:185][0m |          -0.0094 |         104.9205 |        -167.9956 |
[32m[20221214 00:39:43 @agent_ppo2.py:130][0m Policy update time: 1.20 s
[32m[20221214 00:39:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 720.42
[32m[20221214 00:39:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 797.22
[32m[20221214 00:39:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 583.73
[32m[20221214 00:39:44 @agent_ppo2.py:143][0m Total time:      42.22 min
[32m[20221214 00:39:44 @agent_ppo2.py:145][0m 3768320 total steps have happened
[32m[20221214 00:39:44 @agent_ppo2.py:121][0m #------------------------ Iteration 5840 --------------------------#
[32m[20221214 00:39:44 @agent_ppo2.py:127][0m Sampling time: 0.22 s by 5 slaves
[32m[20221214 00:39:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:39:44 @agent_ppo2.py:185][0m |           0.0015 |         112.9693 |        -173.5457 |
[32m[20221214 00:39:44 @agent_ppo2.py:185][0m |          -0.0034 |         107.6273 |        -172.2308 |
[32m[20221214 00:39:44 @agent_ppo2.py:185][0m |           0.0054 |         109.0821 |        -171.6912 |
[32m[20221214 00:39:44 @agent_ppo2.py:185][0m |          -0.0053 |         104.3067 |        -173.0126 |
[32m[20221214 00:39:45 @agent_ppo2.py:185][0m |          -0.0101 |         102.1694 |        -172.2436 |
[32m[20221214 00:39:45 @agent_ppo2.py:185][0m |          -0.0121 |         101.9049 |        -172.4510 |
[32m[20221214 00:39:45 @agent_ppo2.py:185][0m |          -0.0098 |         100.5473 |        -171.7385 |
[32m[20221214 00:39:45 @agent_ppo2.py:185][0m |          -0.0040 |         108.1792 |        -171.3622 |
[32m[20221214 00:39:45 @agent_ppo2.py:185][0m |          -0.0089 |          98.7089 |        -171.4557 |
[32m[20221214 00:39:45 @agent_ppo2.py:185][0m |          -0.0036 |         105.3726 |        -171.6815 |
[32m[20221214 00:39:45 @agent_ppo2.py:130][0m Policy update time: 1.25 s
[32m[20221214 00:39:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 614.43
[32m[20221214 00:39:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 736.63
[32m[20221214 00:39:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 838.67
[32m[20221214 00:39:45 @agent_ppo2.py:143][0m Total time:      42.25 min
[32m[20221214 00:39:45 @agent_ppo2.py:145][0m 3770368 total steps have happened
[32m[20221214 00:39:45 @agent_ppo2.py:121][0m #------------------------ Iteration 5841 --------------------------#
[32m[20221214 00:39:45 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:39:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:39:46 @agent_ppo2.py:185][0m |           0.0038 |         116.7424 |        -172.4018 |
[32m[20221214 00:39:46 @agent_ppo2.py:185][0m |          -0.0014 |         104.2858 |        -172.7188 |
[32m[20221214 00:39:46 @agent_ppo2.py:185][0m |          -0.0044 |          99.0624 |        -172.3646 |
[32m[20221214 00:39:46 @agent_ppo2.py:185][0m |          -0.0110 |          96.0805 |        -171.7366 |
[32m[20221214 00:39:46 @agent_ppo2.py:185][0m |          -0.0110 |          94.8103 |        -171.9763 |
[32m[20221214 00:39:46 @agent_ppo2.py:185][0m |          -0.0108 |          94.6472 |        -172.8000 |
[32m[20221214 00:39:46 @agent_ppo2.py:185][0m |          -0.0149 |          92.5637 |        -172.4723 |
[32m[20221214 00:39:46 @agent_ppo2.py:185][0m |          -0.0080 |          93.6463 |        -172.2682 |
[32m[20221214 00:39:46 @agent_ppo2.py:185][0m |          -0.0145 |          91.9529 |        -172.1716 |
[32m[20221214 00:39:47 @agent_ppo2.py:185][0m |          -0.0094 |          91.3821 |        -171.8505 |
[32m[20221214 00:39:47 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221214 00:39:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 574.38
[32m[20221214 00:39:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 691.41
[32m[20221214 00:39:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 628.80
[32m[20221214 00:39:47 @agent_ppo2.py:143][0m Total time:      42.27 min
[32m[20221214 00:39:47 @agent_ppo2.py:145][0m 3772416 total steps have happened
[32m[20221214 00:39:47 @agent_ppo2.py:121][0m #------------------------ Iteration 5842 --------------------------#
[32m[20221214 00:39:47 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:39:47 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:39:47 @agent_ppo2.py:185][0m |           0.0017 |         125.4883 |        -166.5219 |
[32m[20221214 00:39:47 @agent_ppo2.py:185][0m |          -0.0080 |         118.5232 |        -166.3366 |
[32m[20221214 00:39:47 @agent_ppo2.py:185][0m |          -0.0089 |         115.7267 |        -165.7363 |
[32m[20221214 00:39:47 @agent_ppo2.py:185][0m |          -0.0140 |         114.0823 |        -165.6840 |
[32m[20221214 00:39:48 @agent_ppo2.py:185][0m |          -0.0136 |         112.6336 |        -165.5959 |
[32m[20221214 00:39:48 @agent_ppo2.py:185][0m |          -0.0134 |         111.6900 |        -166.1142 |
[32m[20221214 00:39:48 @agent_ppo2.py:185][0m |          -0.0117 |         110.6291 |        -165.3177 |
[32m[20221214 00:39:48 @agent_ppo2.py:185][0m |          -0.0158 |         109.7817 |        -164.9358 |
[32m[20221214 00:39:48 @agent_ppo2.py:185][0m |          -0.0197 |         109.1840 |        -165.3642 |
[32m[20221214 00:39:48 @agent_ppo2.py:185][0m |          -0.0176 |         109.1718 |        -165.3834 |
[32m[20221214 00:39:48 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221214 00:39:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 565.72
[32m[20221214 00:39:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 663.38
[32m[20221214 00:39:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 699.27
[32m[20221214 00:39:48 @agent_ppo2.py:143][0m Total time:      42.30 min
[32m[20221214 00:39:48 @agent_ppo2.py:145][0m 3774464 total steps have happened
[32m[20221214 00:39:48 @agent_ppo2.py:121][0m #------------------------ Iteration 5843 --------------------------#
[32m[20221214 00:39:48 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:39:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:39:49 @agent_ppo2.py:185][0m |           0.0026 |         110.7645 |        -169.8623 |
[32m[20221214 00:39:49 @agent_ppo2.py:185][0m |           0.0019 |         105.2870 |        -170.0369 |
[32m[20221214 00:39:49 @agent_ppo2.py:185][0m |          -0.0080 |         103.2278 |        -169.8727 |
[32m[20221214 00:39:49 @agent_ppo2.py:185][0m |          -0.0039 |         101.5420 |        -169.3920 |
[32m[20221214 00:39:49 @agent_ppo2.py:185][0m |          -0.0115 |         100.4474 |        -169.2906 |
[32m[20221214 00:39:49 @agent_ppo2.py:185][0m |          -0.0077 |          99.6054 |        -169.1758 |
[32m[20221214 00:39:49 @agent_ppo2.py:185][0m |          -0.0040 |         102.0413 |        -169.3646 |
[32m[20221214 00:39:49 @agent_ppo2.py:185][0m |          -0.0084 |          99.2288 |        -170.1960 |
[32m[20221214 00:39:49 @agent_ppo2.py:185][0m |          -0.0070 |         101.5888 |        -170.2151 |
[32m[20221214 00:39:49 @agent_ppo2.py:185][0m |          -0.0150 |          98.0054 |        -169.4355 |
[32m[20221214 00:39:49 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:39:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 527.62
[32m[20221214 00:39:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 691.05
[32m[20221214 00:39:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 763.28
[32m[20221214 00:39:50 @agent_ppo2.py:143][0m Total time:      42.32 min
[32m[20221214 00:39:50 @agent_ppo2.py:145][0m 3776512 total steps have happened
[32m[20221214 00:39:50 @agent_ppo2.py:121][0m #------------------------ Iteration 5844 --------------------------#
[32m[20221214 00:39:50 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:39:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:39:50 @agent_ppo2.py:185][0m |          -0.0012 |         168.0415 |        -176.2681 |
[32m[20221214 00:39:50 @agent_ppo2.py:185][0m |          -0.0026 |         160.5958 |        -174.4194 |
[32m[20221214 00:39:50 @agent_ppo2.py:185][0m |          -0.0017 |         158.4940 |        -175.4469 |
[32m[20221214 00:39:50 @agent_ppo2.py:185][0m |          -0.0028 |         158.2381 |        -174.9119 |
[32m[20221214 00:39:50 @agent_ppo2.py:185][0m |          -0.0085 |         157.3793 |        -175.6223 |
[32m[20221214 00:39:50 @agent_ppo2.py:185][0m |          -0.0100 |         156.4563 |        -174.2083 |
[32m[20221214 00:39:51 @agent_ppo2.py:185][0m |           0.0007 |         157.4563 |        -173.9031 |
[32m[20221214 00:39:51 @agent_ppo2.py:185][0m |          -0.0085 |         156.0153 |        -173.9065 |
[32m[20221214 00:39:51 @agent_ppo2.py:185][0m |          -0.0073 |         156.0844 |        -174.0608 |
[32m[20221214 00:39:51 @agent_ppo2.py:185][0m |          -0.0096 |         155.6723 |        -173.8242 |
[32m[20221214 00:39:51 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:39:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 686.62
[32m[20221214 00:39:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 843.02
[32m[20221214 00:39:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 464.81
[32m[20221214 00:39:51 @agent_ppo2.py:143][0m Total time:      42.34 min
[32m[20221214 00:39:51 @agent_ppo2.py:145][0m 3778560 total steps have happened
[32m[20221214 00:39:51 @agent_ppo2.py:121][0m #------------------------ Iteration 5845 --------------------------#
[32m[20221214 00:39:51 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:39:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:39:51 @agent_ppo2.py:185][0m |           0.0027 |         164.0180 |        -172.1654 |
[32m[20221214 00:39:51 @agent_ppo2.py:185][0m |          -0.0052 |         151.0826 |        -172.3336 |
[32m[20221214 00:39:51 @agent_ppo2.py:185][0m |           0.0001 |         149.0279 |        -172.5710 |
[32m[20221214 00:39:52 @agent_ppo2.py:185][0m |          -0.0114 |         142.6581 |        -172.6972 |
[32m[20221214 00:39:52 @agent_ppo2.py:185][0m |          -0.0102 |         139.6846 |        -173.1022 |
[32m[20221214 00:39:52 @agent_ppo2.py:185][0m |          -0.0156 |         138.9814 |        -173.4413 |
[32m[20221214 00:39:52 @agent_ppo2.py:185][0m |          -0.0146 |         137.2967 |        -173.2294 |
[32m[20221214 00:39:52 @agent_ppo2.py:185][0m |          -0.0128 |         136.2176 |        -173.8287 |
[32m[20221214 00:39:52 @agent_ppo2.py:185][0m |          -0.0173 |         135.7685 |        -173.9867 |
[32m[20221214 00:39:52 @agent_ppo2.py:185][0m |          -0.0199 |         135.2582 |        -174.1871 |
[32m[20221214 00:39:52 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:39:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 406.59
[32m[20221214 00:39:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 655.41
[32m[20221214 00:39:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 737.25
[32m[20221214 00:39:52 @agent_ppo2.py:143][0m Total time:      42.37 min
[32m[20221214 00:39:52 @agent_ppo2.py:145][0m 3780608 total steps have happened
[32m[20221214 00:39:52 @agent_ppo2.py:121][0m #------------------------ Iteration 5846 --------------------------#
[32m[20221214 00:39:53 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:39:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:39:53 @agent_ppo2.py:185][0m |           0.0010 |         146.3211 |        -173.3777 |
[32m[20221214 00:39:53 @agent_ppo2.py:185][0m |           0.0094 |         150.5375 |        -172.5265 |
[32m[20221214 00:39:53 @agent_ppo2.py:185][0m |          -0.0029 |         139.0871 |        -173.2713 |
[32m[20221214 00:39:53 @agent_ppo2.py:185][0m |          -0.0014 |         138.0501 |        -173.5574 |
[32m[20221214 00:39:53 @agent_ppo2.py:185][0m |          -0.0022 |         137.5156 |        -174.3047 |
[32m[20221214 00:39:53 @agent_ppo2.py:185][0m |           0.0075 |         151.4271 |        -173.8360 |
[32m[20221214 00:39:53 @agent_ppo2.py:185][0m |          -0.0041 |         137.1667 |        -174.6761 |
[32m[20221214 00:39:53 @agent_ppo2.py:185][0m |           0.0066 |         157.6503 |        -175.0987 |
[32m[20221214 00:39:54 @agent_ppo2.py:185][0m |          -0.0098 |         136.3465 |        -174.5194 |
[32m[20221214 00:39:54 @agent_ppo2.py:185][0m |          -0.0071 |         136.1683 |        -174.3116 |
[32m[20221214 00:39:54 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:39:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 702.76
[32m[20221214 00:39:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.14
[32m[20221214 00:39:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 850.15
[32m[20221214 00:39:54 @agent_ppo2.py:143][0m Total time:      42.39 min
[32m[20221214 00:39:54 @agent_ppo2.py:145][0m 3782656 total steps have happened
[32m[20221214 00:39:54 @agent_ppo2.py:121][0m #------------------------ Iteration 5847 --------------------------#
[32m[20221214 00:39:54 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:39:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:39:54 @agent_ppo2.py:185][0m |          -0.0007 |         168.0137 |        -181.6116 |
[32m[20221214 00:39:54 @agent_ppo2.py:185][0m |           0.0024 |         166.4841 |        -182.0647 |
[32m[20221214 00:39:54 @agent_ppo2.py:185][0m |           0.0020 |         165.1472 |        -180.5213 |
[32m[20221214 00:39:54 @agent_ppo2.py:185][0m |           0.0003 |         165.0831 |        -180.8152 |
[32m[20221214 00:39:55 @agent_ppo2.py:185][0m |          -0.0000 |         166.8904 |        -181.4483 |
[32m[20221214 00:39:55 @agent_ppo2.py:185][0m |          -0.0033 |         164.4015 |        -182.2226 |
[32m[20221214 00:39:55 @agent_ppo2.py:185][0m |          -0.0070 |         163.9695 |        -180.4687 |
[32m[20221214 00:39:55 @agent_ppo2.py:185][0m |          -0.0025 |         163.4423 |        -181.4363 |
[32m[20221214 00:39:55 @agent_ppo2.py:185][0m |          -0.0057 |         163.5658 |        -181.9176 |
[32m[20221214 00:39:55 @agent_ppo2.py:185][0m |          -0.0016 |         163.4378 |        -180.5087 |
[32m[20221214 00:39:55 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:39:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 766.48
[32m[20221214 00:39:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.03
[32m[20221214 00:39:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 804.61
[32m[20221214 00:39:55 @agent_ppo2.py:143][0m Total time:      42.41 min
[32m[20221214 00:39:55 @agent_ppo2.py:145][0m 3784704 total steps have happened
[32m[20221214 00:39:55 @agent_ppo2.py:121][0m #------------------------ Iteration 5848 --------------------------#
[32m[20221214 00:39:55 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:39:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:39:56 @agent_ppo2.py:185][0m |          -0.0007 |          90.9491 |        -174.5768 |
[32m[20221214 00:39:56 @agent_ppo2.py:185][0m |          -0.0009 |          86.4316 |        -175.0029 |
[32m[20221214 00:39:56 @agent_ppo2.py:185][0m |          -0.0003 |          84.3030 |        -174.3902 |
[32m[20221214 00:39:56 @agent_ppo2.py:185][0m |          -0.0049 |          85.1656 |        -174.5294 |
[32m[20221214 00:39:56 @agent_ppo2.py:185][0m |          -0.0033 |          83.5815 |        -174.3465 |
[32m[20221214 00:39:56 @agent_ppo2.py:185][0m |          -0.0065 |          82.9921 |        -175.2762 |
[32m[20221214 00:39:56 @agent_ppo2.py:185][0m |          -0.0036 |          82.8746 |        -175.3692 |
[32m[20221214 00:39:56 @agent_ppo2.py:185][0m |          -0.0075 |          82.7882 |        -175.0823 |
[32m[20221214 00:39:56 @agent_ppo2.py:185][0m |           0.0014 |          85.2182 |        -175.9134 |
[32m[20221214 00:39:57 @agent_ppo2.py:185][0m |          -0.0085 |          82.3065 |        -175.8875 |
[32m[20221214 00:39:57 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221214 00:39:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 727.98
[32m[20221214 00:39:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 792.02
[32m[20221214 00:39:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 798.96
[32m[20221214 00:39:57 @agent_ppo2.py:143][0m Total time:      42.44 min
[32m[20221214 00:39:57 @agent_ppo2.py:145][0m 3786752 total steps have happened
[32m[20221214 00:39:57 @agent_ppo2.py:121][0m #------------------------ Iteration 5849 --------------------------#
[32m[20221214 00:39:57 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:39:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:39:57 @agent_ppo2.py:185][0m |           0.0064 |          63.1909 |        -184.2188 |
[32m[20221214 00:39:57 @agent_ppo2.py:185][0m |          -0.0086 |          51.0886 |        -184.3565 |
[32m[20221214 00:39:57 @agent_ppo2.py:185][0m |          -0.0123 |          47.9212 |        -183.8654 |
[32m[20221214 00:39:57 @agent_ppo2.py:185][0m |          -0.0128 |          46.5014 |        -183.7465 |
[32m[20221214 00:39:58 @agent_ppo2.py:185][0m |          -0.0127 |          46.3332 |        -183.4571 |
[32m[20221214 00:39:58 @agent_ppo2.py:185][0m |          -0.0116 |          46.3045 |        -182.8969 |
[32m[20221214 00:39:58 @agent_ppo2.py:185][0m |          -0.0184 |          43.6181 |        -183.5326 |
[32m[20221214 00:39:58 @agent_ppo2.py:185][0m |          -0.0184 |          43.9038 |        -183.7262 |
[32m[20221214 00:39:58 @agent_ppo2.py:185][0m |          -0.0186 |          43.1418 |        -182.9984 |
[32m[20221214 00:39:58 @agent_ppo2.py:185][0m |          -0.0211 |          42.1752 |        -183.4497 |
[32m[20221214 00:39:58 @agent_ppo2.py:130][0m Policy update time: 1.18 s
[32m[20221214 00:39:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 397.66
[32m[20221214 00:39:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 693.31
[32m[20221214 00:39:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 489.23
[32m[20221214 00:39:58 @agent_ppo2.py:143][0m Total time:      42.46 min
[32m[20221214 00:39:58 @agent_ppo2.py:145][0m 3788800 total steps have happened
[32m[20221214 00:39:58 @agent_ppo2.py:121][0m #------------------------ Iteration 5850 --------------------------#
[32m[20221214 00:39:58 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:39:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:39:59 @agent_ppo2.py:185][0m |           0.0060 |          57.3616 |        -185.9271 |
[32m[20221214 00:39:59 @agent_ppo2.py:185][0m |           0.0004 |          53.5178 |        -185.9699 |
[32m[20221214 00:39:59 @agent_ppo2.py:185][0m |          -0.0109 |          52.2010 |        -185.6427 |
[32m[20221214 00:39:59 @agent_ppo2.py:185][0m |          -0.0095 |          51.2600 |        -185.4017 |
[32m[20221214 00:39:59 @agent_ppo2.py:185][0m |           0.0116 |          63.2943 |        -184.9920 |
[32m[20221214 00:39:59 @agent_ppo2.py:185][0m |           0.0104 |          58.1471 |        -185.8732 |
[32m[20221214 00:39:59 @agent_ppo2.py:185][0m |          -0.0063 |          51.4021 |        -184.8895 |
[32m[20221214 00:39:59 @agent_ppo2.py:185][0m |          -0.0117 |          48.3328 |        -185.4460 |
[32m[20221214 00:39:59 @agent_ppo2.py:185][0m |          -0.0124 |          51.3271 |        -185.9125 |
[32m[20221214 00:39:59 @agent_ppo2.py:185][0m |          -0.0114 |          48.4917 |        -185.5899 |
[32m[20221214 00:39:59 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:40:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 597.56
[32m[20221214 00:40:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 696.63
[32m[20221214 00:40:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 671.06
[32m[20221214 00:40:00 @agent_ppo2.py:143][0m Total time:      42.49 min
[32m[20221214 00:40:00 @agent_ppo2.py:145][0m 3790848 total steps have happened
[32m[20221214 00:40:00 @agent_ppo2.py:121][0m #------------------------ Iteration 5851 --------------------------#
[32m[20221214 00:40:00 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:40:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:40:00 @agent_ppo2.py:185][0m |           0.0037 |          78.9221 |        -178.4046 |
[32m[20221214 00:40:00 @agent_ppo2.py:185][0m |          -0.0021 |          70.4273 |        -178.3557 |
[32m[20221214 00:40:00 @agent_ppo2.py:185][0m |          -0.0039 |          68.1141 |        -177.5764 |
[32m[20221214 00:40:00 @agent_ppo2.py:185][0m |           0.0020 |          70.4954 |        -178.5922 |
[32m[20221214 00:40:00 @agent_ppo2.py:185][0m |          -0.0006 |          64.6919 |        -177.9471 |
[32m[20221214 00:40:00 @agent_ppo2.py:185][0m |          -0.0064 |          62.2919 |        -178.5553 |
[32m[20221214 00:40:01 @agent_ppo2.py:185][0m |          -0.0131 |          61.7725 |        -178.4931 |
[32m[20221214 00:40:01 @agent_ppo2.py:185][0m |          -0.0080 |          61.7461 |        -179.1724 |
[32m[20221214 00:40:01 @agent_ppo2.py:185][0m |          -0.0139 |          58.7500 |        -178.7809 |
[32m[20221214 00:40:01 @agent_ppo2.py:185][0m |          -0.0165 |          58.3179 |        -178.3335 |
[32m[20221214 00:40:01 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:40:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 482.82
[32m[20221214 00:40:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 661.82
[32m[20221214 00:40:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 491.26
[32m[20221214 00:40:01 @agent_ppo2.py:143][0m Total time:      42.51 min
[32m[20221214 00:40:01 @agent_ppo2.py:145][0m 3792896 total steps have happened
[32m[20221214 00:40:01 @agent_ppo2.py:121][0m #------------------------ Iteration 5852 --------------------------#
[32m[20221214 00:40:01 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 00:40:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:40:01 @agent_ppo2.py:185][0m |           0.0053 |          72.7265 |        -185.8767 |
[32m[20221214 00:40:02 @agent_ppo2.py:185][0m |          -0.0008 |          69.0368 |        -186.1007 |
[32m[20221214 00:40:02 @agent_ppo2.py:185][0m |           0.0045 |          68.8833 |        -186.3242 |
[32m[20221214 00:40:02 @agent_ppo2.py:185][0m |          -0.0011 |          68.9721 |        -185.8447 |
[32m[20221214 00:40:02 @agent_ppo2.py:185][0m |          -0.0068 |          68.1130 |        -185.8833 |
[32m[20221214 00:40:02 @agent_ppo2.py:185][0m |          -0.0056 |          67.8094 |        -186.4821 |
[32m[20221214 00:40:02 @agent_ppo2.py:185][0m |          -0.0057 |          67.7590 |        -185.9149 |
[32m[20221214 00:40:02 @agent_ppo2.py:185][0m |          -0.0094 |          67.0599 |        -185.7056 |
[32m[20221214 00:40:02 @agent_ppo2.py:185][0m |          -0.0059 |          66.9480 |        -185.4051 |
[32m[20221214 00:40:02 @agent_ppo2.py:185][0m |          -0.0093 |          66.6138 |        -185.2160 |
[32m[20221214 00:40:02 @agent_ppo2.py:130][0m Policy update time: 1.17 s
[32m[20221214 00:40:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 609.24
[32m[20221214 00:40:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 674.20
[32m[20221214 00:40:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 749.91
[32m[20221214 00:40:03 @agent_ppo2.py:143][0m Total time:      42.54 min
[32m[20221214 00:40:03 @agent_ppo2.py:145][0m 3794944 total steps have happened
[32m[20221214 00:40:03 @agent_ppo2.py:121][0m #------------------------ Iteration 5853 --------------------------#
[32m[20221214 00:40:03 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:40:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:40:03 @agent_ppo2.py:185][0m |          -0.0001 |         130.3814 |        -189.3040 |
[32m[20221214 00:40:03 @agent_ppo2.py:185][0m |          -0.0019 |         128.0994 |        -189.7323 |
[32m[20221214 00:40:03 @agent_ppo2.py:185][0m |          -0.0003 |         127.7959 |        -188.8076 |
[32m[20221214 00:40:03 @agent_ppo2.py:185][0m |           0.0120 |         138.3571 |        -189.4246 |
[32m[20221214 00:40:03 @agent_ppo2.py:185][0m |           0.0046 |         132.4675 |        -188.9211 |
[32m[20221214 00:40:03 @agent_ppo2.py:185][0m |          -0.0018 |         125.9796 |        -188.3629 |
[32m[20221214 00:40:04 @agent_ppo2.py:185][0m |           0.0109 |         142.2694 |        -189.2330 |
[32m[20221214 00:40:04 @agent_ppo2.py:185][0m |          -0.0059 |         125.5542 |        -188.8488 |
[32m[20221214 00:40:04 @agent_ppo2.py:185][0m |          -0.0071 |         125.6802 |        -189.0044 |
[32m[20221214 00:40:04 @agent_ppo2.py:185][0m |           0.0133 |         145.2678 |        -188.1154 |
[32m[20221214 00:40:04 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:40:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 660.65
[32m[20221214 00:40:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 774.21
[32m[20221214 00:40:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 654.80
[32m[20221214 00:40:04 @agent_ppo2.py:143][0m Total time:      42.56 min
[32m[20221214 00:40:04 @agent_ppo2.py:145][0m 3796992 total steps have happened
[32m[20221214 00:40:04 @agent_ppo2.py:121][0m #------------------------ Iteration 5854 --------------------------#
[32m[20221214 00:40:04 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:40:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:40:04 @agent_ppo2.py:185][0m |           0.0055 |          35.1790 |        -180.6901 |
[32m[20221214 00:40:04 @agent_ppo2.py:185][0m |          -0.0034 |          22.7017 |        -179.8210 |
[32m[20221214 00:40:05 @agent_ppo2.py:185][0m |          -0.0124 |          19.6234 |        -179.9487 |
[32m[20221214 00:40:05 @agent_ppo2.py:185][0m |          -0.0170 |          18.1074 |        -179.9568 |
[32m[20221214 00:40:05 @agent_ppo2.py:185][0m |          -0.0167 |          16.9364 |        -179.6407 |
[32m[20221214 00:40:05 @agent_ppo2.py:185][0m |          -0.0222 |          16.3282 |        -180.0846 |
[32m[20221214 00:40:05 @agent_ppo2.py:185][0m |          -0.0223 |          15.8628 |        -179.3656 |
[32m[20221214 00:40:05 @agent_ppo2.py:185][0m |          -0.0331 |          15.4320 |        -178.8526 |
[32m[20221214 00:40:05 @agent_ppo2.py:185][0m |          -0.0281 |          14.9379 |        -178.6022 |
[32m[20221214 00:40:05 @agent_ppo2.py:185][0m |          -0.0293 |          14.6490 |        -179.4995 |
[32m[20221214 00:40:05 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:40:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 437.74
[32m[20221214 00:40:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 638.99
[32m[20221214 00:40:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 632.46
[32m[20221214 00:40:05 @agent_ppo2.py:143][0m Total time:      42.58 min
[32m[20221214 00:40:05 @agent_ppo2.py:145][0m 3799040 total steps have happened
[32m[20221214 00:40:05 @agent_ppo2.py:121][0m #------------------------ Iteration 5855 --------------------------#
[32m[20221214 00:40:06 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:40:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:40:06 @agent_ppo2.py:185][0m |           0.0043 |          99.2644 |        -194.2400 |
[32m[20221214 00:40:06 @agent_ppo2.py:185][0m |           0.0061 |         106.9105 |        -194.6855 |
[32m[20221214 00:40:06 @agent_ppo2.py:185][0m |          -0.0122 |          89.6949 |        -195.2083 |
[32m[20221214 00:40:06 @agent_ppo2.py:185][0m |          -0.0120 |          87.3927 |        -195.9889 |
[32m[20221214 00:40:06 @agent_ppo2.py:185][0m |          -0.0080 |          86.7558 |        -196.2731 |
[32m[20221214 00:40:06 @agent_ppo2.py:185][0m |          -0.0074 |          84.6527 |        -195.6260 |
[32m[20221214 00:40:06 @agent_ppo2.py:185][0m |          -0.0104 |          83.9143 |        -195.8254 |
[32m[20221214 00:40:06 @agent_ppo2.py:185][0m |          -0.0123 |          82.7834 |        -196.1127 |
[32m[20221214 00:40:07 @agent_ppo2.py:185][0m |          -0.0162 |          82.2210 |        -196.1214 |
[32m[20221214 00:40:07 @agent_ppo2.py:185][0m |          -0.0176 |          81.5528 |        -196.7715 |
[32m[20221214 00:40:07 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:40:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 549.01
[32m[20221214 00:40:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 755.98
[32m[20221214 00:40:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 745.63
[32m[20221214 00:40:07 @agent_ppo2.py:143][0m Total time:      42.61 min
[32m[20221214 00:40:07 @agent_ppo2.py:145][0m 3801088 total steps have happened
[32m[20221214 00:40:07 @agent_ppo2.py:121][0m #------------------------ Iteration 5856 --------------------------#
[32m[20221214 00:40:07 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:40:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:40:07 @agent_ppo2.py:185][0m |          -0.0003 |         109.9014 |        -196.2783 |
[32m[20221214 00:40:07 @agent_ppo2.py:185][0m |           0.0050 |         104.5632 |        -195.9795 |
[32m[20221214 00:40:07 @agent_ppo2.py:185][0m |          -0.0051 |         102.2489 |        -196.6725 |
[32m[20221214 00:40:07 @agent_ppo2.py:185][0m |          -0.0015 |         100.9113 |        -196.7711 |
[32m[20221214 00:40:08 @agent_ppo2.py:185][0m |          -0.0107 |         100.5354 |        -196.5178 |
[32m[20221214 00:40:08 @agent_ppo2.py:185][0m |          -0.0087 |          99.5705 |        -195.9706 |
[32m[20221214 00:40:08 @agent_ppo2.py:185][0m |          -0.0113 |          99.9599 |        -197.8964 |
[32m[20221214 00:40:08 @agent_ppo2.py:185][0m |          -0.0092 |          99.1971 |        -196.9603 |
[32m[20221214 00:40:08 @agent_ppo2.py:185][0m |          -0.0091 |          99.1985 |        -196.5707 |
[32m[20221214 00:40:08 @agent_ppo2.py:185][0m |          -0.0105 |          98.2096 |        -197.2143 |
[32m[20221214 00:40:08 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:40:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 652.09
[32m[20221214 00:40:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 767.42
[32m[20221214 00:40:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 731.30
[32m[20221214 00:40:08 @agent_ppo2.py:143][0m Total time:      42.63 min
[32m[20221214 00:40:08 @agent_ppo2.py:145][0m 3803136 total steps have happened
[32m[20221214 00:40:08 @agent_ppo2.py:121][0m #------------------------ Iteration 5857 --------------------------#
[32m[20221214 00:40:08 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:40:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:40:09 @agent_ppo2.py:185][0m |           0.0055 |         158.1662 |        -190.8060 |
[32m[20221214 00:40:09 @agent_ppo2.py:185][0m |           0.0003 |         150.6232 |        -190.5006 |
[32m[20221214 00:40:09 @agent_ppo2.py:185][0m |          -0.0046 |         145.9915 |        -189.5337 |
[32m[20221214 00:40:09 @agent_ppo2.py:185][0m |           0.0052 |         149.1520 |        -189.1370 |
[32m[20221214 00:40:09 @agent_ppo2.py:185][0m |          -0.0027 |         142.8226 |        -188.8311 |
[32m[20221214 00:40:09 @agent_ppo2.py:185][0m |          -0.0028 |         141.7420 |        -188.5360 |
[32m[20221214 00:40:09 @agent_ppo2.py:185][0m |          -0.0063 |         141.3722 |        -186.7267 |
[32m[20221214 00:40:09 @agent_ppo2.py:185][0m |          -0.0071 |         138.5778 |        -187.6624 |
[32m[20221214 00:40:09 @agent_ppo2.py:185][0m |           0.0016 |         156.2058 |        -188.2144 |
[32m[20221214 00:40:10 @agent_ppo2.py:185][0m |          -0.0091 |         137.0710 |        -186.0256 |
[32m[20221214 00:40:10 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221214 00:40:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 573.47
[32m[20221214 00:40:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 677.43
[32m[20221214 00:40:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 759.54
[32m[20221214 00:40:10 @agent_ppo2.py:143][0m Total time:      42.66 min
[32m[20221214 00:40:10 @agent_ppo2.py:145][0m 3805184 total steps have happened
[32m[20221214 00:40:10 @agent_ppo2.py:121][0m #------------------------ Iteration 5858 --------------------------#
[32m[20221214 00:40:10 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:40:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:40:10 @agent_ppo2.py:185][0m |           0.0005 |         159.0716 |        -187.6325 |
[32m[20221214 00:40:10 @agent_ppo2.py:185][0m |           0.0006 |         154.9077 |        -188.5209 |
[32m[20221214 00:40:10 @agent_ppo2.py:185][0m |          -0.0005 |         154.0828 |        -188.9002 |
[32m[20221214 00:40:10 @agent_ppo2.py:185][0m |          -0.0026 |         153.2675 |        -189.5190 |
[32m[20221214 00:40:10 @agent_ppo2.py:185][0m |          -0.0022 |         151.9593 |        -189.9939 |
[32m[20221214 00:40:11 @agent_ppo2.py:185][0m |           0.0002 |         152.2206 |        -189.5172 |
[32m[20221214 00:40:11 @agent_ppo2.py:185][0m |          -0.0068 |         151.1222 |        -190.3072 |
[32m[20221214 00:40:11 @agent_ppo2.py:185][0m |          -0.0004 |         150.6112 |        -190.4804 |
[32m[20221214 00:40:11 @agent_ppo2.py:185][0m |          -0.0018 |         150.3673 |        -189.9394 |
[32m[20221214 00:40:11 @agent_ppo2.py:185][0m |          -0.0039 |         150.4906 |        -191.0294 |
[32m[20221214 00:40:11 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:40:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 660.59
[32m[20221214 00:40:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 698.18
[32m[20221214 00:40:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 631.82
[32m[20221214 00:40:11 @agent_ppo2.py:143][0m Total time:      42.68 min
[32m[20221214 00:40:11 @agent_ppo2.py:145][0m 3807232 total steps have happened
[32m[20221214 00:40:11 @agent_ppo2.py:121][0m #------------------------ Iteration 5859 --------------------------#
[32m[20221214 00:40:11 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:40:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:40:11 @agent_ppo2.py:185][0m |           0.0030 |         160.6733 |        -197.5432 |
[32m[20221214 00:40:12 @agent_ppo2.py:185][0m |          -0.0008 |         157.4826 |        -196.1329 |
[32m[20221214 00:40:12 @agent_ppo2.py:185][0m |          -0.0004 |         157.3512 |        -196.8538 |
[32m[20221214 00:40:12 @agent_ppo2.py:185][0m |          -0.0036 |         156.3389 |        -196.9295 |
[32m[20221214 00:40:12 @agent_ppo2.py:185][0m |          -0.0035 |         155.9844 |        -196.8748 |
[32m[20221214 00:40:12 @agent_ppo2.py:185][0m |          -0.0060 |         155.8174 |        -196.3087 |
[32m[20221214 00:40:12 @agent_ppo2.py:185][0m |          -0.0033 |         155.3733 |        -196.5402 |
[32m[20221214 00:40:12 @agent_ppo2.py:185][0m |          -0.0046 |         155.3099 |        -196.9642 |
[32m[20221214 00:40:12 @agent_ppo2.py:185][0m |          -0.0025 |         155.3469 |        -197.2542 |
[32m[20221214 00:40:12 @agent_ppo2.py:185][0m |          -0.0085 |         155.5686 |        -197.8125 |
[32m[20221214 00:40:12 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:40:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 732.67
[32m[20221214 00:40:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 769.97
[32m[20221214 00:40:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 853.09
[32m[20221214 00:40:13 @agent_ppo2.py:143][0m Total time:      42.70 min
[32m[20221214 00:40:13 @agent_ppo2.py:145][0m 3809280 total steps have happened
[32m[20221214 00:40:13 @agent_ppo2.py:121][0m #------------------------ Iteration 5860 --------------------------#
[32m[20221214 00:40:13 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:40:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:40:13 @agent_ppo2.py:185][0m |          -0.0007 |         171.2700 |        -192.7770 |
[32m[20221214 00:40:13 @agent_ppo2.py:185][0m |           0.0041 |         177.8837 |        -192.1672 |
[32m[20221214 00:40:13 @agent_ppo2.py:185][0m |          -0.0026 |         162.6207 |        -191.3613 |
[32m[20221214 00:40:13 @agent_ppo2.py:185][0m |          -0.0084 |         161.0825 |        -191.8492 |
[32m[20221214 00:40:13 @agent_ppo2.py:185][0m |          -0.0126 |         160.4709 |        -190.9499 |
[32m[20221214 00:40:13 @agent_ppo2.py:185][0m |          -0.0134 |         162.7709 |        -190.1141 |
[32m[20221214 00:40:14 @agent_ppo2.py:185][0m |          -0.0071 |         159.6281 |        -190.1481 |
[32m[20221214 00:40:14 @agent_ppo2.py:185][0m |          -0.0104 |         160.4962 |        -189.9865 |
[32m[20221214 00:40:14 @agent_ppo2.py:185][0m |          -0.0116 |         160.0240 |        -189.6984 |
[32m[20221214 00:40:14 @agent_ppo2.py:185][0m |          -0.0068 |         158.1274 |        -189.5190 |
[32m[20221214 00:40:14 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:40:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 578.67
[32m[20221214 00:40:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 813.84
[32m[20221214 00:40:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 716.99
[32m[20221214 00:40:14 @agent_ppo2.py:143][0m Total time:      42.73 min
[32m[20221214 00:40:14 @agent_ppo2.py:145][0m 3811328 total steps have happened
[32m[20221214 00:40:14 @agent_ppo2.py:121][0m #------------------------ Iteration 5861 --------------------------#
[32m[20221214 00:40:14 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:40:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:40:14 @agent_ppo2.py:185][0m |           0.0020 |         155.8690 |        -196.6720 |
[32m[20221214 00:40:14 @agent_ppo2.py:185][0m |          -0.0016 |         150.8397 |        -196.6858 |
[32m[20221214 00:40:15 @agent_ppo2.py:185][0m |          -0.0098 |         149.9057 |        -196.6689 |
[32m[20221214 00:40:15 @agent_ppo2.py:185][0m |          -0.0041 |         149.5370 |        -197.1385 |
[32m[20221214 00:40:15 @agent_ppo2.py:185][0m |          -0.0072 |         148.3746 |        -197.2764 |
[32m[20221214 00:40:15 @agent_ppo2.py:185][0m |          -0.0075 |         147.6257 |        -198.2474 |
[32m[20221214 00:40:15 @agent_ppo2.py:185][0m |           0.0081 |         167.5156 |        -197.1433 |
[32m[20221214 00:40:15 @agent_ppo2.py:185][0m |          -0.0072 |         151.4790 |        -197.7162 |
[32m[20221214 00:40:15 @agent_ppo2.py:185][0m |          -0.0071 |         146.7460 |        -197.1450 |
[32m[20221214 00:40:15 @agent_ppo2.py:185][0m |          -0.0097 |         146.5481 |        -197.9135 |
[32m[20221214 00:40:15 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:40:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 556.11
[32m[20221214 00:40:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 752.28
[32m[20221214 00:40:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 730.70
[32m[20221214 00:40:15 @agent_ppo2.py:143][0m Total time:      42.75 min
[32m[20221214 00:40:15 @agent_ppo2.py:145][0m 3813376 total steps have happened
[32m[20221214 00:40:15 @agent_ppo2.py:121][0m #------------------------ Iteration 5862 --------------------------#
[32m[20221214 00:40:16 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:40:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:40:16 @agent_ppo2.py:185][0m |           0.0018 |         156.8007 |        -197.3932 |
[32m[20221214 00:40:16 @agent_ppo2.py:185][0m |           0.0016 |         154.3539 |        -196.8997 |
[32m[20221214 00:40:16 @agent_ppo2.py:185][0m |          -0.0001 |         152.1935 |        -195.9432 |
[32m[20221214 00:40:16 @agent_ppo2.py:185][0m |           0.0025 |         151.4862 |        -196.1681 |
[32m[20221214 00:40:16 @agent_ppo2.py:185][0m |           0.0008 |         150.4951 |        -196.0138 |
[32m[20221214 00:40:16 @agent_ppo2.py:185][0m |          -0.0026 |         150.3641 |        -196.5450 |
[32m[20221214 00:40:16 @agent_ppo2.py:185][0m |          -0.0010 |         150.7454 |        -195.2020 |
[32m[20221214 00:40:16 @agent_ppo2.py:185][0m |          -0.0027 |         149.3660 |        -195.8512 |
[32m[20221214 00:40:17 @agent_ppo2.py:185][0m |          -0.0001 |         148.6495 |        -195.2560 |
[32m[20221214 00:40:17 @agent_ppo2.py:185][0m |          -0.0027 |         148.8587 |        -196.3735 |
[32m[20221214 00:40:17 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:40:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 621.34
[32m[20221214 00:40:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 686.27
[32m[20221214 00:40:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 689.33
[32m[20221214 00:40:17 @agent_ppo2.py:143][0m Total time:      42.77 min
[32m[20221214 00:40:17 @agent_ppo2.py:145][0m 3815424 total steps have happened
[32m[20221214 00:40:17 @agent_ppo2.py:121][0m #------------------------ Iteration 5863 --------------------------#
[32m[20221214 00:40:17 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:40:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:40:17 @agent_ppo2.py:185][0m |           0.0022 |         148.4512 |        -194.0738 |
[32m[20221214 00:40:17 @agent_ppo2.py:185][0m |          -0.0018 |         148.4470 |        -192.3086 |
[32m[20221214 00:40:17 @agent_ppo2.py:185][0m |          -0.0055 |         147.9014 |        -192.7450 |
[32m[20221214 00:40:17 @agent_ppo2.py:185][0m |           0.0014 |         149.9863 |        -192.0415 |
[32m[20221214 00:40:18 @agent_ppo2.py:185][0m |          -0.0002 |         146.2548 |        -191.7004 |
[32m[20221214 00:40:18 @agent_ppo2.py:185][0m |          -0.0027 |         145.6867 |        -192.2794 |
[32m[20221214 00:40:18 @agent_ppo2.py:185][0m |          -0.0049 |         144.8046 |        -192.8038 |
[32m[20221214 00:40:18 @agent_ppo2.py:185][0m |          -0.0058 |         145.3513 |        -192.3655 |
[32m[20221214 00:40:18 @agent_ppo2.py:185][0m |          -0.0040 |         145.2676 |        -191.7383 |
[32m[20221214 00:40:18 @agent_ppo2.py:185][0m |          -0.0068 |         146.1137 |        -191.4714 |
[32m[20221214 00:40:18 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:40:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 659.82
[32m[20221214 00:40:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 756.66
[32m[20221214 00:40:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 794.04
[32m[20221214 00:40:18 @agent_ppo2.py:143][0m Total time:      42.80 min
[32m[20221214 00:40:18 @agent_ppo2.py:145][0m 3817472 total steps have happened
[32m[20221214 00:40:18 @agent_ppo2.py:121][0m #------------------------ Iteration 5864 --------------------------#
[32m[20221214 00:40:18 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:40:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:40:19 @agent_ppo2.py:185][0m |           0.0035 |         166.8681 |        -188.1938 |
[32m[20221214 00:40:19 @agent_ppo2.py:185][0m |           0.0074 |         169.6009 |        -188.9654 |
[32m[20221214 00:40:19 @agent_ppo2.py:185][0m |           0.0003 |         163.1331 |        -189.2666 |
[32m[20221214 00:40:19 @agent_ppo2.py:185][0m |           0.0017 |         163.5456 |        -189.8547 |
[32m[20221214 00:40:19 @agent_ppo2.py:185][0m |           0.0051 |         170.3273 |        -189.3943 |
[32m[20221214 00:40:19 @agent_ppo2.py:185][0m |          -0.0014 |         162.6927 |        -188.3810 |
[32m[20221214 00:40:19 @agent_ppo2.py:185][0m |          -0.0012 |         161.8165 |        -189.9066 |
[32m[20221214 00:40:19 @agent_ppo2.py:185][0m |          -0.0024 |         163.2899 |        -188.7852 |
[32m[20221214 00:40:19 @agent_ppo2.py:185][0m |          -0.0033 |         162.6934 |        -190.3432 |
[32m[20221214 00:40:19 @agent_ppo2.py:185][0m |          -0.0014 |         163.3484 |        -190.4109 |
[32m[20221214 00:40:20 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:40:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 737.29
[32m[20221214 00:40:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 831.21
[32m[20221214 00:40:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 721.99
[32m[20221214 00:40:20 @agent_ppo2.py:143][0m Total time:      42.82 min
[32m[20221214 00:40:20 @agent_ppo2.py:145][0m 3819520 total steps have happened
[32m[20221214 00:40:20 @agent_ppo2.py:121][0m #------------------------ Iteration 5865 --------------------------#
[32m[20221214 00:40:20 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:40:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:40:20 @agent_ppo2.py:185][0m |           0.0015 |         181.3719 |        -198.0132 |
[32m[20221214 00:40:20 @agent_ppo2.py:185][0m |          -0.0037 |         175.8695 |        -196.7255 |
[32m[20221214 00:40:20 @agent_ppo2.py:185][0m |          -0.0010 |         173.9086 |        -197.4240 |
[32m[20221214 00:40:20 @agent_ppo2.py:185][0m |           0.0177 |         197.4772 |        -197.5361 |
[32m[20221214 00:40:20 @agent_ppo2.py:185][0m |          -0.0018 |         176.0514 |        -197.3489 |
[32m[20221214 00:40:20 @agent_ppo2.py:185][0m |           0.0115 |         196.6410 |        -198.2402 |
[32m[20221214 00:40:21 @agent_ppo2.py:185][0m |          -0.0034 |         171.5796 |        -197.7097 |
[32m[20221214 00:40:21 @agent_ppo2.py:185][0m |          -0.0047 |         170.0699 |        -197.3794 |
[32m[20221214 00:40:21 @agent_ppo2.py:185][0m |          -0.0059 |         170.1644 |        -198.2377 |
[32m[20221214 00:40:21 @agent_ppo2.py:185][0m |          -0.0023 |         169.1336 |        -197.7680 |
[32m[20221214 00:40:21 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:40:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 725.24
[32m[20221214 00:40:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 795.04
[32m[20221214 00:40:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 512.35
[32m[20221214 00:40:21 @agent_ppo2.py:143][0m Total time:      42.84 min
[32m[20221214 00:40:21 @agent_ppo2.py:145][0m 3821568 total steps have happened
[32m[20221214 00:40:21 @agent_ppo2.py:121][0m #------------------------ Iteration 5866 --------------------------#
[32m[20221214 00:40:21 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:40:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:40:21 @agent_ppo2.py:185][0m |          -0.0003 |         163.2328 |        -200.8223 |
[32m[20221214 00:40:21 @agent_ppo2.py:185][0m |           0.0002 |         160.2147 |        -198.8469 |
[32m[20221214 00:40:22 @agent_ppo2.py:185][0m |           0.0063 |         159.9548 |        -199.5759 |
[32m[20221214 00:40:22 @agent_ppo2.py:185][0m |          -0.0015 |         157.6549 |        -199.7822 |
[32m[20221214 00:40:22 @agent_ppo2.py:185][0m |          -0.0041 |         159.9022 |        -198.9595 |
[32m[20221214 00:40:22 @agent_ppo2.py:185][0m |           0.0018 |         156.8110 |        -199.9131 |
[32m[20221214 00:40:22 @agent_ppo2.py:185][0m |           0.0005 |         156.5279 |        -198.5242 |
[32m[20221214 00:40:22 @agent_ppo2.py:185][0m |          -0.0007 |         154.5903 |        -197.5555 |
[32m[20221214 00:40:22 @agent_ppo2.py:185][0m |           0.0010 |         155.4148 |        -199.0255 |
[32m[20221214 00:40:22 @agent_ppo2.py:185][0m |          -0.0022 |         154.1442 |        -198.9316 |
[32m[20221214 00:40:22 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:40:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 800.45
[32m[20221214 00:40:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.49
[32m[20221214 00:40:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 625.64
[32m[20221214 00:40:22 @agent_ppo2.py:143][0m Total time:      42.87 min
[32m[20221214 00:40:22 @agent_ppo2.py:145][0m 3823616 total steps have happened
[32m[20221214 00:40:22 @agent_ppo2.py:121][0m #------------------------ Iteration 5867 --------------------------#
[32m[20221214 00:40:23 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:40:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:40:23 @agent_ppo2.py:185][0m |           0.0094 |         149.6427 |        -197.3095 |
[32m[20221214 00:40:23 @agent_ppo2.py:185][0m |          -0.0047 |         125.3900 |        -196.6344 |
[32m[20221214 00:40:23 @agent_ppo2.py:185][0m |          -0.0049 |         121.1031 |        -196.1825 |
[32m[20221214 00:40:23 @agent_ppo2.py:185][0m |          -0.0106 |         118.9764 |        -195.4187 |
[32m[20221214 00:40:23 @agent_ppo2.py:185][0m |          -0.0064 |         119.4018 |        -195.7637 |
[32m[20221214 00:40:23 @agent_ppo2.py:185][0m |          -0.0073 |         118.6687 |        -196.1823 |
[32m[20221214 00:40:23 @agent_ppo2.py:185][0m |          -0.0135 |         117.0125 |        -196.4894 |
[32m[20221214 00:40:23 @agent_ppo2.py:185][0m |          -0.0115 |         116.3141 |        -195.2374 |
[32m[20221214 00:40:24 @agent_ppo2.py:185][0m |          -0.0132 |         116.0202 |        -195.2489 |
[32m[20221214 00:40:24 @agent_ppo2.py:185][0m |          -0.0118 |         116.0819 |        -196.4011 |
[32m[20221214 00:40:24 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:40:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 531.43
[32m[20221214 00:40:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 786.84
[32m[20221214 00:40:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 739.15
[32m[20221214 00:40:24 @agent_ppo2.py:143][0m Total time:      42.89 min
[32m[20221214 00:40:24 @agent_ppo2.py:145][0m 3825664 total steps have happened
[32m[20221214 00:40:24 @agent_ppo2.py:121][0m #------------------------ Iteration 5868 --------------------------#
[32m[20221214 00:40:24 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:40:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:40:24 @agent_ppo2.py:185][0m |           0.0052 |         141.0979 |        -195.5090 |
[32m[20221214 00:40:24 @agent_ppo2.py:185][0m |          -0.0066 |         131.2136 |        -194.9401 |
[32m[20221214 00:40:24 @agent_ppo2.py:185][0m |          -0.0079 |         128.0077 |        -194.8788 |
[32m[20221214 00:40:24 @agent_ppo2.py:185][0m |          -0.0095 |         125.7586 |        -195.1125 |
[32m[20221214 00:40:25 @agent_ppo2.py:185][0m |          -0.0100 |         125.1300 |        -194.7041 |
[32m[20221214 00:40:25 @agent_ppo2.py:185][0m |          -0.0111 |         124.9937 |        -194.2416 |
[32m[20221214 00:40:25 @agent_ppo2.py:185][0m |          -0.0122 |         124.6688 |        -194.8883 |
[32m[20221214 00:40:25 @agent_ppo2.py:185][0m |          -0.0138 |         122.8825 |        -193.9285 |
[32m[20221214 00:40:25 @agent_ppo2.py:185][0m |          -0.0207 |         122.2662 |        -194.1194 |
[32m[20221214 00:40:25 @agent_ppo2.py:185][0m |          -0.0180 |         121.8969 |        -194.0832 |
[32m[20221214 00:40:25 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:40:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 539.85
[32m[20221214 00:40:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 644.43
[32m[20221214 00:40:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 798.74
[32m[20221214 00:40:25 @agent_ppo2.py:143][0m Total time:      42.91 min
[32m[20221214 00:40:25 @agent_ppo2.py:145][0m 3827712 total steps have happened
[32m[20221214 00:40:25 @agent_ppo2.py:121][0m #------------------------ Iteration 5869 --------------------------#
[32m[20221214 00:40:25 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:40:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:40:26 @agent_ppo2.py:185][0m |           0.0191 |         169.8260 |        -198.7884 |
[32m[20221214 00:40:26 @agent_ppo2.py:185][0m |          -0.0013 |         151.4730 |        -197.8128 |
[32m[20221214 00:40:26 @agent_ppo2.py:185][0m |          -0.0034 |         147.5462 |        -198.9295 |
[32m[20221214 00:40:26 @agent_ppo2.py:185][0m |          -0.0065 |         146.0052 |        -198.4270 |
[32m[20221214 00:40:26 @agent_ppo2.py:185][0m |          -0.0048 |         144.9163 |        -198.3398 |
[32m[20221214 00:40:26 @agent_ppo2.py:185][0m |          -0.0068 |         144.8926 |        -197.0140 |
[32m[20221214 00:40:26 @agent_ppo2.py:185][0m |          -0.0055 |         143.7429 |        -199.1044 |
[32m[20221214 00:40:26 @agent_ppo2.py:185][0m |          -0.0062 |         143.4486 |        -198.5359 |
[32m[20221214 00:40:26 @agent_ppo2.py:185][0m |          -0.0049 |         145.2884 |        -197.6489 |
[32m[20221214 00:40:26 @agent_ppo2.py:185][0m |           0.0000 |         150.9928 |        -198.9387 |
[32m[20221214 00:40:26 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:40:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 681.55
[32m[20221214 00:40:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 776.04
[32m[20221214 00:40:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 554.61
[32m[20221214 00:40:27 @agent_ppo2.py:143][0m Total time:      42.94 min
[32m[20221214 00:40:27 @agent_ppo2.py:145][0m 3829760 total steps have happened
[32m[20221214 00:40:27 @agent_ppo2.py:121][0m #------------------------ Iteration 5870 --------------------------#
[32m[20221214 00:40:27 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:40:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:40:27 @agent_ppo2.py:185][0m |           0.0030 |         173.3379 |        -192.8234 |
[32m[20221214 00:40:27 @agent_ppo2.py:185][0m |           0.0006 |         171.7887 |        -192.7088 |
[32m[20221214 00:40:27 @agent_ppo2.py:185][0m |           0.0028 |         172.1040 |        -192.6760 |
[32m[20221214 00:40:27 @agent_ppo2.py:185][0m |           0.0021 |         169.6235 |        -193.0690 |
[32m[20221214 00:40:27 @agent_ppo2.py:185][0m |          -0.0009 |         169.9938 |        -194.0695 |
[32m[20221214 00:40:27 @agent_ppo2.py:185][0m |          -0.0028 |         169.2887 |        -193.7475 |
[32m[20221214 00:40:28 @agent_ppo2.py:185][0m |           0.0001 |         169.4524 |        -194.2297 |
[32m[20221214 00:40:28 @agent_ppo2.py:185][0m |          -0.0030 |         168.6423 |        -193.7702 |
[32m[20221214 00:40:28 @agent_ppo2.py:185][0m |           0.0060 |         185.6183 |        -193.4212 |
[32m[20221214 00:40:28 @agent_ppo2.py:185][0m |          -0.0021 |         168.0268 |        -192.5386 |
[32m[20221214 00:40:28 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:40:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 745.66
[32m[20221214 00:40:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 793.44
[32m[20221214 00:40:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 659.55
[32m[20221214 00:40:28 @agent_ppo2.py:143][0m Total time:      42.96 min
[32m[20221214 00:40:28 @agent_ppo2.py:145][0m 3831808 total steps have happened
[32m[20221214 00:40:28 @agent_ppo2.py:121][0m #------------------------ Iteration 5871 --------------------------#
[32m[20221214 00:40:28 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:40:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:40:28 @agent_ppo2.py:185][0m |          -0.0015 |         185.3489 |        -193.3961 |
[32m[20221214 00:40:28 @agent_ppo2.py:185][0m |          -0.0071 |         176.6583 |        -193.7195 |
[32m[20221214 00:40:29 @agent_ppo2.py:185][0m |           0.0001 |         178.6538 |        -193.4347 |
[32m[20221214 00:40:29 @agent_ppo2.py:185][0m |           0.0019 |         174.2165 |        -194.1927 |
[32m[20221214 00:40:29 @agent_ppo2.py:185][0m |          -0.0067 |         171.2435 |        -193.6223 |
[32m[20221214 00:40:29 @agent_ppo2.py:185][0m |          -0.0061 |         170.6083 |        -194.4536 |
[32m[20221214 00:40:29 @agent_ppo2.py:185][0m |          -0.0076 |         170.4052 |        -194.0008 |
[32m[20221214 00:40:29 @agent_ppo2.py:185][0m |          -0.0093 |         169.0347 |        -194.3397 |
[32m[20221214 00:40:29 @agent_ppo2.py:185][0m |          -0.0063 |         169.9940 |        -193.3780 |
[32m[20221214 00:40:29 @agent_ppo2.py:185][0m |          -0.0137 |         167.5717 |        -193.8100 |
[32m[20221214 00:40:29 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:40:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 605.88
[32m[20221214 00:40:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 745.69
[32m[20221214 00:40:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 636.73
[32m[20221214 00:40:29 @agent_ppo2.py:143][0m Total time:      42.98 min
[32m[20221214 00:40:29 @agent_ppo2.py:145][0m 3833856 total steps have happened
[32m[20221214 00:40:29 @agent_ppo2.py:121][0m #------------------------ Iteration 5872 --------------------------#
[32m[20221214 00:40:30 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:40:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:40:30 @agent_ppo2.py:185][0m |           0.0069 |         200.7624 |        -197.8898 |
[32m[20221214 00:40:30 @agent_ppo2.py:185][0m |          -0.0008 |         193.1237 |        -197.3898 |
[32m[20221214 00:40:30 @agent_ppo2.py:185][0m |          -0.0011 |         192.4363 |        -197.1913 |
[32m[20221214 00:40:30 @agent_ppo2.py:185][0m |          -0.0002 |         190.5139 |        -197.2190 |
[32m[20221214 00:40:30 @agent_ppo2.py:185][0m |          -0.0025 |         190.4828 |        -197.8165 |
[32m[20221214 00:40:30 @agent_ppo2.py:185][0m |           0.0017 |         190.3603 |        -199.2142 |
[32m[20221214 00:40:30 @agent_ppo2.py:185][0m |           0.0012 |         189.6509 |        -197.5879 |
[32m[20221214 00:40:30 @agent_ppo2.py:185][0m |           0.0123 |         204.0213 |        -198.4063 |
[32m[20221214 00:40:31 @agent_ppo2.py:185][0m |          -0.0030 |         190.2744 |        -199.4151 |
[32m[20221214 00:40:31 @agent_ppo2.py:185][0m |           0.0171 |         209.9058 |        -200.0230 |
[32m[20221214 00:40:31 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:40:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 728.37
[32m[20221214 00:40:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 798.16
[32m[20221214 00:40:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 839.72
[32m[20221214 00:40:31 @agent_ppo2.py:143][0m Total time:      43.01 min
[32m[20221214 00:40:31 @agent_ppo2.py:145][0m 3835904 total steps have happened
[32m[20221214 00:40:31 @agent_ppo2.py:121][0m #------------------------ Iteration 5873 --------------------------#
[32m[20221214 00:40:31 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:40:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:40:31 @agent_ppo2.py:185][0m |          -0.0003 |         170.9622 |        -199.8225 |
[32m[20221214 00:40:31 @agent_ppo2.py:185][0m |          -0.0033 |         167.1656 |        -198.6246 |
[32m[20221214 00:40:31 @agent_ppo2.py:185][0m |           0.0018 |         169.6042 |        -198.8675 |
[32m[20221214 00:40:31 @agent_ppo2.py:185][0m |          -0.0045 |         163.8352 |        -198.9580 |
[32m[20221214 00:40:32 @agent_ppo2.py:185][0m |          -0.0075 |         164.0602 |        -198.4942 |
[32m[20221214 00:40:32 @agent_ppo2.py:185][0m |           0.0063 |         172.7588 |        -198.0130 |
[32m[20221214 00:40:32 @agent_ppo2.py:185][0m |          -0.0070 |         163.2023 |        -198.1951 |
[32m[20221214 00:40:32 @agent_ppo2.py:185][0m |          -0.0079 |         162.6147 |        -197.9788 |
[32m[20221214 00:40:32 @agent_ppo2.py:185][0m |          -0.0077 |         162.2281 |        -198.4573 |
[32m[20221214 00:40:32 @agent_ppo2.py:185][0m |          -0.0068 |         161.3262 |        -198.3041 |
[32m[20221214 00:40:32 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:40:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 768.28
[32m[20221214 00:40:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 827.45
[32m[20221214 00:40:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 619.21
[32m[20221214 00:40:32 @agent_ppo2.py:143][0m Total time:      43.03 min
[32m[20221214 00:40:32 @agent_ppo2.py:145][0m 3837952 total steps have happened
[32m[20221214 00:40:32 @agent_ppo2.py:121][0m #------------------------ Iteration 5874 --------------------------#
[32m[20221214 00:40:32 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:40:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:40:33 @agent_ppo2.py:185][0m |           0.0041 |         154.1061 |        -195.7984 |
[32m[20221214 00:40:33 @agent_ppo2.py:185][0m |          -0.0025 |         146.0205 |        -195.1238 |
[32m[20221214 00:40:33 @agent_ppo2.py:185][0m |          -0.0043 |         140.0877 |        -195.4650 |
[32m[20221214 00:40:33 @agent_ppo2.py:185][0m |           0.0048 |         142.3026 |        -195.6633 |
[32m[20221214 00:40:33 @agent_ppo2.py:185][0m |          -0.0094 |         137.6882 |        -194.6030 |
[32m[20221214 00:40:33 @agent_ppo2.py:185][0m |          -0.0091 |         132.0672 |        -194.7124 |
[32m[20221214 00:40:33 @agent_ppo2.py:185][0m |          -0.0070 |         129.9917 |        -195.1588 |
[32m[20221214 00:40:33 @agent_ppo2.py:185][0m |          -0.0083 |         127.8033 |        -194.9265 |
[32m[20221214 00:40:33 @agent_ppo2.py:185][0m |          -0.0028 |         128.2922 |        -195.6077 |
[32m[20221214 00:40:33 @agent_ppo2.py:185][0m |          -0.0083 |         125.8499 |        -195.7262 |
[32m[20221214 00:40:33 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:40:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 699.01
[32m[20221214 00:40:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.63
[32m[20221214 00:40:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 708.45
[32m[20221214 00:40:34 @agent_ppo2.py:143][0m Total time:      43.05 min
[32m[20221214 00:40:34 @agent_ppo2.py:145][0m 3840000 total steps have happened
[32m[20221214 00:40:34 @agent_ppo2.py:121][0m #------------------------ Iteration 5875 --------------------------#
[32m[20221214 00:40:34 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:40:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:40:34 @agent_ppo2.py:185][0m |           0.0001 |         167.0046 |        -201.5287 |
[32m[20221214 00:40:34 @agent_ppo2.py:185][0m |          -0.0024 |         162.8655 |        -201.4787 |
[32m[20221214 00:40:34 @agent_ppo2.py:185][0m |          -0.0027 |         160.4025 |        -200.9837 |
[32m[20221214 00:40:34 @agent_ppo2.py:185][0m |          -0.0012 |         161.7363 |        -199.9802 |
[32m[20221214 00:40:34 @agent_ppo2.py:185][0m |          -0.0058 |         159.5722 |        -201.5153 |
[32m[20221214 00:40:34 @agent_ppo2.py:185][0m |          -0.0045 |         160.1830 |        -200.4010 |
[32m[20221214 00:40:34 @agent_ppo2.py:185][0m |          -0.0023 |         158.6529 |        -200.2618 |
[32m[20221214 00:40:35 @agent_ppo2.py:185][0m |          -0.0051 |         157.7345 |        -200.8123 |
[32m[20221214 00:40:35 @agent_ppo2.py:185][0m |           0.0046 |         181.0215 |        -201.2914 |
[32m[20221214 00:40:35 @agent_ppo2.py:185][0m |          -0.0055 |         158.8371 |        -201.0994 |
[32m[20221214 00:40:35 @agent_ppo2.py:130][0m Policy update time: 1.04 s
[32m[20221214 00:40:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 767.18
[32m[20221214 00:40:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.35
[32m[20221214 00:40:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 827.70
[32m[20221214 00:40:35 @agent_ppo2.py:143][0m Total time:      43.08 min
[32m[20221214 00:40:35 @agent_ppo2.py:145][0m 3842048 total steps have happened
[32m[20221214 00:40:35 @agent_ppo2.py:121][0m #------------------------ Iteration 5876 --------------------------#
[32m[20221214 00:40:35 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:40:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:40:35 @agent_ppo2.py:185][0m |           0.0016 |         101.7008 |        -187.6883 |
[32m[20221214 00:40:35 @agent_ppo2.py:185][0m |          -0.0016 |          89.8360 |        -186.8613 |
[32m[20221214 00:40:35 @agent_ppo2.py:185][0m |          -0.0021 |          87.4170 |        -186.5326 |
[32m[20221214 00:40:36 @agent_ppo2.py:185][0m |          -0.0003 |          93.3056 |        -185.9656 |
[32m[20221214 00:40:36 @agent_ppo2.py:185][0m |          -0.0108 |          83.0655 |        -186.7487 |
[32m[20221214 00:40:36 @agent_ppo2.py:185][0m |          -0.0110 |          81.5170 |        -186.9211 |
[32m[20221214 00:40:36 @agent_ppo2.py:185][0m |          -0.0122 |          81.3109 |        -186.4134 |
[32m[20221214 00:40:36 @agent_ppo2.py:185][0m |          -0.0093 |          80.2711 |        -185.7792 |
[32m[20221214 00:40:36 @agent_ppo2.py:185][0m |          -0.0133 |          79.4346 |        -185.2091 |
[32m[20221214 00:40:36 @agent_ppo2.py:185][0m |          -0.0133 |          79.7370 |        -184.5264 |
[32m[20221214 00:40:36 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:40:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 689.27
[32m[20221214 00:40:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 834.79
[32m[20221214 00:40:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 849.12
[32m[20221214 00:40:36 @agent_ppo2.py:143][0m Total time:      43.10 min
[32m[20221214 00:40:36 @agent_ppo2.py:145][0m 3844096 total steps have happened
[32m[20221214 00:40:36 @agent_ppo2.py:121][0m #------------------------ Iteration 5877 --------------------------#
[32m[20221214 00:40:36 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:40:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:40:37 @agent_ppo2.py:185][0m |           0.0008 |          96.9474 |        -190.8718 |
[32m[20221214 00:40:37 @agent_ppo2.py:185][0m |          -0.0063 |          81.5697 |        -190.6242 |
[32m[20221214 00:40:37 @agent_ppo2.py:185][0m |          -0.0069 |          78.0378 |        -189.8209 |
[32m[20221214 00:40:37 @agent_ppo2.py:185][0m |          -0.0086 |          76.2561 |        -189.7267 |
[32m[20221214 00:40:37 @agent_ppo2.py:185][0m |          -0.0138 |          75.5850 |        -189.6893 |
[32m[20221214 00:40:37 @agent_ppo2.py:185][0m |          -0.0123 |          75.8774 |        -189.3311 |
[32m[20221214 00:40:37 @agent_ppo2.py:185][0m |          -0.0210 |          73.8683 |        -189.0081 |
[32m[20221214 00:40:37 @agent_ppo2.py:185][0m |          -0.0157 |          73.3046 |        -189.4768 |
[32m[20221214 00:40:37 @agent_ppo2.py:185][0m |          -0.0166 |          72.7812 |        -189.3631 |
[32m[20221214 00:40:38 @agent_ppo2.py:185][0m |          -0.0173 |          72.4465 |        -189.2604 |
[32m[20221214 00:40:38 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:40:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 596.36
[32m[20221214 00:40:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 744.40
[32m[20221214 00:40:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 840.28
[32m[20221214 00:40:38 @agent_ppo2.py:143][0m Total time:      43.12 min
[32m[20221214 00:40:38 @agent_ppo2.py:145][0m 3846144 total steps have happened
[32m[20221214 00:40:38 @agent_ppo2.py:121][0m #------------------------ Iteration 5878 --------------------------#
[32m[20221214 00:40:38 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:40:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:40:38 @agent_ppo2.py:185][0m |          -0.0000 |         190.9265 |        -193.5917 |
[32m[20221214 00:40:38 @agent_ppo2.py:185][0m |          -0.0073 |         182.1137 |        -192.6628 |
[32m[20221214 00:40:38 @agent_ppo2.py:185][0m |          -0.0087 |         178.7021 |        -192.0520 |
[32m[20221214 00:40:38 @agent_ppo2.py:185][0m |           0.0025 |         186.4676 |        -191.5735 |
[32m[20221214 00:40:38 @agent_ppo2.py:185][0m |          -0.0083 |         176.2139 |        -191.5794 |
[32m[20221214 00:40:39 @agent_ppo2.py:185][0m |          -0.0091 |         175.0608 |        -189.6050 |
[32m[20221214 00:40:39 @agent_ppo2.py:185][0m |          -0.0040 |         176.6301 |        -190.5378 |
[32m[20221214 00:40:39 @agent_ppo2.py:185][0m |          -0.0106 |         174.6455 |        -190.8751 |
[32m[20221214 00:40:39 @agent_ppo2.py:185][0m |          -0.0110 |         173.3591 |        -190.7654 |
[32m[20221214 00:40:39 @agent_ppo2.py:185][0m |          -0.0125 |         173.1443 |        -190.3148 |
[32m[20221214 00:40:39 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:40:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 675.88
[32m[20221214 00:40:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 755.32
[32m[20221214 00:40:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 753.10
[32m[20221214 00:40:39 @agent_ppo2.py:143][0m Total time:      43.14 min
[32m[20221214 00:40:39 @agent_ppo2.py:145][0m 3848192 total steps have happened
[32m[20221214 00:40:39 @agent_ppo2.py:121][0m #------------------------ Iteration 5879 --------------------------#
[32m[20221214 00:40:39 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:40:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:40:39 @agent_ppo2.py:185][0m |          -0.0004 |         141.0195 |        -187.6814 |
[32m[20221214 00:40:39 @agent_ppo2.py:185][0m |          -0.0027 |         131.9233 |        -188.9645 |
[32m[20221214 00:40:40 @agent_ppo2.py:185][0m |           0.0001 |         131.1562 |        -188.5442 |
[32m[20221214 00:40:40 @agent_ppo2.py:185][0m |          -0.0044 |         128.0932 |        -188.6335 |
[32m[20221214 00:40:40 @agent_ppo2.py:185][0m |          -0.0038 |         128.1047 |        -188.2798 |
[32m[20221214 00:40:40 @agent_ppo2.py:185][0m |           0.0074 |         143.1395 |        -188.9107 |
[32m[20221214 00:40:40 @agent_ppo2.py:185][0m |          -0.0075 |         127.4664 |        -189.5388 |
[32m[20221214 00:40:40 @agent_ppo2.py:185][0m |          -0.0058 |         126.9768 |        -188.9574 |
[32m[20221214 00:40:40 @agent_ppo2.py:185][0m |          -0.0070 |         126.7631 |        -189.1777 |
[32m[20221214 00:40:40 @agent_ppo2.py:185][0m |          -0.0035 |         126.4968 |        -188.9471 |
[32m[20221214 00:40:40 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:40:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 769.57
[32m[20221214 00:40:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 827.06
[32m[20221214 00:40:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 681.68
[32m[20221214 00:40:40 @agent_ppo2.py:143][0m Total time:      43.17 min
[32m[20221214 00:40:40 @agent_ppo2.py:145][0m 3850240 total steps have happened
[32m[20221214 00:40:40 @agent_ppo2.py:121][0m #------------------------ Iteration 5880 --------------------------#
[32m[20221214 00:40:41 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:40:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:40:41 @agent_ppo2.py:185][0m |           0.0057 |         196.4156 |        -190.4058 |
[32m[20221214 00:40:41 @agent_ppo2.py:185][0m |           0.0077 |         202.8522 |        -191.2453 |
[32m[20221214 00:40:41 @agent_ppo2.py:185][0m |          -0.0021 |         186.9784 |        -192.4413 |
[32m[20221214 00:40:41 @agent_ppo2.py:185][0m |           0.0000 |         185.6133 |        -190.9633 |
[32m[20221214 00:40:41 @agent_ppo2.py:185][0m |          -0.0011 |         184.5062 |        -192.0716 |
[32m[20221214 00:40:41 @agent_ppo2.py:185][0m |          -0.0004 |         184.0363 |        -192.4104 |
[32m[20221214 00:40:41 @agent_ppo2.py:185][0m |          -0.0045 |         183.7796 |        -191.4015 |
[32m[20221214 00:40:41 @agent_ppo2.py:185][0m |           0.0023 |         184.2196 |        -191.3713 |
[32m[20221214 00:40:42 @agent_ppo2.py:185][0m |           0.0057 |         196.3083 |        -191.1110 |
[32m[20221214 00:40:42 @agent_ppo2.py:185][0m |          -0.0037 |         182.7302 |        -190.9522 |
[32m[20221214 00:40:42 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:40:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 756.51
[32m[20221214 00:40:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 827.56
[32m[20221214 00:40:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 671.69
[32m[20221214 00:40:42 @agent_ppo2.py:143][0m Total time:      43.19 min
[32m[20221214 00:40:42 @agent_ppo2.py:145][0m 3852288 total steps have happened
[32m[20221214 00:40:42 @agent_ppo2.py:121][0m #------------------------ Iteration 5881 --------------------------#
[32m[20221214 00:40:42 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:40:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:40:42 @agent_ppo2.py:185][0m |           0.0009 |          86.2823 |        -191.6938 |
[32m[20221214 00:40:42 @agent_ppo2.py:185][0m |           0.0000 |          79.5466 |        -190.6248 |
[32m[20221214 00:40:42 @agent_ppo2.py:185][0m |          -0.0078 |          76.9761 |        -190.7305 |
[32m[20221214 00:40:42 @agent_ppo2.py:185][0m |          -0.0126 |          75.3469 |        -190.8253 |
[32m[20221214 00:40:43 @agent_ppo2.py:185][0m |          -0.0115 |          74.3592 |        -191.2837 |
[32m[20221214 00:40:43 @agent_ppo2.py:185][0m |          -0.0173 |          73.3790 |        -190.4890 |
[32m[20221214 00:40:43 @agent_ppo2.py:185][0m |          -0.0115 |          74.6460 |        -190.2969 |
[32m[20221214 00:40:43 @agent_ppo2.py:185][0m |          -0.0172 |          71.7920 |        -190.8064 |
[32m[20221214 00:40:43 @agent_ppo2.py:185][0m |          -0.0220 |          71.8737 |        -190.9621 |
[32m[20221214 00:40:43 @agent_ppo2.py:185][0m |          -0.0183 |          70.8671 |        -190.2112 |
[32m[20221214 00:40:43 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:40:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 528.20
[32m[20221214 00:40:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 683.72
[32m[20221214 00:40:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 701.36
[32m[20221214 00:40:43 @agent_ppo2.py:143][0m Total time:      43.21 min
[32m[20221214 00:40:43 @agent_ppo2.py:145][0m 3854336 total steps have happened
[32m[20221214 00:40:43 @agent_ppo2.py:121][0m #------------------------ Iteration 5882 --------------------------#
[32m[20221214 00:40:43 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:40:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:40:44 @agent_ppo2.py:185][0m |           0.0058 |         128.7158 |        -191.5864 |
[32m[20221214 00:40:44 @agent_ppo2.py:185][0m |          -0.0029 |         116.5313 |        -191.8084 |
[32m[20221214 00:40:44 @agent_ppo2.py:185][0m |          -0.0099 |         113.5354 |        -192.2789 |
[32m[20221214 00:40:44 @agent_ppo2.py:185][0m |          -0.0070 |         112.2186 |        -191.7968 |
[32m[20221214 00:40:44 @agent_ppo2.py:185][0m |          -0.0096 |         111.4661 |        -191.6820 |
[32m[20221214 00:40:44 @agent_ppo2.py:185][0m |          -0.0138 |         108.9387 |        -191.7476 |
[32m[20221214 00:40:44 @agent_ppo2.py:185][0m |          -0.0143 |         108.3606 |        -191.3569 |
[32m[20221214 00:40:44 @agent_ppo2.py:185][0m |           0.0049 |         114.8784 |        -192.3742 |
[32m[20221214 00:40:44 @agent_ppo2.py:185][0m |          -0.0141 |         109.6352 |        -191.1012 |
[32m[20221214 00:40:44 @agent_ppo2.py:185][0m |          -0.0177 |         106.4030 |        -191.8381 |
[32m[20221214 00:40:44 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:40:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 519.72
[32m[20221214 00:40:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 612.82
[32m[20221214 00:40:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 842.54
[32m[20221214 00:40:45 @agent_ppo2.py:143][0m Total time:      43.24 min
[32m[20221214 00:40:45 @agent_ppo2.py:145][0m 3856384 total steps have happened
[32m[20221214 00:40:45 @agent_ppo2.py:121][0m #------------------------ Iteration 5883 --------------------------#
[32m[20221214 00:40:45 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:40:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:40:45 @agent_ppo2.py:185][0m |           0.0001 |         108.6657 |        -196.4288 |
[32m[20221214 00:40:45 @agent_ppo2.py:185][0m |          -0.0087 |         100.3574 |        -194.9437 |
[32m[20221214 00:40:45 @agent_ppo2.py:185][0m |          -0.0104 |          99.0862 |        -194.4692 |
[32m[20221214 00:40:45 @agent_ppo2.py:185][0m |          -0.0111 |          99.0006 |        -193.7168 |
[32m[20221214 00:40:45 @agent_ppo2.py:185][0m |          -0.0126 |          98.8331 |        -194.5444 |
[32m[20221214 00:40:45 @agent_ppo2.py:185][0m |          -0.0108 |          98.0888 |        -193.7663 |
[32m[20221214 00:40:45 @agent_ppo2.py:185][0m |          -0.0059 |         105.0709 |        -193.7411 |
[32m[20221214 00:40:46 @agent_ppo2.py:185][0m |          -0.0149 |          97.6212 |        -194.0594 |
[32m[20221214 00:40:46 @agent_ppo2.py:185][0m |          -0.0149 |          97.1087 |        -191.6742 |
[32m[20221214 00:40:46 @agent_ppo2.py:185][0m |          -0.0112 |          97.1835 |        -193.2815 |
[32m[20221214 00:40:46 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:40:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 593.51
[32m[20221214 00:40:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 632.86
[32m[20221214 00:40:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 520.84
[32m[20221214 00:40:46 @agent_ppo2.py:143][0m Total time:      43.26 min
[32m[20221214 00:40:46 @agent_ppo2.py:145][0m 3858432 total steps have happened
[32m[20221214 00:40:46 @agent_ppo2.py:121][0m #------------------------ Iteration 5884 --------------------------#
[32m[20221214 00:40:46 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:40:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:40:46 @agent_ppo2.py:185][0m |           0.0047 |         206.7407 |        -195.8634 |
[32m[20221214 00:40:46 @agent_ppo2.py:185][0m |          -0.0028 |         203.9899 |        -196.4495 |
[32m[20221214 00:40:46 @agent_ppo2.py:185][0m |           0.0002 |         202.7399 |        -196.3358 |
[32m[20221214 00:40:47 @agent_ppo2.py:185][0m |           0.0017 |         202.2966 |        -195.5144 |
[32m[20221214 00:40:47 @agent_ppo2.py:185][0m |          -0.0043 |         201.2265 |        -197.1834 |
[32m[20221214 00:40:47 @agent_ppo2.py:185][0m |           0.0035 |         207.4000 |        -196.2999 |
[32m[20221214 00:40:47 @agent_ppo2.py:185][0m |          -0.0034 |         202.1587 |        -196.8821 |
[32m[20221214 00:40:47 @agent_ppo2.py:185][0m |           0.0030 |         203.0008 |        -196.7344 |
[32m[20221214 00:40:47 @agent_ppo2.py:185][0m |           0.0008 |         202.4425 |        -196.8746 |
[32m[20221214 00:40:47 @agent_ppo2.py:185][0m |          -0.0010 |         201.2740 |        -195.7821 |
[32m[20221214 00:40:47 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:40:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 765.63
[32m[20221214 00:40:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 832.38
[32m[20221214 00:40:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 792.52
[32m[20221214 00:40:47 @agent_ppo2.py:143][0m Total time:      43.28 min
[32m[20221214 00:40:47 @agent_ppo2.py:145][0m 3860480 total steps have happened
[32m[20221214 00:40:47 @agent_ppo2.py:121][0m #------------------------ Iteration 5885 --------------------------#
[32m[20221214 00:40:48 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:40:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:40:48 @agent_ppo2.py:185][0m |          -0.0043 |         103.0157 |        -191.3778 |
[32m[20221214 00:40:48 @agent_ppo2.py:185][0m |          -0.0036 |          96.5667 |        -191.4336 |
[32m[20221214 00:40:48 @agent_ppo2.py:185][0m |          -0.0105 |          94.2378 |        -188.9631 |
[32m[20221214 00:40:48 @agent_ppo2.py:185][0m |          -0.0138 |          92.6356 |        -188.4993 |
[32m[20221214 00:40:48 @agent_ppo2.py:185][0m |          -0.0183 |          91.8939 |        -189.9760 |
[32m[20221214 00:40:48 @agent_ppo2.py:185][0m |          -0.0141 |          90.5641 |        -190.1617 |
[32m[20221214 00:40:48 @agent_ppo2.py:185][0m |          -0.0165 |          90.2270 |        -189.9557 |
[32m[20221214 00:40:48 @agent_ppo2.py:185][0m |          -0.0188 |          89.7862 |        -189.5246 |
[32m[20221214 00:40:49 @agent_ppo2.py:185][0m |          -0.0177 |          89.0788 |        -189.0287 |
[32m[20221214 00:40:49 @agent_ppo2.py:185][0m |          -0.0204 |          88.7045 |        -188.7118 |
[32m[20221214 00:40:49 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:40:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 546.24
[32m[20221214 00:40:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 624.26
[32m[20221214 00:40:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 841.05
[32m[20221214 00:40:49 @agent_ppo2.py:143][0m Total time:      43.31 min
[32m[20221214 00:40:49 @agent_ppo2.py:145][0m 3862528 total steps have happened
[32m[20221214 00:40:49 @agent_ppo2.py:121][0m #------------------------ Iteration 5886 --------------------------#
[32m[20221214 00:40:49 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:40:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:40:49 @agent_ppo2.py:185][0m |           0.0048 |         195.6015 |        -186.7869 |
[32m[20221214 00:40:49 @agent_ppo2.py:185][0m |           0.0142 |         213.5847 |        -188.1815 |
[32m[20221214 00:40:49 @agent_ppo2.py:185][0m |           0.0098 |         210.1782 |        -187.6121 |
[32m[20221214 00:40:49 @agent_ppo2.py:185][0m |           0.0085 |         212.1743 |        -188.6573 |
[32m[20221214 00:40:50 @agent_ppo2.py:185][0m |           0.0014 |         192.5036 |        -186.6395 |
[32m[20221214 00:40:50 @agent_ppo2.py:185][0m |          -0.0018 |         192.4037 |        -187.3268 |
[32m[20221214 00:40:50 @agent_ppo2.py:185][0m |          -0.0012 |         192.7425 |        -187.4001 |
[32m[20221214 00:40:50 @agent_ppo2.py:185][0m |          -0.0008 |         192.5849 |        -186.9138 |
[32m[20221214 00:40:50 @agent_ppo2.py:185][0m |          -0.0016 |         191.6765 |        -187.3295 |
[32m[20221214 00:40:50 @agent_ppo2.py:185][0m |          -0.0060 |         191.5302 |        -187.7314 |
[32m[20221214 00:40:50 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:40:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 749.90
[32m[20221214 00:40:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 792.64
[32m[20221214 00:40:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 838.87
[32m[20221214 00:40:50 @agent_ppo2.py:143][0m Total time:      43.33 min
[32m[20221214 00:40:50 @agent_ppo2.py:145][0m 3864576 total steps have happened
[32m[20221214 00:40:50 @agent_ppo2.py:121][0m #------------------------ Iteration 5887 --------------------------#
[32m[20221214 00:40:50 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:40:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:40:51 @agent_ppo2.py:185][0m |           0.0022 |         114.4811 |        -185.1115 |
[32m[20221214 00:40:51 @agent_ppo2.py:185][0m |          -0.0077 |         108.0494 |        -185.3208 |
[32m[20221214 00:40:51 @agent_ppo2.py:185][0m |          -0.0003 |         110.6619 |        -185.8797 |
[32m[20221214 00:40:51 @agent_ppo2.py:185][0m |          -0.0114 |         104.1387 |        -185.2753 |
[32m[20221214 00:40:51 @agent_ppo2.py:185][0m |          -0.0127 |         103.1052 |        -185.2788 |
[32m[20221214 00:40:51 @agent_ppo2.py:185][0m |          -0.0150 |         102.8011 |        -185.3807 |
[32m[20221214 00:40:51 @agent_ppo2.py:185][0m |          -0.0080 |         107.8949 |        -185.6657 |
[32m[20221214 00:40:51 @agent_ppo2.py:185][0m |          -0.0181 |         101.9074 |        -185.7352 |
[32m[20221214 00:40:51 @agent_ppo2.py:185][0m |          -0.0186 |         101.1615 |        -185.0540 |
[32m[20221214 00:40:51 @agent_ppo2.py:185][0m |          -0.0183 |          99.8357 |        -185.9675 |
[32m[20221214 00:40:51 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:40:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 675.29
[32m[20221214 00:40:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 728.84
[32m[20221214 00:40:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 752.25
[32m[20221214 00:40:52 @agent_ppo2.py:143][0m Total time:      43.35 min
[32m[20221214 00:40:52 @agent_ppo2.py:145][0m 3866624 total steps have happened
[32m[20221214 00:40:52 @agent_ppo2.py:121][0m #------------------------ Iteration 5888 --------------------------#
[32m[20221214 00:40:52 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:40:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:40:52 @agent_ppo2.py:185][0m |           0.0061 |         109.5666 |        -192.3166 |
[32m[20221214 00:40:52 @agent_ppo2.py:185][0m |           0.0016 |         102.1024 |        -192.6119 |
[32m[20221214 00:40:52 @agent_ppo2.py:185][0m |          -0.0022 |          99.5856 |        -192.6067 |
[32m[20221214 00:40:52 @agent_ppo2.py:185][0m |          -0.0086 |          98.8787 |        -192.4293 |
[32m[20221214 00:40:52 @agent_ppo2.py:185][0m |          -0.0055 |          97.6139 |        -193.3270 |
[32m[20221214 00:40:52 @agent_ppo2.py:185][0m |          -0.0088 |          95.4792 |        -193.4956 |
[32m[20221214 00:40:53 @agent_ppo2.py:185][0m |          -0.0150 |          94.5744 |        -193.1622 |
[32m[20221214 00:40:53 @agent_ppo2.py:185][0m |          -0.0050 |          93.9208 |        -194.0726 |
[32m[20221214 00:40:53 @agent_ppo2.py:185][0m |          -0.0071 |          92.8160 |        -193.5927 |
[32m[20221214 00:40:53 @agent_ppo2.py:185][0m |          -0.0165 |          92.5398 |        -194.2671 |
[32m[20221214 00:40:53 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:40:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 518.43
[32m[20221214 00:40:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 716.32
[32m[20221214 00:40:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 762.84
[32m[20221214 00:40:53 @agent_ppo2.py:143][0m Total time:      43.38 min
[32m[20221214 00:40:53 @agent_ppo2.py:145][0m 3868672 total steps have happened
[32m[20221214 00:40:53 @agent_ppo2.py:121][0m #------------------------ Iteration 5889 --------------------------#
[32m[20221214 00:40:53 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:40:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:40:53 @agent_ppo2.py:185][0m |           0.0028 |          92.9447 |        -184.9179 |
[32m[20221214 00:40:53 @agent_ppo2.py:185][0m |          -0.0023 |          80.6631 |        -183.9023 |
[32m[20221214 00:40:54 @agent_ppo2.py:185][0m |          -0.0094 |          74.9599 |        -184.2785 |
[32m[20221214 00:40:54 @agent_ppo2.py:185][0m |          -0.0128 |          71.4393 |        -184.1462 |
[32m[20221214 00:40:54 @agent_ppo2.py:185][0m |          -0.0176 |          68.3250 |        -184.7734 |
[32m[20221214 00:40:54 @agent_ppo2.py:185][0m |          -0.0052 |          70.7218 |        -184.7629 |
[32m[20221214 00:40:54 @agent_ppo2.py:185][0m |          -0.0162 |          66.1136 |        -184.5974 |
[32m[20221214 00:40:54 @agent_ppo2.py:185][0m |          -0.0206 |          64.8673 |        -184.8225 |
[32m[20221214 00:40:54 @agent_ppo2.py:185][0m |          -0.0247 |          64.1601 |        -184.3435 |
[32m[20221214 00:40:54 @agent_ppo2.py:185][0m |          -0.0252 |          63.0939 |        -185.3759 |
[32m[20221214 00:40:54 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:40:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 498.34
[32m[20221214 00:40:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 633.71
[32m[20221214 00:40:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 645.08
[32m[20221214 00:40:54 @agent_ppo2.py:143][0m Total time:      43.40 min
[32m[20221214 00:40:54 @agent_ppo2.py:145][0m 3870720 total steps have happened
[32m[20221214 00:40:54 @agent_ppo2.py:121][0m #------------------------ Iteration 5890 --------------------------#
[32m[20221214 00:40:55 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:40:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:40:55 @agent_ppo2.py:185][0m |           0.0019 |         181.7111 |        -191.7115 |
[32m[20221214 00:40:55 @agent_ppo2.py:185][0m |           0.0117 |         175.0520 |        -191.0824 |
[32m[20221214 00:40:55 @agent_ppo2.py:185][0m |           0.0158 |         189.4364 |        -192.6220 |
[32m[20221214 00:40:55 @agent_ppo2.py:185][0m |           0.0000 |         169.3178 |        -191.7287 |
[32m[20221214 00:40:55 @agent_ppo2.py:185][0m |           0.0013 |         168.0089 |        -192.6042 |
[32m[20221214 00:40:55 @agent_ppo2.py:185][0m |          -0.0044 |         166.7860 |        -193.2684 |
[32m[20221214 00:40:55 @agent_ppo2.py:185][0m |           0.0188 |         191.7888 |        -193.3219 |
[32m[20221214 00:40:56 @agent_ppo2.py:185][0m |          -0.0014 |         166.2185 |        -194.1387 |
[32m[20221214 00:40:56 @agent_ppo2.py:185][0m |           0.0009 |         165.4018 |        -194.1601 |
[32m[20221214 00:40:56 @agent_ppo2.py:185][0m |          -0.0029 |         164.6032 |        -194.6577 |
[32m[20221214 00:40:56 @agent_ppo2.py:130][0m Policy update time: 1.30 s
[32m[20221214 00:40:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 704.07
[32m[20221214 00:40:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 836.52
[32m[20221214 00:40:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 848.60
[32m[20221214 00:40:56 @agent_ppo2.py:143][0m Total time:      43.43 min
[32m[20221214 00:40:56 @agent_ppo2.py:145][0m 3872768 total steps have happened
[32m[20221214 00:40:56 @agent_ppo2.py:121][0m #------------------------ Iteration 5891 --------------------------#
[32m[20221214 00:40:56 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:40:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:40:56 @agent_ppo2.py:185][0m |           0.0016 |         104.9362 |        -199.1667 |
[32m[20221214 00:40:56 @agent_ppo2.py:185][0m |          -0.0074 |          94.2991 |        -197.6446 |
[32m[20221214 00:40:57 @agent_ppo2.py:185][0m |          -0.0119 |          92.5331 |        -196.9170 |
[32m[20221214 00:40:57 @agent_ppo2.py:185][0m |          -0.0154 |          91.3775 |        -196.9042 |
[32m[20221214 00:40:57 @agent_ppo2.py:185][0m |          -0.0136 |          90.9026 |        -196.8965 |
[32m[20221214 00:40:57 @agent_ppo2.py:185][0m |          -0.0138 |          90.0505 |        -196.5858 |
[32m[20221214 00:40:57 @agent_ppo2.py:185][0m |          -0.0139 |          89.4070 |        -196.2902 |
[32m[20221214 00:40:57 @agent_ppo2.py:185][0m |          -0.0137 |          88.5204 |        -195.9501 |
[32m[20221214 00:40:57 @agent_ppo2.py:185][0m |          -0.0118 |          91.8619 |        -195.2534 |
[32m[20221214 00:40:57 @agent_ppo2.py:185][0m |          -0.0172 |          88.2159 |        -195.7137 |
[32m[20221214 00:40:57 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:40:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 564.19
[32m[20221214 00:40:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 643.92
[32m[20221214 00:40:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 843.20
[32m[20221214 00:40:57 @agent_ppo2.py:143][0m Total time:      43.45 min
[32m[20221214 00:40:57 @agent_ppo2.py:145][0m 3874816 total steps have happened
[32m[20221214 00:40:57 @agent_ppo2.py:121][0m #------------------------ Iteration 5892 --------------------------#
[32m[20221214 00:40:58 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:40:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:40:58 @agent_ppo2.py:185][0m |           0.0035 |          94.5912 |        -199.8604 |
[32m[20221214 00:40:58 @agent_ppo2.py:185][0m |          -0.0004 |          90.1862 |        -199.2677 |
[32m[20221214 00:40:58 @agent_ppo2.py:185][0m |          -0.0098 |          87.7392 |        -198.8500 |
[32m[20221214 00:40:58 @agent_ppo2.py:185][0m |          -0.0015 |          88.0832 |        -200.3410 |
[32m[20221214 00:40:58 @agent_ppo2.py:185][0m |           0.0021 |          92.8843 |        -200.2721 |
[32m[20221214 00:40:58 @agent_ppo2.py:185][0m |           0.0012 |          94.1216 |        -198.8338 |
[32m[20221214 00:40:58 @agent_ppo2.py:185][0m |          -0.0017 |          91.3587 |        -199.1796 |
[32m[20221214 00:40:59 @agent_ppo2.py:185][0m |          -0.0057 |          84.2427 |        -198.2050 |
[32m[20221214 00:40:59 @agent_ppo2.py:185][0m |          -0.0069 |          84.5046 |        -198.6700 |
[32m[20221214 00:40:59 @agent_ppo2.py:185][0m |          -0.0056 |          84.4057 |        -198.2568 |
[32m[20221214 00:40:59 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:40:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 602.58
[32m[20221214 00:40:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 694.21
[32m[20221214 00:40:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 770.20
[32m[20221214 00:40:59 @agent_ppo2.py:143][0m Total time:      43.48 min
[32m[20221214 00:40:59 @agent_ppo2.py:145][0m 3876864 total steps have happened
[32m[20221214 00:40:59 @agent_ppo2.py:121][0m #------------------------ Iteration 5893 --------------------------#
[32m[20221214 00:40:59 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:40:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:40:59 @agent_ppo2.py:185][0m |           0.0011 |         142.0913 |        -198.0627 |
[32m[20221214 00:40:59 @agent_ppo2.py:185][0m |          -0.0052 |         115.0361 |        -196.8976 |
[32m[20221214 00:40:59 @agent_ppo2.py:185][0m |          -0.0117 |         107.1125 |        -196.7552 |
[32m[20221214 00:41:00 @agent_ppo2.py:185][0m |          -0.0149 |         102.2230 |        -196.9187 |
[32m[20221214 00:41:00 @agent_ppo2.py:185][0m |          -0.0184 |          99.3801 |        -196.6999 |
[32m[20221214 00:41:00 @agent_ppo2.py:185][0m |          -0.0121 |          97.4717 |        -196.5606 |
[32m[20221214 00:41:00 @agent_ppo2.py:185][0m |          -0.0187 |          94.4857 |        -196.2437 |
[32m[20221214 00:41:00 @agent_ppo2.py:185][0m |          -0.0144 |          92.9195 |        -196.0297 |
[32m[20221214 00:41:00 @agent_ppo2.py:185][0m |          -0.0187 |          90.9480 |        -195.9834 |
[32m[20221214 00:41:00 @agent_ppo2.py:185][0m |          -0.0127 |          90.6808 |        -195.6526 |
[32m[20221214 00:41:00 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:41:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 360.72
[32m[20221214 00:41:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 581.54
[32m[20221214 00:41:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 707.47
[32m[20221214 00:41:00 @agent_ppo2.py:143][0m Total time:      43.50 min
[32m[20221214 00:41:00 @agent_ppo2.py:145][0m 3878912 total steps have happened
[32m[20221214 00:41:00 @agent_ppo2.py:121][0m #------------------------ Iteration 5894 --------------------------#
[32m[20221214 00:41:01 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:41:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:41:01 @agent_ppo2.py:185][0m |           0.0040 |          90.0267 |        -195.3625 |
[32m[20221214 00:41:01 @agent_ppo2.py:185][0m |          -0.0077 |          76.5029 |        -194.9746 |
[32m[20221214 00:41:01 @agent_ppo2.py:185][0m |          -0.0091 |          72.1572 |        -195.6018 |
[32m[20221214 00:41:01 @agent_ppo2.py:185][0m |          -0.0057 |          69.4788 |        -195.6708 |
[32m[20221214 00:41:01 @agent_ppo2.py:185][0m |          -0.0099 |          67.9123 |        -195.7440 |
[32m[20221214 00:41:01 @agent_ppo2.py:185][0m |          -0.0028 |          71.3141 |        -195.8056 |
[32m[20221214 00:41:01 @agent_ppo2.py:185][0m |          -0.0077 |          65.9706 |        -196.6015 |
[32m[20221214 00:41:01 @agent_ppo2.py:185][0m |          -0.0132 |          64.8618 |        -196.6989 |
[32m[20221214 00:41:01 @agent_ppo2.py:185][0m |          -0.0135 |          64.5287 |        -195.8456 |
[32m[20221214 00:41:02 @agent_ppo2.py:185][0m |          -0.0140 |          63.4086 |        -196.4243 |
[32m[20221214 00:41:02 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:41:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 621.66
[32m[20221214 00:41:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 709.94
[32m[20221214 00:41:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 786.23
[32m[20221214 00:41:02 @agent_ppo2.py:143][0m Total time:      43.52 min
[32m[20221214 00:41:02 @agent_ppo2.py:145][0m 3880960 total steps have happened
[32m[20221214 00:41:02 @agent_ppo2.py:121][0m #------------------------ Iteration 5895 --------------------------#
[32m[20221214 00:41:02 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:41:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:41:02 @agent_ppo2.py:185][0m |          -0.0005 |          88.6513 |        -203.0653 |
[32m[20221214 00:41:02 @agent_ppo2.py:185][0m |          -0.0033 |          75.7309 |        -201.8616 |
[32m[20221214 00:41:02 @agent_ppo2.py:185][0m |          -0.0085 |          73.5287 |        -201.6434 |
[32m[20221214 00:41:02 @agent_ppo2.py:185][0m |          -0.0068 |          72.9977 |        -200.9754 |
[32m[20221214 00:41:03 @agent_ppo2.py:185][0m |          -0.0140 |          72.5610 |        -200.8040 |
[32m[20221214 00:41:03 @agent_ppo2.py:185][0m |          -0.0155 |          72.0343 |        -201.2458 |
[32m[20221214 00:41:03 @agent_ppo2.py:185][0m |          -0.0117 |          75.5313 |        -200.4601 |
[32m[20221214 00:41:03 @agent_ppo2.py:185][0m |          -0.0128 |          71.5737 |        -200.7190 |
[32m[20221214 00:41:03 @agent_ppo2.py:185][0m |          -0.0169 |          70.3738 |        -200.2226 |
[32m[20221214 00:41:03 @agent_ppo2.py:185][0m |          -0.0035 |          79.4324 |        -200.1461 |
[32m[20221214 00:41:03 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:41:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 681.43
[32m[20221214 00:41:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 824.06
[32m[20221214 00:41:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 516.31
[32m[20221214 00:41:03 @agent_ppo2.py:143][0m Total time:      43.55 min
[32m[20221214 00:41:03 @agent_ppo2.py:145][0m 3883008 total steps have happened
[32m[20221214 00:41:03 @agent_ppo2.py:121][0m #------------------------ Iteration 5896 --------------------------#
[32m[20221214 00:41:03 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:41:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:41:04 @agent_ppo2.py:185][0m |          -0.0036 |          69.9406 |        -209.7200 |
[32m[20221214 00:41:04 @agent_ppo2.py:185][0m |           0.0031 |          65.1434 |        -210.1519 |
[32m[20221214 00:41:04 @agent_ppo2.py:185][0m |          -0.0053 |          56.5426 |        -209.7739 |
[32m[20221214 00:41:04 @agent_ppo2.py:185][0m |          -0.0011 |          55.6859 |        -209.2274 |
[32m[20221214 00:41:04 @agent_ppo2.py:185][0m |          -0.0100 |          53.9922 |        -209.1297 |
[32m[20221214 00:41:04 @agent_ppo2.py:185][0m |          -0.0134 |          53.4063 |        -208.2666 |
[32m[20221214 00:41:04 @agent_ppo2.py:185][0m |          -0.0168 |          52.6546 |        -208.4872 |
[32m[20221214 00:41:04 @agent_ppo2.py:185][0m |          -0.0089 |          61.8926 |        -208.1024 |
[32m[20221214 00:41:04 @agent_ppo2.py:185][0m |          -0.0132 |          51.8234 |        -206.9272 |
[32m[20221214 00:41:04 @agent_ppo2.py:185][0m |          -0.0120 |          51.4342 |        -207.3064 |
[32m[20221214 00:41:04 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:41:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 431.99
[32m[20221214 00:41:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 837.48
[32m[20221214 00:41:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 543.49
[32m[20221214 00:41:05 @agent_ppo2.py:143][0m Total time:      43.57 min
[32m[20221214 00:41:05 @agent_ppo2.py:145][0m 3885056 total steps have happened
[32m[20221214 00:41:05 @agent_ppo2.py:121][0m #------------------------ Iteration 5897 --------------------------#
[32m[20221214 00:41:05 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:41:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:41:05 @agent_ppo2.py:185][0m |           0.0050 |          78.2996 |        -201.8516 |
[32m[20221214 00:41:05 @agent_ppo2.py:185][0m |           0.0036 |          71.9105 |        -200.5792 |
[32m[20221214 00:41:05 @agent_ppo2.py:185][0m |          -0.0083 |          69.6002 |        -201.2445 |
[32m[20221214 00:41:05 @agent_ppo2.py:185][0m |          -0.0069 |          68.1831 |        -201.8357 |
[32m[20221214 00:41:05 @agent_ppo2.py:185][0m |          -0.0124 |          67.1204 |        -201.8600 |
[32m[20221214 00:41:05 @agent_ppo2.py:185][0m |          -0.0106 |          66.4613 |        -200.8759 |
[32m[20221214 00:41:06 @agent_ppo2.py:185][0m |          -0.0028 |          65.8255 |        -199.1907 |
[32m[20221214 00:41:06 @agent_ppo2.py:185][0m |          -0.0149 |          65.7134 |        -200.7715 |
[32m[20221214 00:41:06 @agent_ppo2.py:185][0m |          -0.0114 |          65.3100 |        -201.2630 |
[32m[20221214 00:41:06 @agent_ppo2.py:185][0m |          -0.0146 |          65.0883 |        -200.9801 |
[32m[20221214 00:41:06 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:41:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 639.69
[32m[20221214 00:41:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 839.85
[32m[20221214 00:41:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 807.20
[32m[20221214 00:41:06 @agent_ppo2.py:143][0m Total time:      43.59 min
[32m[20221214 00:41:06 @agent_ppo2.py:145][0m 3887104 total steps have happened
[32m[20221214 00:41:06 @agent_ppo2.py:121][0m #------------------------ Iteration 5898 --------------------------#
[32m[20221214 00:41:06 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:41:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:41:06 @agent_ppo2.py:185][0m |          -0.0013 |         113.1665 |        -203.0473 |
[32m[20221214 00:41:06 @agent_ppo2.py:185][0m |          -0.0039 |         109.0782 |        -202.2252 |
[32m[20221214 00:41:06 @agent_ppo2.py:185][0m |          -0.0027 |         107.0777 |        -201.8045 |
[32m[20221214 00:41:07 @agent_ppo2.py:185][0m |          -0.0038 |         107.0508 |        -202.6185 |
[32m[20221214 00:41:07 @agent_ppo2.py:185][0m |          -0.0021 |         106.2424 |        -201.7860 |
[32m[20221214 00:41:07 @agent_ppo2.py:185][0m |          -0.0090 |         106.1019 |        -202.6747 |
[32m[20221214 00:41:07 @agent_ppo2.py:185][0m |           0.0016 |         109.5613 |        -201.4940 |
[32m[20221214 00:41:07 @agent_ppo2.py:185][0m |          -0.0020 |         108.0603 |        -201.6494 |
[32m[20221214 00:41:07 @agent_ppo2.py:185][0m |          -0.0110 |         106.2176 |        -200.8203 |
[32m[20221214 00:41:07 @agent_ppo2.py:185][0m |          -0.0119 |         105.9068 |        -201.8490 |
[32m[20221214 00:41:07 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:41:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 654.02
[32m[20221214 00:41:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 733.48
[32m[20221214 00:41:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 568.68
[32m[20221214 00:41:07 @agent_ppo2.py:143][0m Total time:      43.62 min
[32m[20221214 00:41:07 @agent_ppo2.py:145][0m 3889152 total steps have happened
[32m[20221214 00:41:07 @agent_ppo2.py:121][0m #------------------------ Iteration 5899 --------------------------#
[32m[20221214 00:41:08 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:41:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:41:08 @agent_ppo2.py:185][0m |           0.0010 |          89.8980 |        -205.7494 |
[32m[20221214 00:41:08 @agent_ppo2.py:185][0m |          -0.0096 |          80.0120 |        -204.9352 |
[32m[20221214 00:41:08 @agent_ppo2.py:185][0m |          -0.0088 |          77.0408 |        -205.2009 |
[32m[20221214 00:41:08 @agent_ppo2.py:185][0m |          -0.0038 |          76.2346 |        -204.7430 |
[32m[20221214 00:41:08 @agent_ppo2.py:185][0m |          -0.0103 |          74.5136 |        -203.8090 |
[32m[20221214 00:41:08 @agent_ppo2.py:185][0m |          -0.0078 |          72.9592 |        -204.9504 |
[32m[20221214 00:41:08 @agent_ppo2.py:185][0m |          -0.0178 |          73.0197 |        -204.2744 |
[32m[20221214 00:41:08 @agent_ppo2.py:185][0m |          -0.0191 |          71.7883 |        -203.5408 |
[32m[20221214 00:41:08 @agent_ppo2.py:185][0m |          -0.0135 |          71.1330 |        -203.9907 |
[32m[20221214 00:41:09 @agent_ppo2.py:185][0m |          -0.0143 |          70.7813 |        -204.7841 |
[32m[20221214 00:41:09 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:41:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 593.67
[32m[20221214 00:41:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 682.55
[32m[20221214 00:41:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 781.19
[32m[20221214 00:41:09 @agent_ppo2.py:143][0m Total time:      43.64 min
[32m[20221214 00:41:09 @agent_ppo2.py:145][0m 3891200 total steps have happened
[32m[20221214 00:41:09 @agent_ppo2.py:121][0m #------------------------ Iteration 5900 --------------------------#
[32m[20221214 00:41:09 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:41:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:41:09 @agent_ppo2.py:185][0m |           0.0057 |          68.3570 |        -201.1481 |
[32m[20221214 00:41:09 @agent_ppo2.py:185][0m |          -0.0048 |          57.3209 |        -200.1392 |
[32m[20221214 00:41:09 @agent_ppo2.py:185][0m |          -0.0023 |          54.6963 |        -201.5794 |
[32m[20221214 00:41:09 @agent_ppo2.py:185][0m |          -0.0130 |          52.4286 |        -200.6328 |
[32m[20221214 00:41:10 @agent_ppo2.py:185][0m |          -0.0105 |          51.5279 |        -200.5399 |
[32m[20221214 00:41:10 @agent_ppo2.py:185][0m |          -0.0158 |          49.7892 |        -200.2213 |
[32m[20221214 00:41:10 @agent_ppo2.py:185][0m |          -0.0128 |          49.6284 |        -200.5645 |
[32m[20221214 00:41:10 @agent_ppo2.py:185][0m |          -0.0135 |          48.8003 |        -199.9876 |
[32m[20221214 00:41:10 @agent_ppo2.py:185][0m |          -0.0131 |          48.4149 |        -199.4139 |
[32m[20221214 00:41:10 @agent_ppo2.py:185][0m |          -0.0184 |          47.4860 |        -199.7851 |
[32m[20221214 00:41:10 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:41:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 513.55
[32m[20221214 00:41:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 669.79
[32m[20221214 00:41:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 690.77
[32m[20221214 00:41:10 @agent_ppo2.py:143][0m Total time:      43.66 min
[32m[20221214 00:41:10 @agent_ppo2.py:145][0m 3893248 total steps have happened
[32m[20221214 00:41:10 @agent_ppo2.py:121][0m #------------------------ Iteration 5901 --------------------------#
[32m[20221214 00:41:10 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:41:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:41:11 @agent_ppo2.py:185][0m |           0.0077 |          85.9903 |        -209.2524 |
[32m[20221214 00:41:11 @agent_ppo2.py:185][0m |          -0.0005 |          78.3187 |        -208.4248 |
[32m[20221214 00:41:11 @agent_ppo2.py:185][0m |          -0.0061 |          77.1071 |        -208.3557 |
[32m[20221214 00:41:11 @agent_ppo2.py:185][0m |          -0.0022 |          76.3563 |        -208.1240 |
[32m[20221214 00:41:11 @agent_ppo2.py:185][0m |          -0.0098 |          73.2820 |        -208.0826 |
[32m[20221214 00:41:11 @agent_ppo2.py:185][0m |          -0.0122 |          73.7221 |        -207.8420 |
[32m[20221214 00:41:11 @agent_ppo2.py:185][0m |          -0.0139 |          71.8036 |        -207.0772 |
[32m[20221214 00:41:11 @agent_ppo2.py:185][0m |          -0.0109 |          72.2216 |        -206.8760 |
[32m[20221214 00:41:11 @agent_ppo2.py:185][0m |          -0.0154 |          70.3539 |        -205.9148 |
[32m[20221214 00:41:11 @agent_ppo2.py:185][0m |          -0.0063 |          69.9877 |        -205.5508 |
[32m[20221214 00:41:11 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:41:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 659.70
[32m[20221214 00:41:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 768.15
[32m[20221214 00:41:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 731.56
[32m[20221214 00:41:12 @agent_ppo2.py:143][0m Total time:      43.69 min
[32m[20221214 00:41:12 @agent_ppo2.py:145][0m 3895296 total steps have happened
[32m[20221214 00:41:12 @agent_ppo2.py:121][0m #------------------------ Iteration 5902 --------------------------#
[32m[20221214 00:41:12 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:41:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:41:12 @agent_ppo2.py:185][0m |           0.0001 |         115.3453 |        -210.5387 |
[32m[20221214 00:41:12 @agent_ppo2.py:185][0m |          -0.0013 |         111.7523 |        -208.8846 |
[32m[20221214 00:41:12 @agent_ppo2.py:185][0m |          -0.0017 |         111.6024 |        -208.4601 |
[32m[20221214 00:41:12 @agent_ppo2.py:185][0m |          -0.0024 |         109.9237 |        -207.4526 |
[32m[20221214 00:41:12 @agent_ppo2.py:185][0m |          -0.0028 |         108.8210 |        -208.4649 |
[32m[20221214 00:41:12 @agent_ppo2.py:185][0m |          -0.0073 |         109.3331 |        -207.9143 |
[32m[20221214 00:41:13 @agent_ppo2.py:185][0m |          -0.0005 |         110.6833 |        -207.0084 |
[32m[20221214 00:41:13 @agent_ppo2.py:185][0m |           0.0016 |         110.0938 |        -204.9722 |
[32m[20221214 00:41:13 @agent_ppo2.py:185][0m |          -0.0031 |         109.7332 |        -206.1415 |
[32m[20221214 00:41:13 @agent_ppo2.py:185][0m |           0.0015 |         113.6859 |        -205.6041 |
[32m[20221214 00:41:13 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:41:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 686.14
[32m[20221214 00:41:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 770.29
[32m[20221214 00:41:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 541.90
[32m[20221214 00:41:13 @agent_ppo2.py:143][0m Total time:      43.71 min
[32m[20221214 00:41:13 @agent_ppo2.py:145][0m 3897344 total steps have happened
[32m[20221214 00:41:13 @agent_ppo2.py:121][0m #------------------------ Iteration 5903 --------------------------#
[32m[20221214 00:41:13 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:41:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:41:13 @agent_ppo2.py:185][0m |           0.0146 |          92.0955 |        -201.7905 |
[32m[20221214 00:41:13 @agent_ppo2.py:185][0m |          -0.0027 |          79.0871 |        -202.6948 |
[32m[20221214 00:41:14 @agent_ppo2.py:185][0m |          -0.0068 |          75.8051 |        -201.9219 |
[32m[20221214 00:41:14 @agent_ppo2.py:185][0m |          -0.0108 |          74.8372 |        -201.9945 |
[32m[20221214 00:41:14 @agent_ppo2.py:185][0m |          -0.0043 |          73.8739 |        -202.1296 |
[32m[20221214 00:41:14 @agent_ppo2.py:185][0m |          -0.0071 |          71.5835 |        -202.4700 |
[32m[20221214 00:41:14 @agent_ppo2.py:185][0m |          -0.0120 |          71.3394 |        -201.7309 |
[32m[20221214 00:41:14 @agent_ppo2.py:185][0m |          -0.0146 |          70.7643 |        -201.6765 |
[32m[20221214 00:41:14 @agent_ppo2.py:185][0m |          -0.0142 |          70.2036 |        -201.5765 |
[32m[20221214 00:41:14 @agent_ppo2.py:185][0m |          -0.0170 |          70.1260 |        -201.3813 |
[32m[20221214 00:41:14 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:41:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 542.61
[32m[20221214 00:41:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 672.09
[32m[20221214 00:41:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 634.15
[32m[20221214 00:41:14 @agent_ppo2.py:143][0m Total time:      43.73 min
[32m[20221214 00:41:14 @agent_ppo2.py:145][0m 3899392 total steps have happened
[32m[20221214 00:41:14 @agent_ppo2.py:121][0m #------------------------ Iteration 5904 --------------------------#
[32m[20221214 00:41:15 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:41:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:41:15 @agent_ppo2.py:185][0m |           0.0061 |          98.9291 |        -202.0897 |
[32m[20221214 00:41:15 @agent_ppo2.py:185][0m |          -0.0033 |          87.6088 |        -201.5328 |
[32m[20221214 00:41:15 @agent_ppo2.py:185][0m |          -0.0051 |          86.4673 |        -201.2594 |
[32m[20221214 00:41:15 @agent_ppo2.py:185][0m |          -0.0073 |          86.1451 |        -201.2484 |
[32m[20221214 00:41:15 @agent_ppo2.py:185][0m |          -0.0111 |          84.6837 |        -200.3884 |
[32m[20221214 00:41:15 @agent_ppo2.py:185][0m |          -0.0085 |          84.0168 |        -201.4557 |
[32m[20221214 00:41:15 @agent_ppo2.py:185][0m |          -0.0051 |          83.1013 |        -201.4234 |
[32m[20221214 00:41:16 @agent_ppo2.py:185][0m |          -0.0113 |          84.1830 |        -200.5603 |
[32m[20221214 00:41:16 @agent_ppo2.py:185][0m |          -0.0111 |          82.0392 |        -201.2627 |
[32m[20221214 00:41:16 @agent_ppo2.py:185][0m |          -0.0090 |          81.7621 |        -200.1358 |
[32m[20221214 00:41:16 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:41:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 618.49
[32m[20221214 00:41:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 738.76
[32m[20221214 00:41:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 338.06
[32m[20221214 00:41:16 @agent_ppo2.py:143][0m Total time:      43.76 min
[32m[20221214 00:41:16 @agent_ppo2.py:145][0m 3901440 total steps have happened
[32m[20221214 00:41:16 @agent_ppo2.py:121][0m #------------------------ Iteration 5905 --------------------------#
[32m[20221214 00:41:16 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:41:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:41:16 @agent_ppo2.py:185][0m |           0.0035 |          51.2173 |        -204.7949 |
[32m[20221214 00:41:16 @agent_ppo2.py:185][0m |           0.0028 |          43.7539 |        -204.1030 |
[32m[20221214 00:41:16 @agent_ppo2.py:185][0m |          -0.0069 |          40.8378 |        -204.4215 |
[32m[20221214 00:41:17 @agent_ppo2.py:185][0m |          -0.0214 |          39.4349 |        -204.1274 |
[32m[20221214 00:41:17 @agent_ppo2.py:185][0m |          -0.0116 |          38.5022 |        -204.0840 |
[32m[20221214 00:41:17 @agent_ppo2.py:185][0m |          -0.0152 |          37.5581 |        -204.3128 |
[32m[20221214 00:41:17 @agent_ppo2.py:185][0m |          -0.0155 |          37.2012 |        -203.7639 |
[32m[20221214 00:41:17 @agent_ppo2.py:185][0m |          -0.0198 |          37.0413 |        -204.6960 |
[32m[20221214 00:41:17 @agent_ppo2.py:185][0m |          -0.0176 |          36.5853 |        -203.8908 |
[32m[20221214 00:41:17 @agent_ppo2.py:185][0m |          -0.0171 |          36.7413 |        -204.1690 |
[32m[20221214 00:41:17 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221214 00:41:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 637.75
[32m[20221214 00:41:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 722.37
[32m[20221214 00:41:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 794.16
[32m[20221214 00:41:17 @agent_ppo2.py:143][0m Total time:      43.78 min
[32m[20221214 00:41:17 @agent_ppo2.py:145][0m 3903488 total steps have happened
[32m[20221214 00:41:17 @agent_ppo2.py:121][0m #------------------------ Iteration 5906 --------------------------#
[32m[20221214 00:41:18 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:41:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:41:18 @agent_ppo2.py:185][0m |          -0.0013 |          82.8633 |        -207.9367 |
[32m[20221214 00:41:18 @agent_ppo2.py:185][0m |          -0.0047 |          78.2299 |        -207.7198 |
[32m[20221214 00:41:18 @agent_ppo2.py:185][0m |          -0.0088 |          75.3223 |        -207.7061 |
[32m[20221214 00:41:18 @agent_ppo2.py:185][0m |          -0.0109 |          75.0460 |        -206.7723 |
[32m[20221214 00:41:18 @agent_ppo2.py:185][0m |          -0.0132 |          74.0379 |        -207.7052 |
[32m[20221214 00:41:18 @agent_ppo2.py:185][0m |          -0.0119 |          74.6361 |        -207.7605 |
[32m[20221214 00:41:18 @agent_ppo2.py:185][0m |          -0.0131 |          73.0139 |        -208.4125 |
[32m[20221214 00:41:18 @agent_ppo2.py:185][0m |          -0.0021 |          73.9775 |        -207.1366 |
[32m[20221214 00:41:18 @agent_ppo2.py:185][0m |          -0.0113 |          72.0042 |        -207.9785 |
[32m[20221214 00:41:19 @agent_ppo2.py:185][0m |          -0.0164 |          73.0389 |        -207.6874 |
[32m[20221214 00:41:19 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:41:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 584.88
[32m[20221214 00:41:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 687.48
[32m[20221214 00:41:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 759.59
[32m[20221214 00:41:19 @agent_ppo2.py:143][0m Total time:      43.81 min
[32m[20221214 00:41:19 @agent_ppo2.py:145][0m 3905536 total steps have happened
[32m[20221214 00:41:19 @agent_ppo2.py:121][0m #------------------------ Iteration 5907 --------------------------#
[32m[20221214 00:41:19 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:41:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:41:19 @agent_ppo2.py:185][0m |           0.0004 |         102.7337 |        -207.4142 |
[32m[20221214 00:41:19 @agent_ppo2.py:185][0m |           0.0056 |         103.3931 |        -207.8955 |
[32m[20221214 00:41:19 @agent_ppo2.py:185][0m |          -0.0048 |          95.7180 |        -208.8809 |
[32m[20221214 00:41:19 @agent_ppo2.py:185][0m |          -0.0061 |          93.7789 |        -207.6673 |
[32m[20221214 00:41:20 @agent_ppo2.py:185][0m |          -0.0001 |          96.5857 |        -207.7569 |
[32m[20221214 00:41:20 @agent_ppo2.py:185][0m |          -0.0106 |          92.8015 |        -207.6434 |
[32m[20221214 00:41:20 @agent_ppo2.py:185][0m |          -0.0136 |          92.0517 |        -208.1270 |
[32m[20221214 00:41:20 @agent_ppo2.py:185][0m |          -0.0126 |          91.7489 |        -208.5287 |
[32m[20221214 00:41:20 @agent_ppo2.py:185][0m |          -0.0139 |          91.1466 |        -208.6535 |
[32m[20221214 00:41:20 @agent_ppo2.py:185][0m |           0.0027 |         107.3268 |        -208.7178 |
[32m[20221214 00:41:20 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:41:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 577.34
[32m[20221214 00:41:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 738.21
[32m[20221214 00:41:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 788.29
[32m[20221214 00:41:20 @agent_ppo2.py:143][0m Total time:      43.83 min
[32m[20221214 00:41:20 @agent_ppo2.py:145][0m 3907584 total steps have happened
[32m[20221214 00:41:20 @agent_ppo2.py:121][0m #------------------------ Iteration 5908 --------------------------#
[32m[20221214 00:41:20 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:41:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:41:21 @agent_ppo2.py:185][0m |           0.0081 |         161.8603 |        -209.3568 |
[32m[20221214 00:41:21 @agent_ppo2.py:185][0m |          -0.0043 |         145.6265 |        -208.6689 |
[32m[20221214 00:41:21 @agent_ppo2.py:185][0m |           0.0005 |         141.4418 |        -209.6935 |
[32m[20221214 00:41:21 @agent_ppo2.py:185][0m |          -0.0072 |         137.1356 |        -208.8348 |
[32m[20221214 00:41:21 @agent_ppo2.py:185][0m |           0.0009 |         141.4840 |        -208.9812 |
[32m[20221214 00:41:21 @agent_ppo2.py:185][0m |          -0.0086 |         134.1233 |        -209.9917 |
[32m[20221214 00:41:21 @agent_ppo2.py:185][0m |          -0.0105 |         131.9496 |        -210.2824 |
[32m[20221214 00:41:21 @agent_ppo2.py:185][0m |          -0.0042 |         141.1622 |        -210.5595 |
[32m[20221214 00:41:21 @agent_ppo2.py:185][0m |          -0.0067 |         132.0227 |        -210.8173 |
[32m[20221214 00:41:21 @agent_ppo2.py:185][0m |          -0.0148 |         130.6813 |        -210.8955 |
[32m[20221214 00:41:21 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:41:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 446.75
[32m[20221214 00:41:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 566.75
[32m[20221214 00:41:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 587.32
[32m[20221214 00:41:22 @agent_ppo2.py:143][0m Total time:      43.85 min
[32m[20221214 00:41:22 @agent_ppo2.py:145][0m 3909632 total steps have happened
[32m[20221214 00:41:22 @agent_ppo2.py:121][0m #------------------------ Iteration 5909 --------------------------#
[32m[20221214 00:41:22 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:41:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:41:22 @agent_ppo2.py:185][0m |           0.0038 |         115.3477 |        -215.0247 |
[32m[20221214 00:41:22 @agent_ppo2.py:185][0m |          -0.0047 |         101.8686 |        -214.8343 |
[32m[20221214 00:41:22 @agent_ppo2.py:185][0m |          -0.0088 |          96.3181 |        -213.6302 |
[32m[20221214 00:41:22 @agent_ppo2.py:185][0m |          -0.0041 |          95.1101 |        -214.6245 |
[32m[20221214 00:41:22 @agent_ppo2.py:185][0m |           0.0023 |          97.0183 |        -213.8873 |
[32m[20221214 00:41:22 @agent_ppo2.py:185][0m |          -0.0062 |          96.0809 |        -214.1789 |
[32m[20221214 00:41:23 @agent_ppo2.py:185][0m |          -0.0147 |          87.1522 |        -213.4067 |
[32m[20221214 00:41:23 @agent_ppo2.py:185][0m |          -0.0111 |          85.0890 |        -213.3575 |
[32m[20221214 00:41:23 @agent_ppo2.py:185][0m |          -0.0169 |          84.0699 |        -212.9700 |
[32m[20221214 00:41:23 @agent_ppo2.py:185][0m |          -0.0180 |          82.8175 |        -212.5412 |
[32m[20221214 00:41:23 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221214 00:41:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 542.94
[32m[20221214 00:41:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 614.82
[32m[20221214 00:41:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 816.67
[32m[20221214 00:41:23 @agent_ppo2.py:143][0m Total time:      43.88 min
[32m[20221214 00:41:23 @agent_ppo2.py:145][0m 3911680 total steps have happened
[32m[20221214 00:41:23 @agent_ppo2.py:121][0m #------------------------ Iteration 5910 --------------------------#
[32m[20221214 00:41:23 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:41:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:41:23 @agent_ppo2.py:185][0m |           0.0035 |         120.3523 |        -208.0470 |
[32m[20221214 00:41:24 @agent_ppo2.py:185][0m |          -0.0031 |         111.5196 |        -206.2062 |
[32m[20221214 00:41:24 @agent_ppo2.py:185][0m |          -0.0011 |         109.9305 |        -206.5745 |
[32m[20221214 00:41:24 @agent_ppo2.py:185][0m |          -0.0061 |         107.7304 |        -207.3941 |
[32m[20221214 00:41:24 @agent_ppo2.py:185][0m |          -0.0029 |         105.8914 |        -207.8090 |
[32m[20221214 00:41:24 @agent_ppo2.py:185][0m |           0.0022 |         113.7874 |        -207.2308 |
[32m[20221214 00:41:24 @agent_ppo2.py:185][0m |          -0.0032 |         105.0571 |        -207.8953 |
[32m[20221214 00:41:24 @agent_ppo2.py:185][0m |          -0.0065 |         104.6918 |        -206.8726 |
[32m[20221214 00:41:24 @agent_ppo2.py:185][0m |          -0.0089 |         103.5963 |        -207.1134 |
[32m[20221214 00:41:24 @agent_ppo2.py:185][0m |          -0.0120 |         103.5704 |        -208.4172 |
[32m[20221214 00:41:24 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 00:41:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 701.01
[32m[20221214 00:41:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 785.41
[32m[20221214 00:41:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 741.23
[32m[20221214 00:41:25 @agent_ppo2.py:143][0m Total time:      43.90 min
[32m[20221214 00:41:25 @agent_ppo2.py:145][0m 3913728 total steps have happened
[32m[20221214 00:41:25 @agent_ppo2.py:121][0m #------------------------ Iteration 5911 --------------------------#
[32m[20221214 00:41:25 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:41:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:41:25 @agent_ppo2.py:185][0m |           0.0071 |         122.0540 |        -212.0862 |
[32m[20221214 00:41:25 @agent_ppo2.py:185][0m |          -0.0019 |         111.4305 |        -211.3453 |
[32m[20221214 00:41:25 @agent_ppo2.py:185][0m |          -0.0094 |         108.3069 |        -210.8521 |
[32m[20221214 00:41:25 @agent_ppo2.py:185][0m |          -0.0108 |         105.4692 |        -210.7315 |
[32m[20221214 00:41:25 @agent_ppo2.py:185][0m |          -0.0128 |         104.4551 |        -210.6192 |
[32m[20221214 00:41:25 @agent_ppo2.py:185][0m |          -0.0183 |         100.6449 |        -211.4811 |
[32m[20221214 00:41:26 @agent_ppo2.py:185][0m |          -0.0146 |          99.5879 |        -210.0033 |
[32m[20221214 00:41:26 @agent_ppo2.py:185][0m |          -0.0151 |          98.5654 |        -209.6580 |
[32m[20221214 00:41:26 @agent_ppo2.py:185][0m |          -0.0163 |          97.1319 |        -209.4233 |
[32m[20221214 00:41:26 @agent_ppo2.py:185][0m |          -0.0205 |          95.8902 |        -209.5118 |
[32m[20221214 00:41:26 @agent_ppo2.py:130][0m Policy update time: 1.20 s
[32m[20221214 00:41:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 504.62
[32m[20221214 00:41:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 762.86
[32m[20221214 00:41:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 579.24
[32m[20221214 00:41:26 @agent_ppo2.py:143][0m Total time:      43.93 min
[32m[20221214 00:41:26 @agent_ppo2.py:145][0m 3915776 total steps have happened
[32m[20221214 00:41:26 @agent_ppo2.py:121][0m #------------------------ Iteration 5912 --------------------------#
[32m[20221214 00:41:26 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:41:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:41:26 @agent_ppo2.py:185][0m |          -0.0045 |         111.6292 |        -204.3082 |
[32m[20221214 00:41:27 @agent_ppo2.py:185][0m |          -0.0006 |         101.4732 |        -204.1604 |
[32m[20221214 00:41:27 @agent_ppo2.py:185][0m |          -0.0133 |          94.4565 |        -203.7718 |
[32m[20221214 00:41:27 @agent_ppo2.py:185][0m |          -0.0140 |          93.7885 |        -204.0439 |
[32m[20221214 00:41:27 @agent_ppo2.py:185][0m |          -0.0133 |          91.9922 |        -204.3151 |
[32m[20221214 00:41:27 @agent_ppo2.py:185][0m |          -0.0193 |          91.1555 |        -202.6828 |
[32m[20221214 00:41:27 @agent_ppo2.py:185][0m |          -0.0182 |          89.8244 |        -203.8105 |
[32m[20221214 00:41:27 @agent_ppo2.py:185][0m |          -0.0150 |          91.8504 |        -203.2850 |
[32m[20221214 00:41:27 @agent_ppo2.py:185][0m |          -0.0218 |          89.9715 |        -203.6320 |
[32m[20221214 00:41:27 @agent_ppo2.py:185][0m |          -0.0069 |          88.7780 |        -202.2554 |
[32m[20221214 00:41:27 @agent_ppo2.py:130][0m Policy update time: 1.16 s
[32m[20221214 00:41:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 691.13
[32m[20221214 00:41:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 794.66
[32m[20221214 00:41:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 684.36
[32m[20221214 00:41:28 @agent_ppo2.py:143][0m Total time:      43.95 min
[32m[20221214 00:41:28 @agent_ppo2.py:145][0m 3917824 total steps have happened
[32m[20221214 00:41:28 @agent_ppo2.py:121][0m #------------------------ Iteration 5913 --------------------------#
[32m[20221214 00:41:28 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:41:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:41:28 @agent_ppo2.py:185][0m |           0.0048 |         148.3280 |        -213.8137 |
[32m[20221214 00:41:28 @agent_ppo2.py:185][0m |          -0.0014 |         145.8201 |        -212.8218 |
[32m[20221214 00:41:28 @agent_ppo2.py:185][0m |          -0.0073 |         137.0604 |        -212.5972 |
[32m[20221214 00:41:28 @agent_ppo2.py:185][0m |          -0.0110 |         136.8662 |        -213.7963 |
[32m[20221214 00:41:28 @agent_ppo2.py:185][0m |          -0.0109 |         134.7345 |        -212.5628 |
[32m[20221214 00:41:28 @agent_ppo2.py:185][0m |          -0.0127 |         134.4925 |        -214.0354 |
[32m[20221214 00:41:29 @agent_ppo2.py:185][0m |          -0.0129 |         133.6126 |        -213.6074 |
[32m[20221214 00:41:29 @agent_ppo2.py:185][0m |          -0.0114 |         136.6331 |        -213.7589 |
[32m[20221214 00:41:29 @agent_ppo2.py:185][0m |          -0.0134 |         132.8642 |        -213.9656 |
[32m[20221214 00:41:29 @agent_ppo2.py:185][0m |          -0.0106 |         132.6087 |        -213.7035 |
[32m[20221214 00:41:29 @agent_ppo2.py:130][0m Policy update time: 1.18 s
[32m[20221214 00:41:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 611.27
[32m[20221214 00:41:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 798.65
[32m[20221214 00:41:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 639.52
[32m[20221214 00:41:29 @agent_ppo2.py:143][0m Total time:      43.98 min
[32m[20221214 00:41:29 @agent_ppo2.py:145][0m 3919872 total steps have happened
[32m[20221214 00:41:29 @agent_ppo2.py:121][0m #------------------------ Iteration 5914 --------------------------#
[32m[20221214 00:41:29 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:41:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:41:29 @agent_ppo2.py:185][0m |          -0.0007 |         165.6272 |        -211.3798 |
[32m[20221214 00:41:30 @agent_ppo2.py:185][0m |           0.0010 |         161.0925 |        -210.7870 |
[32m[20221214 00:41:30 @agent_ppo2.py:185][0m |          -0.0044 |         161.0495 |        -210.7554 |
[32m[20221214 00:41:30 @agent_ppo2.py:185][0m |          -0.0060 |         159.5919 |        -212.2377 |
[32m[20221214 00:41:30 @agent_ppo2.py:185][0m |          -0.0037 |         158.4854 |        -210.5587 |
[32m[20221214 00:41:30 @agent_ppo2.py:185][0m |          -0.0050 |         162.9867 |        -212.5155 |
[32m[20221214 00:41:30 @agent_ppo2.py:185][0m |           0.0003 |         158.3527 |        -211.8075 |
[32m[20221214 00:41:30 @agent_ppo2.py:185][0m |          -0.0008 |         159.0513 |        -212.4805 |
[32m[20221214 00:41:30 @agent_ppo2.py:185][0m |           0.0082 |         163.5100 |        -212.8397 |
[32m[20221214 00:41:30 @agent_ppo2.py:185][0m |          -0.0034 |         158.6441 |        -212.5318 |
[32m[20221214 00:41:30 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:41:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 813.52
[32m[20221214 00:41:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 829.78
[32m[20221214 00:41:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 684.45
[32m[20221214 00:41:31 @agent_ppo2.py:143][0m Total time:      44.00 min
[32m[20221214 00:41:31 @agent_ppo2.py:145][0m 3921920 total steps have happened
[32m[20221214 00:41:31 @agent_ppo2.py:121][0m #------------------------ Iteration 5915 --------------------------#
[32m[20221214 00:41:31 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:41:31 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:41:31 @agent_ppo2.py:185][0m |          -0.0010 |         140.8968 |        -213.7555 |
[32m[20221214 00:41:31 @agent_ppo2.py:185][0m |          -0.0011 |         131.5812 |        -213.7736 |
[32m[20221214 00:41:31 @agent_ppo2.py:185][0m |          -0.0062 |         127.6271 |        -213.5631 |
[32m[20221214 00:41:31 @agent_ppo2.py:185][0m |          -0.0095 |         126.4751 |        -213.6192 |
[32m[20221214 00:41:31 @agent_ppo2.py:185][0m |          -0.0103 |         123.5190 |        -213.9109 |
[32m[20221214 00:41:31 @agent_ppo2.py:185][0m |          -0.0021 |         139.3599 |        -213.1425 |
[32m[20221214 00:41:31 @agent_ppo2.py:185][0m |          -0.0120 |         122.8764 |        -212.8633 |
[32m[20221214 00:41:32 @agent_ppo2.py:185][0m |          -0.0191 |         120.8908 |        -212.9415 |
[32m[20221214 00:41:32 @agent_ppo2.py:185][0m |          -0.0169 |         121.6273 |        -212.6358 |
[32m[20221214 00:41:32 @agent_ppo2.py:185][0m |          -0.0143 |         120.7598 |        -213.5286 |
[32m[20221214 00:41:32 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:41:32 @agent_ppo2.py:138][0m Average TRAINING episode reward: 529.86
[32m[20221214 00:41:32 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 835.67
[32m[20221214 00:41:32 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 832.80
[32m[20221214 00:41:32 @agent_ppo2.py:143][0m Total time:      44.03 min
[32m[20221214 00:41:32 @agent_ppo2.py:145][0m 3923968 total steps have happened
[32m[20221214 00:41:32 @agent_ppo2.py:121][0m #------------------------ Iteration 5916 --------------------------#
[32m[20221214 00:41:32 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:41:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:41:32 @agent_ppo2.py:185][0m |           0.0044 |         165.2559 |        -218.7137 |
[32m[20221214 00:41:32 @agent_ppo2.py:185][0m |           0.0005 |         158.1992 |        -219.7529 |
[32m[20221214 00:41:32 @agent_ppo2.py:185][0m |          -0.0008 |         156.6200 |        -219.1621 |
[32m[20221214 00:41:33 @agent_ppo2.py:185][0m |          -0.0030 |         157.6149 |        -220.2434 |
[32m[20221214 00:41:33 @agent_ppo2.py:185][0m |          -0.0018 |         154.9300 |        -219.6926 |
[32m[20221214 00:41:33 @agent_ppo2.py:185][0m |          -0.0042 |         156.7501 |        -219.6942 |
[32m[20221214 00:41:33 @agent_ppo2.py:185][0m |           0.0061 |         174.2068 |        -220.3098 |
[32m[20221214 00:41:33 @agent_ppo2.py:185][0m |          -0.0021 |         156.9520 |        -218.6548 |
[32m[20221214 00:41:33 @agent_ppo2.py:185][0m |          -0.0042 |         153.1297 |        -219.6357 |
[32m[20221214 00:41:33 @agent_ppo2.py:185][0m |          -0.0034 |         153.5256 |        -218.3404 |
[32m[20221214 00:41:33 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:41:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 677.08
[32m[20221214 00:41:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 752.21
[32m[20221214 00:41:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 780.25
[32m[20221214 00:41:33 @agent_ppo2.py:143][0m Total time:      44.05 min
[32m[20221214 00:41:33 @agent_ppo2.py:145][0m 3926016 total steps have happened
[32m[20221214 00:41:33 @agent_ppo2.py:121][0m #------------------------ Iteration 5917 --------------------------#
[32m[20221214 00:41:34 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:41:34 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:41:34 @agent_ppo2.py:185][0m |           0.0027 |         135.1281 |        -210.1231 |
[32m[20221214 00:41:34 @agent_ppo2.py:185][0m |          -0.0056 |         126.6181 |        -209.6321 |
[32m[20221214 00:41:34 @agent_ppo2.py:185][0m |          -0.0072 |         123.5147 |        -209.3619 |
[32m[20221214 00:41:34 @agent_ppo2.py:185][0m |          -0.0142 |         122.1154 |        -209.5340 |
[32m[20221214 00:41:34 @agent_ppo2.py:185][0m |          -0.0130 |         120.8748 |        -208.7522 |
[32m[20221214 00:41:34 @agent_ppo2.py:185][0m |          -0.0146 |         119.7444 |        -208.1350 |
[32m[20221214 00:41:34 @agent_ppo2.py:185][0m |          -0.0146 |         118.6791 |        -208.4388 |
[32m[20221214 00:41:34 @agent_ppo2.py:185][0m |          -0.0141 |         118.7345 |        -208.8289 |
[32m[20221214 00:41:35 @agent_ppo2.py:185][0m |          -0.0150 |         117.0826 |        -208.1925 |
[32m[20221214 00:41:35 @agent_ppo2.py:185][0m |          -0.0181 |         116.9301 |        -207.3753 |
[32m[20221214 00:41:35 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:41:35 @agent_ppo2.py:138][0m Average TRAINING episode reward: 640.85
[32m[20221214 00:41:35 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 719.34
[32m[20221214 00:41:35 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 580.86
[32m[20221214 00:41:35 @agent_ppo2.py:143][0m Total time:      44.07 min
[32m[20221214 00:41:35 @agent_ppo2.py:145][0m 3928064 total steps have happened
[32m[20221214 00:41:35 @agent_ppo2.py:121][0m #------------------------ Iteration 5918 --------------------------#
[32m[20221214 00:41:35 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:41:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:41:35 @agent_ppo2.py:185][0m |           0.0033 |         159.5590 |        -211.2386 |
[32m[20221214 00:41:35 @agent_ppo2.py:185][0m |          -0.0050 |         148.6826 |        -211.7964 |
[32m[20221214 00:41:35 @agent_ppo2.py:185][0m |          -0.0005 |         149.7076 |        -211.0945 |
[32m[20221214 00:41:35 @agent_ppo2.py:185][0m |          -0.0113 |         142.2679 |        -210.4348 |
[32m[20221214 00:41:36 @agent_ppo2.py:185][0m |          -0.0094 |         140.7721 |        -211.1722 |
[32m[20221214 00:41:36 @agent_ppo2.py:185][0m |          -0.0126 |         140.0651 |        -211.7266 |
[32m[20221214 00:41:36 @agent_ppo2.py:185][0m |          -0.0078 |         140.1490 |        -211.1186 |
[32m[20221214 00:41:36 @agent_ppo2.py:185][0m |          -0.0104 |         138.5292 |        -211.3128 |
[32m[20221214 00:41:36 @agent_ppo2.py:185][0m |          -0.0109 |         136.7881 |        -211.1149 |
[32m[20221214 00:41:36 @agent_ppo2.py:185][0m |          -0.0108 |         136.3181 |        -211.4517 |
[32m[20221214 00:41:36 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:41:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 709.83
[32m[20221214 00:41:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 744.23
[32m[20221214 00:41:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 169.56
[32m[20221214 00:41:36 @agent_ppo2.py:143][0m Total time:      44.10 min
[32m[20221214 00:41:36 @agent_ppo2.py:145][0m 3930112 total steps have happened
[32m[20221214 00:41:36 @agent_ppo2.py:121][0m #------------------------ Iteration 5919 --------------------------#
[32m[20221214 00:41:36 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:41:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:41:37 @agent_ppo2.py:185][0m |           0.0027 |         112.6413 |        -209.5953 |
[32m[20221214 00:41:37 @agent_ppo2.py:185][0m |           0.0063 |         108.0540 |        -209.7723 |
[32m[20221214 00:41:37 @agent_ppo2.py:185][0m |          -0.0002 |         103.5557 |        -209.2359 |
[32m[20221214 00:41:37 @agent_ppo2.py:185][0m |          -0.0046 |         102.4200 |        -209.8067 |
[32m[20221214 00:41:37 @agent_ppo2.py:185][0m |          -0.0053 |         102.6782 |        -209.6038 |
[32m[20221214 00:41:37 @agent_ppo2.py:185][0m |          -0.0024 |         101.1660 |        -211.2968 |
[32m[20221214 00:41:37 @agent_ppo2.py:185][0m |          -0.0110 |         100.7465 |        -210.7584 |
[32m[20221214 00:41:37 @agent_ppo2.py:185][0m |           0.0043 |         105.2929 |        -209.6001 |
[32m[20221214 00:41:37 @agent_ppo2.py:185][0m |          -0.0093 |         100.5638 |        -211.1995 |
[32m[20221214 00:41:37 @agent_ppo2.py:185][0m |          -0.0111 |          98.6237 |        -210.6890 |
[32m[20221214 00:41:37 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:41:38 @agent_ppo2.py:138][0m Average TRAINING episode reward: 702.09
[32m[20221214 00:41:38 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 797.23
[32m[20221214 00:41:38 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 751.70
[32m[20221214 00:41:38 @agent_ppo2.py:143][0m Total time:      44.12 min
[32m[20221214 00:41:38 @agent_ppo2.py:145][0m 3932160 total steps have happened
[32m[20221214 00:41:38 @agent_ppo2.py:121][0m #------------------------ Iteration 5920 --------------------------#
[32m[20221214 00:41:38 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:41:38 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:41:38 @agent_ppo2.py:185][0m |           0.0037 |         137.1915 |        -223.5424 |
[32m[20221214 00:41:38 @agent_ppo2.py:185][0m |          -0.0040 |         131.5268 |        -224.9341 |
[32m[20221214 00:41:38 @agent_ppo2.py:185][0m |          -0.0010 |         129.0159 |        -222.1973 |
[32m[20221214 00:41:38 @agent_ppo2.py:185][0m |           0.0017 |         126.9572 |        -222.4380 |
[32m[20221214 00:41:38 @agent_ppo2.py:185][0m |          -0.0062 |         126.9761 |        -223.4545 |
[32m[20221214 00:41:38 @agent_ppo2.py:185][0m |          -0.0090 |         125.8992 |        -222.9221 |
[32m[20221214 00:41:39 @agent_ppo2.py:185][0m |          -0.0113 |         125.5896 |        -223.1397 |
[32m[20221214 00:41:39 @agent_ppo2.py:185][0m |          -0.0128 |         124.7061 |        -223.1046 |
[32m[20221214 00:41:39 @agent_ppo2.py:185][0m |          -0.0139 |         124.2530 |        -222.9146 |
[32m[20221214 00:41:39 @agent_ppo2.py:185][0m |          -0.0127 |         123.5642 |        -223.1573 |
[32m[20221214 00:41:39 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:41:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 625.61
[32m[20221214 00:41:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 745.14
[32m[20221214 00:41:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 782.02
[32m[20221214 00:41:39 @agent_ppo2.py:143][0m Total time:      44.14 min
[32m[20221214 00:41:39 @agent_ppo2.py:145][0m 3934208 total steps have happened
[32m[20221214 00:41:39 @agent_ppo2.py:121][0m #------------------------ Iteration 5921 --------------------------#
[32m[20221214 00:41:39 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:41:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:41:39 @agent_ppo2.py:185][0m |           0.0052 |         146.6991 |        -226.7767 |
[32m[20221214 00:41:39 @agent_ppo2.py:185][0m |          -0.0002 |         142.1715 |        -227.0869 |
[32m[20221214 00:41:40 @agent_ppo2.py:185][0m |          -0.0029 |         140.6781 |        -226.9563 |
[32m[20221214 00:41:40 @agent_ppo2.py:185][0m |          -0.0041 |         140.2788 |        -226.8204 |
[32m[20221214 00:41:40 @agent_ppo2.py:185][0m |          -0.0008 |         139.5266 |        -224.9025 |
[32m[20221214 00:41:40 @agent_ppo2.py:185][0m |          -0.0022 |         139.6258 |        -226.6218 |
[32m[20221214 00:41:40 @agent_ppo2.py:185][0m |           0.0033 |         147.4422 |        -227.1402 |
[32m[20221214 00:41:40 @agent_ppo2.py:185][0m |          -0.0065 |         139.1479 |        -227.4497 |
[32m[20221214 00:41:40 @agent_ppo2.py:185][0m |           0.0002 |         139.9618 |        -227.7629 |
[32m[20221214 00:41:40 @agent_ppo2.py:185][0m |          -0.0082 |         138.5744 |        -228.0441 |
[32m[20221214 00:41:40 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:41:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 745.47
[32m[20221214 00:41:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 781.90
[32m[20221214 00:41:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 754.74
[32m[20221214 00:41:40 @agent_ppo2.py:143][0m Total time:      44.17 min
[32m[20221214 00:41:40 @agent_ppo2.py:145][0m 3936256 total steps have happened
[32m[20221214 00:41:40 @agent_ppo2.py:121][0m #------------------------ Iteration 5922 --------------------------#
[32m[20221214 00:41:41 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:41:41 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:41:41 @agent_ppo2.py:185][0m |           0.0003 |         143.1744 |        -229.1708 |
[32m[20221214 00:41:41 @agent_ppo2.py:185][0m |          -0.0020 |         136.7480 |        -230.2430 |
[32m[20221214 00:41:41 @agent_ppo2.py:185][0m |           0.0148 |         154.9242 |        -227.8965 |
[32m[20221214 00:41:41 @agent_ppo2.py:185][0m |          -0.0059 |         137.5931 |        -230.9981 |
[32m[20221214 00:41:41 @agent_ppo2.py:185][0m |           0.0079 |         148.7696 |        -230.0794 |
[32m[20221214 00:41:41 @agent_ppo2.py:185][0m |           0.0093 |         147.6739 |        -230.0674 |
[32m[20221214 00:41:41 @agent_ppo2.py:185][0m |           0.0083 |         140.4179 |        -230.0212 |
[32m[20221214 00:41:41 @agent_ppo2.py:185][0m |          -0.0048 |         135.6846 |        -229.2252 |
[32m[20221214 00:41:42 @agent_ppo2.py:185][0m |          -0.0033 |         135.0069 |        -229.3406 |
[32m[20221214 00:41:42 @agent_ppo2.py:185][0m |          -0.0019 |         134.4673 |        -228.8730 |
[32m[20221214 00:41:42 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:41:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 779.89
[32m[20221214 00:41:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 822.56
[32m[20221214 00:41:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 814.61
[32m[20221214 00:41:42 @agent_ppo2.py:143][0m Total time:      44.19 min
[32m[20221214 00:41:42 @agent_ppo2.py:145][0m 3938304 total steps have happened
[32m[20221214 00:41:42 @agent_ppo2.py:121][0m #------------------------ Iteration 5923 --------------------------#
[32m[20221214 00:41:42 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:41:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:41:42 @agent_ppo2.py:185][0m |          -0.0021 |         121.2476 |        -233.0174 |
[32m[20221214 00:41:42 @agent_ppo2.py:185][0m |          -0.0088 |         103.0395 |        -232.5786 |
[32m[20221214 00:41:42 @agent_ppo2.py:185][0m |          -0.0130 |          96.8084 |        -232.3206 |
[32m[20221214 00:41:42 @agent_ppo2.py:185][0m |          -0.0137 |          92.5772 |        -232.6331 |
[32m[20221214 00:41:43 @agent_ppo2.py:185][0m |          -0.0118 |          89.5860 |        -232.3604 |
[32m[20221214 00:41:43 @agent_ppo2.py:185][0m |          -0.0200 |          88.4620 |        -232.1560 |
[32m[20221214 00:41:43 @agent_ppo2.py:185][0m |          -0.0182 |          89.3765 |        -231.2101 |
[32m[20221214 00:41:43 @agent_ppo2.py:185][0m |          -0.0156 |          84.3264 |        -230.8008 |
[32m[20221214 00:41:43 @agent_ppo2.py:185][0m |          -0.0215 |          83.8007 |        -231.2194 |
[32m[20221214 00:41:43 @agent_ppo2.py:185][0m |          -0.0110 |          86.1907 |        -230.1679 |
[32m[20221214 00:41:43 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:41:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 573.97
[32m[20221214 00:41:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 715.76
[32m[20221214 00:41:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 722.97
[32m[20221214 00:41:43 @agent_ppo2.py:143][0m Total time:      44.21 min
[32m[20221214 00:41:43 @agent_ppo2.py:145][0m 3940352 total steps have happened
[32m[20221214 00:41:43 @agent_ppo2.py:121][0m #------------------------ Iteration 5924 --------------------------#
[32m[20221214 00:41:43 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:41:44 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:41:44 @agent_ppo2.py:185][0m |           0.0043 |          28.6060 |        -225.1453 |
[32m[20221214 00:41:44 @agent_ppo2.py:185][0m |           0.0013 |          22.0279 |        -224.7381 |
[32m[20221214 00:41:44 @agent_ppo2.py:185][0m |          -0.0054 |          20.6131 |        -224.4206 |
[32m[20221214 00:41:44 @agent_ppo2.py:185][0m |          -0.0035 |          20.3193 |        -223.9687 |
[32m[20221214 00:41:44 @agent_ppo2.py:185][0m |          -0.0066 |          20.0598 |        -223.2365 |
[32m[20221214 00:41:44 @agent_ppo2.py:185][0m |          -0.0024 |          19.9402 |        -223.8735 |
[32m[20221214 00:41:44 @agent_ppo2.py:185][0m |          -0.0030 |          19.8785 |        -222.2889 |
[32m[20221214 00:41:44 @agent_ppo2.py:185][0m |          -0.0079 |          19.9209 |        -221.9285 |
[32m[20221214 00:41:44 @agent_ppo2.py:185][0m |           0.0006 |          19.7383 |        -221.5865 |
[32m[20221214 00:41:45 @agent_ppo2.py:185][0m |          -0.0028 |          19.7591 |        -221.4811 |
[32m[20221214 00:41:45 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:41:45 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221214 00:41:45 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221214 00:41:45 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 757.05
[32m[20221214 00:41:45 @agent_ppo2.py:143][0m Total time:      44.24 min
[32m[20221214 00:41:45 @agent_ppo2.py:145][0m 3942400 total steps have happened
[32m[20221214 00:41:45 @agent_ppo2.py:121][0m #------------------------ Iteration 5925 --------------------------#
[32m[20221214 00:41:45 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:41:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:41:45 @agent_ppo2.py:185][0m |           0.0015 |         138.9361 |        -215.1807 |
[32m[20221214 00:41:45 @agent_ppo2.py:185][0m |          -0.0009 |         118.4588 |        -215.7458 |
[32m[20221214 00:41:45 @agent_ppo2.py:185][0m |          -0.0144 |         112.3381 |        -215.2408 |
[32m[20221214 00:41:45 @agent_ppo2.py:185][0m |          -0.0083 |         108.7509 |        -214.6528 |
[32m[20221214 00:41:45 @agent_ppo2.py:185][0m |          -0.0138 |         106.6361 |        -214.2346 |
[32m[20221214 00:41:46 @agent_ppo2.py:185][0m |          -0.0089 |         106.5765 |        -214.1239 |
[32m[20221214 00:41:46 @agent_ppo2.py:185][0m |          -0.0129 |         104.4360 |        -214.1281 |
[32m[20221214 00:41:46 @agent_ppo2.py:185][0m |          -0.0180 |         102.4285 |        -214.5354 |
[32m[20221214 00:41:46 @agent_ppo2.py:185][0m |          -0.0154 |         100.4944 |        -213.9275 |
[32m[20221214 00:41:46 @agent_ppo2.py:185][0m |          -0.0189 |          99.7155 |        -213.7130 |
[32m[20221214 00:41:46 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:41:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 541.12
[32m[20221214 00:41:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 675.06
[32m[20221214 00:41:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 833.88
[32m[20221214 00:41:46 @agent_ppo2.py:143][0m Total time:      44.26 min
[32m[20221214 00:41:46 @agent_ppo2.py:145][0m 3944448 total steps have happened
[32m[20221214 00:41:46 @agent_ppo2.py:121][0m #------------------------ Iteration 5926 --------------------------#
[32m[20221214 00:41:46 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:41:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:41:46 @agent_ppo2.py:185][0m |           0.0033 |          56.3516 |        -218.6857 |
[32m[20221214 00:41:47 @agent_ppo2.py:185][0m |          -0.0028 |          45.8156 |        -219.3663 |
[32m[20221214 00:41:47 @agent_ppo2.py:185][0m |          -0.0090 |          42.9569 |        -219.2242 |
[32m[20221214 00:41:47 @agent_ppo2.py:185][0m |          -0.0142 |          41.9190 |        -217.7858 |
[32m[20221214 00:41:47 @agent_ppo2.py:185][0m |          -0.0099 |          40.9201 |        -218.6796 |
[32m[20221214 00:41:47 @agent_ppo2.py:185][0m |          -0.0201 |          39.8874 |        -218.4479 |
[32m[20221214 00:41:47 @agent_ppo2.py:185][0m |          -0.0179 |          39.3182 |        -217.9854 |
[32m[20221214 00:41:47 @agent_ppo2.py:185][0m |          -0.0214 |          38.6159 |        -219.2912 |
[32m[20221214 00:41:47 @agent_ppo2.py:185][0m |          -0.0164 |          38.4525 |        -219.2238 |
[32m[20221214 00:41:47 @agent_ppo2.py:185][0m |          -0.0242 |          38.0975 |        -218.5037 |
[32m[20221214 00:41:47 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:41:48 @agent_ppo2.py:138][0m Average TRAINING episode reward: 619.44
[32m[20221214 00:41:48 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 831.01
[32m[20221214 00:41:48 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 829.81
[32m[20221214 00:41:48 @agent_ppo2.py:143][0m Total time:      44.29 min
[32m[20221214 00:41:48 @agent_ppo2.py:145][0m 3946496 total steps have happened
[32m[20221214 00:41:48 @agent_ppo2.py:121][0m #------------------------ Iteration 5927 --------------------------#
[32m[20221214 00:41:48 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:41:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:41:48 @agent_ppo2.py:185][0m |          -0.0020 |         102.0975 |        -231.4432 |
[32m[20221214 00:41:48 @agent_ppo2.py:185][0m |           0.0005 |          78.9005 |        -230.2218 |
[32m[20221214 00:41:48 @agent_ppo2.py:185][0m |          -0.0058 |          68.2218 |        -230.4869 |
[32m[20221214 00:41:48 @agent_ppo2.py:185][0m |          -0.0094 |          64.2567 |        -230.4363 |
[32m[20221214 00:41:48 @agent_ppo2.py:185][0m |          -0.0117 |          62.2921 |        -230.4681 |
[32m[20221214 00:41:48 @agent_ppo2.py:185][0m |          -0.0060 |          62.1895 |        -229.7092 |
[32m[20221214 00:41:48 @agent_ppo2.py:185][0m |          -0.0122 |          59.3932 |        -230.5757 |
[32m[20221214 00:41:49 @agent_ppo2.py:185][0m |          -0.0111 |          57.6460 |        -230.4821 |
[32m[20221214 00:41:49 @agent_ppo2.py:185][0m |          -0.0160 |          56.3586 |        -229.7755 |
[32m[20221214 00:41:49 @agent_ppo2.py:185][0m |          -0.0164 |          55.3922 |        -229.5901 |
[32m[20221214 00:41:49 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:41:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 617.85
[32m[20221214 00:41:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 705.86
[32m[20221214 00:41:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 849.17
[32m[20221214 00:41:49 @agent_ppo2.py:143][0m Total time:      44.31 min
[32m[20221214 00:41:49 @agent_ppo2.py:145][0m 3948544 total steps have happened
[32m[20221214 00:41:49 @agent_ppo2.py:121][0m #------------------------ Iteration 5928 --------------------------#
[32m[20221214 00:41:49 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:41:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:41:49 @agent_ppo2.py:185][0m |           0.0036 |         168.1274 |        -228.3047 |
[32m[20221214 00:41:49 @agent_ppo2.py:185][0m |           0.0079 |         166.1598 |        -227.9247 |
[32m[20221214 00:41:50 @agent_ppo2.py:185][0m |           0.0118 |         177.6866 |        -229.2097 |
[32m[20221214 00:41:50 @agent_ppo2.py:185][0m |          -0.0039 |         160.0162 |        -229.3971 |
[32m[20221214 00:41:50 @agent_ppo2.py:185][0m |          -0.0081 |         156.9019 |        -230.3013 |
[32m[20221214 00:41:50 @agent_ppo2.py:185][0m |          -0.0065 |         155.4743 |        -230.0836 |
[32m[20221214 00:41:50 @agent_ppo2.py:185][0m |          -0.0075 |         156.1381 |        -229.6410 |
[32m[20221214 00:41:50 @agent_ppo2.py:185][0m |          -0.0065 |         156.6656 |        -229.6857 |
[32m[20221214 00:41:50 @agent_ppo2.py:185][0m |          -0.0056 |         153.6136 |        -228.7455 |
[32m[20221214 00:41:50 @agent_ppo2.py:185][0m |          -0.0053 |         155.3005 |        -230.1553 |
[32m[20221214 00:41:50 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:41:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 617.27
[32m[20221214 00:41:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 754.58
[32m[20221214 00:41:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 0.00
[32m[20221214 00:41:50 @agent_ppo2.py:143][0m Total time:      44.33 min
[32m[20221214 00:41:50 @agent_ppo2.py:145][0m 3950592 total steps have happened
[32m[20221214 00:41:50 @agent_ppo2.py:121][0m #------------------------ Iteration 5929 --------------------------#
[32m[20221214 00:41:51 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:41:51 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:41:51 @agent_ppo2.py:185][0m |          -0.0012 |         179.3998 |        -228.1869 |
[32m[20221214 00:41:51 @agent_ppo2.py:185][0m |          -0.0062 |         175.2675 |        -227.4982 |
[32m[20221214 00:41:51 @agent_ppo2.py:185][0m |           0.0041 |         176.7153 |        -227.9517 |
[32m[20221214 00:41:51 @agent_ppo2.py:185][0m |          -0.0050 |         172.0079 |        -227.1819 |
[32m[20221214 00:41:51 @agent_ppo2.py:185][0m |          -0.0060 |         170.3284 |        -227.1519 |
[32m[20221214 00:41:51 @agent_ppo2.py:185][0m |           0.0063 |         202.8220 |        -227.1161 |
[32m[20221214 00:41:51 @agent_ppo2.py:185][0m |          -0.0110 |         169.9988 |        -227.1244 |
[32m[20221214 00:41:51 @agent_ppo2.py:185][0m |          -0.0021 |         168.1921 |        -226.9187 |
[32m[20221214 00:41:52 @agent_ppo2.py:185][0m |          -0.0105 |         168.0989 |        -228.3184 |
[32m[20221214 00:41:52 @agent_ppo2.py:185][0m |          -0.0131 |         169.1259 |        -227.9487 |
[32m[20221214 00:41:52 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:41:52 @agent_ppo2.py:138][0m Average TRAINING episode reward: 507.48
[32m[20221214 00:41:52 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 673.12
[32m[20221214 00:41:52 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 612.80
[32m[20221214 00:41:52 @agent_ppo2.py:143][0m Total time:      44.36 min
[32m[20221214 00:41:52 @agent_ppo2.py:145][0m 3952640 total steps have happened
[32m[20221214 00:41:52 @agent_ppo2.py:121][0m #------------------------ Iteration 5930 --------------------------#
[32m[20221214 00:41:52 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:41:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:41:52 @agent_ppo2.py:185][0m |           0.0016 |         220.0577 |        -228.4708 |
[32m[20221214 00:41:52 @agent_ppo2.py:185][0m |          -0.0025 |         217.8630 |        -229.2357 |
[32m[20221214 00:41:52 @agent_ppo2.py:185][0m |           0.0044 |         220.9089 |        -226.8431 |
[32m[20221214 00:41:52 @agent_ppo2.py:185][0m |          -0.0047 |         216.3778 |        -226.9198 |
[32m[20221214 00:41:53 @agent_ppo2.py:185][0m |           0.0025 |         216.7687 |        -227.4885 |
[32m[20221214 00:41:53 @agent_ppo2.py:185][0m |          -0.0018 |         215.6217 |        -228.2130 |
[32m[20221214 00:41:53 @agent_ppo2.py:185][0m |          -0.0054 |         215.3684 |        -228.3376 |
[32m[20221214 00:41:53 @agent_ppo2.py:185][0m |          -0.0031 |         215.1650 |        -228.1750 |
[32m[20221214 00:41:53 @agent_ppo2.py:185][0m |          -0.0044 |         214.8833 |        -228.3425 |
[32m[20221214 00:41:53 @agent_ppo2.py:185][0m |          -0.0017 |         217.2094 |        -229.2087 |
[32m[20221214 00:41:53 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:41:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 659.99
[32m[20221214 00:41:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 764.03
[32m[20221214 00:41:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 439.45
[32m[20221214 00:41:53 @agent_ppo2.py:143][0m Total time:      44.38 min
[32m[20221214 00:41:53 @agent_ppo2.py:145][0m 3954688 total steps have happened
[32m[20221214 00:41:53 @agent_ppo2.py:121][0m #------------------------ Iteration 5931 --------------------------#
[32m[20221214 00:41:53 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:41:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:41:54 @agent_ppo2.py:185][0m |          -0.0003 |         210.3205 |        -230.4432 |
[32m[20221214 00:41:54 @agent_ppo2.py:185][0m |          -0.0032 |         208.2644 |        -230.5017 |
[32m[20221214 00:41:54 @agent_ppo2.py:185][0m |          -0.0044 |         207.4895 |        -230.2763 |
[32m[20221214 00:41:54 @agent_ppo2.py:185][0m |          -0.0017 |         207.2800 |        -231.1591 |
[32m[20221214 00:41:54 @agent_ppo2.py:185][0m |          -0.0013 |         206.5541 |        -231.0799 |
[32m[20221214 00:41:54 @agent_ppo2.py:185][0m |          -0.0019 |         206.5175 |        -232.0082 |
[32m[20221214 00:41:54 @agent_ppo2.py:185][0m |          -0.0030 |         206.3170 |        -232.5875 |
[32m[20221214 00:41:54 @agent_ppo2.py:185][0m |          -0.0024 |         206.1556 |        -232.6250 |
[32m[20221214 00:41:54 @agent_ppo2.py:185][0m |          -0.0041 |         205.3903 |        -233.5520 |
[32m[20221214 00:41:54 @agent_ppo2.py:185][0m |          -0.0037 |         208.0799 |        -233.0549 |
[32m[20221214 00:41:54 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:41:55 @agent_ppo2.py:138][0m Average TRAINING episode reward: 766.16
[32m[20221214 00:41:55 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 847.99
[32m[20221214 00:41:55 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 694.92
[32m[20221214 00:41:55 @agent_ppo2.py:143][0m Total time:      44.40 min
[32m[20221214 00:41:55 @agent_ppo2.py:145][0m 3956736 total steps have happened
[32m[20221214 00:41:55 @agent_ppo2.py:121][0m #------------------------ Iteration 5932 --------------------------#
[32m[20221214 00:41:55 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:41:55 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:41:55 @agent_ppo2.py:185][0m |           0.0010 |         206.5306 |        -231.4125 |
[32m[20221214 00:41:55 @agent_ppo2.py:185][0m |           0.0083 |         205.8202 |        -231.4942 |
[32m[20221214 00:41:55 @agent_ppo2.py:185][0m |           0.0048 |         210.1430 |        -232.0544 |
[32m[20221214 00:41:55 @agent_ppo2.py:185][0m |          -0.0019 |         204.5238 |        -231.6885 |
[32m[20221214 00:41:55 @agent_ppo2.py:185][0m |          -0.0017 |         204.3023 |        -230.9221 |
[32m[20221214 00:41:55 @agent_ppo2.py:185][0m |           0.0010 |         204.6431 |        -231.8062 |
[32m[20221214 00:41:56 @agent_ppo2.py:185][0m |          -0.0000 |         203.8411 |        -230.0437 |
[32m[20221214 00:41:56 @agent_ppo2.py:185][0m |          -0.0050 |         204.3485 |        -231.9671 |
[32m[20221214 00:41:56 @agent_ppo2.py:185][0m |           0.0061 |         209.9463 |        -231.2780 |
[32m[20221214 00:41:56 @agent_ppo2.py:185][0m |           0.0021 |         207.7186 |        -230.9403 |
[32m[20221214 00:41:56 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:41:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 797.38
[32m[20221214 00:41:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 836.98
[32m[20221214 00:41:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 727.88
[32m[20221214 00:41:56 @agent_ppo2.py:143][0m Total time:      44.43 min
[32m[20221214 00:41:56 @agent_ppo2.py:145][0m 3958784 total steps have happened
[32m[20221214 00:41:56 @agent_ppo2.py:121][0m #------------------------ Iteration 5933 --------------------------#
[32m[20221214 00:41:56 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:41:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:41:56 @agent_ppo2.py:185][0m |           0.0024 |         198.8491 |        -228.1195 |
[32m[20221214 00:41:56 @agent_ppo2.py:185][0m |          -0.0004 |         196.2903 |        -227.7055 |
[32m[20221214 00:41:57 @agent_ppo2.py:185][0m |          -0.0031 |         195.5958 |        -227.0341 |
[32m[20221214 00:41:57 @agent_ppo2.py:185][0m |          -0.0003 |         195.3224 |        -227.3801 |
[32m[20221214 00:41:57 @agent_ppo2.py:185][0m |          -0.0004 |         194.4545 |        -228.1791 |
[32m[20221214 00:41:57 @agent_ppo2.py:185][0m |           0.0088 |         206.5310 |        -227.8973 |
[32m[20221214 00:41:57 @agent_ppo2.py:185][0m |          -0.0048 |         194.0921 |        -227.0937 |
[32m[20221214 00:41:57 @agent_ppo2.py:185][0m |           0.0202 |         217.2804 |        -226.6915 |
[32m[20221214 00:41:57 @agent_ppo2.py:185][0m |          -0.0016 |         194.5498 |        -224.4729 |
[32m[20221214 00:41:57 @agent_ppo2.py:185][0m |           0.0015 |         193.0343 |        -226.1673 |
[32m[20221214 00:41:57 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:41:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 774.71
[32m[20221214 00:41:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 807.68
[32m[20221214 00:41:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 806.29
[32m[20221214 00:41:57 @agent_ppo2.py:143][0m Total time:      44.45 min
[32m[20221214 00:41:57 @agent_ppo2.py:145][0m 3960832 total steps have happened
[32m[20221214 00:41:57 @agent_ppo2.py:121][0m #------------------------ Iteration 5934 --------------------------#
[32m[20221214 00:41:58 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:41:58 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:41:58 @agent_ppo2.py:185][0m |          -0.0045 |         126.2125 |        -225.2342 |
[32m[20221214 00:41:58 @agent_ppo2.py:185][0m |          -0.0037 |         119.8046 |        -224.5569 |
[32m[20221214 00:41:58 @agent_ppo2.py:185][0m |          -0.0086 |         116.2377 |        -224.0895 |
[32m[20221214 00:41:58 @agent_ppo2.py:185][0m |          -0.0061 |         114.6834 |        -222.7675 |
[32m[20221214 00:41:58 @agent_ppo2.py:185][0m |          -0.0114 |         113.4593 |        -223.3202 |
[32m[20221214 00:41:58 @agent_ppo2.py:185][0m |          -0.0098 |         113.3794 |        -222.9482 |
[32m[20221214 00:41:58 @agent_ppo2.py:185][0m |          -0.0094 |         112.5236 |        -222.6595 |
[32m[20221214 00:41:59 @agent_ppo2.py:185][0m |          -0.0105 |         111.5632 |        -222.8506 |
[32m[20221214 00:41:59 @agent_ppo2.py:185][0m |          -0.0152 |         111.5753 |        -222.6582 |
[32m[20221214 00:41:59 @agent_ppo2.py:185][0m |          -0.0123 |         110.9584 |        -222.8063 |
[32m[20221214 00:41:59 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:41:59 @agent_ppo2.py:138][0m Average TRAINING episode reward: 654.34
[32m[20221214 00:41:59 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 748.65
[32m[20221214 00:41:59 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 691.19
[32m[20221214 00:41:59 @agent_ppo2.py:143][0m Total time:      44.48 min
[32m[20221214 00:41:59 @agent_ppo2.py:145][0m 3962880 total steps have happened
[32m[20221214 00:41:59 @agent_ppo2.py:121][0m #------------------------ Iteration 5935 --------------------------#
[32m[20221214 00:41:59 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:41:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:41:59 @agent_ppo2.py:185][0m |           0.0064 |         201.7873 |        -227.5698 |
[32m[20221214 00:41:59 @agent_ppo2.py:185][0m |           0.0028 |         191.3180 |        -227.4123 |
[32m[20221214 00:41:59 @agent_ppo2.py:185][0m |           0.0004 |         191.1832 |        -227.2873 |
[32m[20221214 00:42:00 @agent_ppo2.py:185][0m |          -0.0015 |         190.2865 |        -227.7076 |
[32m[20221214 00:42:00 @agent_ppo2.py:185][0m |          -0.0029 |         189.5302 |        -228.0817 |
[32m[20221214 00:42:00 @agent_ppo2.py:185][0m |          -0.0018 |         189.2470 |        -227.8859 |
[32m[20221214 00:42:00 @agent_ppo2.py:185][0m |           0.0037 |         188.6959 |        -226.0845 |
[32m[20221214 00:42:00 @agent_ppo2.py:185][0m |          -0.0027 |         189.6580 |        -227.5583 |
[32m[20221214 00:42:00 @agent_ppo2.py:185][0m |           0.0010 |         189.6556 |        -227.8281 |
[32m[20221214 00:42:00 @agent_ppo2.py:185][0m |          -0.0061 |         188.1364 |        -228.4177 |
[32m[20221214 00:42:00 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:42:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 796.64
[32m[20221214 00:42:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 842.53
[32m[20221214 00:42:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 722.52
[32m[20221214 00:42:00 @agent_ppo2.py:143][0m Total time:      44.50 min
[32m[20221214 00:42:00 @agent_ppo2.py:145][0m 3964928 total steps have happened
[32m[20221214 00:42:00 @agent_ppo2.py:121][0m #------------------------ Iteration 5936 --------------------------#
[32m[20221214 00:42:00 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:42:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:42:01 @agent_ppo2.py:185][0m |           0.0073 |         135.6242 |        -229.6916 |
[32m[20221214 00:42:01 @agent_ppo2.py:185][0m |           0.0080 |         133.9626 |        -228.7118 |
[32m[20221214 00:42:01 @agent_ppo2.py:185][0m |           0.0030 |         133.8605 |        -229.1389 |
[32m[20221214 00:42:01 @agent_ppo2.py:185][0m |           0.0027 |         132.9414 |        -228.4734 |
[32m[20221214 00:42:01 @agent_ppo2.py:185][0m |           0.0012 |         129.7382 |        -228.5338 |
[32m[20221214 00:42:01 @agent_ppo2.py:185][0m |          -0.0087 |         129.9863 |        -227.8420 |
[32m[20221214 00:42:01 @agent_ppo2.py:185][0m |          -0.0018 |         129.2253 |        -228.5669 |
[32m[20221214 00:42:01 @agent_ppo2.py:185][0m |          -0.0035 |         130.0400 |        -228.3811 |
[32m[20221214 00:42:01 @agent_ppo2.py:185][0m |           0.0035 |         132.5615 |        -228.0835 |
[32m[20221214 00:42:02 @agent_ppo2.py:185][0m |          -0.0004 |         130.7353 |        -227.1944 |
[32m[20221214 00:42:02 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:42:02 @agent_ppo2.py:138][0m Average TRAINING episode reward: 751.77
[32m[20221214 00:42:02 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 785.86
[32m[20221214 00:42:02 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 788.40
[32m[20221214 00:42:02 @agent_ppo2.py:143][0m Total time:      44.52 min
[32m[20221214 00:42:02 @agent_ppo2.py:145][0m 3966976 total steps have happened
[32m[20221214 00:42:02 @agent_ppo2.py:121][0m #------------------------ Iteration 5937 --------------------------#
[32m[20221214 00:42:02 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:42:02 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:42:02 @agent_ppo2.py:185][0m |          -0.0013 |         131.4431 |        -226.6388 |
[32m[20221214 00:42:02 @agent_ppo2.py:185][0m |           0.0042 |         144.7890 |        -226.9384 |
[32m[20221214 00:42:02 @agent_ppo2.py:185][0m |          -0.0061 |         125.1857 |        -225.8007 |
[32m[20221214 00:42:02 @agent_ppo2.py:185][0m |          -0.0039 |         123.9830 |        -226.4839 |
[32m[20221214 00:42:02 @agent_ppo2.py:185][0m |          -0.0083 |         123.0957 |        -226.6336 |
[32m[20221214 00:42:03 @agent_ppo2.py:185][0m |          -0.0060 |         122.7973 |        -226.1739 |
[32m[20221214 00:42:03 @agent_ppo2.py:185][0m |          -0.0051 |         122.8796 |        -225.7801 |
[32m[20221214 00:42:03 @agent_ppo2.py:185][0m |          -0.0082 |         121.9584 |        -226.1063 |
[32m[20221214 00:42:03 @agent_ppo2.py:185][0m |          -0.0108 |         121.7821 |        -225.5750 |
[32m[20221214 00:42:03 @agent_ppo2.py:185][0m |          -0.0140 |         121.5072 |        -226.3088 |
[32m[20221214 00:42:03 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221214 00:42:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 596.12
[32m[20221214 00:42:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 718.52
[32m[20221214 00:42:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 844.86
[32m[20221214 00:42:03 @agent_ppo2.py:143][0m Total time:      44.55 min
[32m[20221214 00:42:03 @agent_ppo2.py:145][0m 3969024 total steps have happened
[32m[20221214 00:42:03 @agent_ppo2.py:121][0m #------------------------ Iteration 5938 --------------------------#
[32m[20221214 00:42:03 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:42:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:42:04 @agent_ppo2.py:185][0m |           0.0003 |         154.3826 |        -224.7410 |
[32m[20221214 00:42:04 @agent_ppo2.py:185][0m |          -0.0051 |         143.5015 |        -224.9492 |
[32m[20221214 00:42:04 @agent_ppo2.py:185][0m |          -0.0079 |         142.9888 |        -225.5165 |
[32m[20221214 00:42:04 @agent_ppo2.py:185][0m |          -0.0072 |         138.6970 |        -224.8500 |
[32m[20221214 00:42:04 @agent_ppo2.py:185][0m |          -0.0106 |         138.1290 |        -224.6537 |
[32m[20221214 00:42:04 @agent_ppo2.py:185][0m |          -0.0072 |         136.5448 |        -224.3782 |
[32m[20221214 00:42:04 @agent_ppo2.py:185][0m |          -0.0133 |         135.2306 |        -224.1252 |
[32m[20221214 00:42:04 @agent_ppo2.py:185][0m |          -0.0105 |         134.6381 |        -223.9841 |
[32m[20221214 00:42:04 @agent_ppo2.py:185][0m |          -0.0067 |         136.1298 |        -224.6117 |
[32m[20221214 00:42:04 @agent_ppo2.py:185][0m |          -0.0101 |         133.4719 |        -222.5924 |
[32m[20221214 00:42:04 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:42:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 681.47
[32m[20221214 00:42:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 730.72
[32m[20221214 00:42:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 618.68
[32m[20221214 00:42:05 @agent_ppo2.py:143][0m Total time:      44.57 min
[32m[20221214 00:42:05 @agent_ppo2.py:145][0m 3971072 total steps have happened
[32m[20221214 00:42:05 @agent_ppo2.py:121][0m #------------------------ Iteration 5939 --------------------------#
[32m[20221214 00:42:05 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:42:05 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:42:05 @agent_ppo2.py:185][0m |           0.0040 |         126.0947 |        -218.5009 |
[32m[20221214 00:42:05 @agent_ppo2.py:185][0m |          -0.0015 |         120.5270 |        -217.4249 |
[32m[20221214 00:42:05 @agent_ppo2.py:185][0m |          -0.0029 |         119.2919 |        -217.7363 |
[32m[20221214 00:42:05 @agent_ppo2.py:185][0m |          -0.0024 |         118.5873 |        -217.5444 |
[32m[20221214 00:42:05 @agent_ppo2.py:185][0m |          -0.0063 |         117.3609 |        -216.8445 |
[32m[20221214 00:42:05 @agent_ppo2.py:185][0m |          -0.0074 |         116.5182 |        -216.6159 |
[32m[20221214 00:42:06 @agent_ppo2.py:185][0m |          -0.0104 |         116.4144 |        -216.8299 |
[32m[20221214 00:42:06 @agent_ppo2.py:185][0m |          -0.0078 |         115.9667 |        -215.9716 |
[32m[20221214 00:42:06 @agent_ppo2.py:185][0m |          -0.0085 |         115.8756 |        -216.4372 |
[32m[20221214 00:42:06 @agent_ppo2.py:185][0m |          -0.0105 |         115.2897 |        -215.8712 |
[32m[20221214 00:42:06 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:42:06 @agent_ppo2.py:138][0m Average TRAINING episode reward: 612.60
[32m[20221214 00:42:06 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 751.53
[32m[20221214 00:42:06 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 890.41
[32m[20221214 00:42:06 @agent_ppo2.py:143][0m Total time:      44.59 min
[32m[20221214 00:42:06 @agent_ppo2.py:145][0m 3973120 total steps have happened
[32m[20221214 00:42:06 @agent_ppo2.py:121][0m #------------------------ Iteration 5940 --------------------------#
[32m[20221214 00:42:06 @agent_ppo2.py:127][0m Sampling time: 0.20 s by 5 slaves
[32m[20221214 00:42:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:42:06 @agent_ppo2.py:185][0m |           0.0033 |         152.0045 |        -228.6183 |
[32m[20221214 00:42:07 @agent_ppo2.py:185][0m |          -0.0065 |         142.3453 |        -229.3217 |
[32m[20221214 00:42:07 @agent_ppo2.py:185][0m |          -0.0081 |         139.2590 |        -229.2432 |
[32m[20221214 00:42:07 @agent_ppo2.py:185][0m |          -0.0115 |         136.8779 |        -228.3095 |
[32m[20221214 00:42:07 @agent_ppo2.py:185][0m |          -0.0081 |         135.7216 |        -228.8605 |
[32m[20221214 00:42:07 @agent_ppo2.py:185][0m |          -0.0093 |         134.2922 |        -227.8795 |
[32m[20221214 00:42:07 @agent_ppo2.py:185][0m |          -0.0124 |         133.3619 |        -229.2202 |
[32m[20221214 00:42:07 @agent_ppo2.py:185][0m |          -0.0139 |         132.5381 |        -230.0628 |
[32m[20221214 00:42:07 @agent_ppo2.py:185][0m |          -0.0005 |         146.3877 |        -229.6762 |
[32m[20221214 00:42:07 @agent_ppo2.py:185][0m |          -0.0130 |         132.7891 |        -228.7847 |
[32m[20221214 00:42:07 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:42:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 611.81
[32m[20221214 00:42:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 717.09
[32m[20221214 00:42:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 770.70
[32m[20221214 00:42:08 @agent_ppo2.py:143][0m Total time:      44.62 min
[32m[20221214 00:42:08 @agent_ppo2.py:145][0m 3975168 total steps have happened
[32m[20221214 00:42:08 @agent_ppo2.py:121][0m #------------------------ Iteration 5941 --------------------------#
[32m[20221214 00:42:08 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:42:08 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:42:08 @agent_ppo2.py:185][0m |           0.0025 |         146.3092 |        -235.5425 |
[32m[20221214 00:42:08 @agent_ppo2.py:185][0m |          -0.0026 |         143.8846 |        -234.5437 |
[32m[20221214 00:42:08 @agent_ppo2.py:185][0m |           0.0019 |         143.2104 |        -234.8479 |
[32m[20221214 00:42:08 @agent_ppo2.py:185][0m |           0.0070 |         159.4845 |        -234.3596 |
[32m[20221214 00:42:08 @agent_ppo2.py:185][0m |          -0.0021 |         142.3072 |        -233.9887 |
[32m[20221214 00:42:08 @agent_ppo2.py:185][0m |          -0.0003 |         142.5142 |        -234.3869 |
[32m[20221214 00:42:08 @agent_ppo2.py:185][0m |          -0.0046 |         140.8553 |        -235.3408 |
[32m[20221214 00:42:09 @agent_ppo2.py:185][0m |           0.0008 |         142.6192 |        -233.8757 |
[32m[20221214 00:42:09 @agent_ppo2.py:185][0m |          -0.0075 |         140.8082 |        -233.6382 |
[32m[20221214 00:42:09 @agent_ppo2.py:185][0m |          -0.0048 |         141.1885 |        -233.7033 |
[32m[20221214 00:42:09 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:42:09 @agent_ppo2.py:138][0m Average TRAINING episode reward: 696.97
[32m[20221214 00:42:09 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 762.94
[32m[20221214 00:42:09 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 732.62
[32m[20221214 00:42:09 @agent_ppo2.py:143][0m Total time:      44.64 min
[32m[20221214 00:42:09 @agent_ppo2.py:145][0m 3977216 total steps have happened
[32m[20221214 00:42:09 @agent_ppo2.py:121][0m #------------------------ Iteration 5942 --------------------------#
[32m[20221214 00:42:09 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:42:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:42:09 @agent_ppo2.py:185][0m |           0.0011 |         153.9600 |        -227.9320 |
[32m[20221214 00:42:09 @agent_ppo2.py:185][0m |          -0.0044 |         138.6970 |        -225.2168 |
[32m[20221214 00:42:09 @agent_ppo2.py:185][0m |          -0.0095 |         133.2440 |        -225.6199 |
[32m[20221214 00:42:10 @agent_ppo2.py:185][0m |          -0.0147 |         131.2665 |        -225.3571 |
[32m[20221214 00:42:10 @agent_ppo2.py:185][0m |          -0.0126 |         128.2418 |        -225.6229 |
[32m[20221214 00:42:10 @agent_ppo2.py:185][0m |          -0.0042 |         141.7050 |        -225.0889 |
[32m[20221214 00:42:10 @agent_ppo2.py:185][0m |          -0.0137 |         124.2770 |        -224.8831 |
[32m[20221214 00:42:10 @agent_ppo2.py:185][0m |          -0.0174 |         121.5025 |        -223.9854 |
[32m[20221214 00:42:10 @agent_ppo2.py:185][0m |          -0.0178 |         120.1189 |        -223.7949 |
[32m[20221214 00:42:10 @agent_ppo2.py:185][0m |          -0.0201 |         119.1773 |        -223.0711 |
[32m[20221214 00:42:10 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:42:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 619.10
[32m[20221214 00:42:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 721.37
[32m[20221214 00:42:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 673.33
[32m[20221214 00:42:10 @agent_ppo2.py:143][0m Total time:      44.67 min
[32m[20221214 00:42:10 @agent_ppo2.py:145][0m 3979264 total steps have happened
[32m[20221214 00:42:10 @agent_ppo2.py:121][0m #------------------------ Iteration 5943 --------------------------#
[32m[20221214 00:42:11 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:42:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:42:11 @agent_ppo2.py:185][0m |           0.0060 |         145.8954 |        -228.6595 |
[32m[20221214 00:42:11 @agent_ppo2.py:185][0m |          -0.0023 |         136.6051 |        -227.0336 |
[32m[20221214 00:42:11 @agent_ppo2.py:185][0m |          -0.0025 |         134.9808 |        -226.6649 |
[32m[20221214 00:42:11 @agent_ppo2.py:185][0m |           0.0015 |         134.8713 |        -227.8748 |
[32m[20221214 00:42:11 @agent_ppo2.py:185][0m |          -0.0036 |         132.6317 |        -227.5342 |
[32m[20221214 00:42:11 @agent_ppo2.py:185][0m |          -0.0055 |         132.7979 |        -228.0598 |
[32m[20221214 00:42:11 @agent_ppo2.py:185][0m |          -0.0064 |         131.1801 |        -227.4567 |
[32m[20221214 00:42:11 @agent_ppo2.py:185][0m |          -0.0044 |         130.8229 |        -228.8504 |
[32m[20221214 00:42:11 @agent_ppo2.py:185][0m |          -0.0005 |         138.8088 |        -227.5224 |
[32m[20221214 00:42:12 @agent_ppo2.py:185][0m |          -0.0082 |         129.6217 |        -227.7535 |
[32m[20221214 00:42:12 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:42:12 @agent_ppo2.py:138][0m Average TRAINING episode reward: 584.08
[32m[20221214 00:42:12 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 776.32
[32m[20221214 00:42:12 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 113.59
[32m[20221214 00:42:12 @agent_ppo2.py:143][0m Total time:      44.69 min
[32m[20221214 00:42:12 @agent_ppo2.py:145][0m 3981312 total steps have happened
[32m[20221214 00:42:12 @agent_ppo2.py:121][0m #------------------------ Iteration 5944 --------------------------#
[32m[20221214 00:42:12 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:42:12 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:42:12 @agent_ppo2.py:185][0m |           0.0038 |         194.5785 |        -233.0136 |
[32m[20221214 00:42:12 @agent_ppo2.py:185][0m |           0.0015 |         188.7868 |        -231.8825 |
[32m[20221214 00:42:12 @agent_ppo2.py:185][0m |          -0.0044 |         187.8856 |        -232.5619 |
[32m[20221214 00:42:12 @agent_ppo2.py:185][0m |          -0.0031 |         186.8955 |        -231.2146 |
[32m[20221214 00:42:13 @agent_ppo2.py:185][0m |           0.0147 |         205.2266 |        -231.4132 |
[32m[20221214 00:42:13 @agent_ppo2.py:185][0m |          -0.0015 |         185.4383 |        -231.5592 |
[32m[20221214 00:42:13 @agent_ppo2.py:185][0m |           0.0017 |         193.5835 |        -231.7305 |
[32m[20221214 00:42:13 @agent_ppo2.py:185][0m |          -0.0003 |         184.0858 |        -229.5537 |
[32m[20221214 00:42:13 @agent_ppo2.py:185][0m |          -0.0046 |         183.7031 |        -231.5520 |
[32m[20221214 00:42:13 @agent_ppo2.py:185][0m |          -0.0049 |         183.7932 |        -231.2057 |
[32m[20221214 00:42:13 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:42:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 727.96
[32m[20221214 00:42:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 801.69
[32m[20221214 00:42:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 641.18
[32m[20221214 00:42:13 @agent_ppo2.py:143][0m Total time:      44.71 min
[32m[20221214 00:42:13 @agent_ppo2.py:145][0m 3983360 total steps have happened
[32m[20221214 00:42:13 @agent_ppo2.py:121][0m #------------------------ Iteration 5945 --------------------------#
[32m[20221214 00:42:13 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:42:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:42:13 @agent_ppo2.py:185][0m |           0.0035 |         174.2083 |        -238.5034 |
[32m[20221214 00:42:14 @agent_ppo2.py:185][0m |           0.0050 |         171.5051 |        -238.6172 |
[32m[20221214 00:42:14 @agent_ppo2.py:185][0m |           0.0042 |         174.1097 |        -239.3487 |
[32m[20221214 00:42:14 @agent_ppo2.py:185][0m |           0.0006 |         170.1665 |        -237.2014 |
[32m[20221214 00:42:14 @agent_ppo2.py:185][0m |          -0.0008 |         169.6265 |        -238.9074 |
[32m[20221214 00:42:14 @agent_ppo2.py:185][0m |          -0.0034 |         169.6395 |        -239.5767 |
[32m[20221214 00:42:14 @agent_ppo2.py:185][0m |           0.0002 |         169.7397 |        -238.9987 |
[32m[20221214 00:42:14 @agent_ppo2.py:185][0m |          -0.0051 |         168.5352 |        -239.3352 |
[32m[20221214 00:42:14 @agent_ppo2.py:185][0m |           0.0051 |         180.8335 |        -239.0574 |
[32m[20221214 00:42:14 @agent_ppo2.py:185][0m |           0.0063 |         177.7086 |        -239.9178 |
[32m[20221214 00:42:14 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:42:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 688.30
[32m[20221214 00:42:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 832.85
[32m[20221214 00:42:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 838.78
[32m[20221214 00:42:15 @agent_ppo2.py:143][0m Total time:      44.74 min
[32m[20221214 00:42:15 @agent_ppo2.py:145][0m 3985408 total steps have happened
[32m[20221214 00:42:15 @agent_ppo2.py:121][0m #------------------------ Iteration 5946 --------------------------#
[32m[20221214 00:42:15 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:42:15 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:42:15 @agent_ppo2.py:185][0m |           0.0020 |         135.7103 |        -231.7506 |
[32m[20221214 00:42:15 @agent_ppo2.py:185][0m |          -0.0062 |         123.4239 |        -228.7324 |
[32m[20221214 00:42:15 @agent_ppo2.py:185][0m |           0.0011 |         126.5135 |        -230.1460 |
[32m[20221214 00:42:15 @agent_ppo2.py:185][0m |          -0.0043 |         120.6400 |        -229.6249 |
[32m[20221214 00:42:15 @agent_ppo2.py:185][0m |          -0.0089 |         116.3964 |        -229.2408 |
[32m[20221214 00:42:15 @agent_ppo2.py:185][0m |          -0.0074 |         114.6081 |        -229.3306 |
[32m[20221214 00:42:15 @agent_ppo2.py:185][0m |          -0.0090 |         113.2795 |        -228.5804 |
[32m[20221214 00:42:16 @agent_ppo2.py:185][0m |          -0.0079 |         112.6281 |        -228.2327 |
[32m[20221214 00:42:16 @agent_ppo2.py:185][0m |          -0.0151 |         111.2666 |        -228.8172 |
[32m[20221214 00:42:16 @agent_ppo2.py:185][0m |          -0.0126 |         113.5348 |        -227.9161 |
[32m[20221214 00:42:16 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:42:16 @agent_ppo2.py:138][0m Average TRAINING episode reward: 526.95
[32m[20221214 00:42:16 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 656.12
[32m[20221214 00:42:16 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 790.88
[32m[20221214 00:42:16 @agent_ppo2.py:143][0m Total time:      44.76 min
[32m[20221214 00:42:16 @agent_ppo2.py:145][0m 3987456 total steps have happened
[32m[20221214 00:42:16 @agent_ppo2.py:121][0m #------------------------ Iteration 5947 --------------------------#
[32m[20221214 00:42:16 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:42:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:42:16 @agent_ppo2.py:185][0m |           0.0044 |         160.5290 |        -231.2049 |
[32m[20221214 00:42:16 @agent_ppo2.py:185][0m |          -0.0067 |         152.7857 |        -231.0285 |
[32m[20221214 00:42:17 @agent_ppo2.py:185][0m |          -0.0042 |         149.3233 |        -229.6333 |
[32m[20221214 00:42:17 @agent_ppo2.py:185][0m |          -0.0080 |         147.2690 |        -229.0209 |
[32m[20221214 00:42:17 @agent_ppo2.py:185][0m |          -0.0094 |         145.1441 |        -229.5451 |
[32m[20221214 00:42:17 @agent_ppo2.py:185][0m |          -0.0082 |         143.7335 |        -228.1636 |
[32m[20221214 00:42:17 @agent_ppo2.py:185][0m |          -0.0089 |         144.7714 |        -228.9604 |
[32m[20221214 00:42:17 @agent_ppo2.py:185][0m |          -0.0130 |         143.3678 |        -228.1844 |
[32m[20221214 00:42:17 @agent_ppo2.py:185][0m |          -0.0112 |         142.1489 |        -228.4655 |
[32m[20221214 00:42:17 @agent_ppo2.py:185][0m |           0.0045 |         159.1133 |        -227.6469 |
[32m[20221214 00:42:17 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:42:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 605.26
[32m[20221214 00:42:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 707.45
[32m[20221214 00:42:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 692.21
[32m[20221214 00:42:17 @agent_ppo2.py:143][0m Total time:      44.78 min
[32m[20221214 00:42:17 @agent_ppo2.py:145][0m 3989504 total steps have happened
[32m[20221214 00:42:17 @agent_ppo2.py:121][0m #------------------------ Iteration 5948 --------------------------#
[32m[20221214 00:42:18 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:42:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:42:18 @agent_ppo2.py:185][0m |          -0.0032 |         149.8807 |        -228.9515 |
[32m[20221214 00:42:18 @agent_ppo2.py:185][0m |          -0.0016 |         144.2525 |        -228.1600 |
[32m[20221214 00:42:18 @agent_ppo2.py:185][0m |          -0.0076 |         142.2027 |        -227.7017 |
[32m[20221214 00:42:18 @agent_ppo2.py:185][0m |          -0.0067 |         141.3210 |        -227.5138 |
[32m[20221214 00:42:18 @agent_ppo2.py:185][0m |          -0.0054 |         140.8292 |        -227.1353 |
[32m[20221214 00:42:18 @agent_ppo2.py:185][0m |          -0.0052 |         139.7009 |        -227.7088 |
[32m[20221214 00:42:18 @agent_ppo2.py:185][0m |          -0.0100 |         139.5179 |        -227.1880 |
[32m[20221214 00:42:18 @agent_ppo2.py:185][0m |          -0.0069 |         138.7448 |        -226.8992 |
[32m[20221214 00:42:19 @agent_ppo2.py:185][0m |          -0.0114 |         138.2060 |        -227.1276 |
[32m[20221214 00:42:19 @agent_ppo2.py:185][0m |          -0.0118 |         138.1956 |        -226.7619 |
[32m[20221214 00:42:19 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:42:19 @agent_ppo2.py:138][0m Average TRAINING episode reward: 664.07
[32m[20221214 00:42:19 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 697.44
[32m[20221214 00:42:19 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 403.05
[32m[20221214 00:42:19 @agent_ppo2.py:143][0m Total time:      44.81 min
[32m[20221214 00:42:19 @agent_ppo2.py:145][0m 3991552 total steps have happened
[32m[20221214 00:42:19 @agent_ppo2.py:121][0m #------------------------ Iteration 5949 --------------------------#
[32m[20221214 00:42:19 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:42:19 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:42:19 @agent_ppo2.py:185][0m |           0.0035 |         127.0466 |        -233.8770 |
[32m[20221214 00:42:19 @agent_ppo2.py:185][0m |          -0.0017 |         117.3157 |        -232.1324 |
[32m[20221214 00:42:19 @agent_ppo2.py:185][0m |          -0.0029 |         115.8186 |        -232.0260 |
[32m[20221214 00:42:19 @agent_ppo2.py:185][0m |          -0.0069 |         115.2528 |        -232.2507 |
[32m[20221214 00:42:20 @agent_ppo2.py:185][0m |          -0.0136 |         112.5879 |        -233.2299 |
[32m[20221214 00:42:20 @agent_ppo2.py:185][0m |          -0.0119 |         110.5619 |        -232.9083 |
[32m[20221214 00:42:20 @agent_ppo2.py:185][0m |          -0.0098 |         111.3375 |        -234.0360 |
[32m[20221214 00:42:20 @agent_ppo2.py:185][0m |          -0.0150 |         110.3152 |        -233.2949 |
[32m[20221214 00:42:20 @agent_ppo2.py:185][0m |          -0.0021 |         121.2530 |        -233.7361 |
[32m[20221214 00:42:20 @agent_ppo2.py:185][0m |          -0.0095 |         110.0746 |        -233.6985 |
[32m[20221214 00:42:20 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:42:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 535.82
[32m[20221214 00:42:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 653.84
[32m[20221214 00:42:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 765.96
[32m[20221214 00:42:20 @agent_ppo2.py:143][0m Total time:      44.83 min
[32m[20221214 00:42:20 @agent_ppo2.py:145][0m 3993600 total steps have happened
[32m[20221214 00:42:20 @agent_ppo2.py:121][0m #------------------------ Iteration 5950 --------------------------#
[32m[20221214 00:42:20 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:42:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:42:21 @agent_ppo2.py:185][0m |           0.0110 |         145.1009 |        -236.2469 |
[32m[20221214 00:42:21 @agent_ppo2.py:185][0m |           0.0090 |         149.6075 |        -236.0804 |
[32m[20221214 00:42:21 @agent_ppo2.py:185][0m |           0.0024 |         143.2958 |        -236.4659 |
[32m[20221214 00:42:21 @agent_ppo2.py:185][0m |          -0.0067 |         135.8511 |        -235.2847 |
[32m[20221214 00:42:21 @agent_ppo2.py:185][0m |          -0.0064 |         135.0143 |        -235.7590 |
[32m[20221214 00:42:21 @agent_ppo2.py:185][0m |          -0.0078 |         133.8561 |        -235.3931 |
[32m[20221214 00:42:21 @agent_ppo2.py:185][0m |          -0.0104 |         133.7871 |        -235.6520 |
[32m[20221214 00:42:21 @agent_ppo2.py:185][0m |          -0.0085 |         132.6879 |        -236.0887 |
[32m[20221214 00:42:21 @agent_ppo2.py:185][0m |          -0.0107 |         132.1104 |        -235.5632 |
[32m[20221214 00:42:21 @agent_ppo2.py:185][0m |          -0.0145 |         131.7404 |        -236.4206 |
[32m[20221214 00:42:21 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:42:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 586.38
[32m[20221214 00:42:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 789.75
[32m[20221214 00:42:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 27.92
[32m[20221214 00:42:22 @agent_ppo2.py:143][0m Total time:      44.85 min
[32m[20221214 00:42:22 @agent_ppo2.py:145][0m 3995648 total steps have happened
[32m[20221214 00:42:22 @agent_ppo2.py:121][0m #------------------------ Iteration 5951 --------------------------#
[32m[20221214 00:42:22 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:42:22 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:42:22 @agent_ppo2.py:185][0m |          -0.0010 |         156.6996 |        -239.3267 |
[32m[20221214 00:42:22 @agent_ppo2.py:185][0m |           0.0045 |         154.0917 |        -238.5356 |
[32m[20221214 00:42:22 @agent_ppo2.py:185][0m |           0.0004 |         148.1645 |        -237.0554 |
[32m[20221214 00:42:22 @agent_ppo2.py:185][0m |          -0.0041 |         144.0718 |        -239.3994 |
[32m[20221214 00:42:22 @agent_ppo2.py:185][0m |          -0.0061 |         146.5898 |        -239.1319 |
[32m[20221214 00:42:22 @agent_ppo2.py:185][0m |          -0.0110 |         141.6518 |        -239.1007 |
[32m[20221214 00:42:23 @agent_ppo2.py:185][0m |          -0.0055 |         141.2699 |        -239.2724 |
[32m[20221214 00:42:23 @agent_ppo2.py:185][0m |          -0.0114 |         140.8868 |        -239.8131 |
[32m[20221214 00:42:23 @agent_ppo2.py:185][0m |          -0.0122 |         140.3928 |        -239.8948 |
[32m[20221214 00:42:23 @agent_ppo2.py:185][0m |          -0.0154 |         140.2613 |        -239.7661 |
[32m[20221214 00:42:23 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:42:23 @agent_ppo2.py:138][0m Average TRAINING episode reward: 581.95
[32m[20221214 00:42:23 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 682.28
[32m[20221214 00:42:23 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 563.05
[32m[20221214 00:42:23 @agent_ppo2.py:143][0m Total time:      44.88 min
[32m[20221214 00:42:23 @agent_ppo2.py:145][0m 3997696 total steps have happened
[32m[20221214 00:42:23 @agent_ppo2.py:121][0m #------------------------ Iteration 5952 --------------------------#
[32m[20221214 00:42:23 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:42:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:42:23 @agent_ppo2.py:185][0m |           0.0061 |         143.8156 |        -230.6291 |
[32m[20221214 00:42:23 @agent_ppo2.py:185][0m |           0.0048 |         137.4524 |        -229.7023 |
[32m[20221214 00:42:24 @agent_ppo2.py:185][0m |           0.0027 |         137.8538 |        -231.6961 |
[32m[20221214 00:42:24 @agent_ppo2.py:185][0m |          -0.0022 |         131.6947 |        -230.1710 |
[32m[20221214 00:42:24 @agent_ppo2.py:185][0m |           0.0182 |         147.7432 |        -230.9735 |
[32m[20221214 00:42:24 @agent_ppo2.py:185][0m |          -0.0038 |         130.6464 |        -230.5234 |
[32m[20221214 00:42:24 @agent_ppo2.py:185][0m |          -0.0016 |         129.1141 |        -230.9320 |
[32m[20221214 00:42:24 @agent_ppo2.py:185][0m |          -0.0020 |         130.0441 |        -230.6753 |
[32m[20221214 00:42:24 @agent_ppo2.py:185][0m |          -0.0077 |         127.9069 |        -231.3558 |
[32m[20221214 00:42:24 @agent_ppo2.py:185][0m |          -0.0079 |         128.1290 |        -230.5078 |
[32m[20221214 00:42:24 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:42:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 649.61
[32m[20221214 00:42:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 773.84
[32m[20221214 00:42:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 545.45
[32m[20221214 00:42:24 @agent_ppo2.py:143][0m Total time:      44.90 min
[32m[20221214 00:42:24 @agent_ppo2.py:145][0m 3999744 total steps have happened
[32m[20221214 00:42:24 @agent_ppo2.py:121][0m #------------------------ Iteration 5953 --------------------------#
[32m[20221214 00:42:25 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:42:25 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:42:25 @agent_ppo2.py:185][0m |           0.0031 |         148.9287 |        -239.8390 |
[32m[20221214 00:42:25 @agent_ppo2.py:185][0m |          -0.0072 |         143.6678 |        -241.1937 |
[32m[20221214 00:42:25 @agent_ppo2.py:185][0m |           0.0108 |         161.6958 |        -241.3703 |
[32m[20221214 00:42:25 @agent_ppo2.py:185][0m |          -0.0065 |         143.2031 |        -240.3840 |
[32m[20221214 00:42:25 @agent_ppo2.py:185][0m |          -0.0089 |         141.9117 |        -241.4348 |
[32m[20221214 00:42:25 @agent_ppo2.py:185][0m |          -0.0103 |         141.7963 |        -240.7845 |
[32m[20221214 00:42:25 @agent_ppo2.py:185][0m |          -0.0076 |         141.4335 |        -242.7157 |
[32m[20221214 00:42:25 @agent_ppo2.py:185][0m |          -0.0072 |         141.2614 |        -240.2918 |
[32m[20221214 00:42:25 @agent_ppo2.py:185][0m |          -0.0051 |         141.8261 |        -241.2118 |
[32m[20221214 00:42:26 @agent_ppo2.py:185][0m |          -0.0075 |         140.9730 |        -241.4126 |
[32m[20221214 00:42:26 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:42:26 @agent_ppo2.py:138][0m Average TRAINING episode reward: 694.20
[32m[20221214 00:42:26 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 766.15
[32m[20221214 00:42:26 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 657.89
[32m[20221214 00:42:26 @agent_ppo2.py:143][0m Total time:      44.92 min
[32m[20221214 00:42:26 @agent_ppo2.py:145][0m 4001792 total steps have happened
[32m[20221214 00:42:26 @agent_ppo2.py:121][0m #------------------------ Iteration 5954 --------------------------#
[32m[20221214 00:42:26 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:42:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:42:26 @agent_ppo2.py:185][0m |           0.0005 |         169.6615 |        -239.5980 |
[32m[20221214 00:42:26 @agent_ppo2.py:185][0m |           0.0029 |         165.7693 |        -240.6866 |
[32m[20221214 00:42:26 @agent_ppo2.py:185][0m |          -0.0022 |         163.7336 |        -241.5265 |
[32m[20221214 00:42:26 @agent_ppo2.py:185][0m |          -0.0025 |         163.2755 |        -240.8949 |
[32m[20221214 00:42:26 @agent_ppo2.py:185][0m |          -0.0041 |         162.7562 |        -242.0408 |
[32m[20221214 00:42:27 @agent_ppo2.py:185][0m |          -0.0021 |         162.0912 |        -241.5511 |
[32m[20221214 00:42:27 @agent_ppo2.py:185][0m |          -0.0021 |         161.8790 |        -241.0492 |
[32m[20221214 00:42:27 @agent_ppo2.py:185][0m |          -0.0069 |         161.5303 |        -242.0541 |
[32m[20221214 00:42:27 @agent_ppo2.py:185][0m |           0.0021 |         162.4244 |        -241.7018 |
[32m[20221214 00:42:27 @agent_ppo2.py:185][0m |          -0.0011 |         164.0674 |        -242.1363 |
[32m[20221214 00:42:27 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:42:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 640.67
[32m[20221214 00:42:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 740.52
[32m[20221214 00:42:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 827.47
[32m[20221214 00:42:27 @agent_ppo2.py:143][0m Total time:      44.95 min
[32m[20221214 00:42:27 @agent_ppo2.py:145][0m 4003840 total steps have happened
[32m[20221214 00:42:27 @agent_ppo2.py:121][0m #------------------------ Iteration 5955 --------------------------#
[32m[20221214 00:42:27 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:42:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:42:27 @agent_ppo2.py:185][0m |           0.0120 |         164.3831 |        -249.8751 |
[32m[20221214 00:42:28 @agent_ppo2.py:185][0m |          -0.0036 |         142.8732 |        -249.4020 |
[32m[20221214 00:42:28 @agent_ppo2.py:185][0m |          -0.0028 |         141.3646 |        -250.8561 |
[32m[20221214 00:42:28 @agent_ppo2.py:185][0m |          -0.0081 |         140.3373 |        -250.5776 |
[32m[20221214 00:42:28 @agent_ppo2.py:185][0m |           0.0005 |         153.8929 |        -249.1388 |
[32m[20221214 00:42:28 @agent_ppo2.py:185][0m |          -0.0032 |         139.1147 |        -250.4207 |
[32m[20221214 00:42:28 @agent_ppo2.py:185][0m |          -0.0011 |         138.5879 |        -249.2523 |
[32m[20221214 00:42:28 @agent_ppo2.py:185][0m |          -0.0070 |         138.0914 |        -250.5947 |
[32m[20221214 00:42:28 @agent_ppo2.py:185][0m |          -0.0072 |         137.8117 |        -249.9483 |
[32m[20221214 00:42:28 @agent_ppo2.py:185][0m |          -0.0024 |         146.3622 |        -250.1280 |
[32m[20221214 00:42:28 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:42:29 @agent_ppo2.py:138][0m Average TRAINING episode reward: 629.60
[32m[20221214 00:42:29 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 767.92
[32m[20221214 00:42:29 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 767.04
[32m[20221214 00:42:29 @agent_ppo2.py:143][0m Total time:      44.97 min
[32m[20221214 00:42:29 @agent_ppo2.py:145][0m 4005888 total steps have happened
[32m[20221214 00:42:29 @agent_ppo2.py:121][0m #------------------------ Iteration 5956 --------------------------#
[32m[20221214 00:42:29 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:42:29 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:42:29 @agent_ppo2.py:185][0m |          -0.0008 |         127.5519 |        -248.3523 |
[32m[20221214 00:42:29 @agent_ppo2.py:185][0m |          -0.0018 |         122.3448 |        -248.4231 |
[32m[20221214 00:42:29 @agent_ppo2.py:185][0m |          -0.0084 |         120.5285 |        -249.1659 |
[32m[20221214 00:42:29 @agent_ppo2.py:185][0m |           0.0023 |         121.7996 |        -246.5114 |
[32m[20221214 00:42:29 @agent_ppo2.py:185][0m |          -0.0068 |         117.1955 |        -250.0064 |
[32m[20221214 00:42:29 @agent_ppo2.py:185][0m |          -0.0112 |         116.5550 |        -250.5323 |
[32m[20221214 00:42:30 @agent_ppo2.py:185][0m |          -0.0107 |         115.3150 |        -250.1192 |
[32m[20221214 00:42:30 @agent_ppo2.py:185][0m |          -0.0079 |         115.0027 |        -250.2795 |
[32m[20221214 00:42:30 @agent_ppo2.py:185][0m |          -0.0116 |         116.8028 |        -252.4489 |
[32m[20221214 00:42:30 @agent_ppo2.py:185][0m |          -0.0133 |         114.2418 |        -251.8559 |
[32m[20221214 00:42:30 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221214 00:42:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 455.28
[32m[20221214 00:42:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 636.68
[32m[20221214 00:42:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 813.54
[32m[20221214 00:42:30 @agent_ppo2.py:143][0m Total time:      44.99 min
[32m[20221214 00:42:30 @agent_ppo2.py:145][0m 4007936 total steps have happened
[32m[20221214 00:42:30 @agent_ppo2.py:121][0m #------------------------ Iteration 5957 --------------------------#
[32m[20221214 00:42:30 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:42:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:42:30 @agent_ppo2.py:185][0m |           0.0067 |         185.4672 |        -253.1774 |
[32m[20221214 00:42:31 @agent_ppo2.py:185][0m |           0.0043 |         175.1687 |        -253.6577 |
[32m[20221214 00:42:31 @agent_ppo2.py:185][0m |           0.0028 |         173.7188 |        -253.1881 |
[32m[20221214 00:42:31 @agent_ppo2.py:185][0m |           0.0016 |         173.8492 |        -252.3839 |
[32m[20221214 00:42:31 @agent_ppo2.py:185][0m |           0.0001 |         173.0451 |        -251.3027 |
[32m[20221214 00:42:31 @agent_ppo2.py:185][0m |          -0.0013 |         172.7279 |        -252.6756 |
[32m[20221214 00:42:31 @agent_ppo2.py:185][0m |          -0.0014 |         172.5374 |        -251.7411 |
[32m[20221214 00:42:31 @agent_ppo2.py:185][0m |          -0.0038 |         172.7017 |        -251.5583 |
[32m[20221214 00:42:31 @agent_ppo2.py:185][0m |          -0.0009 |         172.9332 |        -248.5903 |
[32m[20221214 00:42:31 @agent_ppo2.py:185][0m |           0.0005 |         172.7981 |        -251.2332 |
[32m[20221214 00:42:31 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:42:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 884.10
[32m[20221214 00:42:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 885.66
[32m[20221214 00:42:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 694.02
[32m[20221214 00:42:31 @agent_ppo2.py:143][0m Total time:      45.02 min
[32m[20221214 00:42:31 @agent_ppo2.py:145][0m 4009984 total steps have happened
[32m[20221214 00:42:31 @agent_ppo2.py:121][0m #------------------------ Iteration 5958 --------------------------#
[32m[20221214 00:42:32 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:42:32 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:42:32 @agent_ppo2.py:185][0m |           0.0010 |         141.1484 |        -259.6776 |
[32m[20221214 00:42:32 @agent_ppo2.py:185][0m |           0.0006 |         138.4713 |        -258.0734 |
[32m[20221214 00:42:32 @agent_ppo2.py:185][0m |          -0.0091 |         133.9894 |        -257.7567 |
[32m[20221214 00:42:32 @agent_ppo2.py:185][0m |          -0.0095 |         133.5004 |        -257.8970 |
[32m[20221214 00:42:32 @agent_ppo2.py:185][0m |          -0.0118 |         132.4041 |        -257.6013 |
[32m[20221214 00:42:32 @agent_ppo2.py:185][0m |          -0.0043 |         133.0700 |        -257.0622 |
[32m[20221214 00:42:32 @agent_ppo2.py:185][0m |          -0.0079 |         130.4012 |        -257.0945 |
[32m[20221214 00:42:33 @agent_ppo2.py:185][0m |          -0.0115 |         129.5522 |        -256.8389 |
[32m[20221214 00:42:33 @agent_ppo2.py:185][0m |          -0.0122 |         129.8047 |        -256.8549 |
[32m[20221214 00:42:33 @agent_ppo2.py:185][0m |          -0.0112 |         129.8307 |        -255.8561 |
[32m[20221214 00:42:33 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:42:33 @agent_ppo2.py:138][0m Average TRAINING episode reward: 549.47
[32m[20221214 00:42:33 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 705.36
[32m[20221214 00:42:33 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 807.80
[32m[20221214 00:42:33 @agent_ppo2.py:143][0m Total time:      45.04 min
[32m[20221214 00:42:33 @agent_ppo2.py:145][0m 4012032 total steps have happened
[32m[20221214 00:42:33 @agent_ppo2.py:121][0m #------------------------ Iteration 5959 --------------------------#
[32m[20221214 00:42:33 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:42:33 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:42:33 @agent_ppo2.py:185][0m |          -0.0033 |         154.7621 |        -233.3644 |
[32m[20221214 00:42:33 @agent_ppo2.py:185][0m |           0.0146 |         175.7743 |        -235.3974 |
[32m[20221214 00:42:33 @agent_ppo2.py:185][0m |          -0.0052 |         149.6521 |        -235.5132 |
[32m[20221214 00:42:34 @agent_ppo2.py:185][0m |          -0.0066 |         146.9502 |        -235.7798 |
[32m[20221214 00:42:34 @agent_ppo2.py:185][0m |          -0.0070 |         147.1943 |        -236.0491 |
[32m[20221214 00:42:34 @agent_ppo2.py:185][0m |          -0.0099 |         146.5690 |        -236.1612 |
[32m[20221214 00:42:34 @agent_ppo2.py:185][0m |          -0.0108 |         146.5678 |        -235.3075 |
[32m[20221214 00:42:34 @agent_ppo2.py:185][0m |          -0.0091 |         145.0291 |        -235.6095 |
[32m[20221214 00:42:34 @agent_ppo2.py:185][0m |          -0.0083 |         144.8285 |        -235.6461 |
[32m[20221214 00:42:34 @agent_ppo2.py:185][0m |          -0.0099 |         144.7867 |        -235.6927 |
[32m[20221214 00:42:34 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:42:34 @agent_ppo2.py:138][0m Average TRAINING episode reward: 629.80
[32m[20221214 00:42:34 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 775.19
[32m[20221214 00:42:34 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 799.42
[32m[20221214 00:42:34 @agent_ppo2.py:143][0m Total time:      45.07 min
[32m[20221214 00:42:34 @agent_ppo2.py:145][0m 4014080 total steps have happened
[32m[20221214 00:42:34 @agent_ppo2.py:121][0m #------------------------ Iteration 5960 --------------------------#
[32m[20221214 00:42:35 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:42:35 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:42:35 @agent_ppo2.py:185][0m |           0.0000 |         164.6455 |        -250.8446 |
[32m[20221214 00:42:35 @agent_ppo2.py:185][0m |          -0.0033 |         155.6542 |        -249.6480 |
[32m[20221214 00:42:35 @agent_ppo2.py:185][0m |          -0.0043 |         152.2282 |        -249.2110 |
[32m[20221214 00:42:35 @agent_ppo2.py:185][0m |          -0.0053 |         148.0171 |        -250.0433 |
[32m[20221214 00:42:35 @agent_ppo2.py:185][0m |          -0.0081 |         147.2567 |        -250.9399 |
[32m[20221214 00:42:35 @agent_ppo2.py:185][0m |          -0.0045 |         145.4364 |        -249.7357 |
[32m[20221214 00:42:35 @agent_ppo2.py:185][0m |          -0.0101 |         144.4206 |        -249.4605 |
[32m[20221214 00:42:35 @agent_ppo2.py:185][0m |          -0.0115 |         143.0923 |        -250.4829 |
[32m[20221214 00:42:36 @agent_ppo2.py:185][0m |          -0.0118 |         142.3566 |        -251.2423 |
[32m[20221214 00:42:36 @agent_ppo2.py:185][0m |          -0.0120 |         141.6820 |        -250.7686 |
[32m[20221214 00:42:36 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:42:36 @agent_ppo2.py:138][0m Average TRAINING episode reward: 632.40
[32m[20221214 00:42:36 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 745.91
[32m[20221214 00:42:36 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 711.35
[32m[20221214 00:42:36 @agent_ppo2.py:143][0m Total time:      45.09 min
[32m[20221214 00:42:36 @agent_ppo2.py:145][0m 4016128 total steps have happened
[32m[20221214 00:42:36 @agent_ppo2.py:121][0m #------------------------ Iteration 5961 --------------------------#
[32m[20221214 00:42:36 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:42:36 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:42:36 @agent_ppo2.py:185][0m |           0.0032 |          34.0025 |        -253.2990 |
[32m[20221214 00:42:36 @agent_ppo2.py:185][0m |           0.0017 |          28.8756 |        -252.7089 |
[32m[20221214 00:42:36 @agent_ppo2.py:185][0m |          -0.0008 |          28.6952 |        -253.5926 |
[32m[20221214 00:42:36 @agent_ppo2.py:185][0m |          -0.0026 |          28.5666 |        -252.6931 |
[32m[20221214 00:42:37 @agent_ppo2.py:185][0m |          -0.0023 |          28.5401 |        -253.1235 |
[32m[20221214 00:42:37 @agent_ppo2.py:185][0m |           0.0068 |          28.4486 |        -253.2144 |
[32m[20221214 00:42:37 @agent_ppo2.py:185][0m |           0.0031 |          28.5056 |        -252.1631 |
[32m[20221214 00:42:37 @agent_ppo2.py:185][0m |          -0.0012 |          28.4031 |        -253.1161 |
[32m[20221214 00:42:37 @agent_ppo2.py:185][0m |           0.0020 |          28.4099 |        -252.5038 |
[32m[20221214 00:42:37 @agent_ppo2.py:185][0m |           0.0010 |          28.8471 |        -252.5256 |
[32m[20221214 00:42:37 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:42:37 @agent_ppo2.py:138][0m Average TRAINING episode reward: 0.00
[32m[20221214 00:42:37 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 0.00
[32m[20221214 00:42:37 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 559.08
[32m[20221214 00:42:37 @agent_ppo2.py:143][0m Total time:      45.11 min
[32m[20221214 00:42:37 @agent_ppo2.py:145][0m 4018176 total steps have happened
[32m[20221214 00:42:37 @agent_ppo2.py:121][0m #------------------------ Iteration 5962 --------------------------#
[32m[20221214 00:42:37 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:42:37 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:42:38 @agent_ppo2.py:185][0m |           0.0045 |         157.4175 |        -251.2638 |
[32m[20221214 00:42:38 @agent_ppo2.py:185][0m |           0.0003 |         152.6086 |        -250.2732 |
[32m[20221214 00:42:38 @agent_ppo2.py:185][0m |          -0.0019 |         151.6453 |        -250.4039 |
[32m[20221214 00:42:38 @agent_ppo2.py:185][0m |           0.0101 |         167.1308 |        -250.6719 |
[32m[20221214 00:42:38 @agent_ppo2.py:185][0m |           0.0016 |         151.3333 |        -251.8291 |
[32m[20221214 00:42:38 @agent_ppo2.py:185][0m |          -0.0051 |         150.5240 |        -250.6532 |
[32m[20221214 00:42:38 @agent_ppo2.py:185][0m |          -0.0024 |         151.8693 |        -251.1189 |
[32m[20221214 00:42:38 @agent_ppo2.py:185][0m |          -0.0062 |         149.1351 |        -250.8658 |
[32m[20221214 00:42:38 @agent_ppo2.py:185][0m |          -0.0043 |         149.0635 |        -251.1777 |
[32m[20221214 00:42:38 @agent_ppo2.py:185][0m |          -0.0005 |         152.2867 |        -251.3594 |
[32m[20221214 00:42:38 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:42:39 @agent_ppo2.py:138][0m Average TRAINING episode reward: 719.14
[32m[20221214 00:42:39 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 777.38
[32m[20221214 00:42:39 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 649.85
[32m[20221214 00:42:39 @agent_ppo2.py:143][0m Total time:      45.14 min
[32m[20221214 00:42:39 @agent_ppo2.py:145][0m 4020224 total steps have happened
[32m[20221214 00:42:39 @agent_ppo2.py:121][0m #------------------------ Iteration 5963 --------------------------#
[32m[20221214 00:42:39 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:42:39 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:42:39 @agent_ppo2.py:185][0m |          -0.0023 |         144.0822 |        -251.8641 |
[32m[20221214 00:42:39 @agent_ppo2.py:185][0m |           0.0034 |         147.3673 |        -250.9877 |
[32m[20221214 00:42:39 @agent_ppo2.py:185][0m |          -0.0027 |         129.7522 |        -250.2988 |
[32m[20221214 00:42:39 @agent_ppo2.py:185][0m |          -0.0125 |         120.7137 |        -249.8787 |
[32m[20221214 00:42:39 @agent_ppo2.py:185][0m |          -0.0091 |         115.9394 |        -248.7206 |
[32m[20221214 00:42:39 @agent_ppo2.py:185][0m |          -0.0117 |         114.1899 |        -248.8191 |
[32m[20221214 00:42:40 @agent_ppo2.py:185][0m |          -0.0134 |         112.0699 |        -250.4122 |
[32m[20221214 00:42:40 @agent_ppo2.py:185][0m |          -0.0142 |         112.4063 |        -249.5539 |
[32m[20221214 00:42:40 @agent_ppo2.py:185][0m |          -0.0089 |         110.8795 |        -249.8002 |
[32m[20221214 00:42:40 @agent_ppo2.py:185][0m |          -0.0182 |         109.9865 |        -249.0707 |
[32m[20221214 00:42:40 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:42:40 @agent_ppo2.py:138][0m Average TRAINING episode reward: 664.17
[32m[20221214 00:42:40 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 781.41
[32m[20221214 00:42:40 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 171.28
[32m[20221214 00:42:40 @agent_ppo2.py:143][0m Total time:      45.16 min
[32m[20221214 00:42:40 @agent_ppo2.py:145][0m 4022272 total steps have happened
[32m[20221214 00:42:40 @agent_ppo2.py:121][0m #------------------------ Iteration 5964 --------------------------#
[32m[20221214 00:42:40 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:42:40 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:42:40 @agent_ppo2.py:185][0m |           0.0029 |         150.3604 |        -253.3088 |
[32m[20221214 00:42:41 @agent_ppo2.py:185][0m |          -0.0078 |         135.2109 |        -253.2118 |
[32m[20221214 00:42:41 @agent_ppo2.py:185][0m |          -0.0094 |         129.9027 |        -252.1839 |
[32m[20221214 00:42:41 @agent_ppo2.py:185][0m |          -0.0097 |         128.5616 |        -253.3037 |
[32m[20221214 00:42:41 @agent_ppo2.py:185][0m |          -0.0111 |         128.3799 |        -252.1363 |
[32m[20221214 00:42:41 @agent_ppo2.py:185][0m |          -0.0109 |         126.3625 |        -252.4710 |
[32m[20221214 00:42:41 @agent_ppo2.py:185][0m |          -0.0143 |         125.5845 |        -252.7573 |
[32m[20221214 00:42:41 @agent_ppo2.py:185][0m |          -0.0161 |         124.9674 |        -252.8119 |
[32m[20221214 00:42:41 @agent_ppo2.py:185][0m |          -0.0163 |         123.5852 |        -252.5319 |
[32m[20221214 00:42:41 @agent_ppo2.py:185][0m |          -0.0149 |         123.2645 |        -252.5310 |
[32m[20221214 00:42:41 @agent_ppo2.py:130][0m Policy update time: 1.15 s
[32m[20221214 00:42:42 @agent_ppo2.py:138][0m Average TRAINING episode reward: 604.24
[32m[20221214 00:42:42 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 675.89
[32m[20221214 00:42:42 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 788.62
[32m[20221214 00:42:42 @agent_ppo2.py:143][0m Total time:      45.19 min
[32m[20221214 00:42:42 @agent_ppo2.py:145][0m 4024320 total steps have happened
[32m[20221214 00:42:42 @agent_ppo2.py:121][0m #------------------------ Iteration 5965 --------------------------#
[32m[20221214 00:42:42 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:42:42 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:42:42 @agent_ppo2.py:185][0m |           0.0014 |          91.8594 |        -259.2896 |
[32m[20221214 00:42:42 @agent_ppo2.py:185][0m |           0.0079 |          85.6947 |        -258.2594 |
[32m[20221214 00:42:42 @agent_ppo2.py:185][0m |          -0.0031 |          76.5900 |        -258.2567 |
[32m[20221214 00:42:42 @agent_ppo2.py:185][0m |          -0.0099 |          74.8461 |        -258.9188 |
[32m[20221214 00:42:42 @agent_ppo2.py:185][0m |          -0.0103 |          74.1004 |        -258.8886 |
[32m[20221214 00:42:42 @agent_ppo2.py:185][0m |          -0.0075 |          73.4654 |        -257.2578 |
[32m[20221214 00:42:43 @agent_ppo2.py:185][0m |          -0.0047 |          73.7813 |        -258.1540 |
[32m[20221214 00:42:43 @agent_ppo2.py:185][0m |          -0.0082 |          72.9859 |        -257.6715 |
[32m[20221214 00:42:43 @agent_ppo2.py:185][0m |          -0.0080 |          72.3110 |        -257.1032 |
[32m[20221214 00:42:43 @agent_ppo2.py:185][0m |          -0.0151 |          71.4117 |        -256.8562 |
[32m[20221214 00:42:43 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 00:42:43 @agent_ppo2.py:138][0m Average TRAINING episode reward: 572.39
[32m[20221214 00:42:43 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 843.74
[32m[20221214 00:42:43 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 629.43
[32m[20221214 00:42:43 @agent_ppo2.py:143][0m Total time:      45.21 min
[32m[20221214 00:42:43 @agent_ppo2.py:145][0m 4026368 total steps have happened
[32m[20221214 00:42:43 @agent_ppo2.py:121][0m #------------------------ Iteration 5966 --------------------------#
[32m[20221214 00:42:43 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:42:43 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:42:43 @agent_ppo2.py:185][0m |           0.0040 |          90.1408 |        -262.8887 |
[32m[20221214 00:42:43 @agent_ppo2.py:185][0m |          -0.0027 |          83.0323 |        -261.4856 |
[32m[20221214 00:42:44 @agent_ppo2.py:185][0m |          -0.0061 |          79.3631 |        -261.3569 |
[32m[20221214 00:42:44 @agent_ppo2.py:185][0m |          -0.0111 |          77.5688 |        -261.0312 |
[32m[20221214 00:42:44 @agent_ppo2.py:185][0m |          -0.0051 |          79.8612 |        -260.5030 |
[32m[20221214 00:42:44 @agent_ppo2.py:185][0m |          -0.0129 |          75.2021 |        -260.3376 |
[32m[20221214 00:42:44 @agent_ppo2.py:185][0m |          -0.0159 |          74.2873 |        -260.0593 |
[32m[20221214 00:42:44 @agent_ppo2.py:185][0m |          -0.0176 |          73.3486 |        -260.5510 |
[32m[20221214 00:42:44 @agent_ppo2.py:185][0m |          -0.0165 |          72.7916 |        -259.2418 |
[32m[20221214 00:42:44 @agent_ppo2.py:185][0m |          -0.0127 |          72.1879 |        -259.8298 |
[32m[20221214 00:42:44 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:42:44 @agent_ppo2.py:138][0m Average TRAINING episode reward: 431.31
[32m[20221214 00:42:44 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 735.03
[32m[20221214 00:42:44 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 742.55
[32m[20221214 00:42:44 @agent_ppo2.py:143][0m Total time:      45.23 min
[32m[20221214 00:42:44 @agent_ppo2.py:145][0m 4028416 total steps have happened
[32m[20221214 00:42:44 @agent_ppo2.py:121][0m #------------------------ Iteration 5967 --------------------------#
[32m[20221214 00:42:45 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:42:45 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:42:45 @agent_ppo2.py:185][0m |           0.0124 |         150.1479 |        -264.1558 |
[32m[20221214 00:42:45 @agent_ppo2.py:185][0m |           0.0001 |         136.6043 |        -262.4469 |
[32m[20221214 00:42:45 @agent_ppo2.py:185][0m |          -0.0104 |         134.1761 |        -261.8877 |
[32m[20221214 00:42:45 @agent_ppo2.py:185][0m |          -0.0129 |         132.8397 |        -262.1228 |
[32m[20221214 00:42:45 @agent_ppo2.py:185][0m |          -0.0064 |         133.8844 |        -260.7538 |
[32m[20221214 00:42:45 @agent_ppo2.py:185][0m |          -0.0078 |         131.3915 |        -259.8088 |
[32m[20221214 00:42:45 @agent_ppo2.py:185][0m |          -0.0080 |         130.7441 |        -260.2874 |
[32m[20221214 00:42:45 @agent_ppo2.py:185][0m |           0.0054 |         150.7068 |        -259.6563 |
[32m[20221214 00:42:46 @agent_ppo2.py:185][0m |           0.0009 |         145.6997 |        -259.3317 |
[32m[20221214 00:42:46 @agent_ppo2.py:185][0m |          -0.0138 |         129.6702 |        -259.0293 |
[32m[20221214 00:42:46 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:42:46 @agent_ppo2.py:138][0m Average TRAINING episode reward: 678.14
[32m[20221214 00:42:46 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 752.71
[32m[20221214 00:42:46 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 390.95
[32m[20221214 00:42:46 @agent_ppo2.py:143][0m Total time:      45.26 min
[32m[20221214 00:42:46 @agent_ppo2.py:145][0m 4030464 total steps have happened
[32m[20221214 00:42:46 @agent_ppo2.py:121][0m #------------------------ Iteration 5968 --------------------------#
[32m[20221214 00:42:46 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:42:46 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:42:46 @agent_ppo2.py:185][0m |           0.0028 |         142.8236 |        -239.9278 |
[32m[20221214 00:42:46 @agent_ppo2.py:185][0m |          -0.0013 |         132.8299 |        -239.3093 |
[32m[20221214 00:42:46 @agent_ppo2.py:185][0m |          -0.0054 |         129.3509 |        -239.0474 |
[32m[20221214 00:42:47 @agent_ppo2.py:185][0m |          -0.0036 |         127.1446 |        -238.1341 |
[32m[20221214 00:42:47 @agent_ppo2.py:185][0m |          -0.0018 |         128.7279 |        -239.0904 |
[32m[20221214 00:42:47 @agent_ppo2.py:185][0m |          -0.0057 |         127.1968 |        -236.3377 |
[32m[20221214 00:42:47 @agent_ppo2.py:185][0m |          -0.0058 |         133.4385 |        -237.9204 |
[32m[20221214 00:42:47 @agent_ppo2.py:185][0m |          -0.0064 |         124.0144 |        -236.6334 |
[32m[20221214 00:42:47 @agent_ppo2.py:185][0m |          -0.0113 |         123.0659 |        -236.0936 |
[32m[20221214 00:42:47 @agent_ppo2.py:185][0m |          -0.0131 |         123.2207 |        -236.4339 |
[32m[20221214 00:42:47 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:42:47 @agent_ppo2.py:138][0m Average TRAINING episode reward: 636.00
[32m[20221214 00:42:47 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 744.33
[32m[20221214 00:42:47 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 788.75
[32m[20221214 00:42:47 @agent_ppo2.py:143][0m Total time:      45.28 min
[32m[20221214 00:42:47 @agent_ppo2.py:145][0m 4032512 total steps have happened
[32m[20221214 00:42:47 @agent_ppo2.py:121][0m #------------------------ Iteration 5969 --------------------------#
[32m[20221214 00:42:47 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:42:48 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:42:48 @agent_ppo2.py:185][0m |           0.0028 |         154.1973 |        -238.0520 |
[32m[20221214 00:42:48 @agent_ppo2.py:185][0m |          -0.0043 |         146.8928 |        -238.4334 |
[32m[20221214 00:42:48 @agent_ppo2.py:185][0m |          -0.0059 |         142.2437 |        -238.2434 |
[32m[20221214 00:42:48 @agent_ppo2.py:185][0m |          -0.0077 |         139.4367 |        -237.8455 |
[32m[20221214 00:42:48 @agent_ppo2.py:185][0m |          -0.0095 |         137.4167 |        -238.8225 |
[32m[20221214 00:42:48 @agent_ppo2.py:185][0m |          -0.0106 |         136.1242 |        -238.2470 |
[32m[20221214 00:42:48 @agent_ppo2.py:185][0m |          -0.0116 |         134.8550 |        -238.2036 |
[32m[20221214 00:42:48 @agent_ppo2.py:185][0m |          -0.0121 |         133.1479 |        -238.2855 |
[32m[20221214 00:42:48 @agent_ppo2.py:185][0m |          -0.0162 |         133.0615 |        -237.6700 |
[32m[20221214 00:42:49 @agent_ppo2.py:185][0m |          -0.0077 |         131.4330 |        -237.8835 |
[32m[20221214 00:42:49 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:42:49 @agent_ppo2.py:138][0m Average TRAINING episode reward: 653.24
[32m[20221214 00:42:49 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 715.22
[32m[20221214 00:42:49 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 658.51
[32m[20221214 00:42:49 @agent_ppo2.py:143][0m Total time:      45.31 min
[32m[20221214 00:42:49 @agent_ppo2.py:145][0m 4034560 total steps have happened
[32m[20221214 00:42:49 @agent_ppo2.py:121][0m #------------------------ Iteration 5970 --------------------------#
[32m[20221214 00:42:49 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:42:49 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:42:49 @agent_ppo2.py:185][0m |           0.0029 |         142.3456 |        -243.4449 |
[32m[20221214 00:42:49 @agent_ppo2.py:185][0m |          -0.0030 |         135.4562 |        -242.8382 |
[32m[20221214 00:42:49 @agent_ppo2.py:185][0m |          -0.0053 |         133.4937 |        -243.2511 |
[32m[20221214 00:42:49 @agent_ppo2.py:185][0m |          -0.0045 |         132.8910 |        -243.2412 |
[32m[20221214 00:42:49 @agent_ppo2.py:185][0m |           0.0049 |         133.9715 |        -243.2649 |
[32m[20221214 00:42:50 @agent_ppo2.py:185][0m |          -0.0043 |         131.4313 |        -243.5412 |
[32m[20221214 00:42:50 @agent_ppo2.py:185][0m |           0.0042 |         138.1864 |        -242.4586 |
[32m[20221214 00:42:50 @agent_ppo2.py:185][0m |          -0.0029 |         131.5448 |        -243.1988 |
[32m[20221214 00:42:50 @agent_ppo2.py:185][0m |          -0.0104 |         129.0146 |        -243.3774 |
[32m[20221214 00:42:50 @agent_ppo2.py:185][0m |          -0.0098 |         128.9094 |        -243.3517 |
[32m[20221214 00:42:50 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:42:50 @agent_ppo2.py:138][0m Average TRAINING episode reward: 671.80
[32m[20221214 00:42:50 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 782.91
[32m[20221214 00:42:50 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 698.42
[32m[20221214 00:42:50 @agent_ppo2.py:143][0m Total time:      45.33 min
[32m[20221214 00:42:50 @agent_ppo2.py:145][0m 4036608 total steps have happened
[32m[20221214 00:42:50 @agent_ppo2.py:121][0m #------------------------ Iteration 5971 --------------------------#
[32m[20221214 00:42:50 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:42:50 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:42:50 @agent_ppo2.py:185][0m |           0.0019 |         110.7702 |        -247.3707 |
[32m[20221214 00:42:51 @agent_ppo2.py:185][0m |           0.0023 |         103.9556 |        -246.2034 |
[32m[20221214 00:42:51 @agent_ppo2.py:185][0m |          -0.0047 |         101.3287 |        -247.0726 |
[32m[20221214 00:42:51 @agent_ppo2.py:185][0m |          -0.0005 |         107.7524 |        -247.0608 |
[32m[20221214 00:42:51 @agent_ppo2.py:185][0m |          -0.0064 |          98.7599 |        -247.2095 |
[32m[20221214 00:42:51 @agent_ppo2.py:185][0m |          -0.0071 |          98.8009 |        -247.9157 |
[32m[20221214 00:42:51 @agent_ppo2.py:185][0m |          -0.0085 |          97.5921 |        -248.2829 |
[32m[20221214 00:42:51 @agent_ppo2.py:185][0m |          -0.0066 |          96.3556 |        -246.7812 |
[32m[20221214 00:42:51 @agent_ppo2.py:185][0m |          -0.0117 |          96.0652 |        -248.0829 |
[32m[20221214 00:42:51 @agent_ppo2.py:185][0m |          -0.0138 |          95.7212 |        -248.0365 |
[32m[20221214 00:42:51 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:42:51 @agent_ppo2.py:138][0m Average TRAINING episode reward: 553.70
[32m[20221214 00:42:51 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 802.82
[32m[20221214 00:42:51 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 838.20
[32m[20221214 00:42:51 @agent_ppo2.py:143][0m Total time:      45.35 min
[32m[20221214 00:42:51 @agent_ppo2.py:145][0m 4038656 total steps have happened
[32m[20221214 00:42:51 @agent_ppo2.py:121][0m #------------------------ Iteration 5972 --------------------------#
[32m[20221214 00:42:52 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:42:52 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:42:52 @agent_ppo2.py:185][0m |          -0.0015 |         184.4817 |        -254.0619 |
[32m[20221214 00:42:52 @agent_ppo2.py:185][0m |           0.0006 |         179.1551 |        -254.2029 |
[32m[20221214 00:42:52 @agent_ppo2.py:185][0m |          -0.0011 |         177.6097 |        -255.5218 |
[32m[20221214 00:42:52 @agent_ppo2.py:185][0m |          -0.0014 |         176.2045 |        -255.5051 |
[32m[20221214 00:42:52 @agent_ppo2.py:185][0m |          -0.0001 |         175.7617 |        -254.8338 |
[32m[20221214 00:42:52 @agent_ppo2.py:185][0m |           0.0134 |         197.7327 |        -255.3220 |
[32m[20221214 00:42:52 @agent_ppo2.py:185][0m |          -0.0046 |         174.7710 |        -255.2648 |
[32m[20221214 00:42:52 @agent_ppo2.py:185][0m |          -0.0050 |         173.4051 |        -254.3222 |
[32m[20221214 00:42:53 @agent_ppo2.py:185][0m |           0.0011 |         173.1462 |        -254.6869 |
[32m[20221214 00:42:53 @agent_ppo2.py:185][0m |           0.0076 |         185.8689 |        -254.8316 |
[32m[20221214 00:42:53 @agent_ppo2.py:130][0m Policy update time: 1.05 s
[32m[20221214 00:42:53 @agent_ppo2.py:138][0m Average TRAINING episode reward: 751.60
[32m[20221214 00:42:53 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 831.69
[32m[20221214 00:42:53 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 809.24
[32m[20221214 00:42:53 @agent_ppo2.py:143][0m Total time:      45.37 min
[32m[20221214 00:42:53 @agent_ppo2.py:145][0m 4040704 total steps have happened
[32m[20221214 00:42:53 @agent_ppo2.py:121][0m #------------------------ Iteration 5973 --------------------------#
[32m[20221214 00:42:53 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:42:53 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:42:53 @agent_ppo2.py:185][0m |           0.0134 |         172.6863 |        -253.2571 |
[32m[20221214 00:42:53 @agent_ppo2.py:185][0m |          -0.0021 |         144.5762 |        -252.9670 |
[32m[20221214 00:42:53 @agent_ppo2.py:185][0m |          -0.0058 |         142.6104 |        -253.4230 |
[32m[20221214 00:42:53 @agent_ppo2.py:185][0m |          -0.0052 |         142.0652 |        -253.7934 |
[32m[20221214 00:42:54 @agent_ppo2.py:185][0m |          -0.0062 |         141.2170 |        -252.4858 |
[32m[20221214 00:42:54 @agent_ppo2.py:185][0m |          -0.0054 |         140.3286 |        -253.7191 |
[32m[20221214 00:42:54 @agent_ppo2.py:185][0m |          -0.0041 |         139.7851 |        -252.3501 |
[32m[20221214 00:42:54 @agent_ppo2.py:185][0m |          -0.0032 |         139.5221 |        -252.9358 |
[32m[20221214 00:42:54 @agent_ppo2.py:185][0m |          -0.0087 |         139.0601 |        -253.5320 |
[32m[20221214 00:42:54 @agent_ppo2.py:185][0m |          -0.0020 |         139.3308 |        -252.0770 |
[32m[20221214 00:42:54 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:42:54 @agent_ppo2.py:138][0m Average TRAINING episode reward: 530.41
[32m[20221214 00:42:54 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 698.04
[32m[20221214 00:42:54 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 773.55
[32m[20221214 00:42:54 @agent_ppo2.py:143][0m Total time:      45.40 min
[32m[20221214 00:42:54 @agent_ppo2.py:145][0m 4042752 total steps have happened
[32m[20221214 00:42:54 @agent_ppo2.py:121][0m #------------------------ Iteration 5974 --------------------------#
[32m[20221214 00:42:54 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:42:54 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:42:55 @agent_ppo2.py:185][0m |           0.0015 |         152.3465 |        -245.4494 |
[32m[20221214 00:42:55 @agent_ppo2.py:185][0m |          -0.0004 |         144.3219 |        -247.5299 |
[32m[20221214 00:42:55 @agent_ppo2.py:185][0m |          -0.0083 |         139.7754 |        -246.8555 |
[32m[20221214 00:42:55 @agent_ppo2.py:185][0m |          -0.0029 |         137.9918 |        -245.3272 |
[32m[20221214 00:42:55 @agent_ppo2.py:185][0m |          -0.0028 |         136.8211 |        -245.7316 |
[32m[20221214 00:42:55 @agent_ppo2.py:185][0m |          -0.0052 |         135.5427 |        -245.3059 |
[32m[20221214 00:42:55 @agent_ppo2.py:185][0m |          -0.0101 |         135.3111 |        -245.1235 |
[32m[20221214 00:42:55 @agent_ppo2.py:185][0m |          -0.0088 |         133.9224 |        -246.0120 |
[32m[20221214 00:42:55 @agent_ppo2.py:185][0m |          -0.0094 |         133.7130 |        -244.9369 |
[32m[20221214 00:42:55 @agent_ppo2.py:185][0m |          -0.0087 |         133.2583 |        -245.8117 |
[32m[20221214 00:42:55 @agent_ppo2.py:130][0m Policy update time: 1.06 s
[32m[20221214 00:42:56 @agent_ppo2.py:138][0m Average TRAINING episode reward: 656.16
[32m[20221214 00:42:56 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 738.11
[32m[20221214 00:42:56 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 635.09
[32m[20221214 00:42:56 @agent_ppo2.py:143][0m Total time:      45.42 min
[32m[20221214 00:42:56 @agent_ppo2.py:145][0m 4044800 total steps have happened
[32m[20221214 00:42:56 @agent_ppo2.py:121][0m #------------------------ Iteration 5975 --------------------------#
[32m[20221214 00:42:56 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:42:56 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:42:56 @agent_ppo2.py:185][0m |           0.0137 |         159.3193 |        -245.0435 |
[32m[20221214 00:42:56 @agent_ppo2.py:185][0m |           0.0038 |         137.6192 |        -244.6268 |
[32m[20221214 00:42:56 @agent_ppo2.py:185][0m |          -0.0004 |         132.7878 |        -242.7498 |
[32m[20221214 00:42:56 @agent_ppo2.py:185][0m |          -0.0014 |         131.5706 |        -242.3176 |
[32m[20221214 00:42:56 @agent_ppo2.py:185][0m |          -0.0033 |         130.3699 |        -242.3000 |
[32m[20221214 00:42:56 @agent_ppo2.py:185][0m |          -0.0104 |         130.0480 |        -241.6950 |
[32m[20221214 00:42:57 @agent_ppo2.py:185][0m |          -0.0054 |         129.9190 |        -240.5092 |
[32m[20221214 00:42:57 @agent_ppo2.py:185][0m |          -0.0076 |         129.5553 |        -241.6342 |
[32m[20221214 00:42:57 @agent_ppo2.py:185][0m |          -0.0025 |         136.2001 |        -240.5829 |
[32m[20221214 00:42:57 @agent_ppo2.py:185][0m |          -0.0072 |         129.0017 |        -239.8862 |
[32m[20221214 00:42:57 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:42:57 @agent_ppo2.py:138][0m Average TRAINING episode reward: 630.36
[32m[20221214 00:42:57 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 771.23
[32m[20221214 00:42:57 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 321.14
[32m[20221214 00:42:57 @agent_ppo2.py:143][0m Total time:      45.44 min
[32m[20221214 00:42:57 @agent_ppo2.py:145][0m 4046848 total steps have happened
[32m[20221214 00:42:57 @agent_ppo2.py:121][0m #------------------------ Iteration 5976 --------------------------#
[32m[20221214 00:42:57 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:42:57 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:42:57 @agent_ppo2.py:185][0m |           0.0036 |         161.8906 |        -249.6361 |
[32m[20221214 00:42:57 @agent_ppo2.py:185][0m |           0.0042 |         157.5620 |        -248.2234 |
[32m[20221214 00:42:58 @agent_ppo2.py:185][0m |          -0.0030 |         156.6390 |        -248.0190 |
[32m[20221214 00:42:58 @agent_ppo2.py:185][0m |           0.0004 |         154.9296 |        -247.5161 |
[32m[20221214 00:42:58 @agent_ppo2.py:185][0m |          -0.0025 |         155.1124 |        -248.0493 |
[32m[20221214 00:42:58 @agent_ppo2.py:185][0m |          -0.0023 |         154.7465 |        -247.5220 |
[32m[20221214 00:42:58 @agent_ppo2.py:185][0m |          -0.0049 |         154.7371 |        -248.0178 |
[32m[20221214 00:42:58 @agent_ppo2.py:185][0m |          -0.0065 |         155.2147 |        -247.0090 |
[32m[20221214 00:42:58 @agent_ppo2.py:185][0m |          -0.0026 |         154.6926 |        -246.6893 |
[32m[20221214 00:42:58 @agent_ppo2.py:185][0m |           0.0011 |         159.2270 |        -246.8032 |
[32m[20221214 00:42:58 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:42:58 @agent_ppo2.py:138][0m Average TRAINING episode reward: 885.93
[32m[20221214 00:42:58 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 886.54
[32m[20221214 00:42:58 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 797.98
[32m[20221214 00:42:58 @agent_ppo2.py:143][0m Total time:      45.47 min
[32m[20221214 00:42:58 @agent_ppo2.py:145][0m 4048896 total steps have happened
[32m[20221214 00:42:58 @agent_ppo2.py:121][0m #------------------------ Iteration 5977 --------------------------#
[32m[20221214 00:42:59 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:42:59 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:42:59 @agent_ppo2.py:185][0m |           0.0016 |         165.6303 |        -241.7862 |
[32m[20221214 00:42:59 @agent_ppo2.py:185][0m |          -0.0019 |         162.3521 |        -240.4970 |
[32m[20221214 00:42:59 @agent_ppo2.py:185][0m |          -0.0027 |         162.0785 |        -241.7516 |
[32m[20221214 00:42:59 @agent_ppo2.py:185][0m |           0.0146 |         185.2141 |        -242.3119 |
[32m[20221214 00:42:59 @agent_ppo2.py:185][0m |          -0.0056 |         161.4805 |        -241.1676 |
[32m[20221214 00:42:59 @agent_ppo2.py:185][0m |          -0.0034 |         159.9708 |        -240.7651 |
[32m[20221214 00:42:59 @agent_ppo2.py:185][0m |          -0.0024 |         159.5994 |        -242.5325 |
[32m[20221214 00:42:59 @agent_ppo2.py:185][0m |          -0.0076 |         160.2905 |        -243.7484 |
[32m[20221214 00:43:00 @agent_ppo2.py:185][0m |          -0.0091 |         159.0782 |        -243.3296 |
[32m[20221214 00:43:00 @agent_ppo2.py:185][0m |          -0.0071 |         159.4067 |        -244.1932 |
[32m[20221214 00:43:00 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:43:00 @agent_ppo2.py:138][0m Average TRAINING episode reward: 741.23
[32m[20221214 00:43:00 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 801.02
[32m[20221214 00:43:00 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 691.80
[32m[20221214 00:43:00 @agent_ppo2.py:143][0m Total time:      45.49 min
[32m[20221214 00:43:00 @agent_ppo2.py:145][0m 4050944 total steps have happened
[32m[20221214 00:43:00 @agent_ppo2.py:121][0m #------------------------ Iteration 5978 --------------------------#
[32m[20221214 00:43:00 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:43:00 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:43:00 @agent_ppo2.py:185][0m |          -0.0009 |         130.0084 |        -253.4007 |
[32m[20221214 00:43:00 @agent_ppo2.py:185][0m |          -0.0046 |         119.0704 |        -252.3879 |
[32m[20221214 00:43:00 @agent_ppo2.py:185][0m |          -0.0131 |         115.2508 |        -252.8626 |
[32m[20221214 00:43:00 @agent_ppo2.py:185][0m |          -0.0111 |         112.6027 |        -252.5564 |
[32m[20221214 00:43:01 @agent_ppo2.py:185][0m |          -0.0170 |         111.6210 |        -252.0165 |
[32m[20221214 00:43:01 @agent_ppo2.py:185][0m |          -0.0096 |         110.5370 |        -251.4794 |
[32m[20221214 00:43:01 @agent_ppo2.py:185][0m |          -0.0136 |         109.9913 |        -251.2431 |
[32m[20221214 00:43:01 @agent_ppo2.py:185][0m |          -0.0195 |         108.8748 |        -250.7556 |
[32m[20221214 00:43:01 @agent_ppo2.py:185][0m |          -0.0171 |         108.0296 |        -250.8016 |
[32m[20221214 00:43:01 @agent_ppo2.py:185][0m |          -0.0176 |         107.2740 |        -250.2188 |
[32m[20221214 00:43:01 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:43:01 @agent_ppo2.py:138][0m Average TRAINING episode reward: 414.78
[32m[20221214 00:43:01 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 649.68
[32m[20221214 00:43:01 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 698.88
[32m[20221214 00:43:01 @agent_ppo2.py:143][0m Total time:      45.51 min
[32m[20221214 00:43:01 @agent_ppo2.py:145][0m 4052992 total steps have happened
[32m[20221214 00:43:01 @agent_ppo2.py:121][0m #------------------------ Iteration 5979 --------------------------#
[32m[20221214 00:43:01 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:43:01 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:43:02 @agent_ppo2.py:185][0m |           0.0031 |         119.1738 |        -251.7276 |
[32m[20221214 00:43:02 @agent_ppo2.py:185][0m |           0.0047 |         111.8536 |        -252.1026 |
[32m[20221214 00:43:02 @agent_ppo2.py:185][0m |          -0.0008 |         109.0647 |        -250.8697 |
[32m[20221214 00:43:02 @agent_ppo2.py:185][0m |          -0.0017 |         109.3864 |        -250.3319 |
[32m[20221214 00:43:02 @agent_ppo2.py:185][0m |          -0.0030 |         105.5668 |        -248.5027 |
[32m[20221214 00:43:02 @agent_ppo2.py:185][0m |          -0.0061 |         104.8196 |        -248.6458 |
[32m[20221214 00:43:02 @agent_ppo2.py:185][0m |          -0.0097 |         104.2593 |        -248.6855 |
[32m[20221214 00:43:02 @agent_ppo2.py:185][0m |          -0.0073 |         103.7911 |        -248.5648 |
[32m[20221214 00:43:02 @agent_ppo2.py:185][0m |          -0.0058 |         103.8563 |        -248.1690 |
[32m[20221214 00:43:02 @agent_ppo2.py:185][0m |          -0.0085 |         102.8387 |        -247.5362 |
[32m[20221214 00:43:02 @agent_ppo2.py:130][0m Policy update time: 1.13 s
[32m[20221214 00:43:03 @agent_ppo2.py:138][0m Average TRAINING episode reward: 569.58
[32m[20221214 00:43:03 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 756.18
[32m[20221214 00:43:03 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 845.73
[32m[20221214 00:43:03 @agent_ppo2.py:143][0m Total time:      45.54 min
[32m[20221214 00:43:03 @agent_ppo2.py:145][0m 4055040 total steps have happened
[32m[20221214 00:43:03 @agent_ppo2.py:121][0m #------------------------ Iteration 5980 --------------------------#
[32m[20221214 00:43:03 @agent_ppo2.py:127][0m Sampling time: 0.18 s by 5 slaves
[32m[20221214 00:43:03 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:43:03 @agent_ppo2.py:185][0m |           0.0059 |         152.4840 |        -258.3007 |
[32m[20221214 00:43:03 @agent_ppo2.py:185][0m |           0.0065 |         142.0161 |        -257.9150 |
[32m[20221214 00:43:03 @agent_ppo2.py:185][0m |          -0.0014 |         138.5838 |        -258.0555 |
[32m[20221214 00:43:03 @agent_ppo2.py:185][0m |          -0.0002 |         136.6314 |        -258.6497 |
[32m[20221214 00:43:03 @agent_ppo2.py:185][0m |          -0.0077 |         136.5758 |        -257.7504 |
[32m[20221214 00:43:03 @agent_ppo2.py:185][0m |          -0.0034 |         135.2328 |        -256.9957 |
[32m[20221214 00:43:04 @agent_ppo2.py:185][0m |          -0.0085 |         135.2062 |        -256.7722 |
[32m[20221214 00:43:04 @agent_ppo2.py:185][0m |          -0.0047 |         134.2263 |        -257.2201 |
[32m[20221214 00:43:04 @agent_ppo2.py:185][0m |          -0.0065 |         134.0706 |        -256.7616 |
[32m[20221214 00:43:04 @agent_ppo2.py:185][0m |          -0.0072 |         134.4036 |        -255.8974 |
[32m[20221214 00:43:04 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:43:04 @agent_ppo2.py:138][0m Average TRAINING episode reward: 703.86
[32m[20221214 00:43:04 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 839.19
[32m[20221214 00:43:04 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 619.23
[32m[20221214 00:43:04 @agent_ppo2.py:143][0m Total time:      45.56 min
[32m[20221214 00:43:04 @agent_ppo2.py:145][0m 4057088 total steps have happened
[32m[20221214 00:43:04 @agent_ppo2.py:121][0m #------------------------ Iteration 5981 --------------------------#
[32m[20221214 00:43:04 @agent_ppo2.py:127][0m Sampling time: 0.15 s by 5 slaves
[32m[20221214 00:43:04 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:43:04 @agent_ppo2.py:185][0m |           0.0006 |         164.9949 |        -244.7129 |
[32m[20221214 00:43:04 @agent_ppo2.py:185][0m |          -0.0037 |         159.5313 |        -244.8348 |
[32m[20221214 00:43:05 @agent_ppo2.py:185][0m |          -0.0065 |         157.5800 |        -243.5183 |
[32m[20221214 00:43:05 @agent_ppo2.py:185][0m |          -0.0066 |         156.6711 |        -243.3603 |
[32m[20221214 00:43:05 @agent_ppo2.py:185][0m |          -0.0071 |         156.1985 |        -242.2402 |
[32m[20221214 00:43:05 @agent_ppo2.py:185][0m |          -0.0078 |         155.2545 |        -241.8479 |
[32m[20221214 00:43:05 @agent_ppo2.py:185][0m |          -0.0085 |         155.1330 |        -240.8809 |
[32m[20221214 00:43:05 @agent_ppo2.py:185][0m |          -0.0100 |         154.2261 |        -241.9907 |
[32m[20221214 00:43:05 @agent_ppo2.py:185][0m |          -0.0041 |         156.4608 |        -241.6912 |
[32m[20221214 00:43:05 @agent_ppo2.py:185][0m |          -0.0097 |         153.8506 |        -240.7480 |
[32m[20221214 00:43:05 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:43:05 @agent_ppo2.py:138][0m Average TRAINING episode reward: 752.65
[32m[20221214 00:43:05 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 827.60
[32m[20221214 00:43:05 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 684.96
[32m[20221214 00:43:05 @agent_ppo2.py:143][0m Total time:      45.58 min
[32m[20221214 00:43:05 @agent_ppo2.py:145][0m 4059136 total steps have happened
[32m[20221214 00:43:05 @agent_ppo2.py:121][0m #------------------------ Iteration 5982 --------------------------#
[32m[20221214 00:43:06 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:43:06 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:43:06 @agent_ppo2.py:185][0m |           0.0065 |         115.8592 |        -235.3099 |
[32m[20221214 00:43:06 @agent_ppo2.py:185][0m |           0.0017 |         107.8424 |        -234.0725 |
[32m[20221214 00:43:06 @agent_ppo2.py:185][0m |          -0.0086 |         103.7976 |        -233.9971 |
[32m[20221214 00:43:06 @agent_ppo2.py:185][0m |          -0.0040 |         102.7178 |        -232.9412 |
[32m[20221214 00:43:06 @agent_ppo2.py:185][0m |          -0.0108 |         102.4396 |        -233.3537 |
[32m[20221214 00:43:06 @agent_ppo2.py:185][0m |          -0.0073 |         102.4455 |        -233.1313 |
[32m[20221214 00:43:06 @agent_ppo2.py:185][0m |          -0.0054 |         100.3614 |        -232.9105 |
[32m[20221214 00:43:06 @agent_ppo2.py:185][0m |          -0.0137 |          99.0103 |        -233.3969 |
[32m[20221214 00:43:07 @agent_ppo2.py:185][0m |          -0.0156 |          99.0160 |        -232.8245 |
[32m[20221214 00:43:07 @agent_ppo2.py:185][0m |          -0.0127 |          98.0151 |        -232.7178 |
[32m[20221214 00:43:07 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:43:07 @agent_ppo2.py:138][0m Average TRAINING episode reward: 516.84
[32m[20221214 00:43:07 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 694.60
[32m[20221214 00:43:07 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 812.06
[32m[20221214 00:43:07 @agent_ppo2.py:143][0m Total time:      45.61 min
[32m[20221214 00:43:07 @agent_ppo2.py:145][0m 4061184 total steps have happened
[32m[20221214 00:43:07 @agent_ppo2.py:121][0m #------------------------ Iteration 5983 --------------------------#
[32m[20221214 00:43:07 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:43:07 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:43:07 @agent_ppo2.py:185][0m |           0.0006 |         144.4220 |        -235.1063 |
[32m[20221214 00:43:07 @agent_ppo2.py:185][0m |          -0.0045 |         139.2577 |        -234.3754 |
[32m[20221214 00:43:07 @agent_ppo2.py:185][0m |          -0.0017 |         138.8897 |        -234.4179 |
[32m[20221214 00:43:07 @agent_ppo2.py:185][0m |          -0.0058 |         138.3774 |        -235.1734 |
[32m[20221214 00:43:08 @agent_ppo2.py:185][0m |          -0.0007 |         140.9649 |        -233.7634 |
[32m[20221214 00:43:08 @agent_ppo2.py:185][0m |           0.0053 |         144.0293 |        -235.1910 |
[32m[20221214 00:43:08 @agent_ppo2.py:185][0m |          -0.0063 |         137.9609 |        -234.3774 |
[32m[20221214 00:43:08 @agent_ppo2.py:185][0m |           0.0029 |         139.0991 |        -233.2387 |
[32m[20221214 00:43:08 @agent_ppo2.py:185][0m |          -0.0032 |         138.1572 |        -231.7497 |
[32m[20221214 00:43:08 @agent_ppo2.py:185][0m |           0.0216 |         159.3874 |        -232.1161 |
[32m[20221214 00:43:08 @agent_ppo2.py:130][0m Policy update time: 1.14 s
[32m[20221214 00:43:08 @agent_ppo2.py:138][0m Average TRAINING episode reward: 790.87
[32m[20221214 00:43:08 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 830.67
[32m[20221214 00:43:08 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 353.20
[32m[20221214 00:43:08 @agent_ppo2.py:143][0m Total time:      45.63 min
[32m[20221214 00:43:08 @agent_ppo2.py:145][0m 4063232 total steps have happened
[32m[20221214 00:43:08 @agent_ppo2.py:121][0m #------------------------ Iteration 5984 --------------------------#
[32m[20221214 00:43:08 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:43:09 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:43:09 @agent_ppo2.py:185][0m |           0.0013 |         112.0056 |        -230.1247 |
[32m[20221214 00:43:09 @agent_ppo2.py:185][0m |           0.0014 |         106.4162 |        -230.3163 |
[32m[20221214 00:43:09 @agent_ppo2.py:185][0m |          -0.0041 |         105.2816 |        -230.1320 |
[32m[20221214 00:43:09 @agent_ppo2.py:185][0m |          -0.0096 |         104.0693 |        -229.8773 |
[32m[20221214 00:43:09 @agent_ppo2.py:185][0m |          -0.0057 |         102.9487 |        -228.8811 |
[32m[20221214 00:43:09 @agent_ppo2.py:185][0m |           0.0000 |         105.9334 |        -229.6042 |
[32m[20221214 00:43:09 @agent_ppo2.py:185][0m |          -0.0113 |         101.8270 |        -228.3199 |
[32m[20221214 00:43:09 @agent_ppo2.py:185][0m |          -0.0094 |         101.5175 |        -228.6186 |
[32m[20221214 00:43:09 @agent_ppo2.py:185][0m |          -0.0133 |         101.2547 |        -229.3823 |
[32m[20221214 00:43:10 @agent_ppo2.py:185][0m |          -0.0122 |         100.7170 |        -228.0518 |
[32m[20221214 00:43:10 @agent_ppo2.py:130][0m Policy update time: 1.12 s
[32m[20221214 00:43:10 @agent_ppo2.py:138][0m Average TRAINING episode reward: 521.54
[32m[20221214 00:43:10 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 759.05
[32m[20221214 00:43:10 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 707.29
[32m[20221214 00:43:10 @agent_ppo2.py:143][0m Total time:      45.66 min
[32m[20221214 00:43:10 @agent_ppo2.py:145][0m 4065280 total steps have happened
[32m[20221214 00:43:10 @agent_ppo2.py:121][0m #------------------------ Iteration 5985 --------------------------#
[32m[20221214 00:43:10 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:43:10 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:43:10 @agent_ppo2.py:185][0m |           0.0078 |         138.5672 |        -226.9449 |
[32m[20221214 00:43:10 @agent_ppo2.py:185][0m |           0.0013 |         118.5643 |        -226.4053 |
[32m[20221214 00:43:10 @agent_ppo2.py:185][0m |          -0.0087 |         113.4852 |        -227.6933 |
[32m[20221214 00:43:10 @agent_ppo2.py:185][0m |          -0.0093 |         109.5717 |        -227.7951 |
[32m[20221214 00:43:10 @agent_ppo2.py:185][0m |          -0.0101 |         105.6373 |        -228.3577 |
[32m[20221214 00:43:11 @agent_ppo2.py:185][0m |          -0.0154 |         103.4000 |        -227.2825 |
[32m[20221214 00:43:11 @agent_ppo2.py:185][0m |          -0.0135 |         102.0432 |        -227.8701 |
[32m[20221214 00:43:11 @agent_ppo2.py:185][0m |          -0.0157 |          99.8059 |        -228.9062 |
[32m[20221214 00:43:11 @agent_ppo2.py:185][0m |          -0.0165 |          98.0775 |        -228.6425 |
[32m[20221214 00:43:11 @agent_ppo2.py:185][0m |          -0.0142 |          97.1191 |        -228.5729 |
[32m[20221214 00:43:11 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:43:11 @agent_ppo2.py:138][0m Average TRAINING episode reward: 322.51
[32m[20221214 00:43:11 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 495.36
[32m[20221214 00:43:11 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 752.53
[32m[20221214 00:43:11 @agent_ppo2.py:143][0m Total time:      45.68 min
[32m[20221214 00:43:11 @agent_ppo2.py:145][0m 4067328 total steps have happened
[32m[20221214 00:43:11 @agent_ppo2.py:121][0m #------------------------ Iteration 5986 --------------------------#
[32m[20221214 00:43:11 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:43:11 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:43:11 @agent_ppo2.py:185][0m |           0.0032 |         117.8906 |        -227.4903 |
[32m[20221214 00:43:12 @agent_ppo2.py:185][0m |          -0.0059 |         106.5878 |        -227.0445 |
[32m[20221214 00:43:12 @agent_ppo2.py:185][0m |          -0.0046 |         103.4194 |        -227.9504 |
[32m[20221214 00:43:12 @agent_ppo2.py:185][0m |          -0.0063 |         101.7808 |        -226.9237 |
[32m[20221214 00:43:12 @agent_ppo2.py:185][0m |          -0.0044 |         101.1569 |        -227.2799 |
[32m[20221214 00:43:12 @agent_ppo2.py:185][0m |          -0.0107 |          99.9330 |        -227.6397 |
[32m[20221214 00:43:12 @agent_ppo2.py:185][0m |          -0.0103 |          99.1688 |        -225.3049 |
[32m[20221214 00:43:12 @agent_ppo2.py:185][0m |           0.0136 |         114.3057 |        -226.6386 |
[32m[20221214 00:43:12 @agent_ppo2.py:185][0m |          -0.0130 |          99.4829 |        -226.8688 |
[32m[20221214 00:43:12 @agent_ppo2.py:185][0m |          -0.0187 |          98.0141 |        -227.0198 |
[32m[20221214 00:43:12 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:43:13 @agent_ppo2.py:138][0m Average TRAINING episode reward: 585.62
[32m[20221214 00:43:13 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 736.02
[32m[20221214 00:43:13 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 776.00
[32m[20221214 00:43:13 @agent_ppo2.py:143][0m Total time:      45.70 min
[32m[20221214 00:43:13 @agent_ppo2.py:145][0m 4069376 total steps have happened
[32m[20221214 00:43:13 @agent_ppo2.py:121][0m #------------------------ Iteration 5987 --------------------------#
[32m[20221214 00:43:13 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:43:13 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:43:13 @agent_ppo2.py:185][0m |           0.0018 |         104.1768 |        -233.5728 |
[32m[20221214 00:43:13 @agent_ppo2.py:185][0m |          -0.0002 |          98.1218 |        -232.6275 |
[32m[20221214 00:43:13 @agent_ppo2.py:185][0m |          -0.0058 |          96.3477 |        -233.5255 |
[32m[20221214 00:43:13 @agent_ppo2.py:185][0m |          -0.0064 |          94.6628 |        -233.2023 |
[32m[20221214 00:43:13 @agent_ppo2.py:185][0m |          -0.0099 |          93.4697 |        -234.5368 |
[32m[20221214 00:43:13 @agent_ppo2.py:185][0m |          -0.0082 |          93.0658 |        -234.6765 |
[32m[20221214 00:43:14 @agent_ppo2.py:185][0m |          -0.0033 |          91.9110 |        -234.3993 |
[32m[20221214 00:43:14 @agent_ppo2.py:185][0m |          -0.0131 |          91.1584 |        -235.5663 |
[32m[20221214 00:43:14 @agent_ppo2.py:185][0m |          -0.0118 |          90.4955 |        -236.0333 |
[32m[20221214 00:43:14 @agent_ppo2.py:185][0m |          -0.0155 |          90.2668 |        -235.5826 |
[32m[20221214 00:43:14 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:43:14 @agent_ppo2.py:138][0m Average TRAINING episode reward: 519.52
[32m[20221214 00:43:14 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 699.43
[32m[20221214 00:43:14 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 770.74
[32m[20221214 00:43:14 @agent_ppo2.py:143][0m Total time:      45.73 min
[32m[20221214 00:43:14 @agent_ppo2.py:145][0m 4071424 total steps have happened
[32m[20221214 00:43:14 @agent_ppo2.py:121][0m #------------------------ Iteration 5988 --------------------------#
[32m[20221214 00:43:14 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:43:14 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:43:14 @agent_ppo2.py:185][0m |           0.0078 |         130.6055 |        -246.6376 |
[32m[20221214 00:43:14 @agent_ppo2.py:185][0m |           0.0013 |         122.9794 |        -246.4080 |
[32m[20221214 00:43:15 @agent_ppo2.py:185][0m |          -0.0045 |         118.3584 |        -246.5150 |
[32m[20221214 00:43:15 @agent_ppo2.py:185][0m |          -0.0027 |         117.4824 |        -245.9746 |
[32m[20221214 00:43:15 @agent_ppo2.py:185][0m |          -0.0025 |         115.6095 |        -245.7938 |
[32m[20221214 00:43:15 @agent_ppo2.py:185][0m |           0.0013 |         114.6903 |        -245.1119 |
[32m[20221214 00:43:15 @agent_ppo2.py:185][0m |          -0.0065 |         114.1751 |        -245.8842 |
[32m[20221214 00:43:15 @agent_ppo2.py:185][0m |          -0.0076 |         113.9414 |        -243.8880 |
[32m[20221214 00:43:15 @agent_ppo2.py:185][0m |          -0.0071 |         113.0043 |        -244.5627 |
[32m[20221214 00:43:15 @agent_ppo2.py:185][0m |          -0.0104 |         113.2451 |        -244.9328 |
[32m[20221214 00:43:15 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:43:15 @agent_ppo2.py:138][0m Average TRAINING episode reward: 690.56
[32m[20221214 00:43:15 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 838.14
[32m[20221214 00:43:15 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 747.42
[32m[20221214 00:43:15 @agent_ppo2.py:143][0m Total time:      45.75 min
[32m[20221214 00:43:15 @agent_ppo2.py:145][0m 4073472 total steps have happened
[32m[20221214 00:43:15 @agent_ppo2.py:121][0m #------------------------ Iteration 5989 --------------------------#
[32m[20221214 00:43:16 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:43:16 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:43:16 @agent_ppo2.py:185][0m |           0.0037 |         132.6384 |        -233.1874 |
[32m[20221214 00:43:16 @agent_ppo2.py:185][0m |          -0.0081 |         122.6282 |        -233.4120 |
[32m[20221214 00:43:16 @agent_ppo2.py:185][0m |          -0.0048 |         120.2794 |        -233.9352 |
[32m[20221214 00:43:16 @agent_ppo2.py:185][0m |          -0.0107 |         118.6451 |        -233.4750 |
[32m[20221214 00:43:16 @agent_ppo2.py:185][0m |          -0.0020 |         121.1709 |        -233.6429 |
[32m[20221214 00:43:16 @agent_ppo2.py:185][0m |           0.0016 |         131.3493 |        -233.5137 |
[32m[20221214 00:43:16 @agent_ppo2.py:185][0m |          -0.0049 |         116.6576 |        -233.5356 |
[32m[20221214 00:43:16 @agent_ppo2.py:185][0m |          -0.0096 |         115.5436 |        -233.5915 |
[32m[20221214 00:43:17 @agent_ppo2.py:185][0m |          -0.0101 |         115.1540 |        -232.5969 |
[32m[20221214 00:43:17 @agent_ppo2.py:185][0m |          -0.0118 |         115.1365 |        -233.1939 |
[32m[20221214 00:43:17 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:43:17 @agent_ppo2.py:138][0m Average TRAINING episode reward: 591.53
[32m[20221214 00:43:17 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 779.89
[32m[20221214 00:43:17 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 759.17
[32m[20221214 00:43:17 @agent_ppo2.py:143][0m Total time:      45.77 min
[32m[20221214 00:43:17 @agent_ppo2.py:145][0m 4075520 total steps have happened
[32m[20221214 00:43:17 @agent_ppo2.py:121][0m #------------------------ Iteration 5990 --------------------------#
[32m[20221214 00:43:17 @agent_ppo2.py:127][0m Sampling time: 0.19 s by 5 slaves
[32m[20221214 00:43:17 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:43:17 @agent_ppo2.py:185][0m |           0.0031 |         160.9300 |        -240.1706 |
[32m[20221214 00:43:17 @agent_ppo2.py:185][0m |          -0.0110 |         153.4686 |        -239.6315 |
[32m[20221214 00:43:17 @agent_ppo2.py:185][0m |          -0.0009 |         152.5814 |        -239.2531 |
[32m[20221214 00:43:17 @agent_ppo2.py:185][0m |          -0.0059 |         147.1990 |        -239.1155 |
[32m[20221214 00:43:18 @agent_ppo2.py:185][0m |          -0.0065 |         146.9888 |        -239.6296 |
[32m[20221214 00:43:18 @agent_ppo2.py:185][0m |          -0.0096 |         146.1430 |        -239.6935 |
[32m[20221214 00:43:18 @agent_ppo2.py:185][0m |          -0.0087 |         147.2229 |        -238.6469 |
[32m[20221214 00:43:18 @agent_ppo2.py:185][0m |          -0.0094 |         144.7230 |        -240.1255 |
[32m[20221214 00:43:18 @agent_ppo2.py:185][0m |           0.0018 |         148.2958 |        -238.7027 |
[32m[20221214 00:43:18 @agent_ppo2.py:185][0m |          -0.0113 |         144.7720 |        -239.1745 |
[32m[20221214 00:43:18 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:43:18 @agent_ppo2.py:138][0m Average TRAINING episode reward: 526.35
[32m[20221214 00:43:18 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 765.65
[32m[20221214 00:43:18 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 701.77
[32m[20221214 00:43:18 @agent_ppo2.py:143][0m Total time:      45.80 min
[32m[20221214 00:43:18 @agent_ppo2.py:145][0m 4077568 total steps have happened
[32m[20221214 00:43:18 @agent_ppo2.py:121][0m #------------------------ Iteration 5991 --------------------------#
[32m[20221214 00:43:18 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:43:18 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:43:19 @agent_ppo2.py:185][0m |           0.0034 |         128.4852 |        -243.2436 |
[32m[20221214 00:43:19 @agent_ppo2.py:185][0m |           0.0031 |         121.0506 |        -243.5180 |
[32m[20221214 00:43:19 @agent_ppo2.py:185][0m |           0.0035 |         119.2551 |        -243.0444 |
[32m[20221214 00:43:19 @agent_ppo2.py:185][0m |          -0.0020 |         116.7074 |        -242.9112 |
[32m[20221214 00:43:19 @agent_ppo2.py:185][0m |          -0.0065 |         115.6327 |        -243.7210 |
[32m[20221214 00:43:19 @agent_ppo2.py:185][0m |          -0.0034 |         114.9757 |        -244.9503 |
[32m[20221214 00:43:19 @agent_ppo2.py:185][0m |          -0.0084 |         114.7346 |        -244.6341 |
[32m[20221214 00:43:19 @agent_ppo2.py:185][0m |          -0.0094 |         114.1738 |        -244.6720 |
[32m[20221214 00:43:19 @agent_ppo2.py:185][0m |          -0.0096 |         113.3268 |        -246.6748 |
[32m[20221214 00:43:19 @agent_ppo2.py:185][0m |          -0.0090 |         113.9728 |        -244.4298 |
[32m[20221214 00:43:19 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:43:20 @agent_ppo2.py:138][0m Average TRAINING episode reward: 643.78
[32m[20221214 00:43:20 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 790.53
[32m[20221214 00:43:20 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 781.59
[32m[20221214 00:43:20 @agent_ppo2.py:143][0m Total time:      45.82 min
[32m[20221214 00:43:20 @agent_ppo2.py:145][0m 4079616 total steps have happened
[32m[20221214 00:43:20 @agent_ppo2.py:121][0m #------------------------ Iteration 5992 --------------------------#
[32m[20221214 00:43:20 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:43:20 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:43:20 @agent_ppo2.py:185][0m |           0.0033 |         217.8168 |        -248.9153 |
[32m[20221214 00:43:20 @agent_ppo2.py:185][0m |           0.0031 |         215.8349 |        -249.2685 |
[32m[20221214 00:43:20 @agent_ppo2.py:185][0m |           0.0096 |         217.5609 |        -248.5072 |
[32m[20221214 00:43:20 @agent_ppo2.py:185][0m |          -0.0031 |         212.3019 |        -248.1514 |
[32m[20221214 00:43:20 @agent_ppo2.py:185][0m |           0.0115 |         239.3212 |        -248.7425 |
[32m[20221214 00:43:20 @agent_ppo2.py:185][0m |           0.0014 |         213.6118 |        -249.6051 |
[32m[20221214 00:43:21 @agent_ppo2.py:185][0m |          -0.0017 |         211.1166 |        -250.5480 |
[32m[20221214 00:43:21 @agent_ppo2.py:185][0m |          -0.0052 |         210.5097 |        -248.8927 |
[32m[20221214 00:43:21 @agent_ppo2.py:185][0m |          -0.0051 |         210.1038 |        -249.5683 |
[32m[20221214 00:43:21 @agent_ppo2.py:185][0m |          -0.0018 |         209.5241 |        -250.7437 |
[32m[20221214 00:43:21 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:43:21 @agent_ppo2.py:138][0m Average TRAINING episode reward: 775.03
[32m[20221214 00:43:21 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 833.05
[32m[20221214 00:43:21 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 840.67
[32m[20221214 00:43:21 @agent_ppo2.py:143][0m Total time:      45.84 min
[32m[20221214 00:43:21 @agent_ppo2.py:145][0m 4081664 total steps have happened
[32m[20221214 00:43:21 @agent_ppo2.py:121][0m #------------------------ Iteration 5993 --------------------------#
[32m[20221214 00:43:21 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:43:21 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:43:21 @agent_ppo2.py:185][0m |           0.0019 |          99.9062 |        -249.3062 |
[32m[20221214 00:43:21 @agent_ppo2.py:185][0m |          -0.0032 |          89.9577 |        -249.2610 |
[32m[20221214 00:43:22 @agent_ppo2.py:185][0m |          -0.0062 |          86.5837 |        -248.9984 |
[32m[20221214 00:43:22 @agent_ppo2.py:185][0m |          -0.0101 |          84.9985 |        -250.9778 |
[32m[20221214 00:43:22 @agent_ppo2.py:185][0m |          -0.0172 |          83.5509 |        -250.8224 |
[32m[20221214 00:43:22 @agent_ppo2.py:185][0m |          -0.0095 |          82.2542 |        -250.2567 |
[32m[20221214 00:43:22 @agent_ppo2.py:185][0m |          -0.0147 |          81.5895 |        -251.2357 |
[32m[20221214 00:43:22 @agent_ppo2.py:185][0m |          -0.0213 |          81.2923 |        -250.5160 |
[32m[20221214 00:43:22 @agent_ppo2.py:185][0m |          -0.0167 |          80.4206 |        -250.9788 |
[32m[20221214 00:43:22 @agent_ppo2.py:185][0m |          -0.0179 |          79.9816 |        -251.3425 |
[32m[20221214 00:43:22 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:43:22 @agent_ppo2.py:138][0m Average TRAINING episode reward: 462.91
[32m[20221214 00:43:22 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 760.68
[32m[20221214 00:43:22 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 806.10
[32m[20221214 00:43:22 @agent_ppo2.py:143][0m Total time:      45.87 min
[32m[20221214 00:43:22 @agent_ppo2.py:145][0m 4083712 total steps have happened
[32m[20221214 00:43:22 @agent_ppo2.py:121][0m #------------------------ Iteration 5994 --------------------------#
[32m[20221214 00:43:23 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:43:23 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:43:23 @agent_ppo2.py:185][0m |           0.0005 |         194.6520 |        -250.1707 |
[32m[20221214 00:43:23 @agent_ppo2.py:185][0m |           0.0122 |         206.0698 |        -249.6679 |
[32m[20221214 00:43:23 @agent_ppo2.py:185][0m |           0.0007 |         187.5310 |        -249.6587 |
[32m[20221214 00:43:23 @agent_ppo2.py:185][0m |          -0.0010 |         186.1822 |        -249.2489 |
[32m[20221214 00:43:23 @agent_ppo2.py:185][0m |           0.0033 |         187.1827 |        -248.4963 |
[32m[20221214 00:43:23 @agent_ppo2.py:185][0m |          -0.0039 |         185.2158 |        -247.9091 |
[32m[20221214 00:43:23 @agent_ppo2.py:185][0m |          -0.0030 |         185.0052 |        -249.0533 |
[32m[20221214 00:43:23 @agent_ppo2.py:185][0m |          -0.0084 |         184.7624 |        -250.7867 |
[32m[20221214 00:43:24 @agent_ppo2.py:185][0m |          -0.0069 |         183.6059 |        -250.0699 |
[32m[20221214 00:43:24 @agent_ppo2.py:185][0m |          -0.0043 |         183.5151 |        -250.5081 |
[32m[20221214 00:43:24 @agent_ppo2.py:130][0m Policy update time: 1.07 s
[32m[20221214 00:43:24 @agent_ppo2.py:138][0m Average TRAINING episode reward: 715.47
[32m[20221214 00:43:24 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 837.11
[32m[20221214 00:43:24 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 717.79
[32m[20221214 00:43:24 @agent_ppo2.py:143][0m Total time:      45.89 min
[32m[20221214 00:43:24 @agent_ppo2.py:145][0m 4085760 total steps have happened
[32m[20221214 00:43:24 @agent_ppo2.py:121][0m #------------------------ Iteration 5995 --------------------------#
[32m[20221214 00:43:24 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:43:24 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:43:24 @agent_ppo2.py:185][0m |           0.0046 |         152.0912 |        -250.5691 |
[32m[20221214 00:43:24 @agent_ppo2.py:185][0m |           0.0020 |         143.6441 |        -249.0557 |
[32m[20221214 00:43:24 @agent_ppo2.py:185][0m |          -0.0039 |         141.4396 |        -249.6211 |
[32m[20221214 00:43:24 @agent_ppo2.py:185][0m |          -0.0048 |         139.7023 |        -249.3216 |
[32m[20221214 00:43:25 @agent_ppo2.py:185][0m |          -0.0062 |         138.6363 |        -249.1932 |
[32m[20221214 00:43:25 @agent_ppo2.py:185][0m |          -0.0049 |         138.0222 |        -249.7551 |
[32m[20221214 00:43:25 @agent_ppo2.py:185][0m |           0.0018 |         144.8954 |        -250.1448 |
[32m[20221214 00:43:25 @agent_ppo2.py:185][0m |          -0.0046 |         136.7304 |        -249.6022 |
[32m[20221214 00:43:25 @agent_ppo2.py:185][0m |          -0.0058 |         136.8066 |        -250.1402 |
[32m[20221214 00:43:25 @agent_ppo2.py:185][0m |          -0.0093 |         136.7974 |        -251.9798 |
[32m[20221214 00:43:25 @agent_ppo2.py:130][0m Policy update time: 1.11 s
[32m[20221214 00:43:25 @agent_ppo2.py:138][0m Average TRAINING episode reward: 577.11
[32m[20221214 00:43:25 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 691.23
[32m[20221214 00:43:25 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 743.64
[32m[20221214 00:43:25 @agent_ppo2.py:143][0m Total time:      45.91 min
[32m[20221214 00:43:25 @agent_ppo2.py:145][0m 4087808 total steps have happened
[32m[20221214 00:43:25 @agent_ppo2.py:121][0m #------------------------ Iteration 5996 --------------------------#
[32m[20221214 00:43:25 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:43:26 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:43:26 @agent_ppo2.py:185][0m |           0.0011 |         161.1529 |        -257.9031 |
[32m[20221214 00:43:26 @agent_ppo2.py:185][0m |          -0.0023 |         153.4251 |        -257.8463 |
[32m[20221214 00:43:26 @agent_ppo2.py:185][0m |           0.0036 |         151.0339 |        -257.5283 |
[32m[20221214 00:43:26 @agent_ppo2.py:185][0m |          -0.0054 |         151.3343 |        -258.7945 |
[32m[20221214 00:43:26 @agent_ppo2.py:185][0m |          -0.0055 |         150.4344 |        -257.9412 |
[32m[20221214 00:43:26 @agent_ppo2.py:185][0m |          -0.0049 |         148.8797 |        -258.6969 |
[32m[20221214 00:43:26 @agent_ppo2.py:185][0m |          -0.0062 |         150.8616 |        -259.0837 |
[32m[20221214 00:43:26 @agent_ppo2.py:185][0m |          -0.0081 |         147.9410 |        -258.5885 |
[32m[20221214 00:43:26 @agent_ppo2.py:185][0m |          -0.0027 |         148.3744 |        -257.1942 |
[32m[20221214 00:43:27 @agent_ppo2.py:185][0m |          -0.0096 |         146.9944 |        -257.7293 |
[32m[20221214 00:43:27 @agent_ppo2.py:130][0m Policy update time: 1.09 s
[32m[20221214 00:43:27 @agent_ppo2.py:138][0m Average TRAINING episode reward: 638.08
[32m[20221214 00:43:27 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 828.32
[32m[20221214 00:43:27 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 297.29
[32m[20221214 00:43:27 @agent_ppo2.py:143][0m Total time:      45.94 min
[32m[20221214 00:43:27 @agent_ppo2.py:145][0m 4089856 total steps have happened
[32m[20221214 00:43:27 @agent_ppo2.py:121][0m #------------------------ Iteration 5997 --------------------------#
[32m[20221214 00:43:27 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:43:27 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:43:27 @agent_ppo2.py:185][0m |           0.0040 |         115.6321 |        -268.9058 |
[32m[20221214 00:43:27 @agent_ppo2.py:185][0m |          -0.0047 |         106.1763 |        -268.1002 |
[32m[20221214 00:43:27 @agent_ppo2.py:185][0m |           0.0024 |         103.6960 |        -266.4930 |
[32m[20221214 00:43:27 @agent_ppo2.py:185][0m |          -0.0003 |         104.5886 |        -264.8791 |
[32m[20221214 00:43:27 @agent_ppo2.py:185][0m |          -0.0036 |         101.9909 |        -265.9864 |
[32m[20221214 00:43:28 @agent_ppo2.py:185][0m |           0.0031 |         115.8914 |        -265.3063 |
[32m[20221214 00:43:28 @agent_ppo2.py:185][0m |          -0.0105 |         100.9560 |        -265.6680 |
[32m[20221214 00:43:28 @agent_ppo2.py:185][0m |          -0.0124 |          99.6161 |        -264.7218 |
[32m[20221214 00:43:28 @agent_ppo2.py:185][0m |          -0.0173 |          99.6407 |        -264.2911 |
[32m[20221214 00:43:28 @agent_ppo2.py:185][0m |          -0.0131 |          99.5061 |        -265.0829 |
[32m[20221214 00:43:28 @agent_ppo2.py:130][0m Policy update time: 1.18 s
[32m[20221214 00:43:28 @agent_ppo2.py:138][0m Average TRAINING episode reward: 524.60
[32m[20221214 00:43:28 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 783.65
[32m[20221214 00:43:28 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 764.32
[32m[20221214 00:43:28 @agent_ppo2.py:143][0m Total time:      45.96 min
[32m[20221214 00:43:28 @agent_ppo2.py:145][0m 4091904 total steps have happened
[32m[20221214 00:43:28 @agent_ppo2.py:121][0m #------------------------ Iteration 5998 --------------------------#
[32m[20221214 00:43:28 @agent_ppo2.py:127][0m Sampling time: 0.17 s by 5 slaves
[32m[20221214 00:43:28 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:43:29 @agent_ppo2.py:185][0m |          -0.0008 |         152.0529 |        -252.7340 |
[32m[20221214 00:43:29 @agent_ppo2.py:185][0m |          -0.0028 |         149.6236 |        -253.6217 |
[32m[20221214 00:43:29 @agent_ppo2.py:185][0m |          -0.0036 |         148.4694 |        -254.3064 |
[32m[20221214 00:43:29 @agent_ppo2.py:185][0m |          -0.0045 |         147.9281 |        -253.5297 |
[32m[20221214 00:43:29 @agent_ppo2.py:185][0m |          -0.0028 |         147.1807 |        -252.8444 |
[32m[20221214 00:43:29 @agent_ppo2.py:185][0m |          -0.0041 |         146.8361 |        -251.7751 |
[32m[20221214 00:43:29 @agent_ppo2.py:185][0m |          -0.0100 |         146.8787 |        -251.4744 |
[32m[20221214 00:43:29 @agent_ppo2.py:185][0m |           0.0063 |         152.8105 |        -253.5491 |
[32m[20221214 00:43:29 @agent_ppo2.py:185][0m |          -0.0026 |         146.5536 |        -252.4777 |
[32m[20221214 00:43:29 @agent_ppo2.py:185][0m |          -0.0076 |         146.1748 |        -253.0595 |
[32m[20221214 00:43:29 @agent_ppo2.py:130][0m Policy update time: 1.08 s
[32m[20221214 00:43:30 @agent_ppo2.py:138][0m Average TRAINING episode reward: 683.86
[32m[20221214 00:43:30 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 780.67
[32m[20221214 00:43:30 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 591.67
[32m[20221214 00:43:30 @agent_ppo2.py:143][0m Total time:      45.99 min
[32m[20221214 00:43:30 @agent_ppo2.py:145][0m 4093952 total steps have happened
[32m[20221214 00:43:30 @agent_ppo2.py:121][0m #------------------------ Iteration 5999 --------------------------#
[32m[20221214 00:43:30 @agent_ppo2.py:127][0m Sampling time: 0.16 s by 5 slaves
[32m[20221214 00:43:30 @agent_ppo2.py:161][0m |      policy_loss |       value_loss |          entropy |
[32m[20221214 00:43:30 @agent_ppo2.py:185][0m |           0.0010 |         160.1741 |        -255.6914 |
[32m[20221214 00:43:30 @agent_ppo2.py:185][0m |          -0.0048 |         151.8372 |        -254.3876 |
[32m[20221214 00:43:30 @agent_ppo2.py:185][0m |          -0.0048 |         149.4194 |        -254.8264 |
[32m[20221214 00:43:30 @agent_ppo2.py:185][0m |          -0.0053 |         146.7225 |        -255.9561 |
[32m[20221214 00:43:30 @agent_ppo2.py:185][0m |          -0.0103 |         144.5350 |        -255.5707 |
[32m[20221214 00:43:30 @agent_ppo2.py:185][0m |          -0.0071 |         142.6547 |        -255.2679 |
[32m[20221214 00:43:31 @agent_ppo2.py:185][0m |           0.0009 |         159.7752 |        -254.8908 |
[32m[20221214 00:43:31 @agent_ppo2.py:185][0m |          -0.0071 |         145.1118 |        -254.8997 |
[32m[20221214 00:43:31 @agent_ppo2.py:185][0m |          -0.0129 |         140.2928 |        -255.2060 |
[32m[20221214 00:43:31 @agent_ppo2.py:185][0m |          -0.0021 |         149.4008 |        -255.6150 |
[32m[20221214 00:43:31 @agent_ppo2.py:130][0m Policy update time: 1.10 s
[32m[20221214 00:43:31 @agent_ppo2.py:138][0m Average TRAINING episode reward: 638.02
[32m[20221214 00:43:31 @agent_ppo2.py:139][0m Maximum TRAINING episode reward: 800.06
[32m[20221214 00:43:31 @agent_ppo2.py:140][0m Average EVALUATION episode reward: 716.04
[32m[20221214 00:43:31 @agent_ppo2.py:103][0m [4m[34mCRITICAL[0m Saving the interval checkpoint with rewards 891.87
[32m[20221214 00:43:31 @agent_ppo2.py:143][0m Total time:      46.01 min
[32m[20221214 00:43:31 @agent_ppo2.py:145][0m 4096000 total steps have happened
[32m[20221214 00:43:31 @train.py:54][0m [4m[34mCRITICAL[0m Training completed!
